<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/jmx.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/jmx.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jmxgodlz.xyz","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="前言首先抛出关键性结论：  RNN模型在时间维度共享参数矩阵，因此RNN模型总的梯度等于各时间的梯度之和，$g&#x3D;\sum{g_t}$。  RNN中总的梯度不会消失，只是远距离梯度消失，梯度被近距离梯度主导，无法捕获远距离特征。  梯度消失的本质：由于RNN模型在时间维度共享参数矩阵，导致针对隐藏状态h求导时，循环计算矩阵乘法，最终梯度上出现了参数矩阵的累乘。  LSTM缓解梯度消失的本质：引入门控">
<meta property="og:type" content="article">
<meta property="og:title" content="公式向-完美解释梯度消失与LSTM.md">
<meta property="og:url" content="https://jmxgodlz.xyz/2022/08/07/2022-08-07-%E8%AF%A6%E8%A7%A3LSTM%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/index.html">
<meta property="og:site_name" content="JMX Blog">
<meta property="og:description" content="前言首先抛出关键性结论：  RNN模型在时间维度共享参数矩阵，因此RNN模型总的梯度等于各时间的梯度之和，$g&#x3D;\sum{g_t}$。  RNN中总的梯度不会消失，只是远距离梯度消失，梯度被近距离梯度主导，无法捕获远距离特征。  梯度消失的本质：由于RNN模型在时间维度共享参数矩阵，导致针对隐藏状态h求导时，循环计算矩阵乘法，最终梯度上出现了参数矩阵的累乘。  LSTM缓解梯度消失的本质：引入门控">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-a2898c8e0c7c45f763256ec8f60b8844_1440w.webp">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-29885f66462e4f48db0aba5722798aa5_1440w.webp">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-62d798b3d85004c5ea63b44dbf9312d5_1440w.webp">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-74080caa295667886f47324200e7bc39_1440w.webp">
<meta property="article:published_time" content="2022-08-06T16:00:00.000Z">
<meta property="article:modified_time" content="2023-02-12T11:51:24.644Z">
<meta property="article:author" content="JMXGODLZZ">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="预训练">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.zhimg.com/80/v2-a2898c8e0c7c45f763256ec8f60b8844_1440w.webp">

<link rel="canonical" href="https://jmxgodlz.xyz/2022/08/07/2022-08-07-%E8%AF%A6%E8%A7%A3LSTM%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>公式向-完美解释梯度消失与LSTM.md | JMX Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">JMX Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-guestbook">

    <a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>guestbook</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jmxgodlz.xyz/2022/08/07/2022-08-07-%E8%AF%A6%E8%A7%A3LSTM%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/jmx.png">
      <meta itemprop="name" content="JMXGODLZZ">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="JMX Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          公式向-完美解释梯度消失与LSTM.md
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-07 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-07T00:00:00+08:00">2022-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-02-12 19:51:24" itemprop="dateModified" datetime="2023-02-12T19:51:24+08:00">2023-02-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/08/07/2022-08-07-%E8%AF%A6%E8%A7%A3LSTM%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/08/07/2022-08-07-%E8%AF%A6%E8%A7%A3LSTM%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>9 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>首先抛出<strong>关键性结论：</strong></p>
<ol>
<li><p><strong>RNN模型在时间维度共享参数矩阵，因此RNN模型总的梯度等于各时间的梯度之和</strong>，$g=\sum{g_t}$。</p>
</li>
<li><p><strong>RNN中总的梯度不会消失，只是远距离梯度消失，梯度被近距离梯度主导，无法捕获远距离特征。</strong></p>
</li>
<li><p><strong>梯度消失的本质：由于RNN模型在时间维度共享参数矩阵，导致针对隐藏状态h求导时，循环计算矩阵乘法，最终梯度上出现了参数矩阵的累乘。</strong></p>
</li>
<li><p><strong>LSTM缓解梯度消失的本质：引入门控机制，将矩阵乘法转为逐元素相乘的哈达马积:</strong>$c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tanh \left(W_{c}\left[h_{t-1}, x_{t}\right]+b_{c}\right)$</p>
</li>
</ol>
<span id="more"></span>
<h1 id="梯度消失分析"><a href="#梯度消失分析" class="headerlink" title="梯度消失分析"></a>梯度消失分析</h1><h2 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h2><p>RNN的状态更新公式如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
h_t=f(Wh_{t-1}+Ux_t) \tag{1}
\end{equation}</script><script type="math/tex; mode=display">
y_t=f(Vh_t) \tag{2}</script><p>其中，h表示隐藏状态，f表示激活函数，W、U表示参数矩阵，x表示输入。从该式中可以看出<strong>不同时间维度的参数矩阵是共享的</strong>。</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>我们在反向传播过程进行梯度求导，以对参数矩阵U求导为例</p>
<script type="math/tex; mode=display">
\frac{\partial y_{t}}{\partial U}=\frac{\partial y_{t}}{\partial h_{t}}\frac{\partial h_{t}}{\partial U} \tag{3}</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{equation}
\frac{\partial h_{t}}{\partial U}=\sum_{s=0}^{t}\frac{\partial h_{t}}{\partial h_s}\frac{\partial h_{s}}{\partial U} \tag{4}
\end{equation}</script><script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{s}}=\frac{\partial h_{t}}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial h_{t-2}} \ldots \frac{\partial h_{s+1}}{\partial h_{s}} \tag{5}</script><p>从式（1）（2）中可以递推到$h_0$，每一个隐藏状态h均与参数矩阵有关，最后的梯度$y_t$<strong>依赖于每一个隐藏状态</strong>。</p>
<p>$以y_2为例,y_2=f(Vh_2)=f(Vf(Wh_1+Ux_1))=f(Vf(Wf(Wh_0+Ux_0)+Ux_1))$,通过全微分求导，$h_t$对$U,h_{t-1}$求偏导数</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\frac{\partial y_{2}}{\partial U}&=\frac{\partial y_{2}}{\partial h_{2}} \frac{\partial h_{2}}{\partial U} 
\\ &= \frac{\partial y_{2}}{\partial h_{2}} (\frac{\partial h_{2}}{\partial h_{1}} * \frac{\partial h_{1}}{\partial U} + \frac{\partial h_{2}}{\partial U})
\\ &=  \frac{\partial y_{2}}{\partial h_{2}} (\frac{\partial h_{2}}{\partial h_{1}} * (\frac{\partial h_{1}}{\partial h_{0}} * \frac{\partial h_{0}}{\partial U} + \frac{\partial h_{1}}{\partial U}) + \frac{\partial h_{2}}{\partial U})
\\ &=\frac{\partial y_{2}}{\partial h_{2}} (\frac{\partial h_{2}}{\partial U} + \frac{\partial h_{2}}{\partial h_1} \frac{\partial h_{1}}{\partial U} + \frac{\partial h_{2}}{\partial h_1} \frac{\partial h_{1}}{\partial h_0} \frac{\partial h_{0}}{\partial U})
\end{aligned}
\tag{6}</script><script type="math/tex; mode=display">
设:z_t=Wh_{t-1}+Ux_t \tag{7}</script><p>$z_t$代表未经过激活函数的神经网络输出，式（1）转化为：</p>
<script type="math/tex; mode=display">
h_t=f(z_t) \tag{8}</script><script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{t-1}}=\frac{\partial h_{t}}{\partial z_{t}} \frac{\partial z_{t}}{\partial h_{t-1}} \tag{9}</script><p>式（8）可以拆分为两部分：</p>
<script type="math/tex; mode=display">
\frac{\partial z_{t}}{\partial h_{t-1}} = W \tag{10}</script><script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial z_{t}}=\left(\begin{array}{cccc}\frac{\partial h_{t, 1}}{\partial z_{t, 1}} & \frac{\partial h_{t, 1}}{\partial z_{t, 2}} & \cdots & \frac{\partial h_{t, 1}}{\partial z_{t, n}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial h_{t, n}}{\partial z_{t, 1}} & \frac{\partial h_{t, n}}{\partial z_{t, 2}} & \cdots & \frac{\partial h_{t, n}}{\partial z_{t, n}}\end{array}\right) \tag{11}</script><p>其中，$h_t$元素由$z_t$逐元素激活得到，因此两者对应元素才具有依赖关系，未对应元素无依赖关系，导数为0，式（10）成为一个对角矩阵.</p>
<script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial z{t}}=\left(\begin{array}{cccc}f^{\prime}\left(\text { z}_{t, 1}\right) & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & f^{\prime}\left(z_{t, n}\right)\end{array}\right)=diag[f^{\prime}(z_t)] \tag{12}</script><p>根据式（9）（11），式（5）求解得到：</p>
<script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{s}}=\prod_{k=s+1}^{t} W^{T} \operatorname{diag}\left[f^{\prime}\left(\operatorname{z}_{k}\right)\right] \tag{13}</script><p>在式（12）中已经出现了<strong>矩阵的连乘</strong>，根据矩阵的相容性”$||X Y|| \le ||X|| ||Y||$”</p>
<script type="math/tex; mode=display">
\begin{aligned} 
||\frac{\partial h_{t}}{\partial h_{s}}||&=\prod_{k=s+1}^{t} || W^{T} \operatorname{diag}\left[f^{\prime}\left(\operatorname{z}_{k}\right)\right]|| 
\\ &\le \prod_{k=s+1}^{t} ||W^{T}|| ||\operatorname{diag}\left[f^{\prime}\left(\operatorname{z}_{k}\right)\right]||  
\\ &\le \prod_{k=s+1}^{t} \sigma_{max} \gamma = (\sigma_{max} \gamma)^{t-s}   
\end{aligned} 
\tag{14}</script><p>其中,<strong>$\sigma$ 代表矩阵W的最大奇异解，$\gamma$代表激活函数f的上界</strong>，例如双曲正切函数的上界为$||tanh^{‘}(x)|| \le 1$，sigmoid函数的上界为$||sigmoid^{‘}(x) \le \frac{1}{4}||$。<br>因此在远距离依赖，即t-s较大的情况下，<strong>当$\sigma_{max} \gamma \lt 1$时，会发生梯度消失;当$\sigma_{max} \gamma \gt 1$时，会发生梯度爆炸。</strong></p>
<p><strong>TIPS：</strong>这里只是不等式情况，因此即使不等式右边远大于1，也有可能发生梯度消失。但是，<strong>在实际情况下，矩阵范数的约束与实际值相当接近。</strong></p>
<h3 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h3><p>对于传统RNN模型，在训练初期避免梯度消失与参数矩阵的初始化，即最大奇异解$\sigma$值有关。</p>
<p><strong>避免梯度消失的矩阵最小初始化方式如下：</strong></p>
<p>以双曲正切函数为例，双曲正切函数的$\gamma = 1$，为了使$\sigma_{max} \gamma = 1$，即$\sigma=1$。</p>
<p>为了使不等式置信度更高，将矩阵W的所有奇异解设置为1.</p>
<p>对于每一列而言，$\Sigma_{i} w_{i j}^{2}=1$，其中j代表第j列，矩阵中每个元素是一个n维向量，i代表矩阵第i行，w代表列向量。</p>
<p>$n \mathbb{E}\left(w^{2}\right)=1$，</p>
<p>我们假设w服从均匀分布，区间为$[-R,R]$,均匀分布的均值为0，方差$\mathbb{E}\left(w^{2}\right)=\frac{R^2}{3}$【均匀分布方差为$\frac{(b-a)^2}{12}$】。</p>
<p>代入得到<script type="math/tex">n\frac{R^2}{3}=1 \tag{15}</script></p>
<p>即<script type="math/tex">R=\frac{\sqrt{3}}{\sqrt{n}} \tag{16}</script></p>
<p>因此w符合的分布为$[-\frac{\sqrt{3}}{\sqrt{n}}, \frac{\sqrt{3}}{\sqrt{n}}]$，当矩阵是方阵时的Xavier-Glorot initialization分布；当矩阵行列不同时，Xavier-Glorot initialization分布为$\left[-\frac{\sqrt{6}}{\sqrt{m+n}}, \frac{\sqrt{6}}{\sqrt{m+n}}\right]$。</p>
<h1 id="LSTM提出"><a href="#LSTM提出" class="headerlink" title="LSTM提出"></a>LSTM提出</h1><p>关于RNN反向传播的一些评论：</p>
<ol>
<li>RNN模型在时间维度共享参数矩阵</li>
<li>权重更新的频率与梯度的准确性需要权衡，越少的更新次数，梯度准确性越高，但训练速度也下降了。【由反向传递时，使用上一时刻状态做近似导致】</li>
<li>梯度消失带来不稳定的梯度流；共享参数带来对最新更新的过度敏感</li>
<li>针对上述三点，进行错误传播过程的梯度截断是有必要的</li>
<li>传播梯度分量也是可以的</li>
</ol>
<p>LSTM缓解梯度消失的根本方法：<strong>write it down</strong>【将状态记录下来】，但是如果无限制的写入也会带来问题，因此升级为有选择的读写，这样带来了LSTM的三个关键机制：</p>
<ol>
<li>有选择的写入，写入关键信息</li>
<li>有选择的读取信息</li>
<li>有选择的遗忘信息</li>
</ol>
<p>我们可以通过门机制实现选择性，但要如何将这三个机制结合起来呢？</p>
<h2 id="LSTM原型"><a href="#LSTM原型" class="headerlink" title="LSTM原型"></a>LSTM原型</h2><p>首先，提出一个LSTM原型，本着先读取状态，再写入的原则，每一次更新状态的增量$\tilde{s}_{t}$，由$o_t$选择性读取上一个状态的内容，$i_t$为选择写入，$f_t$选择性遗忘上一状态的内容：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i} s_{t-1}+U_{i} x_{t}+b_{i}\right) \\ o_{t} &=\sigma\left(W_{o} s_{t-1}+U_{o} x_{t}+b_{o}\right) \\ f_{t} &=\sigma\left(W_{f} s_{t-1}+U_{f} x_{t}+b_{f}\right) \\ \tilde{s_{t}} &=\phi\left(W\left(o_{t} \odot s_{t-1}\right)+U x_{t}+b\right) \\ s_{t} &=f_{t} \odot s_{t-1}+i_{t} \odot \tilde{s}_{t} \end{aligned} \tag{17}</script><p><img src="https://pic1.zhimg.com/80/v2-a2898c8e0c7c45f763256ec8f60b8844_1440w.webp" alt=""></p>
<h2 id="三个起效果的改进版本"><a href="#三个起效果的改进版本" class="headerlink" title="三个起效果的改进版本"></a>三个起效果的改进版本</h2><p>按理说上述LSTM原型能够起效果，但事与愿违，<strong>选择性读取与选择性写入未能很好的协调，导致状态值非常大，紧接着门机制变得饱和。</strong>这种情况源于的$s_t$是无界的，会变得非常大从而导致门机制饱和，因此接下来的三个改进的生效版本均是约束$s_t$的大小，将其约束成有界。</p>
<h3 id="归一化LSTM原型"><a href="#归一化LSTM原型" class="headerlink" title="归一化LSTM原型"></a>归一化LSTM原型</h3><p>针对$s_t$进行正态归一化，$s_t = \frac{s_t-mean(s_t)}{\sqrt{Var(s_t) + 1}}$，也可以类似于层归一化等方式添加缩放与平移分量。</p>
<p>归一化后的$s_t$从无界成为有界。</p>
<h3 id="GRU：将写入与遗忘强绑定"><a href="#GRU：将写入与遗忘强绑定" class="headerlink" title="GRU：将写入与遗忘强绑定"></a>GRU：将写入与遗忘强绑定</h3><script type="math/tex; mode=display">s_{t}=\left(1-i_{t}\right) \odot s_{t-1}+i_{t} \odot \tilde{s}_{t} \tag{18}</script><p>GRU将写入门与遗忘门绑定起来，使之加和为1。将$s_t$变成$s_{t-1}$与$\tilde{s}_{t}$的element-wise加权平均，当两者均有界时，$s_t$也有界。</p>
<p>以下给出GRU的计算公式，与原理图：</p>
<script type="math/tex; mode=display">
\begin{aligned} r_{t} &=\sigma\left(W_{r} s_{t-1}+U_{r} x_{t}+b_{r}\right) \\ z_{t} &=\sigma\left(W_{z} s_{t-1}+U_{z} x_{t}+b_{z}\right) \\ \tilde{s_{t}} &=\phi\left(W\left(r_{t} \odot s_{t-1}\right)+U x_{t}+b\right) \\ s_{t} &=z_{t} \odot s_{t-1}+\left(1-z_{t}\right) \odot \tilde{s}_{t} \end{aligned} \tag{19}</script><p><img src="https://pic2.zhimg.com/80/v2-29885f66462e4f48db0aba5722798aa5_1440w.webp" alt=""></p>
<h3 id="伪LSTM：通过激活函数约束"><a href="#伪LSTM：通过激活函数约束" class="headerlink" title="伪LSTM：通过激活函数约束"></a>伪LSTM：通过激活函数约束</h3><p>通过激活函数，将$s_t$限制到激活函数的上界内。</p>
<p>只有在更新写入时，为了避免信息的变化，未使用激活函数约束。</p>
<p>以下给出伪LSTM的计算公式，与原理图：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i}\left(\phi\left(s_{t-1}\right)\right)+U_{i} x_{t}+b_{i}\right) \\ o_{t} &=\sigma\left(W_{o}\left(\phi\left(s_{t-1}\right)\right)+U_{o} x_{t}+b_{o}\right) \\ f_{t} &=\sigma\left(W_{f}\left(\phi\left(s_{t-1}\right)\right)+U_{f} x_{t}+b_{f}\right) \\ \tilde{s}_{t} &=\phi\left(W\left(o_{t} \odot \phi\left(s_{t-1}\right)\right)+U x_{t}+b\right) \\ s_{t} &=f_{t} \odot s_{t-1}+i_{t} \odot \tilde{s}_{t} \\ \mathbf{r n n}_{\text {out }} &=\phi\left(s_{t}\right) \end{aligned} \tag{20}</script><p><img src="https://pic2.zhimg.com/80/v2-62d798b3d85004c5ea63b44dbf9312d5_1440w.webp" alt=""></p>
<h2 id="LSTM提出-1"><a href="#LSTM提出-1" class="headerlink" title="LSTM提出"></a>LSTM提出</h2><p>LSTM与伪LSTM的几点关键区别如下：</p>
<ol>
<li><p>LSTM是先写后读，因此添加了一个“影子”状态，Hochreiter and Schmidhuber等人认为状态s与剩余的RNN cell是独立的。</p>
</li>
<li><p>使用门控影子状态$h_{t-1}=o_{t-1}\odot \phi(c_{t-1})$计算门结构，替换激活后的$\phi(c_{t-1})$。这样隐藏状态均是当前时间下的信息，与读取信息时$(o_t \odot s_{t-1})$利用前一时刻信息不同。</p>
</li>
<li><p>使用门控影子状态作为RNN cell的输出$h_{t}=o_{t}\odot \phi(c_{t})$，替代$\phi(c_t)$。</p>
</li>
</ol>
<p>这样一来，LSTM的输入就是上一时刻的$c_{t-1},h_{t-1}$，输出为$c_t,h_t$。</p>
<h3 id="基础LSTM"><a href="#基础LSTM" class="headerlink" title="基础LSTM"></a>基础LSTM</h3><p>基础LSTM单元的公式与原理图如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i} h_{t-1}+U_{i} x_{t}+b_{i}\right) \\ o_{t} &=\sigma\left(W_{o} h_{t-1}+U_{o} x_{t}+b_{o}\right) \\ f_{t} &=\sigma\left(W_{f} h_{t-1}+U_{f} x_{t}+b_{f}\right) \\ \tilde{c}_{t} &=\phi\left(W h_{t-1}+U x_{t}+b\right) \\ c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t} \\ h_{t} &=o_{t} \odot \phi\left(c_{t}\right) \\ \mathrm{rnn}_{\text {out }} &=h_{t} \end{aligned} \tag{21}</script><p><img src="https://pic2.zhimg.com/80/v2-74080caa295667886f47324200e7bc39_1440w.webp" alt=""></p>
<h3 id="The-LSTM-with-peepholes"><a href="#The-LSTM-with-peepholes" class="headerlink" title="The LSTM with peepholes"></a>The LSTM with peepholes</h3><p>利用前一状态$c_{t-1}$来进行门控机制的计算，但输出利用实时信息$c_t$。</p>
<p>计算公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i} h_{t-1}+U_{i} x_{t}+P_{i} c_{t-1}+b_{i}\right) \\ f_{t} &=\sigma\left(W_{f} h_{t-1}+U_{f} x_{t}+P_{f} c_{t-1}+b_{f}\right) \\ \tilde{c}_{t} &=\phi\left(W h_{t-1}+U x_{t}+b\right) \\ c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t} \\ o_{t} &=\sigma\left(W_{o} h_{t-1}+U_{o} x_{t}+P_{o} c_{t}+b_{o}\right) \\ h_{t} &=o_{t} \odot \phi\left(c_{t}\right) \\ \operatorname{rnn}_{\text {out }} &=h_{t} \end{aligned} \tag{22}</script><h1 id="LSTM如何解决梯度消失"><a href="#LSTM如何解决梯度消失" class="headerlink" title="LSTM如何解决梯度消失"></a>LSTM如何解决梯度消失</h1><p>从上述分析可以得到，梯度消失中最大原因是需要计算$\frac{\partial h_{t}}{\partial h_{s}}$，如果这个值不随着层数的增加，趋于0或者无穷大，那么就能够捕获到长距离依赖信息。</p>
<p>LSTM将状态与其他部分分开，状态更新部分变成:</p>
<script type="math/tex; mode=display">c_t = f_t \odot c_{t-1} + i_t * \tilde c_t = c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tanh \left(W_{c}\left[h_{t-1}, x_{t}\right]+b_{c}\right) \tag{23}</script><p>针对状态进行求导，同时$c_t$与$c_{t-1},\tilde c_{t-1},f_t,i_t$有关，因此进行全微分求导</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial C_{t}}{\partial C_{t-1}} &= \frac{\partial C_{t}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial \tilde C_{t}} * \frac{\partial \tilde C_{t}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial i_{t}} * \frac{\partial i_{t}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial f_{t}} * \frac{\partial f_{t}}{\partial C_{t-1}} 
\\ &=\frac{\partial C_{t}}{\partial C_{t-1}} +  \frac{\partial C_{t}}{\partial \tilde C_{t}} * \frac{\partial \tilde C_{t}}{\partial h_{t-1}} * \frac{\partial h_{t-1}}{\partial C_{t-1}} 
\\ &+ \frac{\partial C_{t}}{\partial i_{t}} * \frac{\partial i_{t}}{\partial h_{t-1}} * \frac{\partial h_{t-1}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial f_{t}} * \frac{\partial f_{t}}{\partial h_{t-1}} * \frac{\partial h_{t-1}}{\partial C_{t-1}}
\\ &= f_t + i_t * tanh^{'}(*)W_c * o_{t-1} tanh^{'}(C_{t-1}) 
\\ &+ \tilde C_t * \sigma^{'}(*)W_i * o_{t-1} tanh^{'}(C_{t-1}) + C_{t-1} * \sigma^{'}(*)W_f * o_{t-1} tanh^{'}(C_{t-1})
\end{aligned}
\tag{24}</script><p>从上式可以得到，$\frac{\partial C_{t}}{\partial C_{t-1}}$成为上述4部分的加和，在连乘的任意时刻可能是$[0, +\infty)$的范围，并不会一直趋于0，或者$\infty$。同时$f_t,i_t,o_{t-1},\tilde C_t$都是网络学习的值，也就是说由网络自己学习哪些梯度保留，哪些梯度剔除。</p>
<p>在这些机制的帮助下，LSTM很好的<strong>缓解了</strong>梯度消失问题。</p>
<h1 id="LSTM延伸"><a href="#LSTM延伸" class="headerlink" title="LSTM延伸"></a>LSTM延伸</h1><p>Highway网络和residual网络同样包含了LSTM最基本的思想：与原先一层网络输出$x_{t + 1} = Net(x_t)$的计算方式相比，计算增量$x_{t + 1} = x_t + \Delta x_{t + 1}$。</p>
<p>因此这两种方式，同样会遇到LSTM的问题：读写的不协调。</p>
<p>关于这两者的介绍，再后续有时间展开进行介绍。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
<p><a target="_blank" rel="noopener" href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109519044">https://zhuanlan.zhihu.com/p/109519044</a></p>
<p><a target="_blank" rel="noopener" href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1088177386838749184">https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1088177386838749184</a></p>

    </div>

    
    
    
    <div>
    
      <div>
  
    <div style="text-align:center;color:#bfbfbf;font-size:16px;">
      <span>-------- 本文结束 </span>
      <i class="fa fa-paw"></i>
      <span> 感谢阅读 --------</span>
    </div>
  
</div>

    
    </div>
      
  <div class="popular-posts-header">猜你喜欢# Custom header, leave empty to use the default one</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2021/12/31/2021-12-31-2022预训练的下一步是什么/" rel="bookmark">2022预训练的下一步是什么</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2022/07/31/2022-07-31-一文梳理NLP主要模型发展脉络/" rel="bookmark">一文梳理NLP主要模型发展脉络.md</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2022/01/20/2022-01-20-解析NLP中的对抗训练/" rel="bookmark">解析NLP竞赛中的提分点-对抗训练</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2022/06/25/2022-06-25-NLP技能树学习路线-（一）路线总览/" rel="bookmark">NLP技能树学习路线-（一）路线总览.md</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2022/12/07/2022-12-07-【律师函警告】-ChatGPT以鸡你太美主题写小说/" rel="bookmark">【律师函警告】-ChatGPT以鸡你太美主题写小说.md</a></div>
    </li>
  </ul>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>JMX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://jmxgodlz.xyz/2022/08/07/2022-08-07-%E8%AF%A6%E8%A7%A3LSTM%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/" title="公式向-完美解释梯度消失与LSTM.md">https://jmxgodlz.xyz/2022/08/07/2022-08-07-详解LSTM与梯度消失/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
              <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/" rel="tag"><i class="fa fa-tag"></i> 预训练</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/07/31/2022-07-31-%E4%B8%80%E6%96%87%E6%A2%B3%E7%90%86NLP%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/" rel="prev" title="一文梳理NLP主要模型发展脉络.md">
      <i class="fa fa-chevron-left"></i> 一文梳理NLP主要模型发展脉络.md
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/09/04/2022-09-04-Pytorch%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E7%9A%84%E5%A4%9A%E7%A7%8D%E6%89%93%E5%BC%80%E6%96%B9%E5%BC%8F/" rel="next" title="Pytorch多机多卡的多种打开方式">
      Pytorch多机多卡的多种打开方式 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%88%86%E6%9E%90"><span class="nav-number">2.</span> <span class="nav-text">梯度消失分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.1.</span> <span class="nav-text">基本介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">2.2.</span> <span class="nav-text">反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A5%E5%85%85%E8%AF%B4%E6%98%8E"><span class="nav-number">2.2.1.</span> <span class="nav-text">补充说明</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM%E6%8F%90%E5%87%BA"><span class="nav-number">3.</span> <span class="nav-text">LSTM提出</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%E5%8E%9F%E5%9E%8B"><span class="nav-number">3.1.</span> <span class="nav-text">LSTM原型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E4%B8%AA%E8%B5%B7%E6%95%88%E6%9E%9C%E7%9A%84%E6%94%B9%E8%BF%9B%E7%89%88%E6%9C%AC"><span class="nav-number">3.2.</span> <span class="nav-text">三个起效果的改进版本</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96LSTM%E5%8E%9F%E5%9E%8B"><span class="nav-number">3.2.1.</span> <span class="nav-text">归一化LSTM原型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRU%EF%BC%9A%E5%B0%86%E5%86%99%E5%85%A5%E4%B8%8E%E9%81%97%E5%BF%98%E5%BC%BA%E7%BB%91%E5%AE%9A"><span class="nav-number">3.2.2.</span> <span class="nav-text">GRU：将写入与遗忘强绑定</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%AALSTM%EF%BC%9A%E9%80%9A%E8%BF%87%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%BA%A6%E6%9D%9F"><span class="nav-number">3.2.3.</span> <span class="nav-text">伪LSTM：通过激活函数约束</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM%E6%8F%90%E5%87%BA-1"><span class="nav-number">3.3.</span> <span class="nav-text">LSTM提出</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80LSTM"><span class="nav-number">3.3.1.</span> <span class="nav-text">基础LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-LSTM-with-peepholes"><span class="nav-number">3.3.2.</span> <span class="nav-text">The LSTM with peepholes</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">4.</span> <span class="nav-text">LSTM如何解决梯度消失</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM%E5%BB%B6%E4%BC%B8"><span class="nav-number">5.</span> <span class="nav-text">LSTM延伸</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">6.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="JMXGODLZZ"
      src="/images/jmx.png">
  <p class="site-author-name" itemprop="name">JMXGODLZZ</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">29</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/447428054" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;447428054" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jmxgodlzz@gmail.com" title="E-Mail → mailto:jmxgodlzz@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2023/08/23/2023-08-23-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8D%87%E7%BA%A7%E4%B8%8E%E8%AE%BE%E8%AE%A1%E4%B9%8B%E9%81%93%EF%BC%9AChatGLM%E3%80%81LLAMA%E3%80%81Baichuan%E5%8F%8ALLM%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/" title="2023&#x2F;08&#x2F;23&#x2F;2023-08-23-大模型升级与设计之道：ChatGLM、LLAMA、Baichuan及LLM结构解析&#x2F;">大模型升级与设计之道：ChatGLM、LLAMA、Baichuan及LLM结构解析</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/07/22/2023-07-22-RoPE%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC%E3%80%81%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E3%80%81%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8/" title="2023&#x2F;07&#x2F;22&#x2F;2023-07-22-RoPE旋转位置编码深度解析：理论推导、代码实现、长度外推&#x2F;">RoPE旋转位置编码深度解析：理论推导、代码实现、长度外推</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/07/01/2023-07-01-LLM%E8%AE%AD%E7%BB%83%E6%8C%87%E5%8D%97(%E4%BA%8C):%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E3%80%81%E8%AE%A1%E7%AE%97%E9%87%8F%E3%80%81%E6%98%BE%E5%AD%98%E3%80%81%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4%E8%AE%A1%E7%AE%97/" title="2023&#x2F;07&#x2F;01&#x2F;2023-07-01-LLM训练指南(二):模型参数、计算量、显存、计算时间计算&#x2F;">LLM训练指南(二):模型参数、计算量、显存、计算时间计算</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/06/13/2023-06-13-LLM%E8%AE%AD%E7%BB%83%E6%8C%87%E5%8D%97-Token%E5%8F%8A%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E5%87%86%E5%A4%87/" title="2023&#x2F;06&#x2F;13&#x2F;2023-06-13-LLM训练指南-Token及模型参数准备&#x2F;">LLM训练指南-Token及模型参数准备</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2023/05/16/2023-05-08-%E5%A4%8D%E5%88%BBChatGPT%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97-%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%A3%E7%A0%81/" title="2023&#x2F;05&#x2F;16&#x2F;2023-05-08-复刻ChatGPT语言模型系列-（四）文本生成解码&#x2F;">复刻ChatGPT语言模型系列-（四）文本生成解码</a>
        </li>
    </ul>
  </div>


<div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>


      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">JMXGODLZZ</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">248k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:45</span>
</div>
  <div class="powered-by">

  </div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

        
<div class="busuanzi-count">
  <script async src="/js/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1.4.16/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'EqIhOtvzwzCzQNJS178We0en-gzGzoHsz',
      appKey     : 'uGXYV87r0A6miFKVHul24dnC',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>


  <script async src="/js/cursor/fireworks.js"></script>




  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":-15,"vOffset":-15},"mobile":{"show":true},"react":{"opacity":0.9},"log":false});</script></body>
</html>
