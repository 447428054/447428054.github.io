<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>中文情感词典的构建</title>
    <url>/2019/02/28/2019-02-28-%E4%B8%AD%E6%96%87%E6%83%85%E6%84%9F%E8%AF%8D%E5%85%B8%E7%9A%84%E6%9E%84%E5%BB%BA/</url>
    <content><![CDATA[<p>本文介绍了情感词典的构建方式，文章主要分为：通用情感词典的构建、通用情感词典的扩展、领域情感词典的构建</p>
<span id="more"></span>
<blockquote>
<p>首先，国外英文的情感分析已经取得了很好的效果，得益于英文单词自身分析的便捷性与英文大量的数据集 WordNet。但由于中文的多变性，语义的多重性与数据集的缺乏，使得国内的情感分析暂落后于国外。本文将记录博主在项目中构建情感词典的经验，欢迎大家指正。<br>我们首先将情感词典分为通用情感词典与专用情感词典。</p>
</blockquote>
<h1 id="1-通用情感词典的构建"><a href="#1-通用情感词典的构建" class="headerlink" title="1.通用情感词典的构建"></a>1.通用情感词典的构建</h1><p>   通用情感词典的构建主要是通过将目前开源的情感词典整合起来，筛去重复和无用的单词。<br>   目前网上开源的情感词典包含有：知网（HowNet）情感词典、台湾大学（NTSUSD)简体中文情感极性词典、大连理工大学情感词汇本体。<br>   前两个都可以在网上找到，第三个需要到其学校官网申请，说明完用途即可获得。</p>
<h1 id="2-通用情感词典的扩展"><a href="#2-通用情感词典的扩展" class="headerlink" title="2.通用情感词典的扩展"></a>2.通用情感词典的扩展</h1><p>上述情感词典年代都已经比较久远，所以我们可以采取一定方法对其扩展。这里我们采用的方法是将词典的同义词添加到词典里。<br>我们通过使用哈工大整理的同义词词林来获取词典的同义词，需要一提的是第一版的同义词林年代较为久远，现在也有哈工大整理的同义词林扩展版。<br>使用的链接在这里：<a href="https://blog.csdn.net/sinat_33741547/article/details/80016713">哈工大同义词林扩展版</a><br>使用代码编写时也可以利用Python的Synonyms库来获取同义词。<br>其已经开源，链接为：<a href="https://github.com/huyingxi/Synonyms">synonyms</a><br>如：</p>
<pre><code>import synonyms
print(&quot;人脸: %s&quot; % (synonyms.nearby(&quot;人脸&quot;)))
print(&quot;识别: %s&quot; % (synonyms.nearby(&quot;识别&quot;)))
</code></pre><h1 id="3-领域情感词典的构建"><a href="#3-领域情感词典的构建" class="headerlink" title="3.领域情感词典的构建"></a>3.领域情感词典的构建</h1><p>构建特定领域的情感词典大体有两种方法：基于规则的情感词典构建方法、基于统计的情感词典构建方法。</p>
<p>基于规则的情感词典方法一般是用句型固定、句式不多变的情况。通过对语料进行句法分析，词性标注等操作，得到语料中常用的句型并将其抽象出来，根据抽象出来的句型模板来对新的语料进行模板匹配，以此来选择出新的语料中的情感词，将其加入到对应的情感词典中。</p>
<p>基于统计的情感词典构建方法需要利用PMI互信息计算与左右熵来发现所需要的新词。具体方法我们可以添加情感种子词，来计算分好词的语料中各个词语与情感种子词的互信息度与左右熵，再将互信息度与左右熵结合起来，选择出与情感词关联度最高的TopN个词语，将其添加到对应的情感词典。<br>这里可以参考链接<a href="https://www.jianshu.com/p/e9313fd692ef">link</a></p>
<h3 id="互信息度计算"><a href="#互信息度计算" class="headerlink" title="互信息度计算"></a>互信息度计算</h3><p><img src="https://img-blog.csdnimg.cn/20190228172006936.png" alt="互信息度计算"></p>
<ul>
<li>p(x,y)为两个词一起出现的概率</li>
<li>p(x)为词x出现的概率</li>
<li>p(y)为词y出现的概率</li>
</ul>
<hr>
<p>具体例子：4G， 上网卡，4G上网卡;如果4G的词频是2,上网卡的词频是10,4G上网卡的词频是1，那么记单单词的总数有N个，双单词的总数有M个，则有下面的公式<br><img src="https://img-blog.csdnimg.cn/20190228172528100.png" alt="具体例子"></p>
<h3 id="左右熵"><a href="#左右熵" class="headerlink" title="左右熵"></a>左右熵</h3><p>我们这里使用左右熵来衡量主要是想表示预选词的自由程度(4G上网卡为一个预选词），左右熵越大，表示这个词的左边右边的词换的越多，那么它就很有可能是一个单独的词。<br>我们这里的左右熵定义为(以左熵为例):<br><img src="https://img-blog.csdnimg.cn/20190228172807236.png" alt="左熵"><br>这里我们还是举一个具体的例子来理解它<br>假设4G上网卡左右有这么几种搭配<br>[买4G上网卡, 有4G上网卡，有4G上网卡， 丢4G上网卡]<br>那么4G上网卡的左熵为<br><img src="https://img-blog.csdnimg.cn/20190228172830315.png" alt="例子"><br>这里A = [买, 有, 丢]</p>
<blockquote>
<p>后面就是具体的实现了，这里的难点就在如何获得这些概率值，就博主看到的用法有：利用搜索引擎获取词汇共现率即p(x,y)、利用语料库获取各个词出现概率</p>
</blockquote>
<h2 id="最后我们只需要将这三步获得的情感词典进行整合就可以了"><a href="#最后我们只需要将这三步获得的情感词典进行整合就可以了" class="headerlink" title="最后我们只需要将这三步获得的情感词典进行整合就可以了"></a>最后我们只需要将这三步获得的情感词典进行整合就可以了</h2><p>参考文献：<br><a href="https://www.jianshu.com/p/e9313fd692ef">python3实现互信息和左右熵的新词发现</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>情感分析</category>
      </categories>
      <tags>
        <tag>情感分析</tag>
        <tag>情感词典</tag>
      </tags>
  </entry>
  <entry>
    <title>Coursera Machine Learning 学习笔记（一）Introduction</title>
    <url>/2019/03/02/2019-03-02-Coursera%20Machine%20Learning%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89Introduction/</url>
    <content><![CDATA[<p>文章为博主学习Coursera上的Machine Learning课程的笔记，Coursera Machine Learning 学习笔记（一）Introduction</p>
<span id="more"></span>
<blockquote>
<p>文章为博主学习Coursera上的Machine Learning课程的笔记，来记录自己的学习过程，欢迎大家一起学习交流</p>
</blockquote>
<h1 id="01-Introduction"><a href="#01-Introduction" class="headerlink" title="01:Introduction"></a>01:Introduction</h1><h2 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><ul>
<li><p>Arthur Samuel(1959)<br>  <strong>Machine Learning:</strong>“Field of study that gives computers the ability to learn without being explicitly programmed”</p>
</li>
<li><p>Tom Michel(1999)<br>  <strong>Well posed learning problem:</strong>“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”<br>  在下棋的例子中：</p>
</li>
</ul>
<ol>
<li>E经验为1000场下棋游戏</li>
<li>T任务为下棋</li>
<li>P评价准则为是否获胜</li>
</ol>
<ul>
<li><p>学习算法的类型<br>  。 有监督学习（Supervised learning）</p>
<pre><code>      Teach the computer how to do something, then let it use it;s new found knowledge to do it

      注：一般是有标注的训练集
</code></pre><p>  。无监督学习（Unsupervised learning）<br>  Let the computer learn how to do something, and use this to determine structure and patterns in data</p>
<pre><code>      注：一般是无标注训练集，使机器从中提取特征，常见算法为聚类
</code></pre><p>  。强化学习（Reinforcement learning）</p>
<p>  。推荐系统（Recommender systems）</p>
</li>
</ul>
<h3 id="有监督学习介绍"><a href="#有监督学习介绍" class="headerlink" title="有监督学习介绍"></a>有监督学习介绍</h3><h5 id="问题分类："><a href="#问题分类：" class="headerlink" title="问题分类："></a>问题分类：</h5><ul>
<li>预测问题<br>课程内拿房价预测作为示例：具体可以看课程内容<br>预测问题也叫回归问题，具有以下特征：</li>
</ul>
<ol>
<li>预测连续的输出</li>
<li>没有明显得离散划分</li>
</ol>
<ul>
<li>分类问题<br>课程内以肿瘤划分作为示例</li>
</ul>
<h3 id="无监督学习介绍"><a href="#无监督学习介绍" class="headerlink" title="无监督学习介绍"></a>无监督学习介绍</h3><p>在无监督学习里我们获得的是没有标注的数据，将这些数据划分成不同的数据簇</p>
<h5 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h5><p>具体应用示例：</p>
<ol>
<li>新闻划分</li>
<li>基因组排序</li>
<li>分布式计算机集群划分</li>
<li>社交网络划分</li>
<li>天文数据分析</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Introduction</tag>
      </tags>
  </entry>
  <entry>
    <title>Coursera Machine Learning 学习笔记（二）Linear Regression</title>
    <url>/2019/03/02/2019-03-02-Coursera%20Machine%20Learning%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89Linear%20Regression/</url>
    <content><![CDATA[<p>文章为博主学习Coursera上的Machine Learning课程的笔记，Coursera Machine Learning 学习笔记（二）Linear Regression</p>
<span id="more"></span>
<blockquote>
<p>文章为博主学习Coursera上的Machine Learning课程的笔记，来记录自己的学习过程，欢迎大家一起学习交流</p>
</blockquote>
<h1 id="02：Linear-Regression"><a href="#02：Linear-Regression" class="headerlink" title="02：Linear Regression"></a>02：Linear Regression</h1><p>仍然以房价预测作为示例，具体示例仍需见课程内容。<br>符号含义：</p>
<ol>
<li>m 为数据集的大小</li>
<li>x’s为输入数据</li>
<li>y’s为对应的目标输出结果</li>
<li>(x,y)为所有训练数据</li>
<li>(x<sup>i</sup>, y<sup>i</sup>)为具体第i行数据，第i个训练数据</li>
</ol>
<p>假设函数h(x)，以一元线性回归为例：<br><img src="https://img-blog.csdnimg.cn/20190302112701595.png" alt="假设函数"></p>
<script type="math/tex; mode=display">\theta_0：截距  \theta_1：梯度</script><h5 id="Linear-regression-implementation（损失函数cost-function）"><a href="#Linear-regression-implementation（损失函数cost-function）" class="headerlink" title="Linear regression - implementation（损失函数cost function）"></a>Linear regression - implementation（损失函数cost function）</h5><p>计算由不同θ 取值带来的不同损失函数值，本质上是一个最小化问题：使下式取值最小</p>
<script type="math/tex; mode=display">Minimize ：(h_\theta(x)-y)^2</script><p>即可以看成下述式子：</p>
<script type="math/tex; mode=display">J(\theta_0,\theta_1) = \frac 1 {2m}\sum_1^m(h_\theta(x^{(i)})-y^{(i)})^2</script><script type="math/tex; mode=display">\frac1 m 是求平均\frac1 {2m}是为了数学计算方便</script><p>这个损失函数是均方误差，适用于多类回归问题，当然也可以有其他的损失函数。</p>
<h5 id="梯度下降算法（Gradient-descent-algorithm）"><a href="#梯度下降算法（Gradient-descent-algorithm）" class="headerlink" title="梯度下降算法（Gradient descent algorithm）"></a>梯度下降算法（Gradient descent algorithm）</h5><p><strong>目的：</strong> 使损失函数J最小</p>
<h6 id="工作方式"><a href="#工作方式" class="headerlink" title="工作方式"></a>工作方式</h6><ul>
<li>从随机初始化开始</li>
</ul>
<ol>
<li>对θ 随机赋值，可以为任何值</li>
<li>每次一点点改变θ，使J(θ)减小</li>
</ol>
<ul>
<li>每次沿着梯度下降最大的方向</li>
<li>重复上述操作直到达到局部最小值</li>
<li>从哪里开始的可能决定你到达哪个局部最优解<br><img src="https://img-blog.csdnimg.cn/2019030217133153.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjc2MDMz,size_16,color_FFFFFF,t_70" alt="梯度下降"><br><strong>一个更正规的定义：</strong><br>做下述操作直到收敛：<br><img src="https://img-blog.csdnimg.cn/20190302171951356.png" alt="参数更新"><br>符号解释：</li>
</ul>
<ol>
<li><strong>:=</strong><br>表示赋值<br>NB a = b 是一个正确的断言</li>
<li>α (alpha)<br>学习率，控制参数更新的步长<ol>
<li>如果学习率过大，可能无法得到最优解，误差会增大</li>
<li>如果学习率过小，那么达到最优解需要非常多步，耗费很长时间</li>
</ol>
</li>
</ol>
<p>注： 参数的更新必须<strong>同步</strong>即需要一个中间变量保存之前的值，原因是二者的式子中包含了对方，一方的更新会导致第二方式子内值的变化。<br><img src="https://img-blog.csdnimg.cn/20190302173355106.png" alt="在这里插入图片描述"><br><strong>当我们达到局部最优解时：</strong></p>
<ol>
<li>后面部分的梯度为0</li>
<li>各参数值就保持不变了 </li>
</ol>
<h5 id="使用梯度下降的线性回归算法"><a href="#使用梯度下降的线性回归算法" class="headerlink" title="使用梯度下降的线性回归算法"></a>使用梯度下降的线性回归算法</h5><ul>
<li>将梯度下降算法应用到最小化损失函数J(θ)上</li>
</ul>
<script type="math/tex; mode=display">\frac{\partial} {\partial\theta_j}=\frac 1 {2m}\sum_1^m(h_\theta(x^{(i)})-y^{(i)})^2=\frac{\partial} {\partial\theta_j}\frac 1 {2m}\sum_1^m(\theta_0+\theta_1x^{(i)}-y^{(i)})^2</script><p>按照求导公式可以推出：</p>
<script type="math/tex; mode=display">j=0:\frac{\partial} {\partial\theta_0}J(\theta_0,\theta_1)=\frac1 m\sum_1^m(h_\theta(x^{(i)})-y^{(i)})</script><script type="math/tex; mode=display">j=1:\frac{\partial} {\partial\theta_1}J(\theta_0,\theta_1)=\frac1 m\sum_1^m(h_\theta(x^{(i)})-y^{(i)})*x^{(i)}</script><p>注：因为线性回归是一个凸函数，是一个碗形的图，所以会趋于局部最优解</p>
<ul>
<li>现在的这种梯度下降算法又叫Batch Gradient Descent 原因是每一次都遍历了整个数据集，后面会提到取数据集中的部分进行的Gradient Descent</li>
<li>线性回归也有正规方程求解，但其中矩阵运算当数据集过大时不宜使用，这时就可以使用梯度下降</li>
</ul>
<h5 id="数值求解的正规方程法"><a href="#数值求解的正规方程法" class="headerlink" title="数值求解的正规方程法"></a>数值求解的正规方程法</h5><ul>
<li>直接通过数值求解来避免繁琐的迭代过程，从数学上求解出min(J(θ))<br><strong>正规方程的优缺点：</strong><br><strong>优点：</strong><br>1.不需要学习率这个参数<br>2.对某些问题可以很快的解决<br><strong>缺点：</strong><br>会很复杂</li>
</ul>
<h5 id="面对数据量很大的时候"><a href="#面对数据量很大的时候" class="headerlink" title="面对数据量很大的时候"></a>面对数据量很大的时候</h5><p>这个时候就需要<strong>将数据向量化</strong>利用线性代数中矩阵运算来完成计算</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据分析-数据可视化</title>
    <url>/2020/01/03/2020-01-03-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<p>本文将介绍使用Matplotlib工具，完成Python数据分析-数据可视化</p>
<span id="more"></span>
<h1 id="一-Matplotlib-基本概念"><a href="#一-Matplotlib-基本概念" class="headerlink" title="一. Matplotlib 基本概念"></a>一. Matplotlib 基本概念</h1><p>Matplotlib是python的一个数据可视化工具库。</p>
<p>特点：专门用于开发2D图表(包括3D图表)， 操作简单。</p>
<p>可视化是在整个数据挖掘的关键辅助工具，可以清晰的理解数据，从而调整我们的分析方法。</p>
<h1 id="二-Matplotlib三层结构"><a href="#二-Matplotlib三层结构" class="headerlink" title="二. Matplotlib三层结构"></a>二. Matplotlib三层结构</h1><p><img src="https://img-blog.csdnimg.cn/20190313235406342.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="三层结构"></p>
<h1 id="三-Matplotlib-基本使用"><a href="#三-Matplotlib-基本使用" class="headerlink" title="三. Matplotlib 基本使用"></a>三. Matplotlib 基本使用</h1><h2 id="1-折线图"><a href="#1-折线图" class="headerlink" title="1. 折线图"></a>1. 折线图</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"># 图形显示设置</span><br><span class="line">%matplotlib inline   </span><br><span class="line"> </span><br><span class="line"># 绘制画布-容器层  figsize: 画布长宽属性   dpi: 图象的清晰度</span><br><span class="line">plt.figure(figsize=(16,8), dpi=60)</span><br><span class="line"> </span><br><span class="line"># 绘制折线图-图象层</span><br><span class="line">plt.plot([1,2,3,4,5,6], [22,19,18,25,27,19])</span><br><span class="line"> </span><br><span class="line"># 显示图象</span><br><span class="line"># plt.show()</span><br><span class="line"> </span><br><span class="line"># 保存图象 -注：plt.show()会释放figure资源，保存图片需要将plt.show()注释掉</span><br><span class="line"># 图片的保存路径 -- </span><br><span class="line">plt.savefig(&quot;plot.png&quot;)</span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdn.net/20180917191105231?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"></p>
<h2 id="2-绘制多条折线图"><a href="#2-绘制多条折线图" class="headerlink" title="2. 绘制多条折线图"></a>2. 绘制多条折线图</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 中文显示问题-- 下载中文字体，安装字体-修改配置文件下面手动修改配置</span><br><span class="line"># from pylab import mpl</span><br><span class="line"># mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line"># mpl.rcParams[&quot;axes.unicode_minus&quot;] = False # 解决保存图像是负号&#x27;-&#x27;显示为方块的问题</span><br><span class="line"> </span><br><span class="line"># 准备数据</span><br><span class="line">x = range(60)</span><br><span class="line">y_sh = [random.uniform(26,31) for i in x]</span><br><span class="line">y_bj = [random.uniform(27, 35) for i in x]</span><br><span class="line"> </span><br><span class="line"># 创建画布</span><br><span class="line">plt.figure(figsize=(16,8), dpi=60)</span><br><span class="line"> </span><br><span class="line"># 同一坐标内--绘制多条折线图  （新增）</span><br><span class="line">plt.plot(x, y_sh, label=&quot;sh&quot;)</span><br><span class="line">plt.plot(x, y_bj, label=&quot;bj&quot;, linestyle=&quot;--&quot;, color=&quot;y&quot;)  # 线条颜色，线条样式设置 见下图</span><br><span class="line"> </span><br><span class="line"># 自定义x, y轴 刻度 &amp; 刻度标签 (新增)</span><br><span class="line">x_ticks = range(0, 60, 5)</span><br><span class="line">y_ticks = range(20, 40, 5)</span><br><span class="line"> </span><br><span class="line">x_ticks_label = [&quot;11点&#123;&#125;分&quot;.format(i) for i in x_ticks]</span><br><span class="line"> </span><br><span class="line">plt.xticks(x_ticks, x_ticks_label)</span><br><span class="line">plt.yticks(y_ticks)</span><br><span class="line"> </span><br><span class="line"># 添加辅助描述信息-- x，y轴标签 &amp; 图形标题</span><br><span class="line">plt.xlabel(&quot;时间&quot;)</span><br><span class="line">plt.ylabel(&quot;温度&quot;)</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;两地同一时间温度变化图&quot;)</span><br><span class="line"> </span><br><span class="line"># 添加网格线 - alpha:透明度   （新增）</span><br><span class="line">plt.grid(True, linestyle=&quot;--&quot;, alpha=0.6)</span><br><span class="line"> </span><br><span class="line"># 显示图例 -- loc：位置设置,详见下图  （新增）</span><br><span class="line">plt.legend(loc=&quot;best&quot;)</span><br><span class="line"> </span><br><span class="line"># 显示图象</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdn.net/20180917235044993?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br><strong>附参数表:</strong><br><img src="https://img-blog.csdn.net/20180917235125313?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190310015842874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="3-绘制多个坐标系-—-plt-subplots"><a href="#3-绘制多个坐标系-—-plt-subplots" class="headerlink" title="3. 绘制多个坐标系 — plt.subplots"></a>3. 绘制多个坐标系 — plt.subplots</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 中文显示问题--下面手动修改配置 </span><br><span class="line">from pylab import mpl</span><br><span class="line">mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line">mpl.rcParams[&quot;axes.unicode_minus&quot;] = False</span><br><span class="line"> </span><br><span class="line"># 准备x,y轴数据</span><br><span class="line">x = range(60)</span><br><span class="line">y_sh = [random.uniform(15, 18) for i in x ]</span><br><span class="line">y_bj = [random.uniform(5, 12) for i in x ]</span><br><span class="line"> </span><br><span class="line"># 创建画布--多个坐标轴, 绘制折线图</span><br><span class="line">fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), dpi=100)</span><br><span class="line"># Returns:  fig: 图对象    ax: 坐标轴对象列表</span><br><span class="line">axes[0].plot(x, y_sh, label=&#x27;上海&#x27;)</span><br><span class="line">axes[1].plot(x, y_bj, label=&#x27;北京&#x27;, color=&#x27;r&#x27;, linestyle=&#x27;--&#x27;)</span><br><span class="line"> </span><br><span class="line"># 显示图例/坐标轴刻度/网格线</span><br><span class="line">axes[0].legend()</span><br><span class="line">axes[1].legend()</span><br><span class="line"> </span><br><span class="line">x_ticks_label = [&#x27;11点&#123;&#125;分&#x27;.format(i) for i in x]</span><br><span class="line">y_ticks = range(40)</span><br><span class="line">axes[0].set_xticks(x[::5], x_ticks_label[::5])</span><br><span class="line">axes[0].set_yticks(y_ticks[::5])</span><br><span class="line">axes[1].set_xticks(x[::5], x_ticks_label[::5])</span><br><span class="line">axes[1].set_yticks(y_ticks[::5])</span><br><span class="line"> </span><br><span class="line">axes[0].grid(True, linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line">axes[1].grid(True, linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line"> </span><br><span class="line"># 添加 标题/坐标轴描述信息</span><br><span class="line">axes[0].set_title(&#x27;上海11点0分到12点之间的温度变化图&#x27;)</span><br><span class="line">axes[0].set_xlabel(&quot;时间&quot;)</span><br><span class="line">axes[0].set_ylabel(&quot;温度&quot;)</span><br><span class="line"> </span><br><span class="line">axes[1].set_title(&#x27;北京11点0分到12点之间的温度变化图&#x27;)</span><br><span class="line">axes[1].set_xlabel(&#x27;时间&#x27;)</span><br><span class="line">axes[1].set_ylabel(&#x27;温度&#x27;)</span><br><span class="line"> </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="4-绘制sin-函数图像-—-plot"><a href="#4-绘制sin-函数图像-—-plot" class="headerlink" title="4. 绘制sin()函数图像 — plot"></a>4. 绘制sin()函数图像 — plot</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">import numpy as np</span><br><span class="line">x = np.linspace(-10, 10, 1000)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制图像，显示图像</span><br><span class="line">plt.figure(figsize=(10, 1), dpi=100)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.grid(linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2019031201165850.png" alt="在这里插入图片描述"></p>
<h2 id="5-散点图-—-scatter"><a href="#5-散点图-—-scatter" class="headerlink" title="5. 散点图 — scatter"></a>5. 散点图 — scatter</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># -- 案例： 探究房屋面积和房屋价格的关系</span><br><span class="line">from pylab import mpl          # 中文显示问题--下面手动修改配置 </span><br><span class="line">mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line">mpl.rcParams[&quot;axes.unicode_minus&quot;] = False</span><br><span class="line"># 准备数据</span><br><span class="line">x = [225.98, 247.07, 253.14, 457.85, 241.58, 301.01,  20.67, 288.64,</span><br><span class="line">       163.56, 120.06, 207.83, 342.75, 147.9 ,  53.06, 224.72,  29.51,</span><br><span class="line">        21.61, 483.21, 245.25, 399.25, 343.35]</span><br><span class="line">y = [196.63, 203.88, 210.75, 372.74, 202.41, 247.61,  24.9 , 239.34,</span><br><span class="line">       140.32, 104.15, 176.84, 288.23, 128.79,  49.64, 191.74,  33.1 ,</span><br><span class="line">        30.74, 400.02, 205.35, 330.64, 283.45]</span><br><span class="line"> </span><br><span class="line"># 创建画布  -- 绘制散点图 -- 显示图像</span><br><span class="line">plt.figure(figsize=(20,8), dpi=100)</span><br><span class="line">plt.scatter(x, y)</span><br><span class="line"> </span><br><span class="line">plt.title(&#x27;房屋面积和房屋价格的关系--案例测试&#x27;)</span><br><span class="line">plt.xlabel(&#x27;面积&#x27;)</span><br><span class="line">plt.ylabel(&#x27;价格&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190312012238645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="6-柱状图-—-bar"><a href="#6-柱状图-—-bar" class="headerlink" title="6. 柱状图 — bar"></a>6. 柱状图 — bar</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;东方快车谋杀案&#x27;,&#x27;寻梦环游记&#x27;,&#x27;全球风暴&#x27;,&#x27;降魔传&#x27;,&#x27;追捕&#x27;,&#x27;七十七天&#x27;,&#x27;密战&#x27;,&#x27;狂兽&#x27;,&#x27;其它&#x27;]</span><br><span class="line">y = [73853,57767,22354,15969,14839,8725,8716,8318,7916,6764,52222]</span><br><span class="line">x = range(len(movie_name))</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制柱状图，添加标题和格线，显示图像</span><br><span class="line">plt.figure(figsize=(18, 6), dpi=80)</span><br><span class="line">plt.bar(x, y, width=0.5, color=[&#x27;b&#x27;,&#x27;r&#x27;,&#x27;g&#x27;,&#x27;y&#x27;,&#x27;c&#x27;,&#x27;m&#x27;,&#x27;y&#x27;,&#x27;k&#x27;,&#x27;c&#x27;,&#x27;g&#x27;,&#x27;m&#x27;])</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影票房收入对比&quot;)</span><br><span class="line">plt.xticks(x, movie_name)</span><br><span class="line">plt.grid(linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190312014545814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="7-柱状图-—-多个指标对比"><a href="#7-柱状图-—-多个指标对比" class="headerlink" title="7. 柱状图 — 多个指标对比"></a>7. 柱状图 — 多个指标对比</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;寻梦环游记&#x27;]</span><br><span class="line">first_day = [10587.6,10062.5,1275.7]</span><br><span class="line">first_weekend=[36224.9,34479.6,11830]</span><br><span class="line">x = range(len(movie_name))</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制柱状图，添加标题/坐标轴刻度标签/网格线/示例， 显示图像</span><br><span class="line">plt.figure(figsize=(10, 5), dpi=80)</span><br><span class="line">plt.bar(x, first_day, width=0.2, label=&quot;首日票房&quot;)</span><br><span class="line">plt.bar([i+0.2 for i in x], first_weekend, width=0.2, label=&quot;首周票房&quot;)</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影首日和首周的票房对比&quot;)</span><br><span class="line">plt.xticks([i+0.1 for i in x], movie_name)     # 修改x轴刻度显示</span><br><span class="line">plt.grid(linestyle=&quot;--&quot;, alpha=0.5)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190312020011216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="8-直方图-—-hist"><a href="#8-直方图-—-hist" class="headerlink" title="8 直方图 — hist"></a>8 直方图 — hist</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">time = [131,  98, 125, 131, 124, 139, 131, 117, 128, 108, 135, 138, 131, 102, 107, 114, 119, 128, 121, 142, 127, 130, 124, 101, 110, 116, 117, 110, 128, 128, 115,  99, 136, 126, 134,  95, 138, 117, 111,78, 132, 124, 113, 150, 110, 117,  86,  95, 144, 105, 126, 130,126, 130, 126, 116, 123, 106, 112, 138, 123,  86, 101,  99, 136,123, 117, 119, 105, 137, 123, 128, 125, 104, 109, 134, 125, 127,105, 120, 107, 129, 116, 108, 132, 103, 136, 118, 102, 120, 114,105, 115, 132, 145, 119, 121, 112, 139, 125, 138, 109, 132, 134,156, 106, 117, 127, 144, 139, 139, 119, 140,  83, 110, 102,123,107, 143, 115, 136, 118, 139, 123, 112, 118, 125, 109, 119, 133,112, 114, 122, 109, 106, 123, 116, 131, 127, 115, 118, 112, 135,115, 146, 137, 116, 103, 144,  83, 123, 111, 110, 111, 100, 154,136, 100, 118, 119, 133, 134, 106, 129, 126, 110, 111, 109, 141,120, 117, 106, 149, 122, 122, 110, 118, 127, 121, 114, 125, 126,114, 140, 103, 130, 141, 117, 106, 114, 121, 114, 133, 137,  92,121, 112, 146,  97, 137, 105,  98, 117, 112,  81,  97, 139, 113,134, 106, 144, 110, 137, 137, 111, 104, 117, 100, 111, 101, 110,105, 129, 137, 112, 120, 113, 133, 112,  83,  94, 146, 133, 101,131, 116, 111,  84, 137, 115, 122, 106, 144, 109, 123, 116, 111,111, 133, 150]</span><br><span class="line"> </span><br><span class="line"># 创建画布， 绘制直方图， 添加标题/坐标轴刻度标签/网格线</span><br><span class="line">plt.figure(figsize=(20, 8), dpi=80)</span><br><span class="line">distance = 2</span><br><span class="line">group_num = int((max(time)-min(time)) / distance)</span><br><span class="line">plt.hist(time, bins=group_num)</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影时长分布状况&quot;)</span><br><span class="line">plt.xticks(range(min(time), max(time))[::2])</span><br><span class="line">plt.xlabel(&quot;电影时长&quot;)</span><br><span class="line">plt.ylabel(&quot;数量&quot;)</span><br><span class="line">plt.grid(linestyle=&quot;--&quot;, alpha=0.5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190313233637810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="9-饼图-—-pie"><a href="#9-饼图-—-pie" class="headerlink" title="9 饼图 — pie"></a>9 饼图 — pie</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据   -- 案例：电影的排片占比</span><br><span class="line">movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;东方快车谋杀案&#x27;,&#x27;寻梦环游记&#x27;,&#x27;全球风暴&#x27;,&#x27;降魔传&#x27;,&#x27;追捕&#x27;,&#x27;七十七天&#x27;,&#x27;密战&#x27;,&#x27;狂兽&#x27;,&#x27;其它&#x27;]</span><br><span class="line">place_count = [60605,54546,45819,28243,13270,9945,7679,6799,6101,4621,20105]</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制饼图，添加标题/坐标轴刻度标签</span><br><span class="line">plt.figure(figsize=(18, 6), dpi=80)</span><br><span class="line">plt.pie(place_count, labels=movie_name, autopct=&quot;%1.2f%%&quot;)</span><br><span class="line">plt.axis(&quot;equal&quot;)          #  坐标轴长宽相等，保证饼图成圆形</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影的排片占比&quot;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190313235128167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="10-VENN-韦恩图"><a href="#10-VENN-韦恩图" class="headerlink" title="10.VENN 韦恩图"></a>10.VENN 韦恩图</h2><p><strong>需要先下载matplotlib_venn</strong></p>
<h3 id="10-1-具有2个分组的基本的维恩图"><a href="#10-1-具有2个分组的基本的维恩图" class="headerlink" title="10.1 具有2个分组的基本的维恩图"></a>10.1 具有2个分组的基本的维恩图</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn2</span><br><span class="line"></span><br><span class="line"># 第一种方法，10，5为两组的大小，2为两组交叉大小;</span><br><span class="line"># set_labels为组名</span><br><span class="line"># venn2(subsets = (10, 5, 2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;))</span><br><span class="line"># 设置两组数据为ABCD和DEF</span><br><span class="line">venn2([set([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;]), set([&#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;])])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzRfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<h3 id="10-2-具有3个组的基本维恩图"><a href="#10-2-具有3个组的基本维恩图" class="headerlink" title="10.2 具有3个组的基本维恩图"></a>10.2 具有3个组的基本维恩图</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line"># Import the library</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn3</span><br><span class="line"> </span><br><span class="line"># Make the diagram</span><br><span class="line">venn3(subsets = (10, 8, 22, 6,9,4,2)) # 通过直接设置各部分数据来配置韦恩图</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzZfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 设置三组ABCD、DEF、ADG</span><br><span class="line">venn3([set([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;]), set([&#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;]), set([&#x27;A&#x27;, &#x27;D&#x27;, &#x27;G&#x27;,&#x27;F&#x27;])]) # 设置数据来配置韦恩图</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzdfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<h3 id="10-3-自定义维恩图"><a href="#10-3-自定义维恩图" class="headerlink" title="10.3 自定义维恩图"></a>10.3 自定义维恩图</h3><ol>
<li>自定义标签</li>
<li>自定义维恩图上圆的线条</li>
<li>自定义维恩图上的圆</li>
</ol>
<p><strong>自定义标签</strong></p>
<ul>
<li>get_label_by_id 可查看其源代码 表示分类里不同部分<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">## Venn上的自定义标签 Custom label on Venn</span><br><span class="line"># Import the library</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn3</span><br><span class="line">from matplotlib_venn import venn3_circles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Custom text labels: change the label of group A</span><br><span class="line">v=venn3(subsets = (10, 8, 22, 6,9,4,2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;, &#x27;Group C&#x27;))</span><br><span class="line"># 单独改变A的标签</span><br><span class="line">v.get_label_by_id(&#x27;A&#x27;).set_text(&#x27;My Favourite group!&#x27;)</span><br></pre></td></tr></table></figure>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzlfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"><br><strong>自定义维恩图上圆的线条</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">## 自定义维恩图上圆的线条 Custom Circles lines on Venn</span><br><span class="line"># Line style: can be &#x27;dashed&#x27; or &#x27;dotted&#x27; for example</span><br><span class="line"># 设置维恩图</span><br><span class="line">v = venn3(subsets = (10, 8, 22, 6,9,4,2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;, &#x27;Group C&#x27;))</span><br><span class="line"># 画圆，linestyle线条类型，linewith线宽，color线条颜色</span><br><span class="line">c = venn3_circles(subsets = (10, 8, 22, 6,9,4,2), linestyle=&#x27;dashed&#x27;, linewidth=1, color=&quot;grey&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzEwXzAucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"><br><strong>自定义维恩图上的圆</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">## 自定义维恩图上的圆 Custom a circle on Venn</span><br><span class="line"># Change one group only</span><br><span class="line">v=venn3(subsets = (10, 8, 22, 6,9,4,2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;, &#x27;Group C&#x27;))</span><br><span class="line">c=venn3_circles(subsets = (10, 8, 22, 6,9,4,2), linestyle=&#x27;dashed&#x27;, linewidth=1, color=&quot;grey&quot;)</span><br><span class="line"># 设置第一个圆的线宽</span><br><span class="line">c[0].set_lw(8.0)</span><br><span class="line"># 设置第一个圆的线形</span><br><span class="line">c[0].set_ls(&#x27;dotted&#x27;)</span><br><span class="line"># 设置第一个圆的填充颜色</span><br><span class="line">c[0].set_color(&#x27;skyblue&#x27;)</span><br><span class="line"> </span><br><span class="line"># Color</span><br><span class="line"># id号</span><br><span class="line"># 如ABC三个簇，010代表非A和B和非C,100代表A和非B和非C</span><br><span class="line"># 设置透明度</span><br><span class="line">v.get_patch_by_id(&#x27;011&#x27;).set_alpha(1.0)</span><br><span class="line"># 设置颜色</span><br><span class="line">v.get_patch_by_id(&#x27;011&#x27;).set_color(&#x27;red&#x27;)</span><br><span class="line"># 打印id号</span><br><span class="line">#v.id2idx</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzExXzAucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<h3 id="10-4-修改韦恩图数值"><a href="#10-4-修改韦恩图数值" class="headerlink" title="10.4 修改韦恩图数值"></a>10.4 修改韦恩图数值</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn3, venn3_circles</span><br><span class="line">import matplotlib.font_manager as fm</span><br><span class="line">from pathlib import Path</span><br><span class="line">def venn(train_filename=&#x27;biaozhuData/generate_data/train/pkuverb.txt&#x27;, val_filename=&#x27;biaozhuData/generate_data_2/valid/pkuverb.txt&#x27;, test_filename=&#x27;biaozhuData/generate_data_2/test/pkuverb.txt&#x27;):</span><br><span class="line">	&#x27;&#x27;&#x27;</span><br><span class="line">	训练集、验证集、测试集 数据分布的韦恩图</span><br><span class="line">	:param train_filename:</span><br><span class="line">	:param val_filename:</span><br><span class="line">	:param test_filename:</span><br><span class="line">	:return:</span><br><span class="line">	&#x27;&#x27;&#x27;</span><br><span class="line">	path = Path(train_filename)</span><br><span class="line">	set1 = set(open(test_filename, encoding=&#x27;utf-8&#x27;).readlines())</span><br><span class="line">	set2 = set(open(val_filename, encoding=&#x27;utf-8&#x27;).readlines())</span><br><span class="line">	set3 = set(open(train_filename, encoding=&#x27;utf-8&#x27;).readlines())</span><br><span class="line"></span><br><span class="line">	set_len1 = len(set1)</span><br><span class="line">	set_len2 = len(set2)</span><br><span class="line">	set_len3 = len(set3)</span><br><span class="line"></span><br><span class="line">	v = venn3([set1, set2, set3], (&#x27;test&#x27;, &#x27;valid&#x27;, &#x27;train&#x27;))</span><br><span class="line">	a = v.get_label_by_id(&#x27;100&#x27;).get_text()</span><br><span class="line">	b = v.get_label_by_id(&#x27;010&#x27;).get_text()</span><br><span class="line">	c = v.get_label_by_id(&#x27;001&#x27;).get_text()</span><br><span class="line"></span><br><span class="line">	a = str(round(int(a) / set_len1, 3)) + &#x27; &#x27; + a</span><br><span class="line">	b = str(round(int(b) / set_len2, 3)) + &#x27; &#x27; + b</span><br><span class="line">	c = str(round(int(c) / set_len3, 3)) + &#x27; &#x27; + c</span><br><span class="line"></span><br><span class="line">	v.get_label_by_id(&#x27;100&#x27;).set_text(a)</span><br><span class="line">	v.get_label_by_id(&#x27;010&#x27;).set_text(b)</span><br><span class="line">	v.get_label_by_id(&#x27;001&#x27;).set_text(c)</span><br><span class="line">	plt.title(name[path.stem]+&#x27;集合关系&#x27;, fontproperties=myfont)</span><br><span class="line">	sfname = path.name.replace(path.suffix, &#x27;.png&#x27;)</span><br><span class="line">	savepath = Path(venn_savedir)/sfname</span><br><span class="line">	#plt.show()</span><br><span class="line">	plt.savefig(savepath)</span><br><span class="line">	plt.close()</span><br></pre></td></tr></table></figure>
<h1 id="四-Matplotlib-中文无法显示问题"><a href="#四-Matplotlib-中文无法显示问题" class="headerlink" title="四. Matplotlib 中文无法显示问题"></a>四. Matplotlib 中文无法显示问题</h1><ol>
<li>windows下：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.rcParams[&#x27;font.sans-serif&#x27;]=[&#x27;SimHei&#x27;] #用来正常显示中文标签</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 中文显示问题--下面手动修改配置 </span><br><span class="line">from pylab import mpl</span><br><span class="line">mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line">mpl.rcParams[&quot;axes.unicode_minus&quot;] = False</span><br></pre></td></tr></table></figure>
<ol>
<li>Ubuntu/LInux下</li>
</ol>
<p><a href="https://www.cnblogs.com/panlq/p/9270826.html">Ubuntu解决matplotlib中文无法显示</a></p>
<p>其实这些乱码问题，只是路径等配置问题，上述只是修改默认的配置，可以通过指定字体路径等来解决。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">myfont = fm.FontProperties(fname=r&#x27;1031Competition/font/simsun.ttc&#x27;)  # 设置字体</span><br><span class="line">……</span><br><span class="line">plt.xlabel(&#x27;短语类型&#x27;, fontproperties=myfont)</span><br><span class="line">plt.ylabel(&#x27;比例&#x27;, fontproperties=myfont)</span><br><span class="line">plt.legend(loc=&#x27;upper right&#x27;)</span><br><span class="line">plt.savefig(&#x27;test.png&#x27;)</span><br></pre></td></tr></table></figure>
<p><strong>参考文章</strong><br><a href="https://blog.csdn.net/refrain__wg/article/details/82747254">Matplotlib 数据可视化-基本使用教程</a><br><a href="https://blog.csdn.net/LuohenYJ/article/details/103091081">python基于matplotlib_venn实现维恩图的绘制</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>数据分析</category>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch 多卡训练</title>
    <url>/2021/09/12/2021-09-12-Pytorch%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>本文将介绍Pytorch 多卡训练的原理及实现。多卡训练流程一般如下：</p>
<ol>
<li>指定主机节点</li>
<li>主机节点划分数据，一个batch数据平均分到每个机器上</li>
<li>模型从主机拷贝到各个机器</li>
<li>每个机器进行前向传播</li>
<li>每个机器计算loss损失</li>
<li>主机收集所有loss结果，进行参数更新</li>
<li>将更新后参数模型拷贝给各个机器</li>
</ol>
<span id="more"></span>
<h1 id="Pytorch-多卡训练"><a href="#Pytorch-多卡训练" class="headerlink" title="Pytorch 多卡训练"></a>Pytorch 多卡训练</h1><h2 id="一、多卡训练原理"><a href="#一、多卡训练原理" class="headerlink" title="一、多卡训练原理"></a>一、多卡训练原理</h2><p>多卡训练流程一般如下：</p>
<ol>
<li>指定主机节点</li>
<li>主机节点划分数据，一个batch数据平均分到每个机器上</li>
<li>模型从主机拷贝到各个机器</li>
<li>每个机器进行前向传播</li>
<li>每个机器计算loss损失</li>
<li>主机收集所有loss结果，进行参数更新</li>
<li>将更新后参数模型拷贝给各个机器</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/img_convert/1665788acfe3757d493b3d82422035c1.png" alt="image"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/bffacf11040884149347a428d6f63260.png" alt="image"></p>
<h2 id="二、单机多卡训练"><a href="#二、单机多卡训练" class="headerlink" title="二、单机多卡训练"></a>二、单机多卡训练</h2><p>使用<strong>torch.nn.DataParallel</strong>(module, device_ids)模块，module为模型，device_ids为并行的GPU id列表</p>
<p>使用方式：将模型调用该接口执行操作</p>
<p><code>model = torch.nn.DataParallel(model)</code></p>
<p>示例：我们假设模型输入为(32, input_dim)，这里的 32 表示batch_size，模型输出为(32, output_dim)，使用 4 个GPU训练。nn.DataParallel起到的作用是将这 32 个样本拆成 4 份，发送给 4 个GPU 分别做 forward，然后生成 4 个大小为(8, output_dim)的输出，然后再将这 4 个输出都收集到cuda:0上并合并成(32, output_dim)。</p>
<p>可以看出，nn.DataParallel没有改变模型的输入输出，因此其他部分的代码不需要做任何更改，非常方便。但弊端是，后续的loss计算只会在cuda:0上进行，没法并行，因此会导致负载不均衡的问题。</p>
<p>通过在模型内置loss计算可以解决上述负载不均衡的情况，最后所得loss进行取平均。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Net:</span><br><span class="line">    def __init__(self,...):</span><br><span class="line">        # code</span><br><span class="line">    </span><br><span class="line">    def forward(self, inputs, labels=None)</span><br><span class="line">        # outputs = fct(inputs)</span><br><span class="line">        # loss_fct = ...</span><br><span class="line">        if labels is not None:</span><br><span class="line">            loss = loss_fct(outputs, labels)  # 在训练模型时直接将labels传入模型，在forward过程中计算loss</span><br><span class="line">            return loss</span><br><span class="line">        else:</span><br><span class="line">            return outputs</span><br></pre></td></tr></table></figure>
<p>按照我们上面提到的模型并行逻辑，在每个GPU上会计算出一个loss，这些loss会被收集到cuda:0上并合并成长度为 4 的张量。这个时候在做backward的之前，必须对将这个loss张量合并成一个标量，一般直接取mean就可以。这在Pytorch官方文档nn.DataParallel函数中有提到：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</span><br></pre></td></tr></table></figure>
<h2 id="三、多机多卡训练"><a href="#三、多机多卡训练" class="headerlink" title="三、多机多卡训练"></a>三、多机多卡训练</h2><p><strong>该方式也可以实现单机多卡</strong></p>
<p>使用<strong>torch.nn.parallel.DistributedDataParallel</strong>和<strong>torch.utils.data.distributed.DistributedSampler</strong>结合多进程实现。</p>
<ol>
<li><p>从一开始就会启动多个进程(进程数小于等于GPU数)，每个进程独享一个GPU，每个进程都会独立地执行代码。这意味着每个进程都独立地初始化模型、训练，当然，在每次迭代过程中会通过进程间通信共享梯度，整合梯度，然后独立地更新参数。</p>
</li>
<li><p>每个进程都会初始化一份训练数据集，当然它们会使用数据集中的不同记录做训练，这相当于同样的模型喂进去不同的数据做训练，也就是所谓的数据并行。这是通过<strong>torch.utils.data.distributed.DistributedSampler</strong>函数实现的，不过逻辑上也不难想到，只要做一下数据partition，不同进程拿到不同的parition就可以了，官方有一个简单的demo，感兴趣的可以看一下代码实现：Distributed Training</p>
</li>
<li><p>进程通过local_rank变量来标识自己，local_rank为0的为master，其他是slave。这个变量是torch.distributed包帮我们创建的，使用方法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import argparse  # 必须引入 argparse 包</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&quot;--local_rank&quot;, type=int, default=-1)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure>
<p>必须以如下方式运行代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 train.py</span><br></pre></td></tr></table></figure>
<p>这样的话，torch.distributed.launch就以命令行参数的方式将args.local_rank变量注入到每个进程中，每个进程得到的变量值都不相同。比如使用 4 个GPU的话，则 4 个进程获得的args.local_rank值分别为0、1、2、3。</p>
</li>
</ol>
<p>上述命令行参数nproc_per_node表示每个节点需要创建多少个进程(使用几个GPU就创建几个)；nnodes表示使用几个节点，做单机多核训练设为1。</p>
<ol>
<li>因为每个进程都会初始化一份模型，为保证模型初始化过程中生成的随机权重相同，需要设置随机种子。方法如下：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def set_seed(seed):</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>使用方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from torch.utils.data.distributed import DistributedSampler  # 负责分布式dataloader创建，也就是实现上面提到的partition。</span><br><span class="line"></span><br><span class="line"># 负责创建 args.local_rank 变量，并接受 torch.distributed.launch 注入的值</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&quot;--local_rank&quot;, type=int, default=-1)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"># 每个进程根据自己的local_rank设置应该使用的GPU</span><br><span class="line">torch.cuda.set_device(args.local_rank)</span><br><span class="line">device = torch.device(&#x27;cuda&#x27;, args.local_rank)</span><br><span class="line"></span><br><span class="line"># 初始化分布式环境，主要用来帮助进程间通信</span><br><span class="line">torch.distributed.init_process_group(backend=&#x27;nccl&#x27;)</span><br><span class="line"></span><br><span class="line"># 固定随机种子</span><br><span class="line">seed = 42</span><br><span class="line">random.seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line">torch.cuda.manual_seed_all(seed)</span><br><span class="line"></span><br><span class="line"># 初始化模型</span><br><span class="line">model = Net()</span><br><span class="line">model.to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.1)</span><br><span class="line"></span><br><span class="line"># 只 master 进程做 logging，否则输出会很乱</span><br><span class="line">if args.local_rank == 0:</span><br><span class="line">    tb_writer = SummaryWriter(comment=&#x27;ddp-training&#x27;)</span><br><span class="line"></span><br><span class="line"># 分布式数据集</span><br><span class="line">train_sampler = DistributedSampler(train_dataset)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)  # 注意这里的batch_size是每个GPU上的batch_size</span><br><span class="line"></span><br><span class="line"># 分布式模型</span><br><span class="line">model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)</span><br></pre></td></tr></table></figure>
<p><strong>torch.distributed.init_process_group</strong>()包含四个常用的参数：</p>
<ul>
<li>backend: 后端, 实际上是多个机器之间交换数据的协议</li>
<li>init_method: 机器之间交换数据, 需要指定一个主节点, 而这个参数就是指定主节点的</li>
<li>world_size: 介绍都是说是进程, 实际就是机器的个数, 例如两台机器一起训练的话, world_size就设置为2</li>
<li>rank: 区分主节点和从节点的, 主节点为0, 剩余的为了1-(N-1), N为要使用的机器的数量, 也就是world_size</li>
</ul>
<h3 id="后端初始化"><a href="#后端初始化" class="headerlink" title="后端初始化"></a>后端初始化</h3><p>pytorch提供下列常用后端：</p>
<p><img src="https://pic2.zhimg.com/80/v2-b4d4a27387dde5cbd043d883948def09_720w.jpg" alt="image"></p>
<h3 id="初始化init-method"><a href="#初始化init-method" class="headerlink" title="初始化init_method"></a>初始化init_method</h3><ol>
<li>TCP初始化</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line"></span><br><span class="line">dist.init_process_group(backend, init_method=&#x27;tcp://10.1.1.20:23456&#x27;,</span><br><span class="line">                        rank=rank, world_size=world_size)</span><br></pre></td></tr></table></figure>
<p>注意这里使用格式为tcp://ip:端口号, 首先ip地址是你的主节点的ip地址, 也就是rank参数为0的那个主机的ip地址, 然后再选择一个空闲的端口号, 这样就可以初始化init_method了.</p>
<ol>
<li>共享文件系统初始化</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line"></span><br><span class="line">dist.init_process_group(backend, init_method=&#x27;file:///mnt/nfs/sharedfile&#x27;,</span><br><span class="line">                        rank=rank, world_size=world_size)</span><br></pre></td></tr></table></figure>
<h3 id="初始化rank和world-size"><a href="#初始化rank和world-size" class="headerlink" title="初始化rank和world_size"></a>初始化rank和world_size</h3><p>这里其实没有多难, 你需要确保, 不同机器的rank值不同, 但是主机的rank必须为0, 而且使用init_method的ip一定是rank为0的主机, 其次world_size是你的主机数量, 你不能随便设置这个数值, 你的参与训练的主机数量达不到world_size的设置值时, 代码是不会执行的.</p>
<h2 id="四、模型保存"><a href="#四、模型保存" class="headerlink" title="四、模型保存"></a>四、模型保存</h2><p>模型的保存与加载，与单GPU的方式有所不同。这里通通将参数以cpu的方式save进存储, 因为如果是保存的GPU上参数，pth文件中会记录参数属于的GPU号，则加载时会加载到相应的GPU上，这样就会导致如果你GPU数目不够时会在加载模型时报错</p>
<p>或者模型保存时控制进程，只在主进程进行保存。<br>模型保存都是一致的，不过分布式运行有多个进程在同时跑，所以会保存多个模型到存储上，如果使用共享存储就要注意文件名的问题，当然一般只在rank0进程上保存参数即可，因为所有进程的模型参数是同步的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.save(model.module.cpu().state_dict(), &quot;model.pth&quot;)</span><br></pre></td></tr></table></figure>
<p>参数加载：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">param=torch.load(&quot;model.pth&quot;)</span><br></pre></td></tr></table></figure>
<p>以下是huggingface/transformers代码中用到的模型保存代码<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if torch.distributed.get_rank() == 0:</span><br><span class="line">    model_to_save = model.module if hasattr(model, &quot;module&quot;) else model  # Take care of distributed/parallel training</span><br><span class="line">    model_to_save.save_pretrained(args.output_dir)</span><br><span class="line">    tokenizer.save_pretrained(args.output_dir)</span><br></pre></td></tr></table></figure></p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://zhuanlan.zhihu.com/p/86441879">pytorch多gpu并行训练</a></p>
<p><a href="https://github.com/jia-zhuang/pytorch-multi-gpu-training">PyTorch 单机多GPU 训练方法与原理整理</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>Nvidia-Docker配置python3与pytorch环境</title>
    <url>/2021/09/28/2021-09-28-Nvidia-Docker%E9%85%8D%E7%BD%AEpython3%E4%B8%8Epytorch%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<p>本文将介绍Nvidia-Docker配置python3与pytorch环境，完成docker容器内安装GPU深度学习环境。<br>TIPS：为了避免下载源过慢，建议添加中科大源/清华源<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">2. sudo sed -i &#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27; /etc/apt/sources.list</span><br><span class="line">3. sudo apt update</span><br></pre></td></tr></table></figure></p>
<span id="more"></span>
<h1 id="一、Docker与Nvidia-docker安装"><a href="#一、Docker与Nvidia-docker安装" class="headerlink" title="一、Docker与Nvidia-docker安装"></a>一、Docker与Nvidia-docker安装</h1><h2 id="TIPS：为了避免下载源过慢，建议添加中科大源-清华源"><a href="#TIPS：为了避免下载源过慢，建议添加中科大源-清华源" class="headerlink" title="TIPS：为了避免下载源过慢，建议添加中科大源/清华源"></a>TIPS：为了避免下载源过慢，建议添加中科大源/清华源</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">2. sudo sed -i &#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27; /etc/apt/sources.list</span><br><span class="line">3. sudo apt update</span><br></pre></td></tr></table></figure>
<h2 id="1-Docker安装"><a href="#1-Docker安装" class="headerlink" title="1. Docker安装"></a>1. Docker安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. Update the apt package index and install packages to allow apt to use a repository over HTTPS:</span><br><span class="line"></span><br><span class="line"> sudo apt-get update</span><br><span class="line"> sudo apt-get install \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    gnupg \</span><br><span class="line">    lsb-release</span><br><span class="line">    </span><br><span class="line">2. Add Docker’s official GPG key:</span><br><span class="line"></span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg</span><br><span class="line"></span><br><span class="line">3. Use the following command to set up the stable repository. To add the nightly or test repository, add the word nightly or test (or both) after the word stable in the commands below. Learn about nightly and test channels.</span><br><span class="line"></span><br><span class="line"> echo \</span><br><span class="line">  &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \</span><br><span class="line">  $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br><span class="line">  </span><br><span class="line">4. Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version:</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install docker-ce docker-ce-cli containerd.io</span><br><span class="line"></span><br><span class="line">5. To install a specific version of Docker Engine, list the available versions in the repo, then select and install:</span><br><span class="line"></span><br><span class="line">a. List the versions available in your repo:</span><br><span class="line"></span><br><span class="line">apt-cache madison docker-ce</span><br><span class="line"></span><br><span class="line">b. Install a specific version using the version string from the second column, for example, 5:18.09.1~3-0~ubuntu-xenial.</span><br><span class="line"></span><br><span class="line">sudo apt-get install docker-ce=&lt;VERSION_STRING&gt; docker-ce-cli=&lt;VERSION_STRING&gt; containerd.io</span><br><span class="line"></span><br><span class="line">6. Verify that Docker Engine is installed correctly by running the hello-world image.</span><br><span class="line"></span><br><span class="line">sudo docker run hello-world</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-Nvidia-Docker-安装"><a href="#2-Nvidia-Docker-安装" class="headerlink" title="2. Nvidia-Docker 安装"></a>2. Nvidia-Docker 安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. Setup the stable repository and the GPG key:</span><br><span class="line"></span><br><span class="line">distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line"></span><br><span class="line">2. To get access to experimental features such as CUDA on WSL or the new MIG capability on A100, you may want to add the experimental branch to the repository listing:</span><br><span class="line"></span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list</span><br><span class="line"></span><br><span class="line">3. Install the nvidia-docker2 package (and dependencies) after updating the package listing:</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y nvidia-docker2</span><br><span class="line"></span><br><span class="line">4. Restart the Docker daemon to complete the installation after setting the default runtime:</span><br><span class="line"></span><br><span class="line">sudo systemctl restart docker</span><br><span class="line"></span><br><span class="line">5. At this point, a working setup can be tested by running a base CUDA container:</span><br><span class="line">sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi</span><br></pre></td></tr></table></figure>
<h1 id="二、docker内安装python与pytorch环境"><a href="#二、docker内安装python与pytorch环境" class="headerlink" title="二、docker内安装python与pytorch环境"></a>二、docker内安装python与pytorch环境</h1><blockquote>
<p>拉取nvidia包含cuda的基础镜像。拟安装环境：python3.7, pytorch1.6</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 宿主机：提前在宿主机上下载好安装pip3.7要用到的包</span><br><span class="line">curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py</span><br><span class="line"></span><br><span class="line"># 宿主机与容器传输文件</span><br><span class="line">docker cp a.txt containerid:/path</span><br><span class="line"></span><br><span class="line"># 宿主机：运行ubuntu:18.04容器</span><br><span class="line">docker run -it -d --name=lz-ubuntu -v /root/get-pip.py:/root/get-pip.py ubuntu:18.04</span><br><span class="line"></span><br><span class="line"># 宿主机：进入到容器</span><br><span class="line">docker exec -it lz-ubuntu bash</span><br><span class="line"></span><br><span class="line"># 容器内：可选-安装vim</span><br><span class="line">apt-get update</span><br><span class="line">apt-get install vim -y</span><br><span class="line"></span><br><span class="line"># 容器内：配置pip源，用以加速安装</span><br><span class="line">sudo mkdir ~/.pip</span><br><span class="line">sudo vim ~/.pip/pip.conf</span><br><span class="line"></span><br><span class="line">添加以下内容：</span><br><span class="line">[global]</span><br><span class="line">index-url=https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host=mirrors.aliyun.com</span><br><span class="line"></span><br><span class="line">国内源：</span><br><span class="line">清华：</span><br><span class="line">https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">阿里云：</span><br><span class="line">http://mirrors.aliyun.com/pypi/simple/</span><br><span class="line">中国科技大学 </span><br><span class="line">https://pypi.mirrors.ustc.edu.cn/simple/</span><br><span class="line">华中理工大学：</span><br><span class="line">http://pypi.hustunique.com/</span><br><span class="line">山东理工大学：</span><br><span class="line">http://pypi.sdutlinux.org/</span><br><span class="line">豆瓣：</span><br><span class="line">http://pypi.douban.com/simple/</span><br><span class="line"></span><br><span class="line"># 容器内：可选-配置apt源</span><br><span class="line">mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">vim /etc/apt/sources.list</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line"># 容器内：更新软件包列表</span><br><span class="line">apt-get update</span><br><span class="line"></span><br><span class="line"># 容器内：可选-安装调试工具</span><br><span class="line">apt-get install iputils-ping net-tools curl</span><br><span class="line"></span><br><span class="line"># 容器内：安装最主要的python包</span><br><span class="line">apt-get install python3.7 python3.7-dev</span><br><span class="line"></span><br><span class="line"># 容器内：安装pip3.7</span><br><span class="line">apt install python3-distutils</span><br><span class="line">python3.7 get-pip.py</span><br><span class="line"></span><br><span class="line"># 容器内：安装pytorch</span><br><span class="line"># CUDA 10.1</span><br><span class="line">pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line"># 安装其他python包</span><br><span class="line">pip install transformers==2.10.0</span><br><span class="line">pip install pytorch-crf==0.7.2</span><br><span class="line">pip install sklearn</span><br><span class="line">pip install seqeval==1.2.2</span><br><span class="line">pip install pandas</span><br><span class="line"></span><br><span class="line"># 时区设置</span><br><span class="line"># 宿主机：从宿主机中拷贝时区文件到容器内，/usr/share/zoneinfo/UCT这个文件是通过软链追溯到的，时区是亚洲/上海</span><br><span class="line">docker cp /usr/share/zoneinfo/UCT  lyz-ubuntu:/etc/</span><br><span class="line"># 容器内：然后在容器内将其改名为/etc/localtime</span><br><span class="line">mv /etc/UCT /etc/localtime</span><br><span class="line"></span><br><span class="line"># 容器内：清理无用的包</span><br><span class="line">apt-get clean</span><br><span class="line">apt-get autoclean</span><br><span class="line">du -sh /var/cache/apt/</span><br><span class="line">rm -rf /var/cache/apt/archives</span><br><span class="line"></span><br><span class="line"># 容器内：清理pip缓存</span><br><span class="line">rm -rf ~/.cache/pip</span><br><span class="line"></span><br><span class="line"># 容器内：清理命令日志</span><br><span class="line">history -c</span><br><span class="line"></span><br><span class="line"># 宿主机：打包镜像</span><br><span class="line">docker commit -a &#x27;提交人&#x27; -m &#x27;描述&#x27; &lt;容器名/ID&gt; &lt;镜像名称&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="三、nvidia-docker-运行"><a href="#三、nvidia-docker-运行" class="headerlink" title="三、nvidia-docker 运行"></a>三、nvidia-docker 运行</h1><blockquote>
<p>nvidia-docker2版本下nvidia-docker run与docker run —runtime=nvidia效果无太大差异</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># pytorch gpu 可运行</span><br><span class="line">docker run -itd --gpus all --name 容器名 -eNVIDIA_DRIVER_CAPABILITIES=compute,utility -e NVIDIA_VISIBLE_DEVICES=all 镜像名</span><br><span class="line"></span><br><span class="line">多出来的东西其实就是这个家伙：NVIDIA_DRIVER_CAPABILITIES=compute,utility</span><br><span class="line">　　</span><br><span class="line">也就是说，如果你不改这个环境变量，宿主机的nvidia driver在容器内是仅作为utility存在的，如果加上compute，宿主机的英伟达driver将对容器提供计算支持（所谓的计算支持也就是cuda支持）。</span><br><span class="line"></span><br><span class="line"># nvidia-docker2验证</span><br><span class="line">nvidia-docker --version</span><br><span class="line"></span><br><span class="line"># nvidia-docker2 测试</span><br><span class="line">docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi</span><br><span class="line"></span><br><span class="line"># nvidia-docker2启动</span><br><span class="line">启动nvidia-docker：</span><br><span class="line">      $: systemctl start nvidia-docker</span><br><span class="line"> 查看docker服务是否启动：</span><br><span class="line">      $: systemctl status nvidia-docker</span><br></pre></td></tr></table></figure>
<h2 id="1-docker-run-参数介绍"><a href="#1-docker-run-参数介绍" class="headerlink" title="1. docker run 参数介绍"></a>1. docker run 参数介绍</h2><p><strong>docker run 常用参数介绍：</strong></p>
<ul>
<li><p>—rm选项，这样在容器退出时就能够自动清理容器内部的文件系统。</p>
</li>
<li><p>—i选项，打开STDIN，用于控制台交互</p>
</li>
<li><p>—t选项，分配tty设备，该可以支持终端登录，默认为false</p>
</li>
<li><p>—d选项，指定容器运行于前台还是后台，默认为false</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">其他参数介绍：</span><br><span class="line">  -u, --user=&quot;&quot;              指定容器的用户  </span><br><span class="line">  -a, --attach=[]            登录容器（必须是以docker run -d启动的容器）</span><br><span class="line">  -w, --workdir=&quot;&quot;           指定容器的工作目录 </span><br><span class="line">  -c, --cpu-shares=0        设置容器CPU权重，在CPU共享场景使用  </span><br><span class="line">  -e, --env=[]               指定环境变量，容器中可以使用该环境变量  </span><br><span class="line">  -m, --memory=&quot;&quot;            指定容器的内存上限  </span><br><span class="line">  -P, --publish-all=false    指定容器暴露的端口  </span><br><span class="line">  -p, --publish=[]           指定容器暴露的端口 </span><br><span class="line">  -h, --hostname=&quot;&quot;          指定容器的主机名  </span><br><span class="line">  -v, --volume=[]            给容器挂载存储卷，挂载到容器的某个目录  </span><br><span class="line">  --volumes-from=[]          给容器挂载其他容器上的卷，挂载到容器的某个目录</span><br><span class="line">  --cap-add=[]               添加权限，权限清单详见：http://linux.die.net/man/7/capabilities  </span><br><span class="line">  --cap-drop=[]              删除权限，权限清单详见：http://linux.die.net/man/7/capabilities  </span><br><span class="line">  --cidfile=&quot;&quot;               运行容器后，在指定文件中写入容器PID值，一种典型的监控系统用法  </span><br><span class="line">  --cpuset=&quot;&quot;                设置容器可以使用哪些CPU，此参数可以用来容器独占CPU  </span><br><span class="line">  --device=[]                添加主机设备给容器，相当于设备直通  </span><br><span class="line">  --dns=[]                   指定容器的dns服务器  </span><br><span class="line">  --dns-search=[]            指定容器的dns搜索域名，写入到容器的/etc/resolv.conf文件  </span><br><span class="line">  --entrypoint=&quot;&quot;            覆盖image的入口点  </span><br><span class="line">  --env-file=[]              指定环境变量文件，文件格式为每行一个环境变量  </span><br><span class="line">  --expose=[]                指定容器暴露的端口，即修改镜像的暴露端口  </span><br><span class="line">  --link=[]                  指定容器间的关联，使用其他容器的IP、env等信息  </span><br><span class="line">  --lxc-conf=[]              指定容器的配置文件，只有在指定--exec-driver=lxc时使用  </span><br><span class="line">  --name=&quot;&quot;                  指定容器名字，后续可以通过名字进行容器管理，links特性需要使用名字  </span><br><span class="line">  --net=&quot;bridge&quot;             容器网络设置:</span><br><span class="line">				                bridge 使用docker daemon指定的网桥     </span><br><span class="line">				                host 	//容器使用主机的网络  </span><br><span class="line">				                container:NAME_or_ID  &gt;//使用其他容器的网路，共享IP和PORT等网络资源  </span><br><span class="line">				                none 容器使用自己的网络（类似--net=bridge），但是不进行配置 </span><br><span class="line">  --privileged=false         指定容器是否为特权容器，特权容器拥有所有的capabilities  </span><br><span class="line">  --restart=&quot;no&quot;             指定容器停止后的重启策略:</span><br><span class="line">				                no：容器退出时不重启  </span><br><span class="line">				                on-failure：容器故障退出（返回值非零）时重启 </span><br><span class="line">				                always：容器退出时总是重启  </span><br><span class="line">  --rm=false                 指定容器停止后自动删除容器(不支持以docker run -d启动的容器)  </span><br><span class="line">  --sig-proxy=true           设置由代理接受并处理信号，但是SIGCHLD、SIGSTOP和SIGKILL不能被代理  </span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<h2 id="2-docker-常用命令"><a href="#2-docker-常用命令" class="headerlink" title="2. docker 常用命令"></a>2. docker 常用命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> ### 显示版本信息 (与python, nvcc相比少了两个‘--’）</span><br><span class="line">$ docker version</span><br><span class="line"></span><br><span class="line">### 了解当前Docker的使用状态（当前容器，镜像数目信息，存储空间占用信息，</span><br><span class="line"># OS内核版本， 发行版本， 硬件资源等）</span><br><span class="line">$ docker info</span><br><span class="line"></span><br><span class="line">### 拉去一个镜像 ( xxxx 表示某个镜像名字，）</span><br><span class="line">$ docker pull xxxx</span><br><span class="line"># e.g.</span><br><span class="line"># docker pull ubuntu</span><br><span class="line"></span><br><span class="line">### 查看系统中已有的镜像(images要带‘s&#x27;)</span><br><span class="line">$ docker images</span><br><span class="line"># e.g.:</span><br><span class="line"># REPOSITORY  TAG    IMAGES ID   CREATED VIRTUAL SIZE</span><br><span class="line"># ubuntu      latest 4ef6axxxxx   5 day ago  84.0M</span><br><span class="line"></span><br><span class="line">### 从镜像创建docker容器</span><br><span class="line">$ docker run -i -t ubuntu /bin/bash </span><br><span class="line"># or</span><br><span class="line">$ docker run -it 4ef /bin/bash</span><br><span class="line"># 其中 -i, 交互模式，让输入输出都在标准控制台进行；-d，则进入后台</span><br><span class="line"># -t, 为新创建的容器分配一个伪终端</span><br><span class="line"># ubuntu, 用于创建容器的镜像名，可用ID来代替（前3位足够）</span><br><span class="line"># /bin/bash， 在新建容器中运行的命令，可以为任意Linux命令</span><br><span class="line"></span><br><span class="line">### 离开当前容器,返回宿主机终端，使用组合键 &quot;Ctrl+P&quot; 和 &quot;Ctrl+Q&quot;</span><br><span class="line"></span><br><span class="line">### 查看当前活动的容器</span><br><span class="line">$ docker ps</span><br><span class="line"># CONTAINER ID  IMAGE  COMMAND  CREATED   STATUS   PORTS NAME</span><br><span class="line"># 610xxxx  ubuntu:latest  &quot;/bin/bash&quot; 1 minute ago Up 1 minute ago prickly_wilson</span><br><span class="line"></span><br><span class="line">### 宿主机终端与某个容器建立连接</span><br><span class="line">$ docker attach 610</span><br><span class="line"></span><br><span class="line">### 从容器创建Docker镜像</span><br><span class="line">$ docker commit -m &quot;hhahaha&quot; 610 ubuntu:hhh</span><br><span class="line"># -m, 新镜像说明</span><br><span class="line"># 610， 某个容器的ID</span><br><span class="line"># ubuntu:hhh， 命名最好不要这么随意</span><br><span class="line"># 那么接下来可以查看新生成的镜像，命令 docker images</span><br><span class="line"></span><br><span class="line">### 基于新的镜像创建一个新的容器(一样的)</span><br><span class="line">$ docker run -it ubuntu:hhh /bin/bash</span><br><span class="line"></span><br><span class="line">### 给镜像重命名(方便记忆)</span><br><span class="line">$ docker tag IMAGEID(image id) REPOSITORY:TAG</span><br><span class="line"></span><br><span class="line">### 给容器重命名</span><br><span class="line">$ docker rename old-container-name new-container-name</span><br></pre></td></tr></table></figure>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://blog.csdn.net/qq_39698985/article/details/109524762">https://blog.csdn.net/qq_39698985/article/details/109524762</a></p>
<p><a href="https://blog.csdn.net/mumoDM/article/details/82503022">https://blog.csdn.net/mumoDM/article/details/82503022</a></p>
<p><a href="https://blog.csdn.net/qq_29518275/article/details/107486028">https://blog.csdn.net/qq_29518275/article/details/107486028</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>Nvidia-Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习基础-神经网络权重初始化</title>
    <url>/2021/09/29/2021-09-29-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
    <content><![CDATA[<p>本文将介绍神经网络权重初始化的原理与实现。首先解决了两个问题：1. 全零初始化是否可以、2. 参数全部相同初始化是否可以</p>
<p>然后介绍了初始化的方式：</p>
<ol>
<li>预训练初始化</li>
<li>随机初始化</li>
<li>固定初始化</li>
</ol>
<span id="more"></span>
<h1 id="一、两个问题"><a href="#一、两个问题" class="headerlink" title="一、两个问题"></a>一、两个问题</h1><blockquote>
<p>假设3层神经网络,输入节点v0，第一层节点v1,v2,v3 第二层节点v4,v5 第三层节点v6。其中vi=f(ai),i=4,5,6 f为激活函数。</p>
</blockquote>
<p><strong>前向传播：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/83cc1293388e439ab8a1d33e6c67396f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASk1YR09ETFo=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h2 id="1-全零初始化是否可以"><a href="#1-全零初始化是否可以" class="headerlink" title="1. 全零初始化是否可以"></a>1. 全零初始化是否可以</h2><p>一般情况不可以。当全零参数初始化时，除输入节点所有节点值均为0,根据上式除第一层梯度与输入值有关其余均为0.</p>
<p>LR等一层网络可以全零初始化， 网络梯度与输入值有关。仅全零初始化一层也不影响训练，但涉及两层及以上，从涉及层到输入层的梯度都为0,参数无法更新。</p>
<h2 id="2-参数全部相同初始化是否可以"><a href="#2-参数全部相同初始化是否可以" class="headerlink" title="2. 参数全部相同初始化是否可以"></a>2. 参数全部相同初始化是否可以</h2><p>不可以。若初始化为相同的参数，隐藏层所有节点输出相同，梯度也是相同的。相当于输入经过一个节点。</p>
<h1 id="二、参数初始化方式"><a href="#二、参数初始化方式" class="headerlink" title="二、参数初始化方式"></a>二、参数初始化方式</h1><h2 id="1-预训练初始化"><a href="#1-预训练初始化" class="headerlink" title="1. 预训练初始化"></a>1. 预训练初始化</h2><p><strong>pretraining + finetuning</strong>加载已训练好的模型参数，进行下游任务的模型训练。</p>
<h2 id="2-随机初始化"><a href="#2-随机初始化" class="headerlink" title="2. 随机初始化"></a>2. 随机初始化</h2><h3 id="2-1-random-initialization"><a href="#2-1-random-initialization" class="headerlink" title="2.1 random initialization"></a>2.1 random initialization</h3><p><code>random initialization: np.random.randn(m,n)</code></p>
<p>随机产生符合正态分布的m×n维向量</p>
<p>弊端：随机会产生梯度消失，随着网络层次加深，由于链式求导法则，输出越来越接近0</p>
<h3 id="2-2-Xavier-initialization"><a href="#2-2-Xavier-initialization" class="headerlink" title="2.2 Xavier initialization"></a>2.2 Xavier initialization</h3><p><code>tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in)</code></p>
<p><img src="https://pic3.zhimg.com/80/v2-6302a7093b93e1376e54e95033c58086_720w.jpg" alt="image"></p>
<p>M 代表着输入输出维度即node_in,node_out</p>
<p>保证输入输出方差一致</p>
<h3 id="2-3-He-initialization"><a href="#2-3-He-initialization" class="headerlink" title="2.3 He initialization"></a>2.3 He initialization</h3><p><code>tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in/2)</code></p>
<p>适用于RELU激活函数，只有半区有效</p>
<h2 id="3-固定初始化"><a href="#3-固定初始化" class="headerlink" title="3. 固定初始化"></a>3. 固定初始化</h2><p>比如对于偏置（bias）通常用0初始化，LSTM遗忘门偏置通常为1或2，使时序上的梯度变大，对于ReLU神经元，偏置设为0.01，使得训练初期更容易激活。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://note.youdao.com/">https://www.leiphone.com/category/ai/3qMp45aQtbxTdzmK.html</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>权重初始化</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention可视化</title>
    <url>/2021/12/24/2021-12-24-Attention%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<p>本文介绍了Attention可视化的方式，包含两种热力图可视化、文本可视化。</p>
<ol>
<li>使用matplotlib与seaborn完成attention矩阵的热力图绘制</li>
<li>根据注意力矩阵，得到html颜色深浅代表相关性强弱</li>
</ol>
<span id="more"></span>
<h1 id="热力图可视化"><a href="#热力图可视化" class="headerlink" title="热力图可视化"></a>热力图可视化</h1><blockquote>
<p>使用matplotlib与seaborn完成attention矩阵的热力图绘制</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"># author:   LZ</span><br><span class="line"># create time:  2021/11/15 14:54</span><br><span class="line"># file: genpic.py</span><br><span class="line"># IDE:  PyCharm</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.ticker as ticker</span><br><span class="line">import seaborn as sns</span><br><span class="line">import matplotlib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gperes = &#123;</span><br><span class="line">     &#x27;武&#x27;: &#123;&#x27;武&#x27;: 0.1, &#x27;汉&#x27;: 0.9, &#x27;市&#x27;: 1, &#x27;长&#x27;: 0.05, &#x27;江&#x27;: 0.09, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.2&#125;,</span><br><span class="line">     &#x27;汉&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0.2, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.03, &#x27;江&#x27;: 0.04, &#x27;大&#x27;: 0.02, &#x27;桥&#x27;: 0.07&#125;,</span><br><span class="line">     &#x27;市&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.01, &#x27;江&#x27;: 0.02, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.05&#125;,</span><br><span class="line">     &#x27;长&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0.25, &#x27;江&#x27;: 0.1, &#x27;大&#x27;: 0.08, &#x27;桥&#x27;: 0.09&#125;,</span><br><span class="line">     &#x27;江&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0.15, &#x27;大&#x27;: 0.04, &#x27;桥&#x27;: 0.015&#125;,</span><br><span class="line">     &#x27;大&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0.14, &#x27;桥&#x27;: 0.15&#125;,</span><br><span class="line">     &#x27;桥&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0, &#x27;桥&#x27;: 0.12&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">newgperes = &#123;&#125;</span><br><span class="line">for k, v in gperes.items():</span><br><span class="line">     for w, a in v.items():</span><br><span class="line">          newgperes.setdefault(w, &#123;&#125;)</span><br><span class="line">          newgperes[w][k] = a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.rcParams[&#x27;font.sans-serif&#x27;]=[&#x27;Arial Unicode MS&#x27;]</span><br><span class="line">plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False</span><br><span class="line">gpedata = pd.DataFrame(newgperes)</span><br><span class="line">ax = sns.heatmap(gpedata, cmap=&quot;YlOrRd&quot;)</span><br><span class="line">ax.xaxis.tick_top()</span><br><span class="line">ax.set_yticklabels(ax.get_yticklabels(), rotation=0)</span><br><span class="line">plt.savefig(&#x27;loctest.png&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://s3.bmp.ovh/imgs/2021/12/4100fe169b8b9921.png" alt="gpe.png"></p>
<h1 id="NLP文本注意力可视化"><a href="#NLP文本注意力可视化" class="headerlink" title="NLP文本注意力可视化"></a>NLP文本注意力可视化</h1><blockquote>
<p>根据注意力矩阵，得到html颜色深浅代表相关性强弱</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Credits to Lin Zhouhan(@hantek) for the complete visualization code</span><br><span class="line">import random, os, numpy, scipy</span><br><span class="line">from codecs import open</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def createHTML(texts, weights, savepath):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Creates a html file with text heat.</span><br><span class="line">	weights: attention weights for visualizing</span><br><span class="line">	texts: text on which attention weights are to be visualized</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    fOut = open(savepath, &quot;w&quot;, encoding=&quot;utf-8&quot;)</span><br><span class="line">    part1 = &quot;&quot;&quot;</span><br><span class="line">    &lt;html lang=&quot;en&quot;&gt;</span><br><span class="line">    &lt;head&gt;</span><br><span class="line">    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;</span><br><span class="line">    &lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">    font-family: Sans-Serif;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;/style&gt;</span><br><span class="line">    &lt;/head&gt;</span><br><span class="line">    &lt;body&gt;</span><br><span class="line">    &lt;h3&gt;</span><br><span class="line">    Heatmaps</span><br><span class="line">    &lt;/h3&gt;</span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">    &lt;script&gt;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    part2 = &quot;&quot;&quot;</span><br><span class="line">    var color = &quot;255,0,0&quot;;</span><br><span class="line">    var ngram_length = 3;</span><br><span class="line">    var half_ngram = 1;</span><br><span class="line">    for (var k=0; k &lt; any_text.length; k++) &#123;</span><br><span class="line">    var tokens = any_text[k].split(&quot; &quot;);</span><br><span class="line">    var intensity = new Array(tokens.length);</span><br><span class="line">    var max_intensity = Number.MIN_SAFE_INTEGER;</span><br><span class="line">    var min_intensity = Number.MAX_SAFE_INTEGER;</span><br><span class="line">    for (var i = 0; i &lt; intensity.length; i++) &#123;</span><br><span class="line">    intensity[i] = 0.0;</span><br><span class="line">    for (var j = -half_ngram; j &lt; ngram_length-half_ngram; j++) &#123;</span><br><span class="line">    if (i+j &lt; intensity.length &amp;&amp; i+j &gt; -1) &#123;</span><br><span class="line">    intensity[i] += trigram_weights[k][i + j];</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if (i == 0 || i == intensity.length-1) &#123;</span><br><span class="line">    intensity[i] /= 2.0;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">    intensity[i] /= 3.0;</span><br><span class="line">    &#125;</span><br><span class="line">    if (intensity[i] &gt; max_intensity) &#123;</span><br><span class="line">    max_intensity = intensity[i];</span><br><span class="line">    &#125;</span><br><span class="line">    if (intensity[i] &lt; min_intensity) &#123;</span><br><span class="line">    min_intensity = intensity[i];</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    var denominator = max_intensity - min_intensity;</span><br><span class="line">    for (var i = 0; i &lt; intensity.length; i++) &#123;</span><br><span class="line">    intensity[i] = (intensity[i] - min_intensity) / denominator;</span><br><span class="line">    &#125;</span><br><span class="line">    if (k%2 == 0) &#123;</span><br><span class="line">    var heat_text = &quot;&lt;p&gt;&lt;br&gt;&lt;b&gt;Example:&lt;/b&gt;&lt;br&gt;&quot;;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">    var heat_text = &quot;&lt;b&gt;Example:&lt;/b&gt;&lt;br&gt;&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    var space = &quot;&quot;;</span><br><span class="line">    for (var i = 0; i &lt; tokens.length; i++) &#123;</span><br><span class="line">    heat_text += &quot;&lt;span style=&#x27;background-color:rgba(&quot; + color + &quot;,&quot; + intensity[i] + &quot;)&#x27;&gt;&quot; + space + tokens[i] + &quot;&lt;/span&gt;&quot;;</span><br><span class="line">    if (space == &quot;&quot;) &#123;</span><br><span class="line">    space = &quot; &quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //heat_text += &quot;&lt;p&gt;&quot;;</span><br><span class="line">    document.body.innerHTML += heat_text;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;/script&gt;</span><br><span class="line">    &lt;/html&gt;&quot;&quot;&quot;</span><br><span class="line">    putQuote = lambda x: &quot;\&quot;%s\&quot;&quot; % x</span><br><span class="line">    textsString = &quot;var any_text = [%s];\n&quot; % (&quot;,&quot;.join(map(putQuote, texts)))</span><br><span class="line">    weightsString = &quot;var trigram_weights = [%s];\n&quot; % (&quot;,&quot;.join(map(str, weights)))</span><br><span class="line">    fOut.write(part1)</span><br><span class="line">    fOut.write(textsString)</span><br><span class="line">    fOut.write(weightsString)</span><br><span class="line">    fOut.write(part2)</span><br><span class="line">    fOut.close()</span><br><span class="line"></span><br><span class="line">    return</span><br><span class="line"></span><br><span class="line">gperes = &#123;</span><br><span class="line">     &#x27;武&#x27;: &#123;&#x27;武&#x27;: 0.1, &#x27;汉&#x27;: 0.9, &#x27;市&#x27;: 1, &#x27;长&#x27;: 0.05, &#x27;江&#x27;: 0.09, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.2&#125;,</span><br><span class="line">     &#x27;汉&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0.2, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.03, &#x27;江&#x27;: 0.04, &#x27;大&#x27;: 0.02, &#x27;桥&#x27;: 0.07&#125;,</span><br><span class="line">     &#x27;市&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.01, &#x27;江&#x27;: 0.02, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.05&#125;,</span><br><span class="line">     &#x27;长&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0.25, &#x27;江&#x27;: 0.1, &#x27;大&#x27;: 0.08, &#x27;桥&#x27;: 0.09&#125;,</span><br><span class="line">     &#x27;江&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0.15, &#x27;大&#x27;: 0.04, &#x27;桥&#x27;: 0.015&#125;,</span><br><span class="line">     &#x27;大&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0.14, &#x27;桥&#x27;: 0.15&#125;,</span><br><span class="line">     &#x27;桥&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0, &#x27;桥&#x27;: 0.12&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">matrix = []</span><br><span class="line"># 获取矩阵表示</span><br><span class="line">for k, v in gperes.items():</span><br><span class="line">    matrix.append(list(v.values()))</span><br><span class="line">str_ = &#x27;武 汉 市 长 江 大 桥&#x27;</span><br><span class="line">createHTML([str_] * 7,</span><br><span class="line">           matrix,</span><br><span class="line">           &#x27;./visualization/test.html&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.bmp.ovh/imgs/2021/12/04573a3a1f1fb2b6.png" alt="wx20211211-193035@2x.png"></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://blog.csdn.net/qq_38607066/article/details/101345282#t17">https://blog.csdn.net/qq_38607066/article/details/101345282#t17</a></p>
<p><a href="https://blog.csdn.net/u010490755/article/details/89574847">https://blog.csdn.net/u010490755/article/details/89574847</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>2022预训练的下一步是什么</title>
    <url>/2021/12/31/2021-12-31-2022%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5%E6%98%AF%E4%BB%80%E4%B9%88/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文内容为自己对2021年自身算法经历的回顾，同时展望了未来研究的方向。如有理解不对的地方，欢迎指正批评。</p>
<p>2021年研究热点</p>
<ul>
<li>大规模预训练</li>
<li>对比学习</li>
<li>prompt</li>
</ul>
<p>展望未来</p>
<p>回顾自身算法经历</p>
<ol>
<li>需求分析</li>
<li>模型选型及设计</li>
<li>数据分析</li>
<li>模型训练及优化</li>
<li>分析负例<ol>
<li>检查数据质量是否过差</li>
<li>根据指标进行分析<ul>
<li>recall低</li>
<li>precision低</li>
</ul>
</li>
</ol>
</li>
</ol>
<span id="more"></span>
<blockquote>
<p>该内容为自己对2021年自身算法经历的回顾，同时展望了未来研究的方向。如有理解不对的地方，欢迎指针批评。</p>
</blockquote>
<h1 id="2021年研究热点"><a href="#2021年研究热点" class="headerlink" title="2021年研究热点"></a>2021年研究热点</h1><h2 id="大规模预训练"><a href="#大规模预训练" class="headerlink" title="大规模预训练"></a>大规模预训练</h2><p>预训练+微调的做法，在多个下游领域取得优异的表现。而在过去的一年里，预训练模型更是在往<strong>大而深</strong>的方向发展。</p>
<p>目前，国内已有智源研究院、鹏城实验室、中科院自动化所、阿里、百度、华为、浪潮等科研院所和企业研相继发出“悟道”、“盘古”、“紫东 · 太初”、M6、PLUG、ERNIE 3.0 等大模型。</p>
<p>但是模型在往大而深方向发展的同时，也存在如下亟待解决的问题：</p>
<ul>
<li>如何解释预训练模型的理论基础（如大模型智能的参数规模极限存在吗）</li>
<li>如何将大模型高效、低成本的应用于实际系统</li>
<li>如何克服构建大模型的数据质量、训练效率、算力消耗、模型交付等诸多障碍</li>
<li>如何解决目前大部分大模型普遍缺乏认知能力的问题</li>
</ul>
<h2 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h2><p><strong>对比学习的出发点在于避免模型坍塌，理想的模型应该符合alignment和uniformity，即语义相近的句子彼此聚集，语义无关的句子均匀分布。</strong></p>
<p>如果仅仅通过数据增强构建正例，随机句子作为负例，并为其打上0，1标签，存在以下问题：</p>
<ul>
<li>数据增强生成正例的变化有限</li>
<li>随机搭配成负例，含有除正例组合外其他组合全部为0的诱导</li>
<li>0，1标签的赋予太过绝对，对相似性表述不够准确</li>
</ul>
<p>因此对比学习的核心思想转变为：</p>
<script type="math/tex; mode=display">score(X,X^{'}) >> score(X,Y)</script><p>其中，X代表原样本，$X^{‘c}$代表数据增强的正样本，Y代表随机选择的负样本。</p>
<p>根据该思想，对比学习采用InfoNCE损失函数：</p>
<script type="math/tex; mode=display">loss = -log \frac{exp(score(X,X^{'}))}{score(X,X^{'})+\sum_{i=1}^{N}score(X,Y_i)}</script><p>通过该损失函数实现正例拉近，负例推远的效果。</p>
<h2 id="prompt"><a href="#prompt" class="headerlink" title="prompt"></a>prompt</h2><p>prompt被誉为NLP领域的新范式，与预训练+微调的范式相比，其过程分为：”pre-train, prompt, and predict”。</p>
<p><strong>prompt的出发点在于以更轻量化的方式利用预训练模型，避免微调与预训练之间存在的差异。</strong></p>
<p>prompt通过构建模版的方式，将下游任务转为与预训练相似的MLM任务，以该方式充分发挥预训练模型的性能。</p>
<p>以文本情感分类任务中，”I love this movie.”句子为例，prompt按照以下方式进行处理：</p>
<ol>
<li>生成prompt句子</li>
</ol>
<p>该步骤完成输入句子到模型输入的映射：</p>
<script type="math/tex; mode=display">x^{'}=f_{prompt}(x)</script><p>其中，$x^{‘}$为生成的prompt句子，x为输入句子，$f_{prompt}$为prompt函数。</p>
<p>在本例中，使用的模版为： “ [X] Overall, it was a [Z] movie.”</p>
<p>因此，得到的，$x^{‘}$为”I love this movie. Overall it was a [Z] movie.”</p>
<ol>
<li>模型预测</li>
</ol>
<p>该步骤将$x^{‘}$输入模型，模型完成模版空白位置的词语预测。</p>
<p>在本例中，模型可能预测为：”excellent”, “great”, “wonderful” 等词语</p>
<ol>
<li>结果映射</li>
</ol>
<p>通常模型预测的词语与任务输出存在一定差距，因此我们需要完成词语到输出结果的映射。</p>
<script type="math/tex; mode=display">y = f(x^{'})</script><p>在本例中，”excellent”, “great”, “wonderful” 等词语映射为标签 “++”</p>
<h1 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a>展望未来</h1><p>首先我认为当前基于数据驱动方法存在如下的问题：</p>
<ol>
<li>长尾效应：自然界中的数据分布就是长尾的，在学习的过程中，模型容易发生过拟合，泛化性较差。</li>
<li>数据噪声：有标签的数据，在标注过程中就不可避免的存在噪声。尤其是多位人员一起标注时，不同标注人员根据自身的理解完成数据的标注，但不同的人自身理解存在偏差，因此标注结果极易存在误差。归根到底：标注的规范难以确定，无法统一大家的知识库。</li>
</ol>
<p>当前我遇到的一些问题分享：模型仍无法很好地处理下述问题：</p>
<blockquote>
<p>太阳有几只眼睛？</p>
<p>姚明与奥尼尔身高谁比较高？</p>
<p>猫咪可以吃生蛋黄吗？猫咪是可以吃蛋黄的。这里特定煮熟的白水蛋，猫咪不能吃生鸡蛋，因为生鸡蛋中有细菌。</p>
<p>物质都是由分子构成的吗？物质都是由分子构成的，分子又由原子构成-错的！因为有些物质是不含分子的。</p>
</blockquote>
<p>这些问题，我总结为两方面的困难：</p>
<ol>
<li>缺乏知识，由于预训练与微调领域存在偏差，模型在下游任务中缺乏特定知识，同时模型在一些常识问题上表现较差。</li>
<li>缺乏深度语义的理解，模型表现的更像通过字面匹配完成任务，推理的成分更弱。</li>
</ol>
<p>当前研究热点仍然在于挖掘预训练模型的能力，但在基于常识性知识与逻辑推理的问题上，这种基于数据驱动的方式从底层就存在问题。引用一下大咖们对2022年的展望。</p>
<blockquote>
<p>大模型一方面在不少问题上取得了以往难以预期的成功，另一方面其巨大的训练能耗和碳排放是不能忽视的问题。个人以为，大模型未来会在一些事关国计民生的重大任务上发挥作用，而在其他一些场景下或许会通过类似集成学习的手段来利用小模型，尤其是通过很少量训练来 “复用” 和集成已有的小模型来达到不错的性能。</p>
<p>我们提出了一个叫做 “学件” 的思路，目前在做一些这方面的探索。大致思想是，假设很多人已经做了模型并且乐意放到某个市场去共享，市场通过建立规约来组织和管理学件，以后的人再做新应用时，就可以不用从头收集数据训练模型，可以先利用规约去市场里找找看是否有比较接近需求的模型，然后拿回家用自己的数据稍微打磨就能用。这其中还有一些技术挑战需要解决，我们正在研究这个方向。</p>
<p>另一方面，有可能通过利用人类的常识和专业领域知识，使模型得以精简，这就要结合逻辑推理和机器学习。逻辑推理比较善于利用人类知识，机器学习比较善于利用数据事实，如何对两者进行有机结合一直是人工智能中的重大挑战问题。麻烦的是逻辑推理是严密的基于数理逻辑的 “从一般到特殊”的演绎过程，机器学习是不那么严密的概率近似正确的 “从特殊到一般”的归纳过程，在方法论上就非常不一样。已经有的探索大体上是以其中某一方为倚重，引入另一方的某些成分，我们最近在探索双方相对均衡互促利用的方式。</p>
</blockquote>
<p>谈谈自己的理解，<strong>预训练模型的方式归根到底仍然属于数据驱动的任务，其通过在大规模数据上学习，推断未知数据的概率。如果说数据中存在表述不准确、表述有歧义或者词汇本身就有多个含义的话，以概率的方式难以解决这些问题。</strong> 而人脑在未知问题上，推理成分居多，以一词多义为例，人类会考虑该词汇有几种用法，考虑在这种上下文语境下使用哪一种用法，所以是否可以建立一套类似于标准公理的语言规范，以该规范为基础，对未知句子进行拆解推理，理解句子的完整含义。通过了解模型的推理过程，模型的可解释性增强。当预测错误时，我们可以进行溯源分析，对模型依赖的知识进行调整，或者让模型学习的更充分。</p>
<p>接下来对自己2022年的期望：</p>
<ol>
<li>自身学习更多模型结构变化的同时，更多地理解业务的架构，明白模型在业务中起的作用。</li>
<li>在算法研究上能够研究的更加深入，希望能够找到解决上述困难的方法。</li>
</ol>
<h1 id="回顾自身算法经历"><a href="#回顾自身算法经历" class="headerlink" title="回顾自身算法经历"></a>回顾自身算法经历</h1><p>2021年自身的算法经历主要分为：实习、算法比赛、项目、论文四部分。在这些经历里面主要接触分类、阅读理解、信息抽取三种任务，评估方式均采用精确率、召回率及F1值。下面将以这些经历为基础，介绍我处理这些任务的方式。</p>
<h2 id="1-需求分析"><a href="#1-需求分析" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h2><p>开展算法工作之前，首先要搞清楚算法需要满足什么样的需求。包括：</p>
<ul>
<li>业务属于什么样的任务</li>
<li>算法需要侧重的方向</li>
<li>训练数据及线上数据的情况</li>
<li>线上的指标</li>
<li>线下的评估方式</li>
<li>……</li>
</ul>
<p><strong>需求分析的目的在于了解业务的需求与算法在业务中起到的作用。</strong></p>
<h2 id="2-模型选型及设计"><a href="#2-模型选型及设计" class="headerlink" title="2. 模型选型及设计"></a>2. 模型选型及设计</h2><p>在明白需求之后，需要根据任务类型选择模型，并根据需求的不同，对模型结构进行调整。如阅读理解任务下：针对多答案、无答案的情况，我们需要调整模型的结构。</p>
<p><strong>模型选型及设计的目的在于选择或设计能够很好地满足业务需求的模型。</strong></p>
<h2 id="3-数据分析"><a href="#3-数据分析" class="headerlink" title="3. 数据分析"></a>3. 数据分析</h2><p><strong>数据分析这一步是最重要的一步，当前模型主要还是以数据驱动，数据对模型的影响很大。</strong> </p>
<p>我主要从以下角度进行分析：</p>
<ul>
<li>数据是否存在噪声：标点、大小写、特殊符号等</li>
<li>训练集测试集分布是否存在差异，测试集能否反映模型在具体业务下的表现</li>
<li>数据存在哪些特征，通过引入额外的特征，模型可以表现地更好</li>
<li>训练集分布：标签分布、长度分布等，是否会给模型带来类别不均衡、长文本等问题</li>
<li>数据量大小，数据量足够时可以继续预训练</li>
</ul>
<p><strong>数据分析的目的在于数据能否充分发挥模型性能，能否得到符合业务需求的模型</strong></p>
<h2 id="4-模型训练及优化"><a href="#4-模型训练及优化" class="headerlink" title="4. 模型训练及优化"></a>4. 模型训练及优化</h2><p><strong>模型进行训练，开始炼丹【调参】。</strong></p>
<ul>
<li>设置合适的超参数【可以通过一些超参数搜索算法】</li>
<li>选择合适的优化器【adam/adamw/sgd】</li>
<li>学习率调整的策略</li>
</ul>
<p><strong>进阶版：</strong></p>
<ul>
<li>对抗训练</li>
<li>对比学习</li>
<li>UDA等数据增强方式</li>
<li>继续预训练</li>
<li>多任务学习</li>
<li>伪标签</li>
<li>SWA</li>
<li>……</li>
</ul>
<h2 id="5-分析负例"><a href="#5-分析负例" class="headerlink" title="5. 分析负例"></a>5. 分析负例</h2><p>该过程同样重要，我们需要了解模型在测试数据上的表现情况，在什么数据表现较差，如何优化这些负例。</p>
<p><strong>在优化过程中，建议记录每一次优化信息，分析模型的提升/降低是否符合自己预期，充分利用每一次实验</strong></p>
<p>下面总结了我在优化过程常用的分析方式：</p>
<h3 id="1-检查数据质量是否过差"><a href="#1-检查数据质量是否过差" class="headerlink" title="1. 检查数据质量是否过差"></a>1. 检查数据质量是否过差</h3><p>这种情况通常表现为数据质量较差，模型在原始数据上表现不佳，精确率与召回率都很低。针对这种情况，需要对数据做必要的预处理，让模型能够更好地学习。</p>
<h3 id="2-根据指标进行分析"><a href="#2-根据指标进行分析" class="headerlink" title="2. 根据指标进行分析"></a>2. 根据指标进行分析</h3><h4 id="recall低"><a href="#recall低" class="headerlink" title="recall低"></a>recall低</h4><p>召回率表示召回的数量，测试集数据未召回较多，则从下列角度检查数据：</p>
<ol>
<li>训练集测试集数据差异是否较大，即训练集中是否存在类似数据，若不存在则引入更多数据或者对该数据进行数据增强。<strong>这种情况，常见原因为数据分布不均衡-少数数据训练不充分；训练集、测试集分布差异较大导致</strong></li>
<li>训练集中存在类似数据，检查训练集中该种情况有无标注错误：漏标、错标。</li>
</ol>
<h4 id="precision低"><a href="#precision低" class="headerlink" title="precision低"></a>precision低</h4><p>精确率表示预测出的准确率，测试集数据分错的较多：</p>
<ol>
<li>检查数据分布，是否数据分布不均衡。<strong>数据不均衡导致模型倾向于预测数量较多的数据，精确率下降</strong></li>
<li>标签定义是否准确，是否存在两类标签混淆的情况。<strong>这种情况，需要考虑对标签进行融合</strong></li>
</ol>
<p>类别不均衡常用解决方式：</p>
<ul>
<li>数据增强</li>
<li>resample</li>
<li>reweight</li>
<li>集成学习</li>
</ul>
<p>数据错误常用解决方式：</p>
<ul>
<li>交叉验证</li>
<li>置信学习</li>
<li>聚类分析</li>
</ul>
<p>接下来的过程则是迭代分析，直到模型性能符合业务需求。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://mp.weixin.qq.com/s/RqkQzeR5BOVpU7tj_zUgqQ">https://mp.weixin.qq.com/s/RqkQzeR5BOVpU7tj_zUgqQ</a></p>
<p><a href="https://www.zhihu.com/question/480187938/answer/2103245373">https://www.zhihu.com/question/480187938/answer/2103245373</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/399295895">https://zhuanlan.zhihu.com/p/399295895</a></p>
]]></content>
      <categories>
        <category>年度总结</category>
        <category>2021</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>年度总结</tag>
        <tag>NLP</tag>
        <tag>预训练</tag>
        <tag>对比学习</tag>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据分析-数据可视化(二)</title>
    <url>/2022/01/09/2022-01-09-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96(%E4%BA%8C)/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>看到有些论文插图十分简洁美观，于是便摸索一下如何美化一下折线图绘图。本文将在前文<a href="https://jmxgodlz.xyz/2020/01/03/2020-01-03-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/#more">Python数据分析-数据可视化</a>的基础上，介绍折线图格式的调整。</p>
<p>本文使用的画图工具为matplotlib，相关API可访问<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html">python matplotlib文档</a>。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gy7pemcxsnj30m80ecdh1.jpg" alt=""></p>
<span id="more"></span>
<h1 id="Matplotlib-折线图格式调整"><a href="#Matplotlib-折线图格式调整" class="headerlink" title="Matplotlib 折线图格式调整"></a>Matplotlib 折线图格式调整</h1><p>首先，贴一下文档中折线图绘制的附加参数表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>agg_filter</td>
<td>a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array</td>
</tr>
<tr>
<td>alpha</td>
<td>scalar or None</td>
</tr>
<tr>
<td>animated</td>
<td>bool</td>
</tr>
<tr>
<td>antialiased or aa</td>
<td>bool</td>
</tr>
<tr>
<td>clip_box</td>
<td>Bbox</td>
</tr>
<tr>
<td>clip_on</td>
<td>bool</td>
</tr>
<tr>
<td>clip_path</td>
<td>Patch or (Path, Transform) or None</td>
</tr>
<tr>
<td>color or c</td>
<td>color</td>
</tr>
<tr>
<td>dash_capstyle</td>
<td>CapStyle or {‘butt’, ‘projecting’, ‘round’}</td>
</tr>
<tr>
<td>dash_joinstyle</td>
<td>JoinStyle or {‘miter’, ‘round’, ‘bevel’}</td>
</tr>
<tr>
<td>dashes</td>
<td>sequence of floats (on/off ink in points) or (None, None)</td>
</tr>
<tr>
<td>data</td>
<td>(2, N) array or two 1D arrays</td>
</tr>
<tr>
<td>drawstyle or ds</td>
<td>{‘default’, ‘steps’, ‘steps-pre’, ‘steps-mid’, ‘steps-post’}, default: ‘default’</td>
</tr>
<tr>
<td>figure</td>
<td>Figure</td>
</tr>
<tr>
<td>fillstyle</td>
<td>{‘full’, ‘left’, ‘right’, ‘bottom’, ‘top’, ‘none’}</td>
</tr>
<tr>
<td>gid</td>
<td>str</td>
</tr>
<tr>
<td>in_layout</td>
<td>bool</td>
</tr>
<tr>
<td>label</td>
<td>object</td>
</tr>
<tr>
<td>linestyle or ls</td>
<td>{‘-‘, ‘—‘, ‘-.’, ‘:’, ‘’, (offset, on-off-seq), …}</td>
</tr>
<tr>
<td>linewidth or lw</td>
<td>float</td>
</tr>
<tr>
<td>marker</td>
<td>marker style string, Path or MarkerStyle</td>
</tr>
<tr>
<td>markeredgecolor or mec</td>
<td>color</td>
</tr>
<tr>
<td>markeredgewidth or mew</td>
<td>float</td>
</tr>
<tr>
<td>markerfacecolor or mfc</td>
<td>color</td>
</tr>
<tr>
<td>markerfacecoloralt or mfcalt</td>
<td>color</td>
</tr>
<tr>
<td>markersize or ms</td>
<td>float</td>
</tr>
<tr>
<td>markevery</td>
<td>None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]</td>
</tr>
<tr>
<td>path_effects</td>
<td>AbstractPathEffect</td>
</tr>
<tr>
<td>picker</td>
<td>float or callable[[Artist, Event], tuple[bool, dict]]</td>
</tr>
<tr>
<td>pickradius</td>
<td>float</td>
</tr>
<tr>
<td>rasterized</td>
<td>bool</td>
</tr>
<tr>
<td>sketch_params</td>
<td>(scale: float, length: float, randomness: float)</td>
</tr>
<tr>
<td>snap</td>
<td>bool or None</td>
</tr>
<tr>
<td>solid_capstyle</td>
<td>CapStyle or {‘butt’, ‘projecting’, ‘round’}</td>
</tr>
<tr>
<td>solid_joinstyle</td>
<td>JoinStyle or {‘miter’, ‘round’, ‘bevel’}</td>
</tr>
<tr>
<td>transform</td>
<td>unknown</td>
</tr>
<tr>
<td>url</td>
<td>str</td>
</tr>
<tr>
<td>visible</td>
<td>bool</td>
</tr>
<tr>
<td>xdata</td>
<td>1D array</td>
</tr>
<tr>
<td>ydata</td>
<td>1D array</td>
</tr>
<tr>
<td>zorder</td>
<td>float</td>
</tr>
</tbody>
</table>
</div>
<p><strong>接下来，我将挑选几个常用的附加参数介绍使用方式与效果。</strong></p>
<h2 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h2><ol>
<li>附加参数名：label</li>
<li>功能：为绘制曲线命名，该名称会在图例显示</li>
<li>使用方式：plt.plot(x,y,label=’example’)</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic3.zhimg.com/80/v2-b76ae257686d07ce9c469d0053286e92_1440w.webp" alt=""></p>
<h2 id="线条颜色"><a href="#线条颜色" class="headerlink" title="线条颜色"></a>线条颜色</h2><ol>
<li>附加参数名：color</li>
<li>功能：选择绘制线条的颜色</li>
<li>使用方式：plt.plot(x,y,color=’r’)</li>
<li>颜色选取方式分为三种：</li>
</ol>
<ul>
<li>用全名或简称 ，如blue或b</li>
<li>16进制 ，如FF00FF</li>
<li>(r, g, b) 或 (r, g, b, a)，如（1,0,1,1） ，其中 r g b a 取均为[0, 1]之间，[0, 1]之间的浮点数的字符串形式，表示灰度值。0表示黑色，1表示白色</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic3.zhimg.com/80/v2-a1256f476d9e062a706f81f2f38dd67e_1440w.webp" alt=""></p>
<h2 id="线条形状"><a href="#线条形状" class="headerlink" title="线条形状"></a>线条形状</h2><ol>
<li>附加参数名：linestyle(或ls)</li>
<li>功能：选择绘制线条的形状</li>
<li>使用方式：plt.plot(x,y,linestyle=’:’)或者plt.plot(x,y,ls=’:’)</li>
<li>常用形状：</li>
</ol>
<ul>
<li>-      实线(solid)</li>
<li>—     短线(dashed)</li>
<li>-.     短点相间线(dashdot)</li>
<li>：    虚点线(dotted)</li>
<li>‘’, ‘ ‘, None</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;, linestyle=&#x27;-&#x27;)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;, ls=&#x27;--&#x27;)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;, ls=&#x27;:&#x27;)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6), ls=&#x27;&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic3.zhimg.com/80/v2-ea4768baded601f29bd81d25a6220e72_1440w.webp" alt=""></p>
<h2 id="折点样式"><a href="#折点样式" class="headerlink" title="折点样式"></a>折点样式</h2><ol>
<li>附加参数名：<br>(1)marker — 折点形状</li>
</ol>
<p>(2)markeredgecolor 或 mec — 折点外边颜色</p>
<p>(3)markeredgewidth 或 mew — 折点线宽</p>
<p>(4)markerfacecolor 或 mfc —折点实心颜色</p>
<p>(5)markerfacecoloralt 或 mfcalt</p>
<p>(6)markersize 或 ms —折点大小</p>
<p>折点形状选择如下表:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>character</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&#39;-&#39;</code></td>
<td>solid line style</td>
</tr>
<tr>
<td><code>&#39;--&#39;</code></td>
<td>dashed line style</td>
</tr>
<tr>
<td><code>&#39;-.&#39;</code></td>
<td>dash-dot line style</td>
</tr>
<tr>
<td><code>&#39;:&#39;</code></td>
<td>dotted line style</td>
</tr>
<tr>
<td><code>&#39;.&#39;</code></td>
<td>point marker</td>
</tr>
<tr>
<td><code>&#39;,&#39;</code></td>
<td>pixel marker</td>
</tr>
<tr>
<td><code>&#39;o&#39;</code></td>
<td>circle marker</td>
</tr>
<tr>
<td><code>&#39;v&#39;</code></td>
<td>triangle_down marker</td>
</tr>
<tr>
<td><code>&#39;^&#39;</code></td>
<td>triangle_up marker</td>
</tr>
<tr>
<td><code>&#39;&lt;&#39;</code></td>
<td>triangle_left marker</td>
</tr>
<tr>
<td><code>&#39;&gt;&#39;</code></td>
<td>triangle_right marker</td>
</tr>
<tr>
<td><code>&#39;1&#39;</code></td>
<td>tri_down marker</td>
</tr>
<tr>
<td><code>&#39;2&#39;</code></td>
<td>tri_up marker</td>
</tr>
<tr>
<td><code>&#39;3&#39;</code></td>
<td>tri_left marker</td>
</tr>
<tr>
<td><code>&#39;4&#39;</code></td>
<td>tri_right marker</td>
</tr>
<tr>
<td><code>&#39;s&#39;</code></td>
<td>square marker</td>
</tr>
<tr>
<td><code>&#39;p&#39;</code></td>
<td>pentagon marker</td>
</tr>
<tr>
<td><code>&#39;*&#39;</code></td>
<td>star marker</td>
</tr>
<tr>
<td><code>&#39;h&#39;</code></td>
<td>hexagon1 marker</td>
</tr>
<tr>
<td><code>&#39;H&#39;</code></td>
<td>hexagon2 marker</td>
</tr>
<tr>
<td><code>&#39;+&#39;</code></td>
<td>plus marker</td>
</tr>
<tr>
<td><code>&#39;x&#39;</code></td>
<td>x marker</td>
</tr>
<tr>
<td><code>&#39;D&#39;</code></td>
<td>diamond marker</td>
</tr>
<tr>
<td><code>&#39;d&#39;</code></td>
<td>thin_diamond marker</td>
</tr>
<tr>
<td>``’</td>
<td>‘``</td>
<td>vline marker</td>
</tr>
<tr>
<td><code>&#39;_&#39;</code></td>
<td>hline marker</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;, linestyle=&#x27;-&#x27;, marker=&#x27;o&#x27;)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;, ls=&#x27;--&#x27;, marker=&#x27;1&#x27;)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;, ls=&#x27;:&#x27;, marker=&#x27;2&#x27;)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6), marker=&#x27;3&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic1.zhimg.com/80/v2-00fdeaa9405065aa4cd35ff740b99754_1440w.webp" alt=""></p>
<h2 id="线条透明度"><a href="#线条透明度" class="headerlink" title="线条透明度"></a>线条透明度</h2><ol>
<li>附加参数名：alpha,值在[0,1]之间</li>
<li>功能：选择绘制线条的透明度</li>
<li>使用方式：plt.plot(x,y,alpha=’0.9’)</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;, linestyle=&#x27;-&#x27;, alpha=0.3)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;, ls=&#x27;--&#x27;, alpha=0.1)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;, ls=&#x27;:&#x27;, alpha=0.5)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6), ls=&#x27;&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic4.zhimg.com/80/v2-b17579efda53b64ef09cc794ea0fdc27_1440w.webp" alt=""></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>数据分析</category>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>解析NLP竞赛中的提分点-对抗训练</title>
    <url>/2022/01/20/2022-01-20-%E8%A7%A3%E6%9E%90NLP%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在NLP比赛中，对抗训练是常见的提分手段。本文将详细介绍对抗训练的场景、作用、类型、具体实现以及未来的展望。</p>
<p><img src="https://pic2.zhimg.com/80/v2-efaa81e59a5af299791913e5e8f519cd_1440w.jpg" alt=""></p>
<span id="more"></span>
<h1 id="对抗训练应用场景"><a href="#对抗训练应用场景" class="headerlink" title="对抗训练应用场景"></a>对抗训练应用场景</h1><p>Szegedy在14年的ICLR中提出了对抗样本的概念。对抗样本可以用来攻击和防御，而对抗训练其实是“对抗”家族中防御的一种方式，其基本原理为：通过添加扰动构建对抗样本，喂入模型一同训练，提高模型遇到对抗样本时的鲁棒性，同时一定程度也能提高模型的表现和泛化能力。</p>
<p>对抗样本一般需要具有两个特点：</p>
<ol>
<li>相对于原始输入，所添加的扰动是微小的；</li>
<li>能使模型犯错。</li>
</ol>
<p>对抗训练的公式如下：</p>
<script type="math/tex; mode=display">
\min _{\theta} \mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\max _{r_{a d v} \in \mathcal{S}} L\left(\theta, x+r_{a d v}, y\right)\right]</script><p>该过程可以分为两步：</p>
<ol>
<li>内部的max过程：寻找让模型犯错最大的扰动</li>
<li>外部的min过程：寻找整体损失最小的参数</li>
</ol>
<p>在图像领域，扰动可以为图像上的噪点，但是在NLP中，如果直接在词编码上加上扰动，输入会偏离原先的语义。由于向量空间中语义相近的词语相互接近，在向量空间中添加微小的扰动的方式并不会对语义带来较大的破坏，因此当前NLP中的对抗训练均针对embedding做扰动。</p>
<h1 id="对抗训练的作用"><a href="#对抗训练的作用" class="headerlink" title="对抗训练的作用"></a>对抗训练的作用</h1><ol>
<li>提高模型应对恶意对抗样本时的鲁棒性。</li>
<li>作为一种正则化方式(regularization)，减少过拟合(overfitting)，提高泛化能力。</li>
</ol>
<p>在NLP任务中，对抗训练的角色不再是为了防御基于梯度的恶意攻击，更多的是作为一种正则化方式(regularization)，提高模型的泛化能力。</p>
<h1 id="对抗训练具体方式FGM-PGD-FreeLB"><a href="#对抗训练具体方式FGM-PGD-FreeLB" class="headerlink" title="对抗训练具体方式FGM/PGD/FreeLB"></a>对抗训练具体方式FGM/PGD/FreeLB</h1><h2 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h2><p>在介绍对抗训练的具体实现之前，本文先介绍下面Pytorch代码中常见的函数：</p>
<p><strong>一般优化流程：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># zero the parameter gradients</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"># forward + backward + optimize</span><br><span class="line">outputs = net(inputs)</span><br><span class="line">loss = criterion(outputs, labels)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p><strong>具体展开流程：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># gradient descent</span><br><span class="line">weights = [0] * n</span><br><span class="line">alpha = 0.0001</span><br><span class="line">max_Iter = 50000</span><br><span class="line">for i in range(max_Iter):</span><br><span class="line">    loss = 0</span><br><span class="line">    d_weights = [0] * n</span><br><span class="line">    for k in range(m):</span><br><span class="line">        h = dot(input[k], weights)</span><br><span class="line">        d_weights = [d_weights[j] + (label[k] - h) * input[k][j] for j in range(n)] # 梯度下降优化</span><br><span class="line">        loss += (label[k] - h) * (label[k] - h) / 2 # 梯度下降优化</span><br><span class="line">    d_weights = [d_weights[k]/m for k in range(n)]</span><br><span class="line">    weights = [weights[k] + alpha * d_weights[k] for k in range(n)]</span><br><span class="line">    if i%10000 == 0:</span><br><span class="line">        print &quot;Iteration %d loss: %f&quot;%(i, loss/m)</span><br><span class="line">        print weights</span><br></pre></td></tr></table></figure>
<p>可以发现它们实际上是一一对应的：</p>
<ul>
<li><strong>optimizer.zero_grad()对应d_weights = [0] * n</strong></li>
</ul>
<p>该步骤将梯度初始化为零（因为一个batch的loss关于weight的导数是所有sample的loss关于weight的导数的累加和）</p>
<ul>
<li><strong>outputs = net(inputs)对应h = dot(input[k], weights)</strong></li>
</ul>
<p>该步骤即前向传播求出预测的值</p>
<ul>
<li><strong>loss = criterion(outputs, labels)对应loss += (label[k] - h) * (label[k] - h) / 2</strong></li>
</ul>
<p>该步骤为求当前具体loss值</p>
<ul>
<li><strong>loss.backward()对应d_weights = [d_weights[j] + (label[k] - h) * input[k][j] for j in range(n)]</strong></li>
</ul>
<p>该步骤即反向传播求梯度</p>
<ul>
<li><strong>optimizer.step()对应weights = [weights[k] + alpha * d_weights[k] for k in range(n)]</strong></li>
</ul>
<p>该步骤即更新所有参数</p>
<h2 id="FGSM-FGM"><a href="#FGSM-FGM" class="headerlink" title="FGSM/FGM"></a>FGSM/FGM</h2><p>该方式的思想为沿着梯度上升方向对扰动可以对模型带来最大的破坏。</p>
<p>FGSM：采用Sign函数对梯度采取max归一化，max归一化是是说如果梯度某个维度上的值为正，则设为1；如果为负，则设为-1；如果为0，则设为0</p>
<p>FGM：采用L2归一化，L2归一化则将梯度的每个维度的值除以梯度的L2范数。 理论上L2归一化更严格的保留了梯度的方向，但是max归一化则不一定和原始梯度的方向相同。</p>
<script type="math/tex; mode=display">
FGSM：\delta=\epsilon Sign(g)</script><script type="math/tex; mode=display">
FGM: \delta = \epsilon (g/||g_2||)</script><script type="math/tex; mode=display">
其中g为梯度g=\nabla x(L(f_{\theta}(X), y))</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FGM</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attack</span>(<span class="params">self, epsilon=<span class="number">1.</span>, emb_name=<span class="string">&#x27;emb.&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name:</span><br><span class="line">                self.backup[name] = param.data.clone()</span><br><span class="line">                norm = torch.norm(param.grad)</span><br><span class="line">                <span class="keyword">if</span> norm != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> torch.isnan(norm):</span><br><span class="line">                    r_at = epsilon * param.grad / norm</span><br><span class="line">                    param.data.add_(r_at)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore</span>(<span class="params">self, emb_name=<span class="string">&#x27;emb.&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name: </span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.backup</span><br><span class="line">                param.data = self.backup[name]</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">fgm = FGM(model)</span><br><span class="line"><span class="keyword">for</span> batch_input, batch_label <span class="keyword">in</span> data:</span><br><span class="line">    <span class="comment"># 正常训练</span></span><br><span class="line">    loss = model(batch_input, batch_label)</span><br><span class="line">    loss.backward() <span class="comment"># 反向传播，得到正常的grad</span></span><br><span class="line">    <span class="comment"># 对抗训练</span></span><br><span class="line">    fgm.attack() <span class="comment"># 在embedding上添加对抗扰动</span></span><br><span class="line">    loss_adv = model(batch_input, batch_label)</span><br><span class="line">    loss_adv.backward() <span class="comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span></span><br><span class="line">    fgm.restore() <span class="comment"># 恢复embedding参数</span></span><br><span class="line">    <span class="comment"># 梯度下降，更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<h3 id="FGM-FGSM-流程总结"><a href="#FGM-FGSM-流程总结" class="headerlink" title="FGM/FGSM 流程总结"></a>FGM/FGSM 流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>执行对抗训练-根据当前梯度对参数值添加扰动；前向传播得到损失与最终梯度</li>
<li>恢复embedding参数</li>
<li>更新本次迭代的参数</li>
</ol>
<p>根据min-max公式可以看出，对抗训练主要完成内部max的过程。FGM/FGSM思想就是沿着梯度上升的方向，找寻最优解。但是<strong>FGM/FGSM 有假设：损失函数是线性或者局部线性。如果不是线性，那梯度提升方向不一定是最优方向。</strong></p>
<h2 id="PGD"><a href="#PGD" class="headerlink" title="PGD"></a>PGD</h2><p>为了解决FGM中线性假设问题，PGD分多次迭代，若扰动超出范围将扰动映射到规定范围内。</p>
<script type="math/tex; mode=display">
X_{t + 1}=\prod _{X+S}(X_t + \epsilon(g_t/||g_t||))</script><script type="math/tex; mode=display">
其中g为梯度g_t=\nabla x_t(L(f_{\theta}(X_t), y))</script><p>虽然PGD很有效，但效率并不高，若经过m次迭代，PGD需要迭代m*(K + 1)次。其代码展示如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PGD</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.emb_backup = &#123;&#125;</span><br><span class="line">        self.grad_backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attack</span>(<span class="params">self, epsilon=<span class="number">1.</span>, alpha=<span class="number">0.3</span>, emb_name=<span class="string">&#x27;emb.&#x27;</span>, is_first_attack=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">if</span> is_first_attack:</span><br><span class="line">                    self.emb_backup[name] = param.data.clone()</span><br><span class="line">                norm = torch.norm(param.grad)</span><br><span class="line">                <span class="keyword">if</span> norm != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> torch.isnan(norm):</span><br><span class="line">                    r_at = alpha * param.grad / norm</span><br><span class="line">                    param.data.add_(r_at)</span><br><span class="line">                    param.data = self.project(name, param.data, epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore</span>(<span class="params">self, emb_name=<span class="string">&#x27;emb.&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name: </span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.emb_backup</span><br><span class="line">                param.data = self.emb_backup[name]</span><br><span class="line">        self.emb_backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">project</span>(<span class="params">self, param_name, param_data, epsilon</span>):</span></span><br><span class="line">        r = param_data - self.emb_backup[param_name]</span><br><span class="line">        <span class="keyword">if</span> torch.norm(r) &gt; epsilon:</span><br><span class="line">            r = epsilon * r / torch.norm(r)</span><br><span class="line">        <span class="keyword">return</span> self.emb_backup[param_name] + r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backup_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                self.grad_backup[name] = param.grad.clone()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                param.grad = self.grad_backup[name]</span><br><span class="line">                </span><br><span class="line">pgd = PGD(model)</span><br><span class="line">K = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> batch_input, batch_label <span class="keyword">in</span> data:</span><br><span class="line">    <span class="comment"># 正常训练</span></span><br><span class="line">    loss = model(batch_input, batch_label)</span><br><span class="line">    loss.backward() <span class="comment"># 反向传播，得到正常的grad</span></span><br><span class="line">    pgd.backup_grad()</span><br><span class="line">    <span class="comment"># 对抗训练</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        pgd.attack(is_first_attack=(t==<span class="number">0</span>)) <span class="comment"># 在embedding上添加对抗扰动, first attack时备份param.data</span></span><br><span class="line">        <span class="keyword">if</span> t != K-<span class="number">1</span>:</span><br><span class="line">            model.zero_grad()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pgd.restore_grad()</span><br><span class="line">        loss_adv = model(batch_input, batch_label)</span><br><span class="line">        loss_adv.backward() <span class="comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span></span><br><span class="line">    pgd.restore() <span class="comment"># 恢复embedding参数</span></span><br><span class="line">    <span class="comment"># 梯度下降，更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<h3 id="PGD流程总结"><a href="#PGD流程总结" class="headerlink" title="PGD流程总结"></a>PGD流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>备份正常的梯度</li>
<li>执行K次对抗训练<ol>
<li>若t=0，备份参数；梯度清零；前向传播，计算梯度与loss值</li>
<li>若t=K-1；恢复第1步的梯度；前向传播，计算梯度与loss值</li>
</ol>
</li>
<li>恢复3.1中的embedding参数</li>
<li>更新本次迭代的参数</li>
</ol>
<p>PGD执行K次目的为分多步获取<strong>内部max的扰动-扰动表现在参数上</strong>，每一步梯度归零，但是参数值得到了累加:$x^{‘}=x+\sum_{t=0}^{K} r_t$,最后根据参数$x^{‘}$以及初始梯度前向传播计算loss和最终梯度，最后，恢复初始参数，根据最终梯度完成参数更新。</p>
<h2 id="FreeAT"><a href="#FreeAT" class="headerlink" title="FreeAT"></a>FreeAT</h2><p>PGD中进行m次反向传播，m *（K + 1） 次前向传播效率不高</p>
<p>FreeAT把前向传播计算出的梯度也进行回传</p>
<p>对比图为：</p>
<p><img src="https://pic4.zhimg.com/80/v2-b54c33d95d0e8123091297d3c4b89a2f_720w.jpg" alt="image"></p>
<p>进行（m/k）*k=m 次反向传播，（m/k）* k = m次前向传播</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">初始化r=0</span><br><span class="line">对于epoch=1...N/m:</span><br><span class="line">  对于每个x:</span><br><span class="line">    对于每步m:</span><br><span class="line">      1.利用上一步的r，计算x+r的前后向，得到梯度</span><br><span class="line">      2.根据梯度更新参数</span><br><span class="line">      3.根据梯度更新r</span><br></pre></td></tr></table></figure>
<h3 id="FreeAT流程总结"><a href="#FreeAT流程总结" class="headerlink" title="FreeAT流程总结"></a>FreeAT流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>备份正常的梯度</li>
<li>执行K次对抗训练<ol>
<li>前向传播得到梯度与loss值</li>
<li>根据梯度更新参数</li>
<li>根据梯度更新扰动</li>
</ol>
</li>
</ol>
<p><strong>缺点：FreeLB指出，FreeAT的问题在于每次的r对于当前的参数都是次优的（无法最大化loss），因为当前r是由$ r_{t-1} $和$\theta_{t-1}$计算出来的，是对于$\theta_{ t-1 }$的最优。</strong></p>
<h2 id="FreeLB"><a href="#FreeLB" class="headerlink" title="FreeLB"></a>FreeLB</h2><p>FreeLB认为，FreeAT和YOPO对于获得最优r (inner max)的计算都存在问题，因此提出了一种类似PGD的方法。只不过PGD只使用了最后一步x+r输出的梯度，而FreeLB取了每次迭代r输出梯度的平均值，相当于把输入看作一个K倍大的虚拟batch，由[X+r1, X+r2, …, X+rk]拼接而成。具体的公式为：</p>
<script type="math/tex; mode=display">
min_{\theta} E(Z,y) - D(\frac{1}{K} \sum_{t=0}^{K-1}max_{r_t \in L_t} L(f_{\theta}(X+r_t),y))</script><p>PGD公式为:</p>
<script type="math/tex; mode=display">
min_{\theta} E(Z,y) - D(max_{||r|| \le\epsilon} L(f_{\theta}(X+r_t),y))</script><p><strong>FreeLB与PGD区别如下：</strong></p>
<ol>
<li>PGD是迭代K次r后取最后一次扰动的梯度更新参数，FreeLB是取K次迭代中的平均梯度</li>
<li>PGD的扰动范围都在epsilon内，因为流程第3步将梯度归0了，每次投影都会回到以第1步x为圆心，半径是epsilon的圆内，而FreeLB每次的x都会迭代，所以r的范围更加灵活，更可能接近局部最优</li>
</ol>
<p>伪代码为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于每个x:</span><br><span class="line">  1.通过均匀分布初始化r，梯度g为0</span><br><span class="line">  对于每步t=1...K:</span><br><span class="line">    2.根据x+r计算前后向，累计梯度g</span><br><span class="line">    3.更新r</span><br><span class="line">  4.根据g/K更新梯度</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class FreeLB(object):</span><br><span class="line">    def __init__(self, adv_K, adv_lr, adv_init_mag, adv_max_norm=0., adv_norm_type=&#x27;l2&#x27;, base_model=&#x27;bert&#x27;):</span><br><span class="line">        self.adv_K = adv_K</span><br><span class="line">        self.adv_lr = adv_lr</span><br><span class="line">        self.adv_max_norm = adv_max_norm</span><br><span class="line">        self.adv_init_mag = adv_init_mag    # adv-training initialize with what magnitude, 即我们用多大的数值初始化delta</span><br><span class="line">        self.adv_norm_type = adv_norm_type</span><br><span class="line">        self.base_model = base_model</span><br><span class="line">    def attack(self, model, inputs, gradient_accumulation_steps=1):</span><br><span class="line">        input_ids = inputs[&#x27;input_ids&#x27;]</span><br><span class="line">        if isinstance(model, torch.nn.DataParallel):</span><br><span class="line">            embeds_init = getattr(model.module, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">        else:</span><br><span class="line">            embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">        if self.adv_init_mag &gt; 0:   # 影响attack首步是基于原始梯度(delta=0)，还是对抗梯度(delta!=0)</span><br><span class="line">            input_mask = inputs[&#x27;attention_mask&#x27;].to(embeds_init)</span><br><span class="line">            input_lengths = torch.sum(input_mask, 1)</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                delta = torch.zeros_like(embeds_init).uniform_(-1, 1) * input_mask.unsqueeze(2)</span><br><span class="line">                dims = input_lengths * embeds_init.size(-1)</span><br><span class="line">                mag = self.adv_init_mag / torch.sqrt(dims)</span><br><span class="line">                delta = (delta * mag.view(-1, 1, 1)).detach()</span><br><span class="line">            elif self.adv_norm_type == &quot;linf&quot;:</span><br><span class="line">                delta = torch.zeros_like(embeds_init).uniform_(-self.adv_init_mag, self.adv_init_mag)</span><br><span class="line">                delta = delta * input_mask.unsqueeze(2)</span><br><span class="line">        else:</span><br><span class="line">            delta = torch.zeros_like(embeds_init)  # 扰动初始化</span><br><span class="line">        loss, logits = None, None</span><br><span class="line">        for astep in range(self.adv_K):</span><br><span class="line">            delta.requires_grad_()</span><br><span class="line">            inputs[&#x27;inputs_embeds&#x27;] = delta + embeds_init  # 累积一次扰动delta</span><br><span class="line">            inputs[&#x27;input_ids&#x27;] = None</span><br><span class="line">            outputs = model(**inputs)</span><br><span class="line">            loss, logits = outputs[:2]  # model outputs are always tuple in transformers (see doc)</span><br><span class="line">            loss = loss.mean()  # mean() to average on multi-gpu parallel training</span><br><span class="line">            loss = loss / gradient_accumulation_steps</span><br><span class="line">            loss.backward()</span><br><span class="line">            delta_grad = delta.grad.clone().detach()  # 备份扰动的grad</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1).view(-1, 1, 1)</span><br><span class="line">                denorm = torch.clamp(denorm, min=1e-8)</span><br><span class="line">                delta = (delta + self.adv_lr * delta_grad / denorm).detach()</span><br><span class="line">                if self.adv_max_norm &gt; 0:</span><br><span class="line">                    delta_norm = torch.norm(delta.view(delta.size(0), -1).float(), p=2, dim=1).detach()</span><br><span class="line">                    exceed_mask = (delta_norm &gt; self.adv_max_norm).to(embeds_init)</span><br><span class="line">                    reweights = (self.adv_max_norm / delta_norm * exceed_mask + (1 - exceed_mask)).view(-1, 1, 1)</span><br><span class="line">                    delta = (delta * reweights).detach()</span><br><span class="line">            elif self.adv_norm_type == &quot;linf&quot;:</span><br><span class="line">                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1, p=float(&quot;inf&quot;)).view(-1, 1, 1)  # p=&#x27;inf&#x27;,无穷范数，获取绝对值最大者</span><br><span class="line">                denorm = torch.clamp(denorm, min=1e-8)  # 类似np.clip，将数值夹逼到(min, max)之间</span><br><span class="line">                delta = (delta + self.adv_lr * delta_grad / denorm).detach()  # 计算该步的delta，然后累加到原delta值上(梯度上升)</span><br><span class="line">                if self.adv_max_norm &gt; 0:</span><br><span class="line">                    delta = torch.clamp(delta, -self.adv_max_norm, self.adv_max_norm).detach()</span><br><span class="line">            else:</span><br><span class="line">                raise ValueError(&quot;Norm type &#123;&#125; not specified.&quot;.format(self.adv_norm_type))</span><br><span class="line">            if isinstance(model, torch.nn.DataParallel):  </span><br><span class="line">                embeds_init = getattr(model.module, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">            else:</span><br><span class="line">                embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">        return loss, logits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if args.do_adv:</span><br><span class="line">    inputs = &#123;</span><br><span class="line">        &quot;input_ids&quot;: input_ids,</span><br><span class="line">        &quot;bbox&quot;: layout,</span><br><span class="line">        &quot;token_type_ids&quot;: segment_ids,</span><br><span class="line">        &quot;attention_mask&quot;: input_mask,</span><br><span class="line">        &quot;masked_lm_labels&quot;: lm_label_ids</span><br><span class="line">    &#125;</span><br><span class="line">    loss, prediction_scores = freelb.attack(model, inputs)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">scheduler.step()</span><br><span class="line">model.zero_grad()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class FreeLB():</span><br><span class="line">    def __init__(self, model, args, optimizer, base_model=&#x27;xlm-roberta&#x27;):</span><br><span class="line">        self.args = args</span><br><span class="line">        self.model = model</span><br><span class="line">        self.adv_K = self.args.adv_K</span><br><span class="line">        self.adv_lr = self.args.adv_lr</span><br><span class="line">        self.adv_max_norm = self.args.adv_max_norm</span><br><span class="line">        self.adv_init_mag = self.args.adv_init_mag  # adv-training initialize with what magnitude, 即我们用多大的数值初始化delta</span><br><span class="line">        self.adv_norm_type = self.args.adv_norm_type</span><br><span class="line">        self.base_model = base_model</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line"></span><br><span class="line">    def attack(self, model, inputs):</span><br><span class="line">        args = self.args</span><br><span class="line">        input_ids = inputs[&#x27;input_ids&#x27;]</span><br><span class="line">        #获取初始化时的embedding</span><br><span class="line">        embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids.to(args.device))</span><br><span class="line"></span><br><span class="line">        if self.adv_init_mag &gt; 0:   # 影响attack首步是基于原始梯度(delta=0)，还是对抗梯度(delta!=0)</span><br><span class="line">            input_mask = inputs[&#x27;attention_mask&#x27;].to(embeds_init)</span><br><span class="line">            input_lengths = torch.sum(input_mask, 1)</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                delta = torch.zeros_like(embeds_init).uniform_(-1, 1) * input_mask.unsqueeze(2)</span><br><span class="line">                dims = input_lengths * embeds_init.size(-1)</span><br><span class="line">                mag = self.adv_init_mag / torch.sqrt(dims)</span><br><span class="line">                delta = (delta * mag.view(-1, 1, 1)).detach()</span><br><span class="line">        else:</span><br><span class="line">            delta = torch.zeros_like(embeds_init)  # 扰动初始化</span><br><span class="line">        # loss, logits = None, None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        for astep in range(self.adv_K):</span><br><span class="line">            delta.requires_grad_()</span><br><span class="line">            inputs[&#x27;inputs_embeds&#x27;] = delta + embeds_init  # 累积一次扰动delta</span><br><span class="line">            # inputs[&#x27;input_ids&#x27;] = None</span><br><span class="line">            loss, _ = model(input_ids=None,</span><br><span class="line">                            attention_mask=inputs[&quot;attention_mask&quot;].to(args.device),</span><br><span class="line">                             token_type_ids=inputs[&quot;token_type_ids&quot;].to(args.device),</span><br><span class="line">                             labels=inputs[&quot;sl_labels&quot;].to(args.device),</span><br><span class="line">                            inputs_embeds=inputs[&quot;inputs_embeds&quot;].to(args.device))</span><br><span class="line"></span><br><span class="line">            loss = loss / self.adv_K # 求平均的梯度</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            if astep == self.adv_K - 1:</span><br><span class="line">                # further updates on delta</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">            delta_grad = delta.grad.clone().detach()  # 备份扰动的grad</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1).view(-1, 1, 1)</span><br><span class="line">                denorm = torch.clamp(denorm, min=1e-8)</span><br><span class="line">                delta = (delta + self.adv_lr * delta_grad / denorm).detach()</span><br><span class="line">                if self.adv_max_norm &gt; 0:</span><br><span class="line">                    delta_norm = torch.norm(delta.view(delta.size(0), -1).float(), p=2, dim=1).detach()</span><br><span class="line">                    exceed_mask = (delta_norm &gt; self.adv_max_norm).to(embeds_init)</span><br><span class="line">                    reweights = (self.adv_max_norm / delta_norm * exceed_mask + (1 - exceed_mask)).view(-1, 1, 1)</span><br><span class="line">                    delta = (delta * reweights).detach()</span><br><span class="line">            else:</span><br><span class="line">                raise ValueError(&quot;Norm type &#123;&#125; not specified.&quot;.format(self.adv_norm_type))</span><br><span class="line"></span><br><span class="line">            embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids.to(args.device))</span><br><span class="line">        return loss</span><br><span class="line"></span><br><span class="line">for batch_input, batch_label in data:</span><br><span class="line">    # 正常训练</span><br><span class="line">    loss = model(batch_input, batch_label)</span><br><span class="line">    loss.backward() # 反向传播，得到正常的grad</span><br><span class="line">    # 对抗训练</span><br><span class="line">    freelb = FreeLB( model, args, optimizer, base_model)</span><br><span class="line">    loss_adv = freelb.attack(model, batch_input)</span><br><span class="line">    loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span><br><span class="line">    # 梯度下降，更新参数</span><br><span class="line">    optimizer.step()</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<h3 id="FreeLB流程总结"><a href="#FreeLB流程总结" class="headerlink" title="FreeLB流程总结"></a>FreeLB流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>备份正常的梯度</li>
<li>执行K次对抗训练<ol>
<li>前向传播，计算梯度与loss值</li>
<li>梯度累加</li>
<li>根据梯度计算扰动</li>
</ol>
</li>
<li>恢复初始embedding参数</li>
<li>更新本次迭代的参数</li>
</ol>
<p>该方法与FreeAT一样都想高效的利用两种梯度。<strong>不同的是，该方法并不是每次都进行更新，而是将参数梯度累积起来，用累积的梯度对参数更新</strong>。</p>
<h2 id="通用范式"><a href="#通用范式" class="headerlink" title="通用范式"></a>通用范式</h2><p>通过对上述几种对抗训练方式的学习，不难看出对抗训练的目的为完成<strong>内部max的任务，找出最大扰动的最优解</strong>。具体表现为：求解最大扰动更新参数；根据参数进行前向传播得到loss与最终梯度；恢复最初的参数值；利用最终的梯度对最初的参数值进行更新。所以通用流程表示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 正常的前向传播-得到梯度与loss值</span><br><span class="line">2. 备份正常的参数</span><br><span class="line">3. 求解扰动最优值，更新参数</span><br><span class="line">4. 根据更新后参数以及最初梯度，前向传播得到最终梯度</span><br><span class="line">5. 恢复最初的参数</span><br><span class="line">6. 根据最初的参数与最终梯度，完成参数的更新</span><br></pre></td></tr></table></figure>
<p><strong>不同对抗训练方式体现为求解扰动最优值的方式不同：</strong></p>
<ul>
<li>FGM/FGSM最优值求解方式为：一步到位，根据最初的梯度与参数值，得到扰动值</li>
<li>PGD最优值求解方式为：多步走，每一步根据上一步的参数获取扰动并更新参数，最终得到多步累加的扰动值</li>
<li>FreeAT最优值求解方式为：与PGD一样分多步，但是该方法相当于：每一步根据上一步参数和梯度获取的扰动值就是最终扰动值</li>
<li>FreeLB最优值求解方式为：与PGD一样分多步，但是在最后进行梯度更新的时候，最终梯度为初始梯度加上每一步梯度的平均值</li>
</ul>
<h1 id="对抗训练展望"><a href="#对抗训练展望" class="headerlink" title="对抗训练展望"></a>对抗训练展望</h1><h2 id="虚拟对抗训练"><a href="#虚拟对抗训练" class="headerlink" title="虚拟对抗训练"></a>虚拟对抗训练</h2><p><strong>那什么是虚拟对抗训练(VAT)呢</strong>？</p>
<blockquote>
<p>VAT不需要标签信息，可应用于无监督学习，其梯度上升的方向是能使预测的输出分布偏离现状的方向，而传统对抗训练课找的是使模型预测最大地偏离label的方向。因此，VAT不使用真实label，而是“虚拟”label——当前模型的预测结果。</p>
</blockquote>
<p>该部分可以查看JayJay的博客：<a href="https://zhuanlan.zhihu.com/p/345264876">虚拟对抗训练：让预训练模型再次强大！</a></p>
<h1 id="延伸思考"><a href="#延伸思考" class="headerlink" title="延伸思考"></a>延伸思考</h1><h2 id="对抗训练与梯度惩罚"><a href="#对抗训练与梯度惩罚" class="headerlink" title="对抗训练与梯度惩罚"></a>对抗训练与梯度惩罚</h2><p>该内容为苏神在博客<a href="https://kexue.fm/archives/7234">对抗训练浅谈：意义、方法和思考（附Keras实现）</a>中所提及：</p>
<p>假设已经得到对抗扰动Δx，那么我们在更新θ时，考虑对$L(x+Δx,y;θ)$的展开：</p>
<script type="math/tex; mode=display">
\min_{\theta}\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[L(x+\Delta x, y;\theta)\right]\\ 
\approx\, \min_{\theta}\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[L(x, y;\theta)+\langle\nabla_x L(x, y;\theta), \Delta x\rangle\right]</script><p>对应的θ的梯度为：</p>
<script type="math/tex; mode=display">
\nabla_{\theta}L(x, y;\theta)+\langle\nabla_{\theta}\nabla_x L(x, y;\theta), \Delta x\rangle</script><p>代入$\Delta x=\epsilon \nabla_x L(x, y;\theta)$，得到</p>
<script type="math/tex; mode=display">
\nabla_{\theta}L(x, y;\theta)+\epsilon\langle\nabla_{\theta}\nabla_x L(x, y;\theta), \nabla_x L(x, y;\theta)\rangle\\ 
=\,\nabla_{\theta}\left(L(x, y;\theta)+\frac{1}{2}\epsilon\left\Vert\nabla_x L(x, y;\theta)\right\Vert^2\right)</script><p>这个结果表示，对输入样本施加$\epsilon \nabla_x L(x, y;\theta)$的对抗扰动，一定程度上等价于往loss里边加入“梯度惩罚”</p>
<script type="math/tex; mode=display">
\frac{1}{2}\epsilon\left\Vert\nabla_x L(x, y;\theta)\right\Vert^2</script><p>如果对抗扰动是$\nabla_x L(x, y;\theta)/\Vert \nabla_x L(x, y;\theta)\Vert$，那么对应的梯度惩罚项则是$\epsilon\left\Vert\nabla_x L(x, y;\theta)\right\Vert$（少了个1/2，也少了个2次方）。</p>
<p>事实上，这个结果不是新的，它首先出现论文<a href="https://arxiv.org/abs/1711.09404">《Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients》</a>里。只不过这篇文章不容易搜到，因为你一旦搜索“adversarial training gradient penalty”等关键词，出来的结果几乎都是WGAN-GP相关的东西。</p>
<h2 id="词向量空间"><a href="#词向量空间" class="headerlink" title="词向量空间"></a>词向量空间</h2><p>NLP中对抗训练目前的方式均是对embedding向量空间添加扰动，那么向量空间究竟什么样呢？在对比学习的研究中<Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere>，同样提出一个好的对比学习系统应该具体两个特点：</p>
<ul>
<li><strong>Alignment：</strong>指的是相似的例子，也就是正例，映射到单位超球面后，应该有接近的特征，也即是说，在超球面上距离比较近</li>
<li><strong>Uniformity：</strong>指的是系统应该倾向在特征里保留尽可能多的信息，这等价于使得映射到单位超球面的特征，尽可能均匀地分布在球面上，分布得越均匀，意味着保留的信息越充分。分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分。</li>
</ul>
<p><img src="https://pic4.zhimg.com/80/v2-da1e6a090490028ab91e49bbe1484443_1440w.webp" alt=""></p>
<p>极端情况下会出现模型塌缩的情况，即所有特征映射到同一点：</p>
<p><img src="https://pic3.zhimg.com/80/v2-a75ca4a6fcbc48588c3cd643efd6d7fe_1440w.webp" alt=""></p>
<p>笔者认为，对抗训练在词向量层添加扰动，与对比学习类似，实现相似的例子在向量空间中相接近的目的，完成输入发生微小改变，输出改变幅度也不大的任务。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>对抗训练</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>对抗训练</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络调参-warmup and decay</title>
    <url>/2022/01/25/2022-01-25-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82-warmup%20and%20decay/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文将介绍神经网络调参技巧：warmup，decay。反向传播主要完成参数更新：$\theta_t=\theta_{t-1}-\alpha * g_t$，其中$\alpha$为学习率，$g_t$为梯度更新量，而warmup、decay就是调整$\alpha$的方式，优化器决定梯度更新的方式即$g_t$的计算方式。衰减方式如下图所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-e33b13a40632425c6e9ec680d13bcf29_1440w.jpg" alt=""><br><span id="more"></span></p>
<h1 id="warmup-and-decay"><a href="#warmup-and-decay" class="headerlink" title="warmup and decay"></a>warmup and decay</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Warmup and Decay是模型训练过程中，一种学习率（learning rate）的调整策略。</p>
<p>Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，训练了一些epoches或者steps(比如4个epoches,10000steps),再修改为预先设置的学习来进行训练。</p>
<p>同理，Decay是学习率衰减方法，它指定在训练到一定epoches或者steps后，按照线性或者余弦函数等方式，将学习率降低至指定值。一般，使用Warmup and Decay，学习率会遵循从小到大，再减小的规律。</p>
<h2 id="为什么要warmup"><a href="#为什么要warmup" class="headerlink" title="为什么要warmup"></a>为什么要warmup</h2><p>这里引用知乎：<a href="https://www.zhihu.com/question/338066667/answer/771252708的讨论：">https://www.zhihu.com/question/338066667/answer/771252708的讨论：</a><br><strong>SGD训练中常见的方式是初始较大的学习率，然后衰减为小的学习率，而warmup是先以较小的学习率上升到初始学习率，然后再衰减到小的学习率上，那么为什么warmup有效。</strong></p>
<h3 id="直观上解释"><a href="#直观上解释" class="headerlink" title="直观上解释"></a>直观上解释</h3><p>深层网络随机初始化差异较大，如果一开始以较大的学习率，初始学习带来的偏差在后续学习过程中难以纠正。</p>
<p>训练刚开始时梯度更新较大，若学习率设置较大则更新的幅度较大，该类型与传统学习率先大后小方式不同的原因在于起初浅层网络幅度大的更新并不会导致方向错误。</p>
<h3 id="理论上解释"><a href="#理论上解释" class="headerlink" title="理论上解释"></a>理论上解释</h3><p><strong>warmup带来的优点包含：</strong></p>
<ul>
<li>缓解模型在初期对mini-batch过拟合的现象</li>
<li>保持模型深层的稳定性</li>
</ul>
<p><strong>给出三个论文中的结论：</strong></p>
<ol>
<li>当batch大小增加时，学习率也可以成倍增加</li>
<li>限制大batch训练的是高学习率带来的训练不稳定性</li>
<li>warmup主要限制深层的权重变化，并且冻结深层权重的变化可以取得相似的效果</li>
</ol>
<h4 id="batch与学习率大小的关系"><a href="#batch与学习率大小的关系" class="headerlink" title="batch与学习率大小的关系"></a>batch与学习率大小的关系</h4><p>假设现在模型已经train到第t步，权重为$w_t$，我们有k个mini-batch，每个mini-batch大小为n，记为$\mathcal{B}_{1:k}$ 。下面我们来看，以学习率 $\eta$训k次 $\mathcal{B}_{1:k}$ 和以学习率 $\hat{\eta}$ 一次训练$\mathcal{B}$时学习率的关系。</p>
<p>假设我们用的是SGD，那么训k次后我们可以得到：</p>
<script type="math/tex; mode=display">
w_{t+k}=w_{t}-\eta \frac{1}{n} \sum_{j<k} \sum_{x \in \mathcal{B}_{j}} \nabla l\left(x, w_{t+j}\right)</script><p>如果我们一次训练就可以得到：</p>
<script type="math/tex; mode=display">
\hat{w}_{t+1}=w_{t}-\hat{\eta} \frac{1}{k n} \sum_{j<k} \sum_{x \in \mathcal{B}_{j}} \nabla l\left(x, w_{t}\right)</script><p>其中$w_{t+k}$与$\hat{w}_{t+1}$代表按上述方式训练k次与1次，完成参数更新后的参数。显然，这两个是不一样的。但如果我们假设$\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)$，那么令$\hat{\eta}=k\eta $就可以保证 <script type="math/tex">\hat{w}_{t+1} \approx w_{t+k}</script> 。那么，在什么时候 $\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)$ 可能不成立呢？[1]告诉我们有两种情况：</p>
<ul>
<li>在训练的开始阶段，模型权重迅速改变</li>
<li>Mini-batch 大小较小，样本方差较大</li>
</ul>
<p>第一种情况，模型初始参数分布取决于初始化方式，初始数据对于模型都是初次修正，所以梯度更新较大，若一开始以较大的学习率学习，易对数据造成过拟合，需要经过之后更多轮的训练进行修正。</p>
<p>第二种情况，在训练的过程中，如果有mini-batch内的数据分布方差特别大，这会导致模型学习剧烈波动，使其学得的权重很不稳定，这在训练初期最为明显，最后期较为缓解。</p>
<p>针对上述两种情况，并不能简单的成倍增长学习率$\hat{\eta}=k\eta$,因为此时不符合$\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)$假设。此时要么更改学习率增长方式[warmup]，要么解决这两种情况[数据预处理以减小样本方差]。</p>
<h4 id="warmup与模型学习的稳定性"><a href="#warmup与模型学习的稳定性" class="headerlink" title="warmup与模型学习的稳定性"></a>warmup与模型学习的稳定性</h4><p>该部分通过一些论文实验结果，推断有了warmup之后模型能够学习的更稳定。</p>
<p><img src="https://pic3.zhimg.com/80/v2-5d6ffb22059ddf3da93bac83c02482a6_1440w.webp" alt=""></p>
<p><strong>上图表示有了warmup之后，模型能够学习的更加稳定。</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-9440892a277639a91c2468e66ad65072_1440w.webp" alt=""></p>
<p><strong>上图b，c表示有了warmup之后，模型最后几层的相似性增加，避免模型不稳定的改变。</strong></p>
<h1 id="学习率衰减策略"><a href="#学习率衰减策略" class="headerlink" title="学习率衰减策略"></a>学习率衰减策略</h1><h2 id="可视化代码"><a href="#可视化代码" class="headerlink" title="可视化代码"></a>可视化代码</h2><p><strong>下列各种学习率衰减策略均采用warmup，为了图片反应的更加直观：起始学习率设置为1，warmup 步数为20，总步数为100。通常warmup步数可以设置为总步数的10%，参照BERT的经验策略。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># author： JMXGODLZZ</span><br><span class="line"># datetime： 2022/1/23 下午7:10 </span><br><span class="line"># ide： PyCharm</span><br><span class="line">import keras</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">from learningrateSchedules import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_cosine_with_hard_restarts_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_polynomial_decay_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_step_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_exp_schedule_with_warmup</span><br><span class="line">init_lr = 1</span><br><span class="line">warmupsteps = 20</span><br><span class="line">totalsteps = 100</span><br><span class="line"></span><br><span class="line">lrs = get_linear_schedule_with_warmup(1, warmupsteps, totalsteps)</span><br><span class="line">cos_warm_lrs = get_cosine_schedule_with_warmup(1, warmupsteps, totalsteps)</span><br><span class="line">cos_hard_warm_lrs = get_cosine_with_hard_restarts_schedule_with_warmup(1, warmupsteps, totalsteps, 2)</span><br><span class="line">poly_warm_lrs = get_polynomial_decay_schedule_with_warmup(1, warmupsteps, totalsteps, 0, 5)</span><br><span class="line">step_warm_lrs = get_step_schedule_with_warmup(1, warmupsteps, totalsteps)</span><br><span class="line">exp_warm_lrs = get_exp_schedule_with_warmup(1, warmupsteps, totalsteps, 0.9)</span><br><span class="line">x = list(range(totalsteps))</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, lrs, label=&#x27;linear_warmup&#x27;, color=&#x27;k&#x27;)</span><br><span class="line">plt.plot(x, cos_warm_lrs, label=&#x27;cosine_warmup&#x27;, color=&#x27;b&#x27;)</span><br><span class="line">plt.plot(x, cos_hard_warm_lrs, label=&#x27;cosine_cy2_warmup&#x27;, color=&#x27;g&#x27;)</span><br><span class="line">plt.plot(x, poly_warm_lrs, label=&#x27;polynomial_warmup_pw5&#x27;, color=&#x27;r&#x27;)</span><br><span class="line">plt.plot(x, step_warm_lrs, label=&#x27;step_warmup&#x27;, color=&#x27;purple&#x27;)</span><br><span class="line">plt.plot(x, exp_warm_lrs, label=&#x27;exp_warmup&#x27;, color=&#x27;orange&#x27;)</span><br><span class="line">plt.xlabel(&#x27;steps&#x27;)</span><br><span class="line">plt.ylabel(&#x27;learning rate&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://pic2.zhimg.com/80/v2-e33b13a40632425c6e9ec680d13bcf29_1440w.webp" alt=""></p>
<h2 id="指数衰减学习率"><a href="#指数衰减学习率" class="headerlink" title="指数衰减学习率"></a>指数衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_exp_schedule_with_warmup(learning_rate, num_warmup_steps, num_training_steps, gamma, last_epoch=-1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span><br><span class="line">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        stepmi = (current_step - num_warmup_steps)</span><br><span class="line">        return pow(gamma, stepmi)</span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="余弦衰减学习率"><a href="#余弦衰减学习率" class="headerlink" title="余弦衰减学习率"></a>余弦衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_cosine_schedule_with_warmup(</span><br><span class="line">    learning_rate, num_warmup_steps: int, num_training_steps: int, num_cycles: float = 0.5, last_epoch: int = -1</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases following the values of the cosine function between the</span><br><span class="line">    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the</span><br><span class="line">    initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        num_cycles (:obj:`float`, `optional`, defaults to 0.5):</span><br><span class="line">            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0</span><br><span class="line">            following a half-cosine).</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))</span><br><span class="line">        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))</span><br><span class="line"></span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="线性衰减学习率"><a href="#线性衰减学习率" class="headerlink" title="线性衰减学习率"></a>线性衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_linear_schedule_with_warmup(learning_rate, num_warmup_steps, num_training_steps, last_epoch=-1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span><br><span class="line">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        return max(</span><br><span class="line">            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))</span><br><span class="line">        )</span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="阶梯衰减学习率"><a href="#阶梯衰减学习率" class="headerlink" title="阶梯衰减学习率"></a>阶梯衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_step_schedule_with_warmup(learning_rate, num_warmup_steps, num_training_steps, last_epoch=-1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span><br><span class="line">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        stepmi = (current_step - num_warmup_steps) // 20 + 1</span><br><span class="line">        return pow(0.5, stepmi)</span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="多项式衰减学习率"><a href="#多项式衰减学习率" class="headerlink" title="多项式衰减学习率"></a>多项式衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_polynomial_decay_schedule_with_warmup(</span><br><span class="line">    learning_rate, num_warmup_steps, num_training_steps, lr_end=1e-7, power=1.0, last_epoch=-1</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the</span><br><span class="line">    optimizer to end lr defined by `lr_end`, after a warmup period during which it increases linearly from 0 to the</span><br><span class="line">    initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        lr_end (:obj:`float`, `optional`, defaults to 1e-7):</span><br><span class="line">            The end LR.</span><br><span class="line">        power (:obj:`float`, `optional`, defaults to 1.0):</span><br><span class="line">            Power factor.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Note: `power` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT</span><br><span class="line">    implementation at</span><br><span class="line">    https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    lr_init = learning_rate</span><br><span class="line">    if not (lr_init &gt; lr_end):</span><br><span class="line">        raise ValueError(f&quot;lr_end (&#123;lr_end&#125;) must be be smaller than initial lr (&#123;lr_init&#125;)&quot;)</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        elif current_step &gt; num_training_steps:</span><br><span class="line">            return lr_end / lr_init  # as LambdaLR multiplies by lr_init</span><br><span class="line">        else:</span><br><span class="line">            lr_range = lr_init - lr_end</span><br><span class="line">            decay_steps = num_training_steps - num_warmup_steps</span><br><span class="line">            pct_remaining = 1 - (current_step - num_warmup_steps) / decay_steps</span><br><span class="line">            decay = lr_range * pct_remaining ** power + lr_end</span><br><span class="line">            return decay / lr_init  # as LambdaLR multiplies by lr_init</span><br><span class="line"></span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="余弦循环衰减学习率"><a href="#余弦循环衰减学习率" class="headerlink" title="余弦循环衰减学习率"></a>余弦循环衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_cosine_with_hard_restarts_schedule_with_warmup(</span><br><span class="line">    learning_rate, num_warmup_steps: int, num_training_steps: int, num_cycles: int = 1, last_epoch: int = -1</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases following the values of the cosine function between the</span><br><span class="line">    initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases</span><br><span class="line">    linearly between 0 and the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        num_cycles (:obj:`int`, `optional`, defaults to 1):</span><br><span class="line">            The number of hard restarts to use.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))</span><br><span class="line">        if progress &gt;= 1.0:</span><br><span class="line">            return 0.0</span><br><span class="line">        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))</span><br><span class="line"></span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h1 id="学习率衰减实现"><a href="#学习率衰减实现" class="headerlink" title="学习率衰减实现"></a>学习率衰减实现</h1><h2 id="Pytorch学习率策略"><a href="#Pytorch学习率策略" class="headerlink" title="Pytorch学习率策略"></a>Pytorch学习率策略</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if args.scheduler == &quot;constant_schedule&quot;:</span><br><span class="line">    scheduler = get_constant_schedule(optimizer)</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;constant_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_constant_schedule_with_warmup(</span><br><span class="line">        optimizer, num_warmup_steps=args.warmup_steps</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;linear_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;cosine_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_cosine_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">        num_cycles=args.cosine_schedule_num_cycles,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;cosine_with_hard_restarts_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">        num_cycles=args.cosine_schedule_num_cycles,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;polynomial_decay_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_polynomial_decay_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">        lr_end=args.polynomial_decay_schedule_lr_end,</span><br><span class="line">        power=args.polynomial_decay_schedule_power,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">else:</span><br><span class="line">    raise ValueError(&quot;&#123;&#125; is not a valid scheduler.&quot;.format(args.scheduler))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="keras学习率策略"><a href="#keras学习率策略" class="headerlink" title="keras学习率策略"></a>keras学习率策略</h2><ul>
<li>Keras提供了四种衰减策略分别是ExponentialDecay(指数衰减)、 PiecewiseConstantDecay(分段常数衰减) 、 PolynomialDecay(多项式衰减)和InverseTimeDecay(逆时间衰减)。只要在Optimizer中指定衰减策略，一行代码就能实现，在以下方法一中详细介绍。</li>
<li>如果想要自定义学习率的衰减，有第二种方法，更加灵活，需要使用callbacks来实现动态、自定义学习率衰减策略，方法二中将详细介绍。</li>
<li>如果两种方法同时使用，默认优先使用第二种，第一种方法将被忽略。</li>
</ul>
<h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><h4 id="指数衰减"><a href="#指数衰减" class="headerlink" title="指数衰减"></a>指数衰减</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(</span><br><span class="line">    initial_learning_rate=1e-2,</span><br><span class="line">    decay_steps=10000,</span><br><span class="line">    decay_rate=0.96)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h4 id="分段衰减"><a href="#分段衰减" class="headerlink" title="分段衰减"></a>分段衰减</h4><p>[0~1000]的steps，学习率为1.0,[10001～9000]的steps，学习率为0.5，其他steps，学习率为0.1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">step = tf.Variable(0, trainable=False)</span><br><span class="line">boundaries = [1000, 10000]</span><br><span class="line">values = [1.0, 0.5, 0.1]</span><br><span class="line">learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)</span><br><span class="line">lr_scheduler = learning_rate_fn(step)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h4 id="多项式衰减"><a href="#多项式衰减" class="headerlink" title="多项式衰减"></a>多项式衰减</h4><p>在10000步中从0.1衰减到0.001，使用开根式( power=0.5)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start_lr = 0.1</span><br><span class="line">end_lr = 0.001</span><br><span class="line">decay_steps = 10000</span><br><span class="line">lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(</span><br><span class="line">    start_lr,</span><br><span class="line">    decay_steps,</span><br><span class="line">    end_lr,</span><br><span class="line">    power=0.5)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h4 id="逆时间衰减"><a href="#逆时间衰减" class="headerlink" title="逆时间衰减"></a>逆时间衰减</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">initial_lr = 0.1</span><br><span class="line">decay_steps = 1.0</span><br><span class="line">decay_rate = 0.5</span><br><span class="line">lr_scheduler = keras.optimizers.schedules.InverseTimeDecay(</span><br><span class="line">  initial_lr, decay_steps, decay_rate)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><h4 id="自定义指数衰减"><a href="#自定义指数衰减" class="headerlink" title="自定义指数衰减"></a>自定义指数衰减</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 第一步：自定义指数衰减策略</span><br><span class="line">def step_decay(epoch):</span><br><span class="line">    init_lr = 0.1</span><br><span class="line">    drop=0.5</span><br><span class="line">    epochs_drop=10</span><br><span class="line">    if epoch&lt;100:</span><br><span class="line">        return init_lr</span><br><span class="line">    else:</span><br><span class="line">        return init_lr*pow(drop,floor(1+epoch)/epochs_drop)</span><br><span class="line">        </span><br><span class="line"># ……</span><br><span class="line"># 第二步：用LearningRateScheduler封装学习率衰减策略</span><br><span class="line">lr_callback = LearningRateScheduler(step_decay)</span><br><span class="line"># 第三步：加入callbacks</span><br><span class="line">model = KerasClassifier(build_fn = create_model,epochs=200,batch_size=5,verbose=1,callbacks=[checkpoint,lr_callback])</span><br><span class="line">model.fit(X,Y)</span><br></pre></td></tr></table></figure>
<h4 id="动态修改学习率"><a href="#动态修改学习率" class="headerlink" title="动态修改学习率"></a>动态修改学习率</h4><p>ReduceLROnPlateau(monitor=’val_acc’, mode=’max’,min_delta=0.1,factor=0.2,patience=5, min_lr=0.001)</p>
<p>训练集连续patience个epochs的val_acc小于min_delta时，学习率将会乘以factor。mode可以选择max或者min，根据monitor的选择而灵活设定。min_lr是学习率的最低值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 第一步：ReduceLROnPlateau定义学习动态变化策略</span><br><span class="line">reduce_lr_callback = ReduceLROnPlateau(monitor=&#x27;val_acc&#x27;, factor=0.2,patience=5, min_lr=0.001)</span><br><span class="line"># 第二步：加入callbacks</span><br><span class="line">model = KerasClassifier(build_fn = create_model,epochs=200,batch_size=5,verbose=1,callbacks=[checkpoint,reduce_lr_callback])</span><br><span class="line">model.fit(X,Y)</span><br></pre></td></tr></table></figure>
<h3 id="Keras学习率回显代码"><a href="#Keras学习率回显代码" class="headerlink" title="Keras学习率回显代码"></a>Keras学习率回显代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_lr_metric(optimizer):</span><br><span class="line">    def lr(y_true, y_pred):</span><br><span class="line">        return optimizer.lr</span><br><span class="line">    return lr</span><br><span class="line"> </span><br><span class="line">x = Input((50,))</span><br><span class="line">out = Dense(1, activation=&#x27;sigmoid&#x27;)(x)</span><br><span class="line">model = Model(x, out)</span><br><span class="line"> </span><br><span class="line">optimizer = Adam(lr=0.001)</span><br><span class="line">lr_metric = get_lr_metric(optimizer)</span><br><span class="line">model.compile(loss=&#x27;binary_crossentropy&#x27;, optimizer=optimizer, metrics=[&#x27;acc&#x27;, lr_metric])</span><br><span class="line"> </span><br><span class="line"># reducing the learning rate by half every 2 epochs</span><br><span class="line">cbks = [LerningRateScheduler(lambda epoch: 0.001 * 0.5 ** (epoch // 2)),</span><br><span class="line">        TensorBoard(write_graph=False)]</span><br><span class="line">X = np.random.rand(1000, 50)</span><br><span class="line">Y = np.random.randint(2, size=1000)</span><br><span class="line">model.fit(X, Y, epochs=10, callbacks=cbks)</span><br></pre></td></tr></table></figure>
<h1 id="分层学习率设置"><a href="#分层学习率设置" class="headerlink" title="分层学习率设置"></a>分层学习率设置</h1><p>有时候我们需要为模型中不同层设置不同学习率大小，比如微调预训练模型时，预训练层数设置较小的学习率进行学习，而其他层以正常大小进行学习。这里给出苏神给出的keras实现，其通过参数变换实现调整学习率的目的：</p>
<p>梯度下降公式如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{\theta}_{n+1}=\boldsymbol{\theta}_{n}-\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\label{eq:sgd-1}</script><p>考虑变换$\boldsymbol{\theta}=\lambda \boldsymbol{\phi}$,其中λ是一个固定的标量，<strong>ϕ</strong>也是参数。现在来优化<strong>ϕ</strong>，相应的更新公式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}\boldsymbol{\phi}_{n+1}=&\boldsymbol{\phi}_{n}-\alpha \frac{\partial L(\lambda\boldsymbol{\phi}_{n})}{\partial \boldsymbol{\phi}_n}\\ 
=&\boldsymbol{\phi}_{n}-\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\frac{\partial \boldsymbol{\theta}_{n}}{\partial \boldsymbol{\phi}_n}\\ 
=&\boldsymbol{\phi}_{n}-\lambda\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\end{aligned}</script><p>然后通过链式求导法则，再上述等式两边同时乘以λ：</p>
<script type="math/tex; mode=display">
\lambda\boldsymbol{\phi}_{n+1}=\lambda\boldsymbol{\phi}_{n}-\lambda^2\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\quad\Rightarrow\quad\boldsymbol{\theta}_{n+1}=\boldsymbol{\theta}_{n}-\lambda^2\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\label{eq:sgd-2}</script><blockquote>
<p>在SGD优化器中，如果做参数变换<strong>θ</strong>=λ<strong>ϕ</strong>，那么等价的结果是学习率从α变成了$\lambda^2\alpha$。</p>
<p>不过，在自适应学习率优化器（比如RMSprop、Adam等），情况有点不一样，因为自适应学习率使用梯度（作为分母）来调整了学习率，抵消了一个λ</p>
<p>在RMSprop、Adam等自适应学习率优化器中，如果做参数变换<strong>θ</strong>=λ<strong>ϕ</strong>，那么等价的结果是学习率从α变成了λα。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import keras.backend as K</span><br><span class="line"></span><br><span class="line">class SetLearningRate:</span><br><span class="line">    &quot;&quot;&quot;层的一个包装，用来设置当前层的学习率</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, layer, lamb, is_ada=False):</span><br><span class="line">        self.layer = layer</span><br><span class="line">        self.lamb = lamb # 学习率比例</span><br><span class="line">        self.is_ada = is_ada # 是否自适应学习率优化器</span><br><span class="line"></span><br><span class="line">    def __call__(self, inputs):</span><br><span class="line">        with K.name_scope(self.layer.name):</span><br><span class="line">            if not self.layer.built:</span><br><span class="line">                input_shape = K.int_shape(inputs)</span><br><span class="line">                self.layer.build(input_shape)</span><br><span class="line">                self.layer.built = True</span><br><span class="line">                if self.layer._initial_weights is not None:</span><br><span class="line">                    self.layer.set_weights(self.layer._initial_weights)</span><br><span class="line">        for key in [&#x27;kernel&#x27;, &#x27;bias&#x27;, &#x27;embeddings&#x27;, &#x27;depthwise_kernel&#x27;, &#x27;pointwise_kernel&#x27;, &#x27;recurrent_kernel&#x27;, &#x27;gamma&#x27;, &#x27;beta&#x27;]:</span><br><span class="line">            if hasattr(self.layer, key):</span><br><span class="line">                weight = getattr(self.layer, key)</span><br><span class="line">                if self.is_ada:</span><br><span class="line">                    lamb = self.lamb # 自适应学习率优化器直接保持lamb比例</span><br><span class="line">                else:</span><br><span class="line">                    lamb = self.lamb**0.5 # SGD（包括动量加速），lamb要开平方</span><br><span class="line">                K.set_value(weight, K.eval(weight) / lamb) # 更改初始化</span><br><span class="line">                setattr(self.layer, key, weight * lamb) # 按比例替换</span><br><span class="line">        return self.layer(inputs)</span><br><span class="line"> </span><br><span class="line">x_in = Input(shape=(None,))</span><br><span class="line">x = x_in</span><br><span class="line"></span><br><span class="line"># 默认情况下是x = Embedding(100, 1000, weights=[word_vecs])(x)</span><br><span class="line"># 下面这一句表示：后面将会用自适应学习率优化器，并且Embedding层以总体的十分之一的学习率更新。</span><br><span class="line"># word_vecs是预训练好的词向量</span><br><span class="line">x = SetLearningRate(Embedding(100, 1000, weights=[word_vecs]), 0.1, True)(x)</span><br><span class="line"></span><br><span class="line"># 后面部分自己想象了～</span><br><span class="line">x = LSTM(100)(x)</span><br><span class="line"></span><br><span class="line">model = Model(x_in, x)</span><br><span class="line">model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;) # 用自适应学习率优化器优化</span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://jishuin.proginn.com/p/763bfbd51f6b">https://jishuin.proginn.com/p/763bfbd51f6b</a></p>
<p><a href="https://www.zhihu.com/question/338066667/answer/771252708">https://www.zhihu.com/question/338066667/answer/771252708</a></p>
<p><a href="https://kexue.fm/archives/6418">https://kexue.fm/archives/6418</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>warmup</tag>
        <tag>decay</tag>
        <tag>学习率</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP技能树学习路线-（一）路线总览.md</title>
    <url>/2022/06/25/2022-06-25-NLP%E6%8A%80%E8%83%BD%E6%A0%91%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF-%EF%BC%88%E4%B8%80%EF%BC%89%E8%B7%AF%E7%BA%BF%E6%80%BB%E8%A7%88/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>偶然发现一位韩国小哥整理了NLP的学习路线：<a href="https://github.com/graykode/nlp-roadmap">nlp-roadmap</a>，其中知识点覆盖很全面。由于里面内容都是英文的，笔者重新用xmind绘制学习路线图，并结合自己的理解，将一些名词翻译为中文名词。</p>
<p>笔者打算根据该学习路线，对自己的知识体系进行查漏补缺，并在该系列文章记录自己学习过程。本部分内容旨在贴出各部分学习路线图，总览待学习内容。</p>
<p><img src="https://picx.zhimg.com/v2-c39bf0e8abf032430c470985e1aae7b1_1440w.jpg?source=172ae18b" alt=""></p>
<span id="more"></span>
<h1 id="概率统计"><a href="#概率统计" class="headerlink" title="概率统计"></a>概率统计</h1><p><img src="https://pic4.zhimg.com/80/v2-a20a27c1c86bbe6d940e95d4e636a6bf_1440w.jpg" alt=""></p>
<p>概率统计是人工智能算法的基础，图中分为贝叶斯、信息理论、模型、采样、基础五部分。笔者知识体系在“基础、采样、贝叶斯部分”存在缺漏，将在后面对该部分内容展开学习。</p>
<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><p><img src="https://pic3.zhimg.com/80/v2-76d9137fd2807eef14d1da51242719b2_1440w.webp" alt=""></p>
<p>机器学习算法也是NLP的基石，含有许多基础概念，图中分为训练、降维、聚类、非概率、线性回归、逻辑回归、正则化七部分。笔者知识体系在“降维、聚类”存在缺漏，将在后面对该部分内容展开学习。</p>
<h1 id="NLP下的数据挖掘"><a href="#NLP下的数据挖掘" class="headerlink" title="NLP下的数据挖掘"></a>NLP下的数据挖掘</h1><p><img src="https://pic4.zhimg.com/80/v2-38e9b7bb97abc054eb3275ef1cf43c5b_1440w.webp" alt=""></p>
<p>NLP下的数据挖掘包含常见NLP数据处理操作，图中分为基础流程、序列标注、词嵌入、NLP基本假设、图、文档六部分。笔者知识体系在“主题模型、NLP基本假设、图”存在缺漏，将在后面对该部分内容展开学习。</p>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><p><img src="https://pic4.zhimg.com/80/v2-8ca2aec0135b78a750a6fef2f2858683_1440w.webp" alt=""></p>
<p>该部分正式介绍NLP的学习路线，分为基础学习、分布式特征、具体任务、语言模型四部分。笔者知识体系在一些具体模型存在缺漏，将在后面对该部分内容展开学习。</p>
<p>若大家需要思维导图源文件可以私我，有兴趣一起学习也可以私我加好友～</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>学习路线</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>不要停止预训练实战-Roberta与Albert</title>
    <url>/2022/03/20/2022-03-20-%E9%A2%84%E8%AE%AD%E7%BB%83%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文在LCQMC数据集上，再次对roberta、albert模型进行预训练，详细介绍了预训练的过程并对比了预训练前后的结果。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">roberta</td>
<td style="text-align:center">0.88503</td>
<td style="text-align:center">0.86344</td>
</tr>
<tr>
<td style="text-align:center">albert</td>
<td style="text-align:center">0.85662</td>
<td style="text-align:center">0.84960</td>
</tr>
<tr>
<td style="text-align:center">预训练后roberta</td>
<td style="text-align:center"><strong>0.89343</strong></td>
<td style="text-align:center">0.85328</td>
</tr>
<tr>
<td style="text-align:center">预训练后albert</td>
<td style="text-align:center">0.84958</td>
<td style="text-align:center"><strong>0.85224</strong></td>
</tr>
</tbody>
</table>
</div>
<span id="more"></span>
<h1 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h1><p>根据选取数据集，转为预训练格式数据，完成roberta、albert的预训练，并对比在该数据集上，预训练前后的具体任务指标。</p>
<h1 id="任务数据集"><a href="#任务数据集" class="headerlink" title="任务数据集"></a>任务数据集</h1><p>LCQMC数据集</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">训练集</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">238766</td>
<td style="text-align:center">8802</td>
<td style="text-align:center">12500</td>
</tr>
</tbody>
</table>
</div>
<p>LCQMC数据集的长度分布如下：</p>
<p><img src="https://pic4.zhimg.com/80/v2-c5971c7538e7972a3ad872b3e4bea8bb_1440w.jpg" alt=""></p>
<h1 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h1><p>代码链接：<a href="https://github.com/447428054/Pretrain/tree/master/LcqmcExample">https://github.com/447428054/Pretrain/tree/master/LcqmcExample</a></p>
<p>预训练环境：tensorflow1.14</p>
<p>预训练代码执行顺序：</p>
<ol>
<li>bash create_pretrain_data_lz.sh</li>
<li>bash pretrain_lz.sh</li>
</ol>
<p>LCQMC微调代码:</p>
<ol>
<li>python task_sentence_similarity_lcqmc_roberta.py</li>
<li>python task_sentence_similarity_lcqmc_albert.py</li>
</ol>
<p>TIPS:</p>
<p>记得修改文件路径</p>
<h1 id="预训练数据生成"><a href="#预训练数据生成" class="headerlink" title="预训练数据生成"></a>预训练数据生成</h1><p>预训练代码读取生成的record，数据处理代码首先读取不同文件，每个文件格式为：<strong>每一行存放一个句子，不同文档之间以空行分割</strong></p>
<p>我们将LCQMC中相似的句子作为一个文档，不相似的分开</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">谁有狂三这张高清的</span><br><span class="line"></span><br><span class="line">这张高清图，谁有</span><br><span class="line"></span><br><span class="line">英雄联盟什么英雄最好</span><br><span class="line">英雄联盟最好英雄是什么</span><br><span class="line"></span><br><span class="line">这是什么意思，被蹭网吗</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="roberta的预训练数据处理"><a href="#roberta的预训练数据处理" class="headerlink" title="roberta的预训练数据处理"></a>roberta的预训练数据处理</h2><ol>
<li><p>每个文件中，一个sentence占一行，不同document之间加一个空行分割<br>[[‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘？’], [‘有’, ‘人’, ‘知’, ‘道’, ‘名’, ‘字’, ‘吗’]]</p>
</li>
<li><p>从一个文档中连续的获得文本，直到达到最大长度。如果是从下一个文档中获得，那么加上一个分隔符.将长度限制修改了，因为lcqmc句子都偏短<br>[‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘？’, ‘有’, ‘人’, ‘知’, ‘道’, ‘名’, ‘字’, ‘吗’]</p>
</li>
<li><p>对于获取之后的文本，进行全词分词： 判断每个字符起始长度3以内的，是否在分词里面，在的话添加##标记<br>[‘有’, ‘##人’, ‘知’, ‘##道’, ‘叫’, ‘什’, ‘##么’, ‘名’, ‘##字’, ‘吗’, ‘[UNK]’, ‘？’, ‘有’, ‘##人’, ‘知’, ‘##道’, ‘名’, ‘##字’, ‘吗’]</p>
</li>
<li><p>对获得的token序列，进行掩码:返回 掩码结果，掩码的位置，掩码的标签<br>[‘[CLS]’, ‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘[MASK]’, ‘有’, ‘人’, ‘[MASK]’, ‘[MASK]’, ‘名’, ‘字’, ‘吗’, ‘[SEP]’]<br>[12, 15, 16]<br>[‘？’, ‘知’, ‘##道’]</p>
</li>
</ol>
<p>run_pretraining.py需要注释TPU的引用<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># *tpu_cluster_resolver* = tf.contrib.cluster_resolver.TPUClusterResolver( # TODO</span><br><span class="line">#       tpu=FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)</span><br></pre></td></tr></table></figure></p>
<h2 id="albert的预训练数据处理"><a href="#albert的预训练数据处理" class="headerlink" title="albert的预训练数据处理"></a>albert的预训练数据处理</h2><ol>
<li><p>每个文件中，一个sentence占一行，不同document之间加一个空行分割<br>[[‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘？’], [‘有’, ‘人’, ‘知’, ‘道’, ‘名’, ‘字’, ‘吗’]]</p>
</li>
<li><p>从一个文档中获取sentence,sentece进行全词分词，当长度达到最大长度或者遍历完整个文档了，A[SEP]B 随机分割句子，50%概率交换顺序，得到SOP标签<br>tokenA:[‘有’, ‘##人’, ‘知’, ‘##道’, ‘叫’, ‘什’, ‘##么’, ‘名’, ‘##字’, ‘吗’, ‘[UNK]’, ‘？’]<br>tokenB:[‘有’, ‘##人’, ‘知’, ‘##道’, ‘名’, ‘##字’, ‘吗’]</p>
</li>
</ol>
<p>只有一句话,构不成SOP任务的就continue</p>
<ol>
<li>对获得的token序列，进行掩码:返回 掩码结果，掩码的位置，掩码的标签<br>tokens:[‘[CLS]’, ‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘[MASK]’, ‘吗’, ‘[UNK]’, ‘？’, ‘[SEP]’, ‘[MASK]’, ‘人’, ‘知’, ‘[MASK]’, ‘名’, ‘字’, ‘吗’, ‘[SEP]’]<br>masked_lm_positions:[9, 14, 17]<br>masked_lm_labels:[‘##字’, ‘有’, ‘##道’]<br>is_random_next:False</li>
</ol>
<h1 id="预训练代码"><a href="#预训练代码" class="headerlink" title="预训练代码"></a>预训练代码</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="Roberta"><a href="#Roberta" class="headerlink" title="Roberta"></a>Roberta</h3><p><strong>整个模型结构与BERT相同，整体流程如下：</strong></p>
<ol>
<li><strong>输入的token 经过embedding</strong></li>
<li><strong>再加上token type id 与position embedding</strong></li>
<li><strong>进入transfomer层，每一个transformer又由多头attention、层归一化、残差结构、前馈神经网络构成</strong></li>
<li><strong>获取CLS输出与整个句子的输出</strong></li>
</ol>
<p><strong>整个代码结构跟流程相同：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">with tf.variable_scope(scope, default_name=&quot;bert&quot;):</span><br><span class="line">  with tf.variable_scope(&quot;embeddings&quot;):</span><br><span class="line">    # Perform embedding lookup on the word ids.</span><br><span class="line">    (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        vocab_size=config.vocab_size,</span><br><span class="line">        embedding_size=config.hidden_size,</span><br><span class="line">        initializer_range=config.initializer_range,</span><br><span class="line">        word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    # Add positional embeddings and token type embeddings, then layer</span><br><span class="line">    # normalize and perform dropout.</span><br><span class="line">    self.embedding_output = embedding_postprocessor(</span><br><span class="line">        input_tensor=self.embedding_output,</span><br><span class="line">        use_token_type=True,</span><br><span class="line">        token_type_ids=token_type_ids,</span><br><span class="line">        token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">        token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">        use_position_embeddings=True,</span><br><span class="line">        position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">        initializer_range=config.initializer_range,</span><br><span class="line">        max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">        dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">  with tf.variable_scope(&quot;encoder&quot;):</span><br><span class="line">    # This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span><br><span class="line">    # mask of shape [batch_size, seq_length, seq_length] which is used</span><br><span class="line">    # for the attention scores.</span><br><span class="line">    attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">        input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">    # Run the stacked transformer.</span><br><span class="line">    # `sequence_output` shape = [batch_size, seq_length, hidden_size].</span><br><span class="line">    self.all_encoder_layers = transformer_model(</span><br><span class="line">        input_tensor=self.embedding_output,</span><br><span class="line">        attention_mask=attention_mask,</span><br><span class="line">        hidden_size=config.hidden_size,</span><br><span class="line">        num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">        num_attention_heads=config.num_attention_heads,</span><br><span class="line">        intermediate_size=config.intermediate_size,</span><br><span class="line">        intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">        hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">        attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">        initializer_range=config.initializer_range,</span><br><span class="line">        do_return_all_layers=True)</span><br><span class="line"></span><br><span class="line">  self.sequence_output = self.all_encoder_layers[-1] # [batch_size, seq_length, hidden_size]</span><br><span class="line">  # The &quot;pooler&quot; converts the encoded sequence tensor of shape</span><br><span class="line">  # [batch_size, seq_length, hidden_size] to a tensor of shape</span><br><span class="line">  # [batch_size, hidden_size]. This is necessary for segment-level</span><br><span class="line">  # (or segment-pair-level) classification tasks where we need a fixed</span><br><span class="line">  # dimensional representation of the segment.</span><br><span class="line">  with tf.variable_scope(&quot;pooler&quot;):</span><br><span class="line">    # We &quot;pool&quot; the model by simply taking the hidden state corresponding</span><br><span class="line">    # to the first token. We assume that this has been pre-trained</span><br><span class="line">    first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)</span><br><span class="line">    self.pooled_output = tf.layers.dense(</span><br><span class="line">        first_token_tensor,</span><br><span class="line">        config.hidden_size,</span><br><span class="line">        activation=tf.tanh,</span><br><span class="line">        kernel_initializer=create_initializer(config.initializer_range))</span><br></pre></td></tr></table></figure>
<h4 id="embedding-lookup"><a href="#embedding-lookup" class="headerlink" title="embedding_lookup"></a>embedding_lookup</h4><p><strong>与常规神经网络中词嵌入类似</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def embedding_lookup(input_ids,</span><br><span class="line">                     vocab_size,</span><br><span class="line">                     embedding_size=128,</span><br><span class="line">                     initializer_range=0.02,</span><br><span class="line">                     word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">                     use_one_hot_embeddings=False):</span><br><span class="line">  &quot;&quot;&quot;Looks up words embeddings for id tensor.</span><br><span class="line">  Args:</span><br><span class="line">    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span><br><span class="line">      ids.</span><br><span class="line">    vocab_size: int. Size of the embedding vocabulary.</span><br><span class="line">    embedding_size: int. Width of the word embeddings.</span><br><span class="line">    initializer_range: float. Embedding initialization range.</span><br><span class="line">    word_embedding_name: string. Name of the embedding table.</span><br><span class="line">    use_one_hot_embeddings: bool. If True, use one-hot method for word</span><br><span class="line">      embeddings. If False, use `tf.gather()`.</span><br><span class="line">  Returns:</span><br><span class="line">    float Tensor of shape [batch_size, seq_length, embedding_size].</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # This function assumes that the input is of shape [batch_size, seq_length,</span><br><span class="line">  # num_inputs].</span><br><span class="line">  #</span><br><span class="line">  # If the input is a 2D tensor of shape [batch_size, seq_length], we</span><br><span class="line">  # reshape to [batch_size, seq_length, 1].</span><br><span class="line">  if input_ids.shape.ndims == 2:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[-1])</span><br><span class="line"></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [-1])</span><br><span class="line">  if use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  else:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[0:-1] + [input_shape[-1] * embedding_size])</span><br><span class="line">  return (output, embedding_table)</span><br></pre></td></tr></table></figure>
<h4 id="embedding-postprocessor"><a href="#embedding-postprocessor" class="headerlink" title="embedding_postprocessor"></a>embedding_postprocessor</h4><p><strong>加上token type id与可学习的position id 词嵌入，postprocessor指在embedding之后进行层归一化与dropout</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def embedding_postprocessor(input_tensor,</span><br><span class="line">                            use_token_type=False,</span><br><span class="line">                            token_type_ids=None,</span><br><span class="line">                            token_type_vocab_size=16,</span><br><span class="line">                            token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">                            use_position_embeddings=True,</span><br><span class="line">                            position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">                            initializer_range=0.02,</span><br><span class="line">                            max_position_embeddings=512,</span><br><span class="line">                            dropout_prob=0.1):</span><br><span class="line">  &quot;&quot;&quot;Performs various post-processing on a word embedding tensor.</span><br><span class="line">  Args:</span><br><span class="line">    input_tensor: float Tensor of shape [batch_size, seq_length,</span><br><span class="line">      embedding_size].</span><br><span class="line">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span><br><span class="line">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span><br><span class="line">      Must be specified if `use_token_type` is True.</span><br><span class="line">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span><br><span class="line">    token_type_embedding_name: string. The name of the embedding table variable</span><br><span class="line">      for token type ids.</span><br><span class="line">    use_position_embeddings: bool. Whether to add position embeddings for the</span><br><span class="line">      position of each token in the sequence.</span><br><span class="line">    position_embedding_name: string. The name of the embedding table variable</span><br><span class="line">      for positional embeddings.</span><br><span class="line">    initializer_range: float. Range of the weight initialization.</span><br><span class="line">    max_position_embeddings: int. Maximum sequence length that might ever be</span><br><span class="line">      used with this model. This can be longer than the sequence length of</span><br><span class="line">      input_tensor, but cannot be shorter.</span><br><span class="line">    dropout_prob: float. Dropout probability applied to the final output tensor.</span><br><span class="line">  Returns:</span><br><span class="line">    float tensor with same shape as `input_tensor`.</span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: One of the tensor shapes or input values is invalid.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">  batch_size = input_shape[0]</span><br><span class="line">  seq_length = input_shape[1]</span><br><span class="line">  width = input_shape[2]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  if use_token_type:</span><br><span class="line">    if token_type_ids is None:</span><br><span class="line">      raise ValueError(&quot;`token_type_ids` must be specified if&quot;</span><br><span class="line">                       &quot;`use_token_type` is True.&quot;)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    # This vocab will be small so we always do one-hot here, since it is always</span><br><span class="line">    # faster for a small vocabulary.</span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [-1])</span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  if use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    with tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      # Since the position embedding table is a learned variable, we create it</span><br><span class="line">      # using a (long) sequence length `max_position_embeddings`. The actual</span><br><span class="line">      # sequence length might be shorter than this, for faster training of</span><br><span class="line">      # tasks that do not have long sequences.</span><br><span class="line">      #</span><br><span class="line">      # So `full_position_embeddings` is effectively an embedding table</span><br><span class="line">      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span><br><span class="line">      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span><br><span class="line">      # perform a slice.</span><br><span class="line">      position_embeddings = tf.slice(full_position_embeddings, [0, 0],</span><br><span class="line">                                     [seq_length, -1])</span><br><span class="line">      num_dims = len(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      # Only the last two dimensions are relevant (`seq_length` and `width`), so</span><br><span class="line">      # we broadcast among the first dimensions, which is typically just</span><br><span class="line">      # the batch size.</span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      for _ in range(num_dims - 2):</span><br><span class="line">        position_broadcast_shape.append(1)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  return </span><br></pre></td></tr></table></figure>
<h4 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h4><p><strong>对于每一个Transformer结构如下：</strong></p>
<ol>
<li><strong>多头attention</strong></li>
<li><strong>拼接多头输出，经过隐藏层映射</strong></li>
<li><strong>经过dropout+残差+层归一化</strong></li>
<li><strong>前馈神经网络</strong></li>
<li><strong>经过dropout+残差+层归一化</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def transformer_model(input_tensor,</span><br><span class="line">                      attention_mask=None,</span><br><span class="line">                      hidden_size=768,</span><br><span class="line">                      num_hidden_layers=12,</span><br><span class="line">                      num_attention_heads=12,</span><br><span class="line">                      intermediate_size=3072,</span><br><span class="line">                      intermediate_act_fn=gelu,</span><br><span class="line">                      hidden_dropout_prob=0.1,</span><br><span class="line">                      attention_probs_dropout_prob=0.1,</span><br><span class="line">                      initializer_range=0.02,</span><br><span class="line">                      do_return_all_layers=False):</span><br><span class="line">  &quot;&quot;&quot;Multi-headed, multi-layer Transformer from &quot;Attention is All You Need&quot;.</span><br><span class="line">  This is almost an exact implementation of the original Transformer encoder.</span><br><span class="line">  See the original paper:</span><br><span class="line">  https://arxiv.org/abs/1706.03762</span><br><span class="line">  Also see:</span><br><span class="line">  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span><br><span class="line">  Args:</span><br><span class="line">    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span><br><span class="line">    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span><br><span class="line">      seq_length], with 1 for positions that can be attended to and 0 in</span><br><span class="line">      positions that should not be.</span><br><span class="line">    hidden_size: int. Hidden size of the Transformer.</span><br><span class="line">    num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span><br><span class="line">    num_attention_heads: int. Number of attention heads in the Transformer.</span><br><span class="line">    intermediate_size: int. The size of the &quot;intermediate&quot; (a.k.a., feed</span><br><span class="line">      forward) layer.</span><br><span class="line">    intermediate_act_fn: function. The non-linear activation function to apply</span><br><span class="line">      to the output of the intermediate/feed-forward layer.</span><br><span class="line">    hidden_dropout_prob: float. Dropout probability for the hidden layers.</span><br><span class="line">    attention_probs_dropout_prob: float. Dropout probability of the attention</span><br><span class="line">      probabilities.</span><br><span class="line">    initializer_range: float. Range of the initializer (stddev of truncated</span><br><span class="line">      normal).</span><br><span class="line">    do_return_all_layers: Whether to also return all layers or just the final</span><br><span class="line">      layer.</span><br><span class="line">  Returns:</span><br><span class="line">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span><br><span class="line">    hidden layer of the Transformer.</span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: A Tensor shape or parameter is invalid.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  if hidden_size % num_attention_heads != 0:</span><br><span class="line">    raise ValueError(</span><br><span class="line">        &quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">        &quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">  batch_size = input_shape[0]</span><br><span class="line">  seq_length = input_shape[1]</span><br><span class="line">  input_width = input_shape[2]</span><br><span class="line"></span><br><span class="line">  # The Transformer performs sum residuals on all layers so the input needs</span><br><span class="line">  # to be the same as the hidden size.</span><br><span class="line">  if input_width != hidden_size:</span><br><span class="line">    raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  # We keep the representation as a 2D tensor to avoid re-shaping it back and</span><br><span class="line">  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span><br><span class="line">  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span><br><span class="line">  # help the optimizer.</span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  for layer_idx in range(num_hidden_layers):</span><br><span class="line">    with tf.variable_scope(&quot;layer_%d&quot; % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      with tf.variable_scope(&quot;attention&quot;):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=True,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = None</span><br><span class="line">        if len(attention_heads) == 1:</span><br><span class="line">          attention_output = attention_heads[0]</span><br><span class="line">        else:</span><br><span class="line">          # In the case where we have other sequences, we just concatenate</span><br><span class="line">          # them to the self-attention head before the projection.</span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=-1)</span><br><span class="line"></span><br><span class="line">        # Run a linear projection of `hidden_size` then add a residual</span><br><span class="line">        # with `layer_input`.</span><br><span class="line">        with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      # The activation is only applied to the &quot;intermediate&quot; hidden layer.</span><br><span class="line">      with tf.variable_scope(&quot;intermediate&quot;):</span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      # Down-project back to `hidden_size` then add the residual.</span><br><span class="line">      with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  if do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    for layer_output in all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    return final_outputs</span><br><span class="line">  else:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    return final_output</span><br></pre></td></tr></table></figure>
<p><strong>其中，多头attention结构如下：</strong></p>
<ol>
<li><p><strong>针对输入向量与输出向量，生成Q、K、V向量，隐藏层维度为num heads * head size</strong></p>
<p><strong>self-attention中 输入输出来源相同。Q来源于输入向量，V来源于输出向量。“目的在于计算输入向量 对于 不同输出的 权重</strong>”</p>
<p><strong>若需要对注意力进行掩码，对得分减去很大的值，最终softmax之后得到的影响就非常小</strong></p>
</li>
<li><p><strong>针对Q，K 进行缩放点积计算，再经过softmax</strong></p>
</li>
<li><p><strong>将第二步结果与V相乘得到上下文向量</strong></p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def attention_layer(from_tensor,</span><br><span class="line">                    to_tensor,</span><br><span class="line">                    attention_mask=None,</span><br><span class="line">                    num_attention_heads=1,</span><br><span class="line">                    size_per_head=512,</span><br><span class="line">                    query_act=None,</span><br><span class="line">                    key_act=None,</span><br><span class="line">                    value_act=None,</span><br><span class="line">                    attention_probs_dropout_prob=0.0,</span><br><span class="line">                    initializer_range=0.02,</span><br><span class="line">                    do_return_2d_tensor=False,</span><br><span class="line">                    batch_size=None,</span><br><span class="line">                    from_seq_length=None,</span><br><span class="line">                    to_seq_length=None):</span><br><span class="line">  &quot;&quot;&quot;Performs multi-headed attention from `from_tensor` to `to_tensor`.</span><br><span class="line">  This is an implementation of multi-headed attention based on &quot;Attention</span><br><span class="line">  is all you Need&quot;. If `from_tensor` and `to_tensor` are the same, then</span><br><span class="line">  this is self-attention. Each timestep in `from_tensor` attends to the</span><br><span class="line">  corresponding sequence in `to_tensor`, and returns a fixed-with vector.</span><br><span class="line">  This function first projects `from_tensor` into a &quot;query&quot; tensor and</span><br><span class="line">  `to_tensor` into &quot;key&quot; and &quot;value&quot; tensors. These are (effectively) a list</span><br><span class="line">  of tensors of length `num_attention_heads`, where each tensor is of shape</span><br><span class="line">  [batch_size, seq_length, size_per_head].</span><br><span class="line">  Then, the query and key tensors are dot-producted and scaled. These are</span><br><span class="line">  softmaxed to obtain attention probabilities. The value tensors are then</span><br><span class="line">  interpolated by these probabilities, then concatenated back to a single</span><br><span class="line">  tensor and returned.</span><br><span class="line">  In practice, the multi-headed attention are done with transposes and</span><br><span class="line">  reshapes rather than actual separate tensors.</span><br><span class="line">  Args:</span><br><span class="line">    from_tensor: float Tensor of shape [batch_size, from_seq_length,</span><br><span class="line">      from_width].</span><br><span class="line">    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].</span><br><span class="line">    attention_mask: (optional) int32 Tensor of shape [batch_size,</span><br><span class="line">      from_seq_length, to_seq_length]. The values should be 1 or 0. The</span><br><span class="line">      attention scores will effectively be set to -infinity for any positions in</span><br><span class="line">      the mask that are 0, and will be unchanged for positions that are 1.</span><br><span class="line">    num_attention_heads: int. Number of attention heads.</span><br><span class="line">    size_per_head: int. Size of each attention head.</span><br><span class="line">    query_act: (optional) Activation function for the query transform.</span><br><span class="line">    key_act: (optional) Activation function for the key transform.</span><br><span class="line">    value_act: (optional) Activation function for the value transform.</span><br><span class="line">    attention_probs_dropout_prob: (optional) float. Dropout probability of the</span><br><span class="line">      attention probabilities.</span><br><span class="line">    initializer_range: float. Range of the weight initializer.</span><br><span class="line">    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size</span><br><span class="line">      * from_seq_length, num_attention_heads * size_per_head]. If False, the</span><br><span class="line">      output will be of shape [batch_size, from_seq_length, num_attention_heads</span><br><span class="line">      * size_per_head].</span><br><span class="line">    batch_size: (Optional) int. If the input is 2D, this might be the batch size</span><br><span class="line">      of the 3D version of the `from_tensor` and `to_tensor`.</span><br><span class="line">    from_seq_length: (Optional) If the input is 2D, this might be the seq length</span><br><span class="line">      of the 3D version of the `from_tensor`.</span><br><span class="line">    to_seq_length: (Optional) If the input is 2D, this might be the seq length</span><br><span class="line">      of the 3D version of the `to_tensor`.</span><br><span class="line">  Returns:</span><br><span class="line">    float Tensor of shape [batch_size, from_seq_length,</span><br><span class="line">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span><br><span class="line">      true, this will be of shape [batch_size * from_seq_length,</span><br><span class="line">      num_attention_heads * size_per_head]).</span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: Any of the arguments or tensor shapes are invalid.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,</span><br><span class="line">                           seq_length, width):</span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])</span><br><span class="line">    return output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])</span><br><span class="line"></span><br><span class="line">  if len(from_shape) != len(to_shape):</span><br><span class="line">    raise ValueError(</span><br><span class="line">        &quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;)</span><br><span class="line"></span><br><span class="line">  if len(from_shape) == 3:</span><br><span class="line">    batch_size = from_shape[0]</span><br><span class="line">    from_seq_length = from_shape[1]</span><br><span class="line">    to_seq_length = to_shape[1]</span><br><span class="line">  elif len(from_shape) == 2:</span><br><span class="line">    if (batch_size is None or from_seq_length is None or to_seq_length is None):</span><br><span class="line">      raise ValueError(</span><br><span class="line">          &quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span><br><span class="line">          &quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot;</span><br><span class="line">          &quot;must all be specified.&quot;)</span><br><span class="line"></span><br><span class="line">  # Scalar dimensions referenced here:</span><br><span class="line">  #   B = batch size (number of sequences)</span><br><span class="line">  #   F = `from_tensor` sequence length</span><br><span class="line">  #   T = `to_tensor` sequence length</span><br><span class="line">  #   N = `num_attention_heads`</span><br><span class="line">  #   H = `size_per_head`</span><br><span class="line"></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">  # `query_layer` = [B*F, N*H]</span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=&quot;query&quot;,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # `key_layer` = [B*T, N*H]</span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=&quot;key&quot;,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # `value_layer` = [B*T, N*H]</span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=&quot;value&quot;,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # `query_layer` = [B, N, F, H]</span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  # `key_layer` = [B, N, T, H]</span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw</span><br><span class="line">  # attention scores.</span><br><span class="line">  # `attention_scores` = [B, N, F, T]</span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)</span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 1.0 / math.sqrt(float(size_per_head)))</span><br><span class="line"></span><br><span class="line">  if attention_mask is not None:</span><br><span class="line">    # `attention_mask` = [B, 1, F, T]</span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[1])</span><br><span class="line"></span><br><span class="line">    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span><br><span class="line">    # masked positions, this operation will create a tensor which is 0.0 for</span><br><span class="line">    # positions we want to attend and -10000.0 for masked positions.</span><br><span class="line">    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0</span><br><span class="line"></span><br><span class="line">    # Since we are adding it to the raw scores before the softmax, this is</span><br><span class="line">    # effectively the same as removing these entirely.</span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  # Normalize the attention scores to probabilities.</span><br><span class="line">  # `attention_probs` = [B, N, F, T]</span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  # This is actually dropping out entire tokens to attend to, which might</span><br><span class="line">  # seem a bit unusual, but is taken from the original Transformer paper.</span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  # `value_layer` = [B, T, N, H]</span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  # `value_layer` = [B, N, T, H]</span><br><span class="line">  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])</span><br><span class="line"></span><br><span class="line">  # `context_layer` = [B, N, F, H]</span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  # `context_layer` = [B, F, N, H]</span><br><span class="line">  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])</span><br><span class="line"></span><br><span class="line">  if do_return_2d_tensor:</span><br><span class="line">    # `context_layer` = [B*F, N*H]</span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  else:</span><br><span class="line">    # `context_layer` = [B, F, N*H]</span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  return context_layer</span><br></pre></td></tr></table></figure>
<h3 id="Albert"><a href="#Albert" class="headerlink" title="Albert"></a>Albert</h3><p><strong>Albert整体结构与BERT相似，改动有三点：</strong></p>
<ol>
<li><p><strong>词嵌入层由Vocab <em> Hidden 分解为 Vocab </em> Embedding + Embedding * Hidden</strong></p>
</li>
<li><p><strong>跨层参数共享，主要是全连接层与注意力层的共享</strong></p>
<p><strong>Tensorflow 中 通过get variable 与 变量域Variable Scope完成参数共享</strong></p>
</li>
<li><p><strong>段落连续的SOP任务替换原先NSP任务，SOP任务中文档连续语句为正例，调换顺序后为负例</strong></p>
</li>
</ol>
<p><strong>代码中同时更新了层归一化的顺序：pre-Layer Normalization can converge fast and better. check paper: ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE</strong></p>
<p><strong>模型结构改动主要涉及前两点，接下来我们从代码层面来看这些改动：</strong></p>
<h4 id="embedding-lookup-factorized"><a href="#embedding-lookup-factorized" class="headerlink" title="embedding_lookup_factorized"></a>embedding_lookup_factorized</h4><p><strong>主要拆分为两次矩阵运算，embedding size在中间过渡</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def embedding_lookup_factorized(input_ids, # Factorized embedding parameterization provide by albert</span><br><span class="line">                     vocab_size,</span><br><span class="line">                     hidden_size,</span><br><span class="line">                     embedding_size=128,</span><br><span class="line">                     initializer_range=0.02,</span><br><span class="line">                     word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">                     use_one_hot_embeddings=False):</span><br><span class="line">    &quot;&quot;&quot;Looks up words embeddings for id tensor, but in a factorized style followed by albert. it is used to reduce much percentage of parameters previous exists.</span><br><span class="line">       Check &quot;Factorized embedding parameterization&quot; session in the paper.</span><br><span class="line">     Args:</span><br><span class="line">       input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span><br><span class="line">         ids.</span><br><span class="line">       vocab_size: int. Size of the embedding vocabulary.</span><br><span class="line">       embedding_size: int. Width of the word embeddings.</span><br><span class="line">       initializer_range: float. Embedding initialization range.</span><br><span class="line">       word_embedding_name: string. Name of the embedding table.</span><br><span class="line">       use_one_hot_embeddings: bool. If True, use one-hot method for word</span><br><span class="line">         embeddings. If False, use `tf.gather()`.</span><br><span class="line">     Returns:</span><br><span class="line">       float Tensor of shape [batch_size, seq_length, embedding_size].</span><br><span class="line">     &quot;&quot;&quot;</span><br><span class="line">    # This function assumes that the input is of shape [batch_size, seq_length,</span><br><span class="line">    # num_inputs].</span><br><span class="line">    #</span><br><span class="line">    # If the input is a 2D tensor of shape [batch_size, seq_length], we</span><br><span class="line">    # reshape to [batch_size, seq_length, 1].</span><br><span class="line"></span><br><span class="line">    # 1.first project one-hot vectors into a lower dimensional embedding space of size E</span><br><span class="line">    print(&quot;embedding_lookup_factorized. factorized embedding parameterization is used.&quot;)</span><br><span class="line">    if input_ids.shape.ndims == 2:</span><br><span class="line">        input_ids = tf.expand_dims(input_ids, axis=[-1])  # shape of input_ids is:[ batch_size, seq_length, 1]</span><br><span class="line"></span><br><span class="line">    embedding_table = tf.get_variable(  # [vocab_size, embedding_size]</span><br><span class="line">        name=word_embedding_name,</span><br><span class="line">        shape=[vocab_size, embedding_size],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">    flat_input_ids = tf.reshape(input_ids, [-1])  # one rank. shape as (batch_size * sequence_length,)</span><br><span class="line">    if use_one_hot_embeddings:</span><br><span class="line">        one_hot_input_ids = tf.one_hot(flat_input_ids,depth=vocab_size)  # one_hot_input_ids=[batch_size * sequence_length,vocab_size]</span><br><span class="line">        output_middle = tf.matmul(one_hot_input_ids, embedding_table)  # output=[batch_size * sequence_length,embedding_size]</span><br><span class="line">    else:</span><br><span class="line">        output_middle = tf.gather(embedding_table,flat_input_ids)  # [vocab_size, embedding_size]*[batch_size * sequence_length,]---&gt;[batch_size * sequence_length,embedding_size]</span><br><span class="line"></span><br><span class="line">    # 2. project vector(output_middle) to the hidden space</span><br><span class="line">    project_variable = tf.get_variable(  # [embedding_size, hidden_size]</span><br><span class="line">        name=word_embedding_name+&quot;_2&quot;,</span><br><span class="line">        shape=[embedding_size, hidden_size],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    output = tf.matmul(output_middle, project_variable) # ([batch_size * sequence_length, embedding_size] * [embedding_size, hidden_size])---&gt;[batch_size * sequence_length, hidden_size]</span><br><span class="line">    # reshape back to 3 rank</span><br><span class="line">    input_shape = get_shape_list(input_ids)  # input_shape=[ batch_size, seq_length, 1]</span><br><span class="line">    batch_size, sequene_length, _=input_shape</span><br><span class="line">    output = tf.reshape(output, (batch_size,sequene_length,hidden_size))  # output=[batch_size, sequence_length, hidden_size]</span><br><span class="line">    return (output, embedding_table, project_variable)</span><br></pre></td></tr></table></figure>
<h4 id="prelln-transformer-model"><a href="#prelln-transformer-model" class="headerlink" title="prelln_transformer_model"></a>prelln_transformer_model</h4><p>将Layer Norm放在Attention前面，使训练过程收敛的更快更好。使用Tensorflow的变量域，完成参数共享。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def prelln_transformer_model(input_tensor,</span><br><span class="line">						attention_mask=None,</span><br><span class="line">						hidden_size=768,</span><br><span class="line">						num_hidden_layers=12,</span><br><span class="line">						num_attention_heads=12,</span><br><span class="line">						intermediate_size=3072,</span><br><span class="line">						intermediate_act_fn=gelu,</span><br><span class="line">						hidden_dropout_prob=0.1,</span><br><span class="line">						attention_probs_dropout_prob=0.1,</span><br><span class="line">						initializer_range=0.02,</span><br><span class="line">						do_return_all_layers=False,</span><br><span class="line">						shared_type=&#x27;all&#x27;, # None,</span><br><span class="line">						adapter_fn=None):</span><br><span class="line">	&quot;&quot;&quot;Multi-headed, multi-layer Transformer from &quot;Attention is All You Need&quot;.</span><br><span class="line">	This is almost an exact implementation of the original Transformer encoder.</span><br><span class="line">	See the original paper:</span><br><span class="line">	https://arxiv.org/abs/1706.03762</span><br><span class="line">	Also see:</span><br><span class="line">	https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span><br><span class="line">	Args:</span><br><span class="line">		input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span><br><span class="line">		attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span><br><span class="line">			seq_length], with 1 for positions that can be attended to and 0 in</span><br><span class="line">			positions that should not be.</span><br><span class="line">		hidden_size: int. Hidden size of the Transformer.</span><br><span class="line">		num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span><br><span class="line">		num_attention_heads: int. Number of attention heads in the Transformer.</span><br><span class="line">		intermediate_size: int. The size of the &quot;intermediate&quot; (a.k.a., feed</span><br><span class="line">			forward) layer.</span><br><span class="line">		intermediate_act_fn: function. The non-linear activation function to apply</span><br><span class="line">			to the output of the intermediate/feed-forward layer.</span><br><span class="line">		hidden_dropout_prob: float. Dropout probability for the hidden layers.</span><br><span class="line">		attention_probs_dropout_prob: float. Dropout probability of the attention</span><br><span class="line">			probabilities.</span><br><span class="line">		initializer_range: float. Range of the initializer (stddev of truncated</span><br><span class="line">			normal).</span><br><span class="line">		do_return_all_layers: Whether to also return all layers or just the final</span><br><span class="line">			layer.</span><br><span class="line">	Returns:</span><br><span class="line">		float Tensor of shape [batch_size, seq_length, hidden_size], the final</span><br><span class="line">		hidden layer of the Transformer.</span><br><span class="line">	Raises:</span><br><span class="line">		ValueError: A Tensor shape or parameter is invalid.</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	if hidden_size % num_attention_heads != 0:</span><br><span class="line">		raise ValueError(</span><br><span class="line">				&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">				&quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">	attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line"></span><br><span class="line">	input_shape = bert_utils.get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">	batch_size = input_shape[0]</span><br><span class="line">	seq_length = input_shape[1]</span><br><span class="line">	input_width = input_shape[2]</span><br><span class="line"></span><br><span class="line">	# The Transformer performs sum residuals on all layers so the input needs</span><br><span class="line">	# to be the same as the hidden size.</span><br><span class="line">	if input_width != hidden_size:</span><br><span class="line">		raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %</span><br><span class="line">										 (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">	# We keep the representation as a 2D tensor to avoid re-shaping it back and</span><br><span class="line">	# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span><br><span class="line">	# the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span><br><span class="line">	# help the optimizer.</span><br><span class="line">	prev_output = bert_utils.reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">	all_layer_outputs = []</span><br><span class="line"></span><br><span class="line">	def layer_scope(idx, shared_type):</span><br><span class="line">		if shared_type == &#x27;all&#x27;:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_shared&quot;,</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention&#x27;,</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate&#x27;,</span><br><span class="line">				&#x27;output&#x27;:&#x27;output&#x27;</span><br><span class="line">			&#125;</span><br><span class="line">		elif shared_type == &#x27;attention&#x27;:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_shared&quot;,</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention&#x27;,</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate_&#123;&#125;&#x27;.format(idx),</span><br><span class="line">				&#x27;output&#x27;:&#x27;output_&#123;&#125;&#x27;.format(idx)</span><br><span class="line">			&#125;</span><br><span class="line">		elif shared_type == &#x27;ffn&#x27;:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_shared&quot;,</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention_&#123;&#125;&#x27;.format(idx),</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate&#x27;,</span><br><span class="line">				&#x27;output&#x27;:&#x27;output&#x27;</span><br><span class="line">			&#125;</span><br><span class="line">		else:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_&#123;&#125;&quot;.format(idx),</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention&#x27;,</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate&#x27;,</span><br><span class="line">				&#x27;output&#x27;:&#x27;output&#x27;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">		return tmp</span><br><span class="line"></span><br><span class="line">	all_layer_outputs = []</span><br><span class="line"></span><br><span class="line">	for layer_idx in range(num_hidden_layers):</span><br><span class="line"></span><br><span class="line">		idx_scope = layer_scope(layer_idx, shared_type)</span><br><span class="line"></span><br><span class="line">		with tf.variable_scope(idx_scope[&#x27;layer&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">			layer_input = prev_output</span><br><span class="line"></span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;attention&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				attention_heads = []</span><br><span class="line"></span><br><span class="line">				with tf.variable_scope(&quot;output&quot;, reuse=tf.AUTO_REUSE):</span><br><span class="line">					layer_input_pre = layer_norm(layer_input)</span><br><span class="line"></span><br><span class="line">				with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">					attention_head = attention_layer(</span><br><span class="line">							from_tensor=layer_input_pre,</span><br><span class="line">							to_tensor=layer_input_pre,</span><br><span class="line">							attention_mask=attention_mask,</span><br><span class="line">							num_attention_heads=num_attention_heads,</span><br><span class="line">							size_per_head=attention_head_size,</span><br><span class="line">							attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">							initializer_range=initializer_range,</span><br><span class="line">							do_return_2d_tensor=True,</span><br><span class="line">							batch_size=batch_size,</span><br><span class="line">							from_seq_length=seq_length,</span><br><span class="line">							to_seq_length=seq_length)</span><br><span class="line">					attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">				attention_output = None</span><br><span class="line">				if len(attention_heads) == 1:</span><br><span class="line">					attention_output = attention_heads[0]</span><br><span class="line">				else:</span><br><span class="line">					# In the case where we have other sequences, we just concatenate</span><br><span class="line">					# them to the self-attention head before the projection.</span><br><span class="line">					attention_output = tf.concat(attention_heads, axis=-1)</span><br><span class="line"></span><br><span class="line">				# Run a linear projection of `hidden_size` then add a residual</span><br><span class="line">				# with `layer_input`.</span><br><span class="line">				with tf.variable_scope(&quot;output&quot;, reuse=tf.AUTO_REUSE):</span><br><span class="line">					attention_output = tf.layers.dense(</span><br><span class="line">							attention_output,</span><br><span class="line">							hidden_size,</span><br><span class="line">							kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">					attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">					# attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line">					attention_output = attention_output + layer_input</span><br><span class="line"></span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;output&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				attention_output_pre = layer_norm(attention_output)</span><br><span class="line"></span><br><span class="line">			# The activation is only applied to the &quot;intermediate&quot; hidden layer.</span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;intermediate&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				intermediate_output = tf.layers.dense(</span><br><span class="line">						attention_output_pre,</span><br><span class="line">						intermediate_size,</span><br><span class="line">						activation=intermediate_act_fn,</span><br><span class="line">						kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">			# Down-project back to `hidden_size` then add the residual.</span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;output&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				layer_output = tf.layers.dense(</span><br><span class="line">						intermediate_output,</span><br><span class="line">						hidden_size,</span><br><span class="line">						kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">				layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">				# layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">				layer_output = layer_output + attention_output</span><br><span class="line">				prev_output = layer_output</span><br><span class="line">				all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">	if do_return_all_layers:</span><br><span class="line">		final_outputs = []</span><br><span class="line">		for layer_output in all_layer_outputs:</span><br><span class="line">			final_output = bert_utils.reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">			final_outputs.append(final_output)</span><br><span class="line">		return final_outputs</span><br><span class="line">	else:</span><br><span class="line">		final_output = bert_utils.reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">		return final_output</span><br></pre></td></tr></table></figure>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="MLM"><a href="#MLM" class="headerlink" title="MLM"></a>MLM</h3><p><strong>主要流程为：</strong></p>
<ol>
<li><strong>提取模型输出中mask position位置的向量</strong></li>
<li><strong>经过变换 每一个输出vocab size大小</strong></li>
<li><strong>与标签计算交叉熵损失</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_masked_lm_output(albert_config, input_tensor, output_weights, positions,</span><br><span class="line">                         label_ids, label_weights):</span><br><span class="line">  &quot;&quot;&quot;Get loss and log probs for the masked LM.&quot;&quot;&quot;</span><br><span class="line">  input_tensor = gather_indexes(input_tensor, positions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  with tf.variable_scope(&quot;cls/predictions&quot;):</span><br><span class="line">    # We apply one more non-linear transformation before the output layer.</span><br><span class="line">    # This matrix is not used after pre-training.</span><br><span class="line">    with tf.variable_scope(&quot;transform&quot;):</span><br><span class="line">      input_tensor = tf.layers.dense(</span><br><span class="line">          input_tensor,</span><br><span class="line">          units=albert_config.embedding_size,</span><br><span class="line">          activation=modeling.get_activation(albert_config.hidden_act),</span><br><span class="line">          kernel_initializer=modeling.create_initializer(</span><br><span class="line">              albert_config.initializer_range))</span><br><span class="line">      input_tensor = modeling.layer_norm(input_tensor)</span><br><span class="line"></span><br><span class="line">    # The output weights are the same as the input embeddings, but there is</span><br><span class="line">    # an output-only bias for each token.</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        &quot;output_bias&quot;,</span><br><span class="line">        shape=[albert_config.vocab_size],</span><br><span class="line">        initializer=tf.zeros_initializer())</span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=-1)</span><br><span class="line"></span><br><span class="line">    label_ids = tf.reshape(label_ids, [-1])</span><br><span class="line">    label_weights = tf.reshape(label_weights, [-1])</span><br><span class="line"></span><br><span class="line">    one_hot_labels = tf.one_hot(</span><br><span class="line">        label_ids, depth=albert_config.vocab_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    # The `positions` tensor might be zero-padded (if the sequence is too</span><br><span class="line">    # short to have the maximum number of predictions). The `label_weights`</span><br><span class="line">    # tensor has a value of 1.0 for every real prediction and 0.0 for the</span><br><span class="line">    # padding predictions.</span><br><span class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])</span><br><span class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</span><br><span class="line">    denominator = tf.reduce_sum(label_weights) + 1e-5</span><br><span class="line">    loss = numerator / denominator</span><br><span class="line"></span><br><span class="line">  return (loss, per_example_loss, log_probs)</span><br></pre></td></tr></table></figure>
<h3 id="SOP"><a href="#SOP" class="headerlink" title="SOP"></a>SOP</h3><p>主要流程为：</p>
<ol>
<li>模型输出向量 转换为 输出为2维的向量</li>
<li>与标签计算交叉熵损失</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_sentence_order_output(albert_config, input_tensor, labels):</span><br><span class="line">  &quot;&quot;&quot;Get loss and log probs for the next sentence prediction.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  # Simple binary classification. Note that 0 is &quot;next sentence&quot; and 1 is</span><br><span class="line">  # &quot;random sentence&quot;. This weight matrix is not used after pre-training.</span><br><span class="line">  with tf.variable_scope(&quot;cls/seq_relationship&quot;):</span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        &quot;output_weights&quot;,</span><br><span class="line">        shape=[2, albert_config.hidden_size],</span><br><span class="line">        initializer=modeling.create_initializer(</span><br><span class="line">            albert_config.initializer_range))</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        &quot;output_bias&quot;, shape=[2], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=-1)</span><br><span class="line">    labels = tf.reshape(labels, [-1])</span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)</span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)</span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line">    return (loss, per_example_loss, log_probs)</span><br></pre></td></tr></table></figure>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">roberta</td>
<td style="text-align:center">0.88503</td>
<td style="text-align:center">0.86344</td>
</tr>
<tr>
<td style="text-align:center">albert</td>
<td style="text-align:center">0.85662</td>
<td style="text-align:center">0.84960</td>
</tr>
<tr>
<td style="text-align:center">预训练后roberta</td>
<td style="text-align:center"><strong>0.89343</strong></td>
<td style="text-align:center">0.85328</td>
</tr>
<tr>
<td style="text-align:center">预训练后albert</td>
<td style="text-align:center">0.84958</td>
<td style="text-align:center"><strong>0.85224</strong></td>
</tr>
</tbody>
</table>
</div>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>模型根据验证集结果保存最优模型，因此测试集上表现不一定是最优的，我们主要看在验证集上的表现。以上模型在相同参数下只跑了一次，因此结果会略有浮动。</p>
<ol>
<li>roberta在预训练后效果取得提升，经过再次预训练，模型领域与微调领域更加接近，效果更好</li>
<li>Albert预训练后效果下降，可能与我们构建数据的方式有关，构建的数据与SOP任务并不符合，可以尝试更符合要求的数据进行测试。</li>
</ol>
<h1 id="TO-DO"><a href="#TO-DO" class="headerlink" title="TO DO"></a>TO DO</h1><ul>
<li>MACBert-20220319暂未开源预训练代码</li>
<li>根据SpanBert改为n-gram 掩码与SBO任务</li>
<li>Pytorch与keras的预训练</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>预训练</category>
      </categories>
      <tags>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>真香～BERT在MAC Pytorch的使用.md</title>
    <url>/2022/07/16/2022-07-16-%E7%9C%9F%E9%A6%99%EF%BD%9EBERT%E5%9C%A8MAC%20Pytorch%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>终于，Pytorch也支持MAC的硬件加速，两个字评价一下感受：真香～</p>
<p>周末笔者在自己机器上完成环境安装，笔者机器环境如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-6ac54749fc06a77883b3095e62d36a60_1440w.jpg" alt=""></p>
<p>接着，笔者在该文用卷积、BERT模型对比了有无MAC硬件加速的模型运行时间</p>
<span id="more"></span>
<h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><p>按照官网给出的命令，即可完成安装MAC硬件加速版pytorch。</p>
<p><a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p>
<blockquote>
<p>conda install pytorch torchvision torchaudio -c pytorch</p>
</blockquote>
<h1 id="简单测试"><a href="#简单测试" class="headerlink" title="简单测试"></a>简单测试</h1><p>利用卷积操作，测试有无硬件加速的效果。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev = &#x27;mps:0&#x27;</span><br><span class="line"></span><br><span class="line">conv = torch.nn.Conv2d(10, 10, 3).to(dev)</span><br><span class="line"></span><br><span class="line">img = torch.randn(64, 10, 64, 64).to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line"></span><br><span class="line">for i in range(1000):</span><br><span class="line"></span><br><span class="line">    conv(img)</span><br><span class="line"></span><br><span class="line">t1 = time.time()</span><br><span class="line"></span><br><span class="line">print(&#x27;Use mps, time:&#123;&#125;&#x27;.format(t1-t0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev = &#x27;cpu&#x27;</span><br><span class="line"></span><br><span class="line">conv = torch.nn.Conv2d(10, 10, 3).to(dev)</span><br><span class="line"></span><br><span class="line">img = torch.randn(64, 10, 64, 64).to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line"></span><br><span class="line">for i in range(1000):</span><br><span class="line"></span><br><span class="line">    conv(img)</span><br><span class="line"></span><br><span class="line">t1 = time.time()</span><br><span class="line"></span><br><span class="line">print(&#x27;Use cpu, time:&#123;&#125;&#x27;.format(t1-t0))</span><br></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="https://pic2.zhimg.com/80/v2-371933b35c5a6dc4f9c4c9cd18805f1d_1440w.webp" alt=""></p>
<h1 id="BERT测试"><a href="#BERT测试" class="headerlink" title="BERT测试"></a>BERT测试</h1><p>使用huggingface的glue代码作示例。</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>运行下述代码完成数据下载工作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;&#x27;&#x27; Script for downloading all GLUE data.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Note: for legal reasons, we are unable to host MRPC.</span><br><span class="line"></span><br><span class="line">You can either use the version hosted by the SentEval team, which is already tokenized,</span><br><span class="line"></span><br><span class="line">or you can download the original data from (https://download.microsoft.com/download/D/4/6/D46FF87A-F6B9-4252-AA8B-3604ED519838/MSRParaphraseCorpus.msi) and extract the data from it manually.</span><br><span class="line"></span><br><span class="line">For Windows users, you can run the .msi file. For Mac and Linux users, consider an external library such as &#x27;cabextract&#x27; (see below for an example).</span><br><span class="line"></span><br><span class="line">You should then rename and place specific files in a folder (see below for an example).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mkdir MRPC</span><br><span class="line"></span><br><span class="line">cabextract MSRParaphraseCorpus.msi -d MRPC</span><br><span class="line"></span><br><span class="line">cat MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D | tr -d $&#x27;\r&#x27; &gt; MRPC/msr_paraphrase_train.txt</span><br><span class="line"></span><br><span class="line">cat MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61 | tr -d $&#x27;\r&#x27; &gt; MRPC/msr_paraphrase_test.txt</span><br><span class="line"></span><br><span class="line">rm MRPC/_*</span><br><span class="line"></span><br><span class="line">rm MSRParaphraseCorpus.msi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1/30/19: It looks like SentEval is no longer hosting their extracted and tokenized MRPC data, so you&#x27;ll need to download the data from the original source for now.</span><br><span class="line"></span><br><span class="line">2/11/19: It looks like SentEval actually *is* hosting the extracted data. Hooray!</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">import tempfile</span><br><span class="line"></span><br><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">import zipfile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASKS = [&quot;CoLA&quot;, &quot;SST&quot;, &quot;MRPC&quot;, &quot;QQP&quot;, &quot;STS&quot;, &quot;MNLI&quot;, &quot;QNLI&quot;, &quot;RTE&quot;, &quot;WNLI&quot;, &quot;diagnostic&quot;]</span><br><span class="line"></span><br><span class="line">TASK2PATH = &#123;&quot;CoLA&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/CoLA.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;SST&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/SST-2.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;QQP&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;STS&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/STS-B.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;MNLI&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/MNLI.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;QNLI&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;RTE&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/RTE.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;WNLI&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/WNLI.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;diagnostic&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/AX.tsv&#x27;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MRPC_TRAIN = &#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt&#x27;</span><br><span class="line"></span><br><span class="line">MRPC_TEST = &#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def download_and_extract(task, data_dir):</span><br><span class="line"></span><br><span class="line">    print(&quot;Downloading and extracting %s...&quot; % task)</span><br><span class="line"></span><br><span class="line">    if task == &quot;MNLI&quot;:</span><br><span class="line"></span><br><span class="line">        print(</span><br><span class="line"></span><br><span class="line">            &quot;\tNote (12/10/20): This script no longer downloads SNLI. You will need to manually download and format the data to use SNLI.&quot;)</span><br><span class="line"></span><br><span class="line">    data_file = &quot;%s.zip&quot; % task</span><br><span class="line"></span><br><span class="line">    urllib.request.urlretrieve(TASK2PATH[task], data_file)</span><br><span class="line"></span><br><span class="line">    with zipfile.ZipFile(data_file) as zip_ref:</span><br><span class="line"></span><br><span class="line">        zip_ref.extractall(data_dir)</span><br><span class="line"></span><br><span class="line">    os.remove(data_file)</span><br><span class="line"></span><br><span class="line">    print(&quot;\tCompleted!&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def format_mrpc(data_dir, path_to_data):</span><br><span class="line"></span><br><span class="line">    print(&quot;Processing MRPC...&quot;)</span><br><span class="line"></span><br><span class="line">    mrpc_dir = os.path.join(data_dir, &quot;MRPC&quot;)</span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(mrpc_dir):</span><br><span class="line"></span><br><span class="line">        os.mkdir(mrpc_dir)</span><br><span class="line"></span><br><span class="line">    if path_to_data:</span><br><span class="line"></span><br><span class="line">        mrpc_train_file = os.path.join(path_to_data, &quot;msr_paraphrase_train.txt&quot;)</span><br><span class="line"></span><br><span class="line">        mrpc_test_file = os.path.join(path_to_data, &quot;msr_paraphrase_test.txt&quot;)</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line"></span><br><span class="line">            mrpc_train_file = os.path.join(mrpc_dir, &quot;msr_paraphrase_train.txt&quot;)</span><br><span class="line"></span><br><span class="line">            mrpc_test_file = os.path.join(mrpc_dir, &quot;msr_paraphrase_test.txt&quot;)</span><br><span class="line"></span><br><span class="line">            URLLIB.urlretrieve(MRPC_TRAIN, mrpc_train_file)</span><br><span class="line"></span><br><span class="line">            URLLIB.urlretrieve(MRPC_TEST, mrpc_test_file)</span><br><span class="line"></span><br><span class="line">        except urllib.error.HTTPError:</span><br><span class="line"></span><br><span class="line">            print(&quot;Error downloading MRPC&quot;)</span><br><span class="line"></span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">    assert os.path.isfile(mrpc_train_file), &quot;Train data not found at %s&quot; % mrpc_train_file</span><br><span class="line"></span><br><span class="line">    assert os.path.isfile(mrpc_test_file), &quot;Test data not found at %s&quot; % mrpc_test_file</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    with io.open(mrpc_test_file, encoding=&#x27;utf-8&#x27;) as data_fh, \</span><br><span class="line"></span><br><span class="line">            io.open(os.path.join(mrpc_dir, &quot;test.tsv&quot;), &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as test_fh:</span><br><span class="line"></span><br><span class="line">        header = data_fh.readline()</span><br><span class="line"></span><br><span class="line">        test_fh.write(&quot;index\t#1 ID\t#2 ID\t#1 String\t#2 String\n&quot;)</span><br><span class="line"></span><br><span class="line">        for idx, row in enumerate(data_fh):</span><br><span class="line"></span><br><span class="line">            label, id1, id2, s1, s2 = row.strip().split(&#x27;\t&#x27;)</span><br><span class="line"></span><br><span class="line">            test_fh.write(&quot;%d\t%s\t%s\t%s\t%s\n&quot; % (idx, id1, id2, s1, s2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line"></span><br><span class="line">        URLLIB.urlretrieve(TASK2PATH[&quot;MRPC&quot;], os.path.join(mrpc_dir, &quot;dev_ids.tsv&quot;))</span><br><span class="line"></span><br><span class="line">    except KeyError or urllib.error.HTTPError:</span><br><span class="line"></span><br><span class="line">        print(&quot;\tError downloading standard development IDs for MRPC. You will need to manually split your data.&quot;)</span><br><span class="line"></span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    dev_ids = []</span><br><span class="line"></span><br><span class="line">    with io.open(os.path.join(mrpc_dir, &quot;dev_ids.tsv&quot;), encoding=&#x27;utf-8&#x27;) as ids_fh:</span><br><span class="line"></span><br><span class="line">        for row in ids_fh:</span><br><span class="line"></span><br><span class="line">            dev_ids.append(row.strip().split(&#x27;\t&#x27;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    with io.open(mrpc_train_file, encoding=&#x27;utf-8&#x27;) as data_fh, \</span><br><span class="line"></span><br><span class="line">            io.open(os.path.join(mrpc_dir, &quot;train.tsv&quot;), &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as train_fh, \</span><br><span class="line"></span><br><span class="line">            io.open(os.path.join(mrpc_dir, &quot;dev.tsv&quot;), &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as dev_fh:</span><br><span class="line"></span><br><span class="line">        header = data_fh.readline()</span><br><span class="line"></span><br><span class="line">        train_fh.write(header)</span><br><span class="line"></span><br><span class="line">        dev_fh.write(header)</span><br><span class="line"></span><br><span class="line">        for row in data_fh:</span><br><span class="line"></span><br><span class="line">            label, id1, id2, s1, s2 = row.strip().split(&#x27;\t&#x27;)</span><br><span class="line"></span><br><span class="line">            if [id1, id2] in dev_ids:</span><br><span class="line"></span><br><span class="line">                dev_fh.write(&quot;%s\t%s\t%s\t%s\t%s\n&quot; % (label, id1, id2, s1, s2))</span><br><span class="line"></span><br><span class="line">            else:</span><br><span class="line"></span><br><span class="line">                train_fh.write(&quot;%s\t%s\t%s\t%s\t%s\n&quot; % (label, id1, id2, s1, s2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(&quot;\tCompleted!&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def download_diagnostic(data_dir):</span><br><span class="line"></span><br><span class="line">    print(&quot;Downloading and extracting diagnostic...&quot;)</span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(os.path.join(data_dir, &quot;diagnostic&quot;)):</span><br><span class="line"></span><br><span class="line">        os.mkdir(os.path.join(data_dir, &quot;diagnostic&quot;))</span><br><span class="line"></span><br><span class="line">    data_file = os.path.join(data_dir, &quot;diagnostic&quot;, &quot;diagnostic.tsv&quot;)</span><br><span class="line"></span><br><span class="line">    urllib.request.urlretrieve(TASK2PATH[&quot;diagnostic&quot;], data_file)</span><br><span class="line"></span><br><span class="line">    print(&quot;\tCompleted!&quot;)</span><br><span class="line"></span><br><span class="line">    return</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tasks(task_names):</span><br><span class="line"></span><br><span class="line">    task_names = task_names.split(&#x27;,&#x27;)</span><br><span class="line"></span><br><span class="line">    if &quot;all&quot; in task_names:</span><br><span class="line"></span><br><span class="line">        tasks = TASKS</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line"></span><br><span class="line">        tasks = []</span><br><span class="line"></span><br><span class="line">        for task_name in task_names:</span><br><span class="line"></span><br><span class="line">            assert task_name in TASKS, &quot;Task %s not found!&quot; % task_name</span><br><span class="line"></span><br><span class="line">            tasks.append(task_name)</span><br><span class="line"></span><br><span class="line">    return tasks</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(arguments):</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">    parser.add_argument(&#x27;--data_dir&#x27;, help=&#x27;directory to save data to&#x27;, type=str, default=&#x27;glue_data&#x27;)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(&#x27;--tasks&#x27;, help=&#x27;tasks to download data for as a comma separated string&#x27;,</span><br><span class="line"></span><br><span class="line">                        type=str, default=&#x27;all&#x27;)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(&#x27;--path_to_mrpc&#x27;,</span><br><span class="line"></span><br><span class="line">                        help=&#x27;path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt&#x27;,</span><br><span class="line"></span><br><span class="line">                        type=str, default=&#x27;&#x27;)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args(arguments)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(args.data_dir):</span><br><span class="line"></span><br><span class="line">        os.mkdir(args.data_dir)</span><br><span class="line"></span><br><span class="line">    tasks = get_tasks(args.tasks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    for task in tasks:</span><br><span class="line"></span><br><span class="line">        if task == &#x27;MRPC&#x27;:</span><br><span class="line"></span><br><span class="line">            format_mrpc(args.data_dir, args.path_to_mrpc)</span><br><span class="line"></span><br><span class="line">        elif task == &#x27;diagnostic&#x27;:</span><br><span class="line"></span><br><span class="line">            download_diagnostic(args.data_dir)</span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line"></span><br><span class="line">            download_and_extract(task, args.data_dir)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">    sys.exit(main(sys.argv[1:]))</span><br></pre></td></tr></table></figure>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>requirements内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">accelerate</span><br><span class="line"></span><br><span class="line">datasets &gt;= 1.8.0</span><br><span class="line"></span><br><span class="line">sentencepiece != 0.1.92</span><br><span class="line"></span><br><span class="line">scipy</span><br><span class="line"></span><br><span class="line">scikit-learn</span><br><span class="line"></span><br><span class="line">protobuf</span><br><span class="line"></span><br><span class="line">numpy==1.17.3</span><br><span class="line"></span><br><span class="line">#torch &gt;= 1.3</span><br></pre></td></tr></table></figure>
<h2 id="代码准备"><a href="#代码准备" class="headerlink" title="代码准备"></a>代码准备</h2><p>利用huggingface的<strong>run_glue_no_trainer.py</strong>。</p>
<p>运行脚本如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export TASK_NAME=mrpc</span><br><span class="line"></span><br><span class="line">python run_glue_no_trainer.py \</span><br><span class="line">  --model_name_or_path Pretrained_LMs/bert-base-cased \</span><br><span class="line">  --task_name $TASK_NAME \</span><br><span class="line">  --max_length 128 \</span><br><span class="line">  --per_device_train_batch_size 32 \</span><br><span class="line">  --learning_rate 2e-5 \</span><br><span class="line">  --num_train_epochs 3 \</span><br><span class="line">  --output_dir ./output/$TASK_NAME/</span><br></pre></td></tr></table></figure>
<p>在代码中修改运行设备方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    accelerator.state.device = &#x27;mps&#x27;</span><br><span class="line"></span><br><span class="line">    print(&#x27;-&#x27; * 100)</span><br><span class="line"></span><br><span class="line">    print(accelerator.state.device)</span><br><span class="line"></span><br><span class="line">    print(&#x27;-&#x27; * 100)</span><br></pre></td></tr></table></figure>
<h2 id="运行结果-1"><a href="#运行结果-1" class="headerlink" title="运行结果"></a>运行结果</h2><p>CPU下运行时间约1h：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Num processes: 1</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: cpu</span><br><span class="line">...</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ - ***** Running training *****</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Num examples = 3668</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Num Epochs = 3</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Instantaneous batch size per device = 32</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Total train batch size (w. parallel, distributed &amp; accumulation) = 32</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Gradient Accumulation steps = 1</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Total optimization steps = 345</span><br><span class="line">  2%|███▌                                                                                                                                                                                                       | 6/345 [01:06&lt;1:03:49, 11.30s/it]</span><br></pre></td></tr></table></figure>
<p>硬件加速下运行时间约20min：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Num processes: 1</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: mps</span><br><span class="line">...</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ - ***** Running training *****</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Num examples = 3668</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Num Epochs = 3</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Instantaneous batch size per device = 32</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Total train batch size (w. parallel, distributed &amp; accumulation) = 32</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Gradient Accumulation steps = 1</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Total optimization steps = 345</span><br><span class="line">  5%|██████████▋                                                                                                                                                                                                 | 18/345 [01:03&lt;20:14,  3.71s/it]</span><br></pre></td></tr></table></figure>
<p>观察MAC活动监视器，可以看到程序确实有用到GPU硬件加速。</p>
<p><img src="https://pic3.zhimg.com/80/v2-105dfbe9e73e8dc800994e94e9a08e2e_1440w.webp" alt=""></p>
<h2 id="bug-fix"><a href="#bug-fix" class="headerlink" title="bug fix"></a>bug fix</h2><p>在运行过程中出现如下错误：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OMP: Error #15: Initializing libomp.dylib, but found libiomp5.dylib already initialize异常</span><br></pre></td></tr></table></figure>
<p>参照该<a href="http://t.zoukankan.com/yxym2016-p-13900887.html">链接</a>解决了问题，<strong>如果Python是基本于Conda安装的，则Conda上的numpy包中的mkl很容易与系统内库发生冲突，可选择update numpy package in Conda或者设置为系统库。</strong></p>
<p><strong>解决方案</strong>:降低numpy的版本，此处笔者将版本降低到1.17.3</p>
<blockquote>
<p>pip install numpy==1.17.3</p>
</blockquote>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>不要停止预训练实战(二)-一日看尽MLM</title>
    <url>/2022/05/30/2022-05-30-%E4%B8%8D%E8%A6%81%E5%81%9C%E6%AD%A2%E9%A2%84%E8%AE%AD%E7%BB%83%E5%AE%9E%E6%88%98(%E4%BA%8C)-%E4%B8%80%E6%97%A5%E7%9C%8B%E5%B0%BDMLM/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><img src="https://picx.zhimg.com/v2-79c0d08709031f923a7fb3305ae8ff59_1440w.jpg?source=172ae18b" alt=""></p>
<p>本文在上文<a href="https://jmxgodlz.xyz/2022/03/20/2022-03-20-预训练实战/#more">不要停止预训练实战-Roberta与Albert</a>的基础上，进一步完成以下内容：</p>
<ul>
<li>keras预训练</li>
<li>N-gram掩码任务 </li>
<li>Span掩码任务 </li>
</ul>
<span id="more"></span>
<h1 id="掩码任务"><a href="#掩码任务" class="headerlink" title="掩码任务"></a>掩码任务</h1><p>BERT等预训练模型中掩码任务主要涉及下列要素：</p>
<ul>
<li>掩码比例</li>
<li>替换策略</li>
<li>掩码方式</li>
</ul>
<h2 id="掩码比例"><a href="#掩码比例" class="headerlink" title="掩码比例"></a>掩码比例</h2><p>常用掩码比例设置为15%，该比例经过许多研究，已证明该比例能够取得很好的效果。</p>
<p>从理论上来说，笔者从网上找到的说法为：“当取15%时，恰好大概7个词mask一个，正好就是CBOW中，长度为7的滑动窗口的中心词，因此会有比较好的效果”</p>
<p>而近日丹琦大佬等人的论文<a href="https://arxiv.org/abs/2202.08005">Should You Mask 15% in Masked Language Modeling?</a>表明掩码40%能够取得与15%差不多的效果。</p>
<p>该论文表明<strong>“所谓的optimal masking rate并不是一个一成不变的神奇数字，而是一个随着模型大小、mask策略、训练recipe、下游任务变化的函数。”</strong></p>
<h2 id="替换策略"><a href="#替换策略" class="headerlink" title="替换策略"></a>替换策略</h2><p>常用的替换策略如下：</p>
<ul>
<li>80%词语替换为[MASK]</li>
<li>10%词语保持不变</li>
<li>10%词语随机替换为其他词语</li>
</ul>
<p>这样做的目的在于强迫模型学习词语上下文的语义信息。任何一个词语都有可能被替换，不仅靠当前词语，还需要利用上下文的信息预测当前词语。</p>
<p>但是[MASK]标签并未出现在下游任务中，因此<strong>存在预训练与微调的不一致问题。</strong></p>
<p>MacBERT提出<strong>MLM as correction</strong>的方法，替换策略如下：</p>
<ul>
<li>80%词语替换为同义词</li>
<li>10%词语保持不变</li>
<li>10%词语随机替换为其他词语</li>
</ul>
<p>MacBERT论文中与下列替换策略进行对比，对比结果如图所示：</p>
<ul>
<li>MacBERT：80%替换为同义词，10%替换为随机词语，10%保持不变；</li>
<li>Random Replace：90%替换为随机词语，10%保持不变；</li>
<li>Partial Mask：同原生的BERT一样，80%替换为[MASK]，10%替换为随机词语，10%保持不变；</li>
<li>ALL Mask：90%替换为[MASK]，10%保持不变。</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-11ceecc7dceba69da862e6bebac0f4c1_1440w.webp" alt="ss"></p>
<p>图中横坐标代表训练步数，纵坐标代表EM值。第一幅图是CMRC数据集结果，第二幅图是DRCD数据集结果。</p>
<h2 id="掩码方式"><a href="#掩码方式" class="headerlink" title="掩码方式"></a>掩码方式</h2><p>目前的掩码方式主要分为以下几种：</p>
<ul>
<li>单词掩码</li>
<li>全词掩码</li>
<li>实体掩码</li>
<li>N-gram掩码</li>
<li>Span掩码</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">中文</th>
<th style="text-align:center">英文</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">原句</td>
<td style="text-align:center">使用语言模型来预测下一个词的概率。</td>
<td style="text-align:center">we use a language model to predict the probability of the next word.</td>
</tr>
<tr>
<td style="text-align:center">分词</td>
<td style="text-align:center">使用 语言 模型 来 预测 下 一个 词 的 概率 。</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">BERT Tokenizer</td>
<td style="text-align:center">使 用 语 言 模 型 来 预 测 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a language <strong>model</strong> to <strong>pre ##di ##ct</strong> the <strong>pro ##ba ##bility</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">单词掩码</td>
<td style="text-align:center">使 用 语 言 <strong>[M]</strong> 型 来 <strong>[M]</strong> 测 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a language <strong>[M]</strong> to <strong>[M] ##di ##ct</strong> the <strong>pro [M] ##bility</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">全词掩码</td>
<td style="text-align:center">使 用 语 言 <strong>[M] [M]</strong> 来 <strong>[M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a language <strong>[M]</strong> to <strong>[M] [M] [M]</strong> the <strong>[M] [M] [M]</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">实体掩码</td>
<td style="text-align:center">使 用 <strong>[M] [M] [M] [M]</strong> 来 <strong>[M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a <strong>[M] [M]</strong> to <strong>[M] [M] [M]</strong> the <strong>[M] [M] [M]</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">N-gram掩码</td>
<td style="text-align:center">使 用 <strong>[M] [M] [M] [M]</strong> 来 <strong>[M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a <strong>[M] [M]</strong> to <strong>[M] [M] [M]</strong> the <strong>[M] [M] [M] [M] [M]</strong> next word.</td>
</tr>
<tr>
<td style="text-align:center">Span掩码</td>
<td style="text-align:center">使 用 <strong>[M] [M] [M] [M] [M] [M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a <strong>[M] [M] [M] [M] [M] [M]</strong> the <strong>[M] [M] [M] [M] [M]</strong> next word.</td>
</tr>
<tr>
<td style="text-align:center">MAC掩码</td>
<td style="text-align:center">使 用 语 法 建 模 来 预 见 下 一 个 词 的 几 率 。</td>
<td style="text-align:center">we use a <strong>text system</strong> to <strong>ca ##lc ##ulate</strong> the <strong>po ##si ##bility</strong> of the next word.</td>
</tr>
</tbody>
</table>
</div>
<h3 id="全词掩码"><a href="#全词掩码" class="headerlink" title="全词掩码"></a>全词掩码</h3><p>以分词结果为最小粒度，完成掩码任务。</p>
<h3 id="N-gram掩码"><a href="#N-gram掩码" class="headerlink" title="N-gram掩码"></a>N-gram掩码</h3><p>同样以分词结果为最小粒度，以n-gram取词语进行掩码。</p>
<p>例如MacBERT采用基于分词的n-gram masking，1-gram~4gram Masking的概率分别是40%、30%、20%、10%。</p>
<h3 id="实体掩码"><a href="#实体掩码" class="headerlink" title="实体掩码"></a>实体掩码</h3><p>代表模型为：<strong>ERNIE</strong></p>
<p>引入命名实体信息，将实体作为最小粒度，进行掩码。</p>
<h3 id="Span掩码"><a href="#Span掩码" class="headerlink" title="Span掩码"></a>Span掩码</h3><p>代表模型为：<strong>SpanBERT</strong></p>
<p>以上做法让人认为，或许必须得引入类似词边界信息才能帮助训练。但前不久的 MASS 模型，却表明可能并不需要，随机遮盖可能效果也很好，于是就有SpanBERT的 idea：</p>
<p>根据<strong>几何分布</strong>，先随机选择一段（span）的<strong>长度</strong>，之后再根据均匀分布随机选择这一段的<strong>起始位置</strong>，最后按照长度遮盖。文中使用几何分布取 <em>p=0.2</em>，最大长度只能是 10，利用此方案获得平均采样长度分布。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>相关代码实现可见：</p>
<p><a href="https://github.com/447428054/Pretrain/tree/master/KerasExample/pretraining">https://github.com/447428054/Pretrain/tree/master/KerasExample/pretraining</a></p>
<p>Span掩码核心代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def __init__(</span><br><span class="line">    self, tokenizer, word_segment, lower=1, upper=10, p=0.3, mask_rate=0.15, sequence_length=512</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;参数说明：</span><br><span class="line">        tokenizer必须是bert4keras自带的tokenizer类；</span><br><span class="line">        word_segment是任意分词函数。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    super(TrainingDatasetRoBERTa, self).__init__(tokenizer, sequence_length)</span><br><span class="line">    self.word_segment = word_segment</span><br><span class="line">    self.mask_rate = mask_rate</span><br><span class="line"></span><br><span class="line">    self.lower = lower</span><br><span class="line">    self.upper = upper</span><br><span class="line">    self.p = p</span><br><span class="line"></span><br><span class="line">    self.lens = list(range(self.lower, self.upper + 1))</span><br><span class="line">    self.len_distrib = [self.p * (1-self.p)**(i - self.lower) for i in range(self.lower, self.upper + 1)] if self.p &gt;= 0 else None</span><br><span class="line">    self.len_distrib = [x / (sum(self.len_distrib)) for x in self.len_distrib]</span><br><span class="line">    print(self.len_distrib, self.lens)</span><br><span class="line"></span><br><span class="line">def sentence_process(self, text):</span><br><span class="line">    &quot;&quot;&quot;单个文本的处理函数</span><br><span class="line">    流程：分词，然后转id，按照mask_rate构建全词mask的序列</span><br><span class="line">          来指定哪些token是否要被mask</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    word_tokens = self.tokenizer.tokenize(text=text)[1:-1]</span><br><span class="line">    word_token_ids = self.tokenizer.tokens_to_ids(word_tokens)</span><br><span class="line"></span><br><span class="line">    sent_length = len(word_tokens)</span><br><span class="line">    mask_num = math.ceil(sent_length * self.mask_rate)</span><br><span class="line">    mask = set()</span><br><span class="line">    spans = []</span><br><span class="line"></span><br><span class="line">    while len(mask) &lt; mask_num:</span><br><span class="line">        span_len = np.random.choice(self.lens, p=self.len_distrib) # 随机选择span长度</span><br><span class="line"></span><br><span class="line">        anchor = np.random.choice(sent_length)</span><br><span class="line">        if anchor in mask: # 随机生成起点</span><br><span class="line">            continue</span><br><span class="line">        left1 = anchor</span><br><span class="line">        spans.append([left1, left1])</span><br><span class="line">        right1 = min(anchor + span_len, sent_length)</span><br><span class="line">        for i in range(left1, right1):</span><br><span class="line">            if len(mask) &gt;= mask_num:</span><br><span class="line">                break</span><br><span class="line">            mask.add(i)</span><br><span class="line">            spans[-1][-1] = i</span><br><span class="line"></span><br><span class="line">    spans = merge_intervals(spans)</span><br><span class="line">    word_mask_ids = [0] * len(word_tokens)</span><br><span class="line">    for (st, ed) in spans:</span><br><span class="line">        for idx in range(st, ed + 1):</span><br><span class="line">            wid = word_token_ids[idx]</span><br><span class="line">            word_mask_ids[idx] = self.token_process(wid) + 1</span><br><span class="line"></span><br><span class="line">    return [word_token_ids, word_mask_ids]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>预训练</category>
      </categories>
      <tags>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>一文梳理NLP主要模型发展脉络.md</title>
    <url>/2022/07/31/2022-07-31-%E4%B8%80%E6%96%87%E6%A2%B3%E7%90%86NLP%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文根据笔者所学知识，对NLP主要模型的发展脉络作梳理，目的在于了解主流技术的前世今生，如有理解错误的地方，麻烦指正～</p>
<p><img src="https://pic1.zhimg.com/80/v2-4f13c1d7a7fda98d177c406106177c98_1440w.jpg" alt=""></p>
<p>下面将依次介绍RNN、LSTM、GRU、Encoder-Deocder、Transformer、BERT设计的出发点，模型结构不作详细介绍。</p>
<span id="more"></span>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>自然语言处理的数据类型多为文本类型，文本数据的上下文关系具有较强的序列特征。同时，RNN模型具有“上一时刻输出作为下一时刻的输入”的特征，该特征能够很好的处理序列数据。因此，RNN模型相较于其他模型，更适合处理自然语言处理任务。</p>
<p>当待处理序列长度较长时，RNN模型在反向传播的过程中，受链式求导法则的影响，当求导过程导数过小或过大时，会导致梯度消失或梯度爆炸。</p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>RNN模型的权重矩阵在时间维度上是共享的。LSTM相较于RNN模型，通过引入门控机制，缓解梯度消失，那么<strong>LSTM如何避免梯度消失？</strong></p>
<p>这里给出几个关键结论，详细分析见<a href="https://jmxgodlz.xyz/2022/08/07/2022-08-07-详解LSTM与梯度消失/#more">详解LSTM与梯度消失</a>。</p>
<ul>
<li><p>RNN模型在时间维度共享参数矩阵，因此RNN模型总的梯度等于各时间的梯度之和，$g=\sum{g_t}$。</p>
</li>
<li><p>RNN中总的梯度不会消失，只是远距离梯度消失，梯度被近距离梯度主导，无法捕获远距离特征。</p>
</li>
<li><p>梯度消失的本质：由于RNN模型在时间维度共享参数矩阵，导致针对隐藏状态h求导时，循环计算矩阵乘法，最终梯度上出现了参数矩阵的累乘。</p>
</li>
<li><p>LSTM缓解梯度消失的本质：引入门控机制，将矩阵乘法转为逐元素相乘的哈达马积:$c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tanh \left(W_{c}\left[h_{t-1}, x_{t}\right]+b_{c}\right)$</p>
</li>
</ul>
<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>GRU与LSTM模型相同，引入门控机制，避免梯度消失。区别在于，GRU只用了重置门与更新门两个门结构，参数量较LSTM少，训练速度更快。</p>
<h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p>RNN模型“上一时刻输出作为下一时刻的输入”的特征，也存在模型输入输出一直是等长的问题。Encoder-Decoder模型通过编码器与解码器两个部分，解决了输入输出定长的问题。其中Encoder端负责文本序列的特征表示获取，Decoder端根据特征向量解码输出序列。</p>
<p>但Encoder-Decoder模型仍然存在以下问题：</p>
<ul>
<li><p>文本序列的特征表示向量选取</p>
</li>
<li><p>特征表示向量包含特征的有限性</p>
</li>
<li><p>OOV问题</p>
</li>
</ul>
<p>第一个与第二个问题通过<strong>注意力机制</strong>解决，利用注意力机制有选择的关注文本序列重要的特征。</p>
<p>第三个问题则通过<strong>拷贝机制</strong>以及<strong>Subword编码</strong>解决。</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer模型主要包含多头自注意力模块、前馈神经网络、残差结构与Dropout，其中核心模块为<strong>多头自注意力模块</strong>，各组件的功能如下：</p>
<ul>
<li><p><strong>自注意力机制</strong>在编码器端有选择的关注文本序列重要的特征，解决文本序列的特征表示向量选取及该向量包含特征的有限性问题。</p>
</li>
<li><p><strong>多头机制</strong>中每一头映射到不同空间，得到不同侧重点的特征表示，使得特征表示的更充分。</p>
</li>
<li><p><strong>残差结构</strong>有效避免梯度消失。</p>
</li>
<li><p><strong>Dropout</strong>有效避免过拟合。</p>
</li>
<li><p><strong>前馈神经网络</strong>完成隐含层到输出空间的映射</p>
</li>
</ul>
<p>接下来将重点介绍<strong>Transformer</strong>模型的<strong>优点</strong>。</p>
<h2 id="1-Transformer能够实现长距离依赖"><a href="#1-Transformer能够实现长距离依赖" class="headerlink" title="1. Transformer能够实现长距离依赖"></a>1. Transformer能够实现长距离依赖</h2><p>在自注意力机制中，每个字符能够与其他所有字符计算注意力得分。这种计算方式未考虑时序特征，能够捕获长距离依赖。</p>
<p>但该方式缺点在于，注意力得分计算的时间复杂度为$O(n^2)$，当输入序列较长时，时间复杂度过高，因此不适合处理过长数据。</p>
<h2 id="2-Transformer能够实现并行化"><a href="#2-Transformer能够实现并行化" class="headerlink" title="2. Transformer能够实现并行化"></a>2. Transformer能够实现并行化</h2><p>假设输入序列为（a,b,c,d）</p>
<p>传统RNN需要计算a的embedding向量得到$e_a$,再经过特征抽取得到$h_a$,然后以相同方式计算b,c,d。</p>
<p>Transformer通过self-attention机制，每个词都可以与全部序列交互，应此模型可以同时处理整个序列，得到$e_a,e_b,e_c,e_d$，然后再一起计算$h_a,h_b,h_c,h_d$。</p>
<h2 id="3-Transformer适合预训练"><a href="#3-Transformer适合预训练" class="headerlink" title="3. Transformer适合预训练"></a>3. Transformer适合预训练</h2><ul>
<li><p>RNN模型作出输入数据具有时序性的假设，将前一时刻的输出作为下一时刻的输入。</p>
</li>
<li><p>CNN模型基于输入数据为图像的假设，在结构中加入一些特质【如卷积生成特征】，使前向传播更加高效，降低网络的参数量。</p>
</li>
</ul>
<p>与CNN、RNN模型不同， Transformer 模型是一种灵活的架构，对输入数据的结构无限制，因此适合在大规模数据上进行预训练，但该点也带来了Transformer模型在小规模数据集上泛化性差的问题。改进方法包括引入结构偏差或正则化，对大规模未标记数据进行预训练等。</p>
<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>首先，BERT模型参照GPT模型，采样预训练-微调两阶段的训练方式。但是，与GPT使用Transformer解码器部分不同，BERT为了充分利用上下文信息，使用Transformer编码器部分作为模型结构。</p>
<h3 id="训练任务"><a href="#训练任务" class="headerlink" title="训练任务"></a>训练任务</h3><p>如果BERT与GPT同样使用语言模型作为学习任务，则模型存在标签泄露的问题【一个词的上下文包含了另一个词的预测目标】。因此为了利用上下文信息，BERT提出MLM掩码语言模型任务，通过上下文预测遮盖词，MLM有关介绍可见-<a href="https://jmxgodlz.xyz/2022/05/30/2022-05-30-不要停止预训练实战(二">不要停止预训练实战(二)-一日看尽MLM</a>-一日看尽MLM/#more)。</p>
<h3 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h3><p>针对Transformer结构及预训练方式，BERT模型仍存在以下改进点：</p>
<ul>
<li><p>训练方式：针对掩码方式与多任务训练方式进行改进，调整NSP训练任务与掩码的方式</p>
</li>
<li><p>模型结构调整：针对Transformer $O(n^2)$的时间复杂度以及输入结构无假设的两点，调整模型结构</p>
</li>
<li><p>架构调整：轻量化结构、加强跨块连接、自适应计算时间、分治策略的Transformer</p>
</li>
<li><p>预训练：使用完整Encoder模型，如T5，BART模型</p>
</li>
<li><p>多模态等下游任务应用</p>
</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-b90f773588e3119ca7c8ff505ed52785_1440w.webp" alt=""></p>
<p>BERT模型未来发展方向主要包含：更大更深的模型、多模态、跨语言、小样本、模型蒸馏，有关讨论可见-<a href="https://jmxgodlz.xyz/2021/12/31/2021-12-31-2022预训练的下一步是什么/">2022预训练的下一步是什么</a></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://arxiv.org/pdf/2106.04554.pdf">https://arxiv.org/pdf/2106.04554.pdf</a></p>
<p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>公式向-完美解释梯度消失与LSTM.md</title>
    <url>/2022/08/07/2022-08-07-%E8%AF%A6%E8%A7%A3LSTM%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>首先抛出<strong>关键性结论：</strong></p>
<ol>
<li><p><strong>RNN模型在时间维度共享参数矩阵，因此RNN模型总的梯度等于各时间的梯度之和</strong>，$g=\sum{g_t}$。</p>
</li>
<li><p><strong>RNN中总的梯度不会消失，只是远距离梯度消失，梯度被近距离梯度主导，无法捕获远距离特征。</strong></p>
</li>
<li><p><strong>梯度消失的本质：由于RNN模型在时间维度共享参数矩阵，导致针对隐藏状态h求导时，循环计算矩阵乘法，最终梯度上出现了参数矩阵的累乘。</strong></p>
</li>
<li><p><strong>LSTM缓解梯度消失的本质：引入门控机制，将矩阵乘法转为逐元素相乘的哈达马积:</strong>$c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tanh \left(W_{c}\left[h_{t-1}, x_{t}\right]+b_{c}\right)$</p>
</li>
</ol>
<span id="more"></span>
<h1 id="梯度消失分析"><a href="#梯度消失分析" class="headerlink" title="梯度消失分析"></a>梯度消失分析</h1><h2 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h2><p>RNN的状态更新公式如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
h_t=f(Wh_{t-1}+Ux_t) \tag{1}
\end{equation}</script><script type="math/tex; mode=display">
y_t=f(Vh_t) \tag{2}</script><p>其中，h表示隐藏状态，f表示激活函数，W、U表示参数矩阵，x表示输入。从该式中可以看出<strong>不同时间维度的参数矩阵是共享的</strong>。</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>我们在反向传播过程进行梯度求导，以对参数矩阵U求导为例</p>
<script type="math/tex; mode=display">
\frac{\partial y_{t}}{\partial U}=\frac{\partial y_{t}}{\partial h_{t}}\frac{\partial h_{t}}{\partial U} \tag{3}</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{equation}
\frac{\partial h_{t}}{\partial U}=\sum_{s=0}^{t}\frac{\partial h_{t}}{\partial h_s}\frac{\partial h_{s}}{\partial U} \tag{4}
\end{equation}</script><script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{s}}=\frac{\partial h_{t}}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial h_{t-2}} \ldots \frac{\partial h_{s+1}}{\partial h_{s}} \tag{5}</script><p>从式（1）（2）中可以递推到$h_0$，每一个隐藏状态h均与参数矩阵有关，最后的梯度$y_t$<strong>依赖于每一个隐藏状态</strong>。</p>
<p>$以y_2为例,y_2=f(Vh_2)=f(Vf(Wh_1+Ux_1))=f(Vf(Wf(Wh_0+Ux_0)+Ux_1))$,通过全微分求导，$h_t$对$U,h_{t-1}$求偏导数</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\frac{\partial y_{2}}{\partial U}&=\frac{\partial y_{2}}{\partial h_{2}} \frac{\partial h_{2}}{\partial U} 
\\ &= \frac{\partial y_{2}}{\partial h_{2}} (\frac{\partial h_{2}}{\partial h_{1}} * \frac{\partial h_{1}}{\partial U} + \frac{\partial h_{2}}{\partial U})
\\ &=  \frac{\partial y_{2}}{\partial h_{2}} (\frac{\partial h_{2}}{\partial h_{1}} * (\frac{\partial h_{1}}{\partial h_{0}} * \frac{\partial h_{0}}{\partial U} + \frac{\partial h_{1}}{\partial U}) + \frac{\partial h_{2}}{\partial U})
\\ &=\frac{\partial y_{2}}{\partial h_{2}} (\frac{\partial h_{2}}{\partial U} + \frac{\partial h_{2}}{\partial h_1} \frac{\partial h_{1}}{\partial U} + \frac{\partial h_{2}}{\partial h_1} \frac{\partial h_{1}}{\partial h_0} \frac{\partial h_{0}}{\partial U})
\end{aligned}
\tag{6}</script><script type="math/tex; mode=display">
设:z_t=Wh_{t-1}+Ux_t \tag{7}</script><p>$z_t$代表未经过激活函数的神经网络输出，式（1）转化为：</p>
<script type="math/tex; mode=display">
h_t=f(z_t) \tag{8}</script><script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{t-1}}=\frac{\partial h_{t}}{\partial z_{t}} \frac{\partial z_{t}}{\partial h_{t-1}} \tag{9}</script><p>式（8）可以拆分为两部分：</p>
<script type="math/tex; mode=display">
\frac{\partial z_{t}}{\partial h_{t-1}} = W \tag{10}</script><script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial z_{t}}=\left(\begin{array}{cccc}\frac{\partial h_{t, 1}}{\partial z_{t, 1}} & \frac{\partial h_{t, 1}}{\partial z_{t, 2}} & \cdots & \frac{\partial h_{t, 1}}{\partial z_{t, n}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial h_{t, n}}{\partial z_{t, 1}} & \frac{\partial h_{t, n}}{\partial z_{t, 2}} & \cdots & \frac{\partial h_{t, n}}{\partial z_{t, n}}\end{array}\right) \tag{11}</script><p>其中，$h_t$元素由$z_t$逐元素激活得到，因此两者对应元素才具有依赖关系，未对应元素无依赖关系，导数为0，式（10）成为一个对角矩阵.</p>
<script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial z{t}}=\left(\begin{array}{cccc}f^{\prime}\left(\text { z}_{t, 1}\right) & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & f^{\prime}\left(z_{t, n}\right)\end{array}\right)=diag[f^{\prime}(z_t)] \tag{12}</script><p>根据式（9）（11），式（5）求解得到：</p>
<script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{s}}=\prod_{k=s+1}^{t} W^{T} \operatorname{diag}\left[f^{\prime}\left(\operatorname{z}_{k}\right)\right] \tag{13}</script><p>在式（12）中已经出现了<strong>矩阵的连乘</strong>，根据矩阵的相容性”$||X Y|| \le ||X|| ||Y||$”</p>
<script type="math/tex; mode=display">
\begin{aligned} 
||\frac{\partial h_{t}}{\partial h_{s}}||&=\prod_{k=s+1}^{t} || W^{T} \operatorname{diag}\left[f^{\prime}\left(\operatorname{z}_{k}\right)\right]|| 
\\ &\le \prod_{k=s+1}^{t} ||W^{T}|| ||\operatorname{diag}\left[f^{\prime}\left(\operatorname{z}_{k}\right)\right]||  
\\ &\le \prod_{k=s+1}^{t} \sigma_{max} \gamma = (\sigma_{max} \gamma)^{t-s}   
\end{aligned} 
\tag{14}</script><p>其中,<strong>$\sigma$ 代表矩阵W的最大奇异解，$\gamma$代表激活函数f的上界</strong>，例如双曲正切函数的上界为$||tanh^{‘}(x)|| \le 1$，sigmoid函数的上界为$||sigmoid^{‘}(x) \le \frac{1}{4}||$。<br>因此在远距离依赖，即t-s较大的情况下，<strong>当$\sigma_{max} \gamma \lt 1$时，会发生梯度消失;当$\sigma_{max} \gamma \gt 1$时，会发生梯度爆炸。</strong></p>
<p><strong>TIPS：</strong>这里只是不等式情况，因此即使不等式右边远大于1，也有可能发生梯度消失。但是，<strong>在实际情况下，矩阵范数的约束与实际值相当接近。</strong></p>
<h3 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h3><p>对于传统RNN模型，在训练初期避免梯度消失与参数矩阵的初始化，即最大奇异解$\sigma$值有关。</p>
<p><strong>避免梯度消失的矩阵最小初始化方式如下：</strong></p>
<p>以双曲正切函数为例，双曲正切函数的$\gamma = 1$，为了使$\sigma_{max} \gamma = 1$，即$\sigma=1$。</p>
<p>为了使不等式置信度更高，将矩阵W的所有奇异解设置为1.</p>
<p>对于每一列而言，$\Sigma_{i} w_{i j}^{2}=1$，其中j代表第j列，矩阵中每个元素是一个n维向量，i代表矩阵第i行，w代表列向量。</p>
<p>$n \mathbb{E}\left(w^{2}\right)=1$，</p>
<p>我们假设w服从均匀分布，区间为$[-R,R]$,均匀分布的均值为0，方差$\mathbb{E}\left(w^{2}\right)=\frac{R^2}{3}$【均匀分布方差为$\frac{(b-a)^2}{12}$】。</p>
<p>代入得到<script type="math/tex">n\frac{R^2}{3}=1 \tag{15}</script></p>
<p>即<script type="math/tex">R=\frac{\sqrt{3}}{\sqrt{n}} \tag{16}</script></p>
<p>因此w符合的分布为$[-\frac{\sqrt{3}}{\sqrt{n}}, \frac{\sqrt{3}}{\sqrt{n}}]$，当矩阵是方阵时的Xavier-Glorot initialization分布；当矩阵行列不同时，Xavier-Glorot initialization分布为$\left[-\frac{\sqrt{6}}{\sqrt{m+n}}, \frac{\sqrt{6}}{\sqrt{m+n}}\right]$。</p>
<h1 id="LSTM提出"><a href="#LSTM提出" class="headerlink" title="LSTM提出"></a>LSTM提出</h1><p>关于RNN反向传播的一些评论：</p>
<ol>
<li>RNN模型在时间维度共享参数矩阵</li>
<li>权重更新的频率与梯度的准确性需要权衡，越少的更新次数，梯度准确性越高，但训练速度也下降了。【由反向传递时，使用上一时刻状态做近似导致】</li>
<li>梯度消失带来不稳定的梯度流；共享参数带来对最新更新的过度敏感</li>
<li>针对上述三点，进行错误传播过程的梯度截断是有必要的</li>
<li>传播梯度分量也是可以的</li>
</ol>
<p>LSTM缓解梯度消失的根本方法：<strong>write it down</strong>【将状态记录下来】，但是如果无限制的写入也会带来问题，因此升级为有选择的读写，这样带来了LSTM的三个关键机制：</p>
<ol>
<li>有选择的写入，写入关键信息</li>
<li>有选择的读取信息</li>
<li>有选择的遗忘信息</li>
</ol>
<p>我们可以通过门机制实现选择性，但要如何将这三个机制结合起来呢？</p>
<h2 id="LSTM原型"><a href="#LSTM原型" class="headerlink" title="LSTM原型"></a>LSTM原型</h2><p>首先，提出一个LSTM原型，本着先读取状态，再写入的原则，每一次更新状态的增量$\tilde{s}_{t}$，由$o_t$选择性读取上一个状态的内容，$i_t$为选择写入，$f_t$选择性遗忘上一状态的内容：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i} s_{t-1}+U_{i} x_{t}+b_{i}\right) \\ o_{t} &=\sigma\left(W_{o} s_{t-1}+U_{o} x_{t}+b_{o}\right) \\ f_{t} &=\sigma\left(W_{f} s_{t-1}+U_{f} x_{t}+b_{f}\right) \\ \tilde{s_{t}} &=\phi\left(W\left(o_{t} \odot s_{t-1}\right)+U x_{t}+b\right) \\ s_{t} &=f_{t} \odot s_{t-1}+i_{t} \odot \tilde{s}_{t} \end{aligned} \tag{17}</script><p><img src="https://pic1.zhimg.com/80/v2-a2898c8e0c7c45f763256ec8f60b8844_1440w.webp" alt=""></p>
<h2 id="三个起效果的改进版本"><a href="#三个起效果的改进版本" class="headerlink" title="三个起效果的改进版本"></a>三个起效果的改进版本</h2><p>按理说上述LSTM原型能够起效果，但事与愿违，<strong>选择性读取与选择性写入未能很好的协调，导致状态值非常大，紧接着门机制变得饱和。</strong>这种情况源于的$s_t$是无界的，会变得非常大从而导致门机制饱和，因此接下来的三个改进的生效版本均是约束$s_t$的大小，将其约束成有界。</p>
<h3 id="归一化LSTM原型"><a href="#归一化LSTM原型" class="headerlink" title="归一化LSTM原型"></a>归一化LSTM原型</h3><p>针对$s_t$进行正态归一化，$s_t = \frac{s_t-mean(s_t)}{\sqrt{Var(s_t) + 1}}$，也可以类似于层归一化等方式添加缩放与平移分量。</p>
<p>归一化后的$s_t$从无界成为有界。</p>
<h3 id="GRU：将写入与遗忘强绑定"><a href="#GRU：将写入与遗忘强绑定" class="headerlink" title="GRU：将写入与遗忘强绑定"></a>GRU：将写入与遗忘强绑定</h3><script type="math/tex; mode=display">s_{t}=\left(1-i_{t}\right) \odot s_{t-1}+i_{t} \odot \tilde{s}_{t} \tag{18}</script><p>GRU将写入门与遗忘门绑定起来，使之加和为1。将$s_t$变成$s_{t-1}$与$\tilde{s}_{t}$的element-wise加权平均，当两者均有界时，$s_t$也有界。</p>
<p>以下给出GRU的计算公式，与原理图：</p>
<script type="math/tex; mode=display">
\begin{aligned} r_{t} &=\sigma\left(W_{r} s_{t-1}+U_{r} x_{t}+b_{r}\right) \\ z_{t} &=\sigma\left(W_{z} s_{t-1}+U_{z} x_{t}+b_{z}\right) \\ \tilde{s_{t}} &=\phi\left(W\left(r_{t} \odot s_{t-1}\right)+U x_{t}+b\right) \\ s_{t} &=z_{t} \odot s_{t-1}+\left(1-z_{t}\right) \odot \tilde{s}_{t} \end{aligned} \tag{19}</script><p><img src="https://pic2.zhimg.com/80/v2-29885f66462e4f48db0aba5722798aa5_1440w.webp" alt=""></p>
<h3 id="伪LSTM：通过激活函数约束"><a href="#伪LSTM：通过激活函数约束" class="headerlink" title="伪LSTM：通过激活函数约束"></a>伪LSTM：通过激活函数约束</h3><p>通过激活函数，将$s_t$限制到激活函数的上界内。</p>
<p>只有在更新写入时，为了避免信息的变化，未使用激活函数约束。</p>
<p>以下给出伪LSTM的计算公式，与原理图：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i}\left(\phi\left(s_{t-1}\right)\right)+U_{i} x_{t}+b_{i}\right) \\ o_{t} &=\sigma\left(W_{o}\left(\phi\left(s_{t-1}\right)\right)+U_{o} x_{t}+b_{o}\right) \\ f_{t} &=\sigma\left(W_{f}\left(\phi\left(s_{t-1}\right)\right)+U_{f} x_{t}+b_{f}\right) \\ \tilde{s}_{t} &=\phi\left(W\left(o_{t} \odot \phi\left(s_{t-1}\right)\right)+U x_{t}+b\right) \\ s_{t} &=f_{t} \odot s_{t-1}+i_{t} \odot \tilde{s}_{t} \\ \mathbf{r n n}_{\text {out }} &=\phi\left(s_{t}\right) \end{aligned} \tag{20}</script><p><img src="https://pic2.zhimg.com/80/v2-62d798b3d85004c5ea63b44dbf9312d5_1440w.webp" alt=""></p>
<h2 id="LSTM提出-1"><a href="#LSTM提出-1" class="headerlink" title="LSTM提出"></a>LSTM提出</h2><p>LSTM与伪LSTM的几点关键区别如下：</p>
<ol>
<li><p>LSTM是先写后读，因此添加了一个“影子”状态，Hochreiter and Schmidhuber等人认为状态s与剩余的RNN cell是独立的。</p>
</li>
<li><p>使用门控影子状态$h_{t-1}=o_{t-1}\odot \phi(c_{t-1})$计算门结构，替换激活后的$\phi(c_{t-1})$。这样隐藏状态均是当前时间下的信息，与读取信息时$(o_t \odot s_{t-1})$利用前一时刻信息不同。</p>
</li>
<li><p>使用门控影子状态作为RNN cell的输出$h_{t}=o_{t}\odot \phi(c_{t})$，替代$\phi(c_t)$。</p>
</li>
</ol>
<p>这样一来，LSTM的输入就是上一时刻的$c_{t-1},h_{t-1}$，输出为$c_t,h_t$。</p>
<h3 id="基础LSTM"><a href="#基础LSTM" class="headerlink" title="基础LSTM"></a>基础LSTM</h3><p>基础LSTM单元的公式与原理图如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i} h_{t-1}+U_{i} x_{t}+b_{i}\right) \\ o_{t} &=\sigma\left(W_{o} h_{t-1}+U_{o} x_{t}+b_{o}\right) \\ f_{t} &=\sigma\left(W_{f} h_{t-1}+U_{f} x_{t}+b_{f}\right) \\ \tilde{c}_{t} &=\phi\left(W h_{t-1}+U x_{t}+b\right) \\ c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t} \\ h_{t} &=o_{t} \odot \phi\left(c_{t}\right) \\ \mathrm{rnn}_{\text {out }} &=h_{t} \end{aligned} \tag{21}</script><p><img src="https://pic2.zhimg.com/80/v2-74080caa295667886f47324200e7bc39_1440w.webp" alt=""></p>
<h3 id="The-LSTM-with-peepholes"><a href="#The-LSTM-with-peepholes" class="headerlink" title="The LSTM with peepholes"></a>The LSTM with peepholes</h3><p>利用前一状态$c_{t-1}$来进行门控机制的计算，但输出利用实时信息$c_t$。</p>
<p>计算公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i} h_{t-1}+U_{i} x_{t}+P_{i} c_{t-1}+b_{i}\right) \\ f_{t} &=\sigma\left(W_{f} h_{t-1}+U_{f} x_{t}+P_{f} c_{t-1}+b_{f}\right) \\ \tilde{c}_{t} &=\phi\left(W h_{t-1}+U x_{t}+b\right) \\ c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t} \\ o_{t} &=\sigma\left(W_{o} h_{t-1}+U_{o} x_{t}+P_{o} c_{t}+b_{o}\right) \\ h_{t} &=o_{t} \odot \phi\left(c_{t}\right) \\ \operatorname{rnn}_{\text {out }} &=h_{t} \end{aligned} \tag{22}</script><h1 id="LSTM如何解决梯度消失"><a href="#LSTM如何解决梯度消失" class="headerlink" title="LSTM如何解决梯度消失"></a>LSTM如何解决梯度消失</h1><p>从上述分析可以得到，梯度消失中最大原因是需要计算$\frac{\partial h_{t}}{\partial h_{s}}$，如果这个值不随着层数的增加，趋于0或者无穷大，那么就能够捕获到长距离依赖信息。</p>
<p>LSTM将状态与其他部分分开，状态更新部分变成:</p>
<script type="math/tex; mode=display">c_t = f_t \odot c_{t-1} + i_t * \tilde c_t = c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tanh \left(W_{c}\left[h_{t-1}, x_{t}\right]+b_{c}\right) \tag{23}</script><p>针对状态进行求导，同时$c_t$与$c_{t-1},\tilde c_{t-1},f_t,i_t$有关，因此进行全微分求导</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial C_{t}}{\partial C_{t-1}} &= \frac{\partial C_{t}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial \tilde C_{t}} * \frac{\partial \tilde C_{t}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial i_{t}} * \frac{\partial i_{t}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial f_{t}} * \frac{\partial f_{t}}{\partial C_{t-1}} 
\\ &=\frac{\partial C_{t}}{\partial C_{t-1}} +  \frac{\partial C_{t}}{\partial \tilde C_{t}} * \frac{\partial \tilde C_{t}}{\partial h_{t-1}} * \frac{\partial h_{t-1}}{\partial C_{t-1}} 
\\ &+ \frac{\partial C_{t}}{\partial i_{t}} * \frac{\partial i_{t}}{\partial h_{t-1}} * \frac{\partial h_{t-1}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial f_{t}} * \frac{\partial f_{t}}{\partial h_{t-1}} * \frac{\partial h_{t-1}}{\partial C_{t-1}}
\\ &= f_t + i_t * tanh^{'}(*)W_c * o_{t-1} tanh^{'}(C_{t-1}) 
\\ &+ \tilde C_t * \sigma^{'}(*)W_i * o_{t-1} tanh^{'}(C_{t-1}) + C_{t-1} * \sigma^{'}(*)W_f * o_{t-1} tanh^{'}(C_{t-1})
\end{aligned}
\tag{24}</script><p>从上式可以得到，$\frac{\partial C_{t}}{\partial C_{t-1}}$成为上述4部分的加和，在连乘的任意时刻可能是$[0, +\infty)$的范围，并不会一直趋于0，或者$\infty$。同时$f_t,i_t,o_{t-1},\tilde C_t$都是网络学习的值，也就是说由网络自己学习哪些梯度保留，哪些梯度剔除。</p>
<p>在这些机制的帮助下，LSTM很好的<strong>缓解了</strong>梯度消失问题。</p>
<h1 id="LSTM延伸"><a href="#LSTM延伸" class="headerlink" title="LSTM延伸"></a>LSTM延伸</h1><p>Highway网络和residual网络同样包含了LSTM最基本的思想：与原先一层网络输出$x_{t + 1} = Net(x_t)$的计算方式相比，计算增量$x_{t + 1} = x_t + \Delta x_{t + 1}$。</p>
<p>因此这两种方式，同样会遇到LSTM的问题：读写的不协调。</p>
<p>关于这两者的介绍，再后续有时间展开进行介绍。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
<p><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html</a></p>
<p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/109519044">https://zhuanlan.zhihu.com/p/109519044</a></p>
<p><a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1088177386838749184">https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1088177386838749184</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>进击！BERT句向量表征.md</title>
    <url>/2022/11/05/2022-11-05-%E8%BF%9B%E5%87%BB%EF%BC%81BERT%E5%8F%A5%E5%90%91%E9%87%8F%E8%A1%A8%E5%BE%81/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><img src="https://pic1.zhimg.com/v2-48949d2c35c494d5d03b7de832151835_1440w.jpg?source=172ae18b" alt=""></p>
<p>BERT等语言模型在多数NLP任务中取得优异的表现，但如果直接取BERT输出的句向量作表征，取得的效果甚至还不如Glove词向量。<br>Bert-flow论文中指出，产生该现象的原因是BERT模型的各向异性过高，Transformer模型的输出中，高频词汇分布集中，低频词汇分布分散，整个向量空间类似于锥形结构。<br><img src="https://pic1.zhimg.com/80/v2-8b4df6fe6c7491b6d435dd94c0729e00_1440w.jpg" alt=""><br><span id="more"></span><br>余弦相似度使用的前提是向量空间在标准正交基下，而BERT输出的句向量很明显 不符合该条件，因此直接使用BERT输出的句向量计算余弦相似度，效果表现很差。</p>
<p>针对该问题，BERT-flow、BERT-Whitening模型将输出的句向量映射到标准正交的高斯空间。SimCSE引入对比学习的方式， 在保证向量空间的uniformity情况下，也提高alignment。此后，多种对比学习方式层出不穷，其核心改进点在于正负例的生成方式，提高正负样本的难度，本文主要介绍ConSERT、SimCSE、ESimCSE、DiffCSE、PromptBert、SNCSE、EASE。</p>
<h1 id="Sentence-Bert（EMNLP-2019）"><a href="#Sentence-Bert（EMNLP-2019）" class="headerlink" title="Sentence-Bert（EMNLP 2019）"></a>Sentence-Bert（EMNLP 2019）</h1><p><strong>题目</strong>：Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a></p>
<p><strong>代码</strong>：<a href="https://github.com/UKPLab/sentence-transformers">https://github.com/UKPLab/sentence-transformers</a></p>
<h2 id="核心思路"><a href="#核心思路" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>解决文本匹配推理次数过多的问题，BERT的传统文本匹配任务是输入sentence1与sentence2 作一个分类任务，给定一个语料库查找最相似句子的时间复杂度高达O(n^2)。<br>直接使用BERT的embdeeing向量，存在各向异性问题，相似度计算效果甚至低于Glove词向量。</p>
<p><strong>解决方式</strong>：</p>
<p>采用孪生网络的双塔结构，这样推理的次数降低到了O(n)，计算相似度的点积复杂度还是O(n^2)</p>
<p><strong>其他</strong>：</p>
<p>论文比较了三种pooling方式：CLS、AVG、MAX。也设计了分类任务、回归任务、Triplet形式的任务。</p>
<p><img src="https://pic3.zhimg.com/80/v2-01aeeae898873dd7d7dfb418a927806a_1440w.webp" alt=""></p>
<p>SBERT通过图1的方式进行训练，微调学习BERT句向量的embedding，通过图2的方式完成推理，计算文本相似度。</p>
<h1 id="BERT-flow（EMNLP-2020）"><a href="#BERT-flow（EMNLP-2020）" class="headerlink" title="BERT-flow（EMNLP 2020）"></a>BERT-flow（EMNLP 2020）</h1><p><strong>题目</strong>：On the Sentence Embeddings from Pre-trained Language Models</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2011.05864.pdf">https://arxiv.org/pdf/2011.05864.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/bohanli/BERT-flow">https://github.com/bohanli/BERT-flow</a></p>
<h2 id="核心思路-1"><a href="#核心思路-1" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>指出BERT句向量存在的问题：各向异性，高频词汇分布集中</p>
<p><strong>解决方式</strong>：</p>
<p>引入flow流式模型，映射为同向性，将BERT句向量映射到标准高斯空间.</p>
<p><strong>计算公式</strong>如下：<br><img src="https://pic4.zhimg.com/80/v2-999025acb512c8f3b97c4bd4ec45065b_1440w.webp" alt=""></p>
<h1 id="BERT-whitening"><a href="#BERT-whitening" class="headerlink" title="BERT-whitening"></a>BERT-whitening</h1><p><strong>题目</strong>：Whitening Sentence Representations for Better Semantics and Faster Retrieval</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/abs/2103.15316">https://arxiv.org/abs/2103.15316</a></p>
<p><strong>代码</strong>：<a href="https://github.com/bojone/BERT-whitening">https://github.com/bojone/BERT-whitening</a></p>
<h2 id="核心思路-2"><a href="#核心思路-2" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：在句向量维度，通过一个白化的操作直接校正局向量的协方差矩阵。</p>
<p><strong>解决方式</strong>：</p>
<p>PCA白化，并且可以降维实现更好的效果。<br>将句向量进行标准化映射，即白化操作：</p>
<script type="math/tex; mode=display">\tilde{\boldsymbol{x}}_i=\left(\boldsymbol{x}_i-\boldsymbol{\mu}\right) W</script><p>均值与协方差矩阵计算方式如下：</p>
<script type="math/tex; mode=display">\boldsymbol{\mu}=\frac{1}{N} \sum_{i=1}^N \boldsymbol{x}_i</script><script type="math/tex; mode=display">\boldsymbol{\Sigma}=\boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U}^{\top},即原向量矩阵的协方差矩阵SVD分解</script><script type="math/tex; mode=display">\boldsymbol{W}=\boldsymbol{U} \sqrt{\boldsymbol{\Lambda}^{-1}}</script><p><strong>代码如下</strong>：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def compute_kernel_bias(vecs):</span><br><span class="line">    &quot;&quot;&quot;计算kernel和bias</span><br><span class="line">    vecs.shape = [num_samples, embedding_size]，</span><br><span class="line">    最后的变换：y = (x + bias).dot(kernel)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    mu = vecs.mean(axis=0, keepdims=True)</span><br><span class="line">    cov = np.cov(vecs.T)</span><br><span class="line">    u, s, vh = np.linalg.svd(cov)</span><br><span class="line">    W = np.dot(u, np.diag(1 / np.sqrt(s)))</span><br><span class="line">    return W, -mu</span><br></pre></td></tr></table></figure></p>
<h1 id="ConSERT（ACL-2021）"><a href="#ConSERT（ACL-2021）" class="headerlink" title="ConSERT（ACL 2021）"></a>ConSERT（ACL 2021）</h1><p><strong>题目</strong>：ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/abs/2105.11741">https://arxiv.org/abs/2105.11741</a></p>
<p><strong>代码</strong>：<a href="https://github.com/yym6472/ConSERT">https://github.com/yym6472/ConSERT</a></p>
<h2 id="核心思路-3"><a href="#核心思路-3" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>解决BERT本身表征的语义塌缩问题。再来看对比学习，它是通过拉近相同样本的距离、拉远不同样本的距离，来刻画样本本身的表示，正好可以解决BERT表示的塌缩问题。</p>
<p><strong>解决方式</strong>：</p>
<p>NLP的数据增强方式很重要，使用了5种数据增强方式进行对比：Adversarial attack、shuffle、token cutoff、feature cutoff、dropout</p>
<p>损失函数：对比学习损失</p>
<script type="math/tex; mode=display">\mathcal{L}_{i, j}=-\log \frac{\exp \left(\operatorname{sim}\left(r_i, r_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbb{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(r_i, r_k\right) / \tau\right)}</script><p><img src="https://pic3.zhimg.com/80/v2-60b3b4f2d2753a786449186812c4271a_1440w.webp" alt=""></p>
<p><img src="https://pic4.zhimg.com/80/v2-9d0e06beb31ee2fcf5ff536c58d6eca3_1440w.webp" alt=""></p>
<h2 id="正例构建方式"><a href="#正例构建方式" class="headerlink" title="正例构建方式"></a>正例构建方式</h2><ul>
<li>shuffle：更换position id的顺序</li>
<li>token cutoff：在某个token维度把embedding置为0</li>
<li>feature cutoff：在embedding矩阵中，有768个维度，把某个维度的feature置为0</li>
<li>dropout：dropout</li>
<li>Adversarial attack：对抗扰动</li>
</ul>
<h2 id="负例构建方式"><a href="#负例构建方式" class="headerlink" title="负例构建方式"></a>负例构建方式</h2><p>同一batch内其他样本</p>
<h1 id="SimCSE（EMNLP-2021）"><a href="#SimCSE（EMNLP-2021）" class="headerlink" title="SimCSE（EMNLP 2021）"></a>SimCSE（EMNLP 2021）</h1><p><strong>题目</strong>：SimCSE: Simple Contrastive Learning of Sentence Embeddings</p>
<p><strong>地址</strong>：<a href="https://aclanthology.org/2021.emnlp-main.552.pdf">https://aclanthology.org/2021.emnlp-main.552.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/princeton-nlp/SimCSE">https://github.com/princeton-nlp/SimCSE</a></p>
<h2 id="核心思路-4"><a href="#核心思路-4" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>将对比学习方式引入句向量表征领域。</p>
<p><strong>解决方式</strong>：</p>
<p>引入对比损失的难点在于正样本的构建，该文提出dropout的正样本构建方式。</p>
<p><strong>无监督SimCSE</strong>：</p>
<p>Dropout构建正例样本，batch内其他样本作为负样本</p>
<p><img src="https://pic4.zhimg.com/80/v2-e0e1aab2f104c2e1df6f801d86509987_1440w.webp" alt=""></p>
<p><strong>有监督SimCSE</strong>：</p>
<p>句子蕴含任务中同一标签的作为正样本，同一batch的其他样本的是负样本</p>
<p><img src="https://pic2.zhimg.com/80/v2-330c834b6c87b47cce0e314deefb4ad1_1440w.webp" alt=""></p>
<h2 id="正例构建方式-1"><a href="#正例构建方式-1" class="headerlink" title="正例构建方式"></a>正例构建方式</h2><p>Dropout两次产生的数据</p>
<h2 id="负例构建方式-1"><a href="#负例构建方式-1" class="headerlink" title="负例构建方式"></a>负例构建方式</h2><p>同一batch内其他样本</p>
<p><img src="https://pic4.zhimg.com/80/v2-3ab1f8ef586853f48950c102e6c4b23f_1440w.webp" alt=""></p>
<h1 id="ESimCSE（COLING-2022）"><a href="#ESimCSE（COLING-2022）" class="headerlink" title="ESimCSE（COLING 2022）"></a>ESimCSE（COLING 2022）</h1><p><strong>题目</strong>：ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2109.04380.pdf">https://arxiv.org/pdf/2109.04380.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/caskcsg/sentemb/tree/main/ESimCSE">https://github.com/caskcsg/sentemb/tree/main/ESimCSE</a></p>
<h2 id="核心思路-5"><a href="#核心思路-5" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>解决SimCSE的两个问题：</p>
<ul>
<li>Dropout构建的正例均是相同长度的，会导致模型认为相同句子长度的句子更相似</li>
<li>SimCSE增加batchsize，引入更多负例，反而引起效果下降【猜测：更多的负例中，部分与正例接近质量不高】</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-941a073e5de189c9b4286d2e92739415_1440w.webp" alt=""><br><strong>核心改动点</strong>：</p>
<p><strong>正例生成方式</strong>：重复一定单词</p>
<p><strong>负例生成方式</strong>：动量序列，扩展负样本数量</p>
<h2 id="正例生成方式"><a href="#正例生成方式" class="headerlink" title="正例生成方式"></a>正例生成方式</h2><p>重复单词</p>
<h2 id="负例生成方式"><a href="#负例生成方式" class="headerlink" title="负例生成方式"></a>负例生成方式</h2><p>动量序列，扩展负样本数量</p>
<h1 id="DiffCSE（NAACL2022）"><a href="#DiffCSE（NAACL2022）" class="headerlink" title="DiffCSE（NAACL2022）"></a>DiffCSE（NAACL2022）</h1><p><strong>题目</strong>：DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2204.10298.pdf">https://arxiv.org/pdf/2204.10298.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/voidism/DiffCSE">https://github.com/voidism/DiffCSE</a></p>
<h2 id="核心思路（将敏感变化作为负例，非敏感变化的dropout作为正例）"><a href="#核心思路（将敏感变化作为负例，非敏感变化的dropout作为正例）" class="headerlink" title="核心思路（将敏感变化作为负例，非敏感变化的dropout作为正例）"></a>核心思路（将敏感变化作为负例，非敏感变化的dropout作为正例）</h2><p><strong>出发点</strong>：</p>
<p>NLP任务中，词语EDA的数据增强方式是敏感变化，dropout方式是 不敏感变化。SimCSE的成功也说明了dropout masks机制来构建正样本，比基于同义词或掩码语言模型的删除或替换等更复杂的增强效果要好得多。“。这一现象也说明，<strong>「直接增强（删除或替换）往往改变句子本身语义」。</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-6a25b5c6d3eef374dd84d6f3e712b887_1440w.webp" alt=""><br><img src="https://pic4.zhimg.com/80/v2-74343b4576af98fc90bc171a16f058f3_1440w.webp" alt=""></p>
<h2 id="正例生成方式-1"><a href="#正例生成方式-1" class="headerlink" title="正例生成方式"></a>正例生成方式</h2><p>与SimCSE相同，Dropout产生正例</p>
<h2 id="负例生成方式-1"><a href="#负例生成方式-1" class="headerlink" title="负例生成方式"></a>负例生成方式</h2><p>通过ELECTRA模型，完成句子改写任务</p>
<h1 id="PromptBert（EMNLP-2022）"><a href="#PromptBert（EMNLP-2022）" class="headerlink" title="PromptBert（EMNLP 2022）"></a>PromptBert（EMNLP 2022）</h1><p><strong>题目</strong>：PromptBERT: Improving BERT Sentence Embeddings with Prompts</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2201.04337.pdf">https://arxiv.org/pdf/2201.04337.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/kongds/Prompt-BERT">https://github.com/kongds/Prompt-BERT</a></p>
<h2 id="核心思路-6"><a href="#核心思路-6" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>BERT句向量存在各向异性，并受到词频、字母大小写、子词等影响。</p>
<p><strong>解决方式</strong>：</p>
<p>通过Prompt模版，得到句子的表征向量，与CLS、AVG、MAX取表征向量不同，没有用到具体某个词语的信息。</p>
<p><strong>模版去噪对比学习</strong>：</p>
<p>通过不同模版产生句子表征向量，构建正样本。为了剔除不同模版的影响，减去纯模版得到的句子表征向量。</p>
<p><img src="https://pic3.zhimg.com/80/v2-03707325180ec050cf39849c4030e586_1440w.webp" alt=""></p>
<h2 id="正例构建方式-2"><a href="#正例构建方式-2" class="headerlink" title="正例构建方式"></a>正例构建方式</h2><p>通过不同模版产生的句子表征向量</p>
<h2 id="负例构建方式-2"><a href="#负例构建方式-2" class="headerlink" title="负例构建方式"></a>负例构建方式</h2><p>同一batch内其他样本</p>
<h1 id="SNCSE"><a href="#SNCSE" class="headerlink" title="SNCSE"></a>SNCSE</h1><p><strong>题目</strong>：SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2201.05979.pdf">https://arxiv.org/pdf/2201.05979.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/Sense-GVT/SNCSE">https://github.com/Sense-GVT/SNCSE</a></p>
<h2 id="核心思路-7"><a href="#核心思路-7" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>目前的数据增强方法，获取的正样本均极为相似，导致模型存在特征抑制，即<strong>「模型不能区分文本相似度和语义相似度，并更偏向具有相似文本，而不考虑它们之间的实际语义差异」。</strong> 导致模型更多关注的是字面匹配，而非语义匹配。</p>
<p><strong>解决方式</strong>：</p>
<p>新增软负例以及双向边际损失，限制样本与软负例相似度到一定范围内。 并利用PromptBert方式获取词向量。</p>
<p><strong>计算公式</strong>如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-6d6852c52edb927f3e6f90a4780360f0_1440w.webp" alt=""></p>
<p><img src="https://pic2.zhimg.com/80/v2-da3c2a097a0fa631f88a913dccadd541_1440w.webp" alt=""></p>
<h2 id="正例生成方式-2"><a href="#正例生成方式-2" class="headerlink" title="正例生成方式"></a>正例生成方式</h2><p>与SimCSE相同，Dropout产生正例</p>
<h2 id="负例生成方式-2"><a href="#负例生成方式-2" class="headerlink" title="负例生成方式"></a>负例生成方式</h2><ul>
<li>纯负例：batch内其他数据</li>
<li>软负例：通过spacy进行语法解析，为动词添加否定词前缀。</li>
</ul>
<h1 id="EASE（NAACL2022）"><a href="#EASE（NAACL2022）" class="headerlink" title="EASE（NAACL2022）"></a>EASE（NAACL2022）</h1><p><strong>题目</strong>：EASE: Entity-Aware Contrastive Learning of Sentence Embedding</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2205.04260.pdf">https://arxiv.org/pdf/2205.04260.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/studio-ousia/ease">https://github.com/studio-ousia/ease</a></p>
<h2 id="核心思路-8"><a href="#核心思路-8" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：实体是一个句子的重要部分，可以作为一个句子的指示器，通过学习实体与句子之间的差异，可以为句子向量的学习提供额外信息。</p>
<p><strong>解决方式</strong>：通过维基百科的实体超链接引入实体信息。</p>
<p><strong>损失函数</strong>：实体句子对比损失+自监督对比损失</p>
<h2 id="正例生成方式-3"><a href="#正例生成方式-3" class="headerlink" title="正例生成方式"></a>正例生成方式</h2><ul>
<li><strong>实体正例</strong>：维基百科超链接实体。(为了提高实体质量，仅保留超链接实体出现次数超过10次的实体。)</li>
<li><strong>样本正例</strong>：与SimCSE相同，Dropout产生正例</li>
</ul>
<h2 id="负例生成方式-3"><a href="#负例生成方式-3" class="headerlink" title="负例生成方式"></a>负例生成方式</h2><ul>
<li>实体负例：<br>负例实体需要与正例实体具有相同的类型；<br>负例实体不能与正例实体出现在同一维基百科页面中。<br>随机在满足上诉条件的候选实体中选择一个实体作为硬负例数据，构建（句子，正例实体，负例实体）的triple数据。</li>
<li>样本负例：batch内其他数据</li>
</ul>
<h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><ul>
<li>在中文数据集上，验证各方法效果</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>句向量</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>句向量</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch多机多卡的多种打开方式</title>
    <url>/2022/09/04/2022-09-04-Pytorch%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E7%9A%84%E5%A4%9A%E7%A7%8D%E6%89%93%E5%BC%80%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在上一篇介绍<a href="https://jmxgodlz.xyz/2021/09/12/2021-09-12-Pytorch多卡训练原理/#more">多卡训练原理</a>的基础上，本篇主要介绍Pytorch多机多卡的几种实现方式：<strong>DDP、multiprocessing、Accelerate</strong>。</p>
<p>在介绍具体实现之前，torch.distributed 涉及的分布式概念如下：</p>
<ul>
<li><strong>group：</strong>进程组，通常一个job只有一个组，即一个world，使用多机时，一个group产生了多个world。</li>
<li><strong>world_size：</strong>一个job的全局进程数量</li>
<li><strong>rank：</strong>进程的序号，一般设置rank=0的主机为master节点。</li>
<li><strong>local_rank：</strong>进程内部的GPU序号。</li>
</ul>
<p>比如，有两台8卡机器，这时具有一个group，2个world，每个world_size为8，第一个主机rank=0，显卡编号依次为0,…,7，第二个主机rank=1，显卡编号依次为0,…,7。</p>
<p>在多机多卡的分布式训练过程中，为每个进程的模型、数据配置好这些参数至关重要。</p>
<span id="more"></span>
<h2 id="DDP"><a href="#DDP" class="headerlink" title="DDP"></a>DDP</h2><p>Pytorch分布式执行流程如下：</p>
<ol>
<li><strong>init_process_group</strong> 初始化进程组，同时初始化 distributed 包。</li>
<li>创建分布式模型<strong>model = DDP(model)</strong></li>
<li>创建分布式数据采样的<strong>datasampler</strong></li>
<li>利用<strong>torch.distributed.launch</strong>控制进程训练</li>
<li><strong>destory_process_group</strong>销毁进程组</li>
</ol>
<h3 id="进程组初始化"><a href="#进程组初始化" class="headerlink" title="进程组初始化"></a>进程组初始化</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">init_process_group(backend, </span><br><span class="line">                   init_method=None, </span><br><span class="line">                   timeout=datetime.timedelta(0, 1800), </span><br><span class="line">                   world_size=-1, </span><br><span class="line">                   rank=-1, </span><br><span class="line">                   store=None)</span><br></pre></td></tr></table></figure>
<h4 id="TCP初始化"><a href="#TCP初始化" class="headerlink" title="TCP初始化"></a>TCP初始化</h4><p>使用<strong>TCP初始化</strong>时，需要指定下列<strong>参数</strong>：</p>
<ol>
<li><code>rank</code> 为当前进程的进程号</li>
<li><code>word_size</code> 为当前 <code>job</code> 的总进程数</li>
<li><code>init_method</code> 内指定 <code>tcp</code> 模式，且所有进程的 <code>ip:port</code> 必须一致，设定为主进程的 <code>ip:port</code></li>
</ol>
<p>初始化时，<strong>需要注意下列事项：</strong></p>
<ol>
<li><p>在 <code>rank==0</code> 的进程内保存参数,一般是rank0主节点来分发广播梯度。</p>
</li>
<li><p>若程序内未根据 <code>rank</code> 设定当前进程使用的 <code>GPUs</code>，则默认使用全部 <code>GPU</code>，且以数据并行的方式使用。</p>
</li>
<li><p>每条命令表示一个进程，若已开启的进程未达到 <code>word_size</code> 的数量，则所有进程会一直等待。</p>
</li>
<li><p>每台主机上可以开启多个进程。但是，若未为每个进程分配合适的 <code>GPU</code>，则同机不同进程可能会共用 <code>GPU</code>，应该坚决避免这种情况，容易爆显存。</p>
</li>
<li><p>使用 <code>gloo</code> 后端进行 <code>GPU</code> 训练时，会报错。</p>
</li>
</ol>
<p>参考代码如下，<strong>需要在args里面添加指定的参数：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line">import torch.utils.data.distributed</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">parser = argparse.ArgumentParser(description=&#x27;PyTorch distributed training on cifar-10&#x27;)</span><br><span class="line">parser.add_argument(&#x27;--rank&#x27;, default=0,</span><br><span class="line">                    help=&#x27;rank of current process&#x27;)</span><br><span class="line">parser.add_argument(&#x27;--word_size&#x27;, default=2,</span><br><span class="line">                    help=&quot;word size&quot;)</span><br><span class="line">parser.add_argument(&#x27;--init_method&#x27;, default=&#x27;tcp://127.0.0.1:23456&#x27;,</span><br><span class="line">                    help=&quot;init-method&quot;)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">dist.init_process_group(backend=&#x27;nccl&#x27;, init_method=args.init_method, rank=args.rank, world_size=args.word_size)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True, download=download, transform=transform)</span><br><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">net = Net()</span><br><span class="line">net = net.cuda()</span><br><span class="line">net = torch.nn.parallel.DistributedDataParallel(net)</span><br></pre></td></tr></table></figure>
<p><strong>执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Node 1 : ip 192.168.1.201  port : 12345</span><br><span class="line">python tcp_init.py --init_method tcp://192.168.1.201:12345 --rank 0 --word_size 3</span><br><span class="line"></span><br><span class="line"># Node 2 : </span><br><span class="line">python tcp_init.py --init_method tcp://192.168.1.201:12345 --rank 1 --word_size 3</span><br><span class="line"></span><br><span class="line"># Node 3 : </span><br><span class="line">python tcp_init.py --init_method tcp://192.168.1.201:12345 --rank 2 --word_size 3</span><br></pre></td></tr></table></figure>
<h4 id="ENV初始化"><a href="#ENV初始化" class="headerlink" title="ENV初始化"></a>ENV初始化</h4><p>在ENV初始化方式中，init中无需指定参数，主要从机器的环境变量中获取参数。</p>
<ol>
<li><p>该初始化中需要设定local_rank参数，确定单机进程的序号。</p>
</li>
<li><p>然后，通过<strong>torch.distributed.launch</strong>设定nnodes节点数，node_rank当前主机进程序号，nproc_per_node每个节点的进程数量，master_addr主节点地址，master_port主节点端口，在环境变量中获取这些参数。</p>
</li>
</ol>
<p><strong>注意事项如下：</strong></p>
<ol>
<li><p>使用 <code>torch.distributed.launch</code> 工具时，将会为当前主机创建 <code>nproc_per_node</code> 个进程，每个进程独立执行训练脚本。同时，它还会为每个进程分配一个 <code>local_rank</code> 参数，表示当前进程在当前主机上的编号。例如：<code>rank=2, local_rank=0</code> 表示第 <code>3</code> 个节点上的第 <code>1</code> 个进程。</p>
</li>
<li><p>在 <code>rank==0</code> 的进程内保存参数。</p>
</li>
<li><p><code>Env</code> 方式中，在 <code>init_process_group</code> 中，无需指定任何参数</p>
</li>
<li><p>合理利用 <code>local_rank</code> 参数，来合理分配本地的 <code>GPU</code> 资源</p>
</li>
<li><p>每条命令表示一个进程。若已开启的进程未达到 <code>word_size</code> 的数量，则所有进程会一直等待。</p>
</li>
</ol>
<p><strong>参考代码如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line">import torch.utils.data.distributed</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">import argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"># 注意这个参数，必须要以这种形式指定，即使代码中不使用。因为 launch 工具默认传递该参数</span><br><span class="line">parser.add_argument(&quot;--local_rank&quot;, type=int)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">dist.init_process_group(backend=&#x27;nccl&#x27;, init_method=&#x27;env://&#x27;)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True, download=download, transform=transform)</span><br><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line"># 根据 local_rank，配置当前进程使用的 GPU</span><br><span class="line">net = Net()</span><br><span class="line">device = torch.device(&#x27;cuda&#x27;, args.local_rank)</span><br><span class="line">net = net.to(device)</span><br><span class="line">net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.local_rank], output_device=args.local_rank)</span><br></pre></td></tr></table></figure>
<p><strong>执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=2 --nnodes=3 --node_rank=0 --master_addr=&quot;192.168.1.201&quot; --master_port=23456 env_init.py</span><br><span class="line"></span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=2 --nnodes=3 --node_rank=1 --master_addr=&quot;192.168.1.201&quot; --master_port=23456 env_init.py</span><br><span class="line"></span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=2 --nnodes=3 --node_rank=2 --master_addr=&quot;192.168.1.201&quot; --master_port=23456 env_init.py</span><br></pre></td></tr></table></figure>
<h4 id="共享文件系统初始化"><a href="#共享文件系统初始化" class="headerlink" title="共享文件系统初始化"></a>共享文件系统初始化</h4><p>使用<strong>共享文件系统初始化</strong>时，与<strong>TCP初始化</strong>类似，需要指定下列参数：</p>
<ol>
<li><code>rank</code> 为当前进程的进程号</li>
<li><code>word_size</code> 为当前 <code>job</code> 的总进程数</li>
<li><code>init_method</code> 内指定 <code>文件系统</code> 模式，以 <code>file://</code> 为前缀，表示文件系统各式初始化。<code>/xxx</code> 表示共享的文件，各个进程在共享文件系统中通过该文件进行同步或异步。因此，所有进程必须对该文件具有读写权限。</li>
</ol>
<p><strong>参考代码如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mport torch.distributed as dist</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">parser = argparse.ArgumentParser(description=&#x27;PyTorch distributed training on cifar-10&#x27;)</span><br><span class="line">parser.add_argument(&#x27;--rank&#x27;, default=0,</span><br><span class="line">                    help=&#x27;rank of current process&#x27;)</span><br><span class="line">parser.add_argument(&#x27;--word_size&#x27;, default=2,</span><br><span class="line">                    help=&quot;word size&quot;)</span><br><span class="line">parser.add_argument(&#x27;--init_method&#x27;, default=&#x27;file:///mnt/nfs/sharedfile&#x27;,</span><br><span class="line">                    help=&quot;init-method&quot;)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"># rank should always be specified</span><br><span class="line">dist.init_process_group(backend, init_method=&#x27;file:///mnt/nfs/sharedfile&#x27;,</span><br><span class="line">                        world_size=4, rank=args.rank)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True, download=download, transform=transform)</span><br><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line"># 根据 local_rank，配置当前进程使用的 GPU</span><br><span class="line">net = Net()</span><br><span class="line">device = torch.device(&#x27;cuda&#x27;, args.local_rank)</span><br><span class="line">net = net.to(device)</span><br><span class="line">net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.local_rank], output_device=args.local_rank</span><br></pre></td></tr></table></figure>
<p><strong>执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Node1:</span><br><span class="line">python mnsit.py --init-method file://PathToShareFile/MultiNode --rank 0 --world_size 2</span><br><span class="line">Node2:</span><br><span class="line">python mnsit.py --init-method file://PathToShareFile/MultiNode --rank 1 --world_size 2</span><br></pre></td></tr></table></figure>
<h3 id="DistributedDataParallel"><a href="#DistributedDataParallel" class="headerlink" title="DistributedDataParallel"></a>DistributedDataParallel</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.nn.parallel.DistributedDataParallel(module, </span><br><span class="line">                                          device_ids=None, </span><br><span class="line">                                          output_device=None, </span><br><span class="line">                                          dim=0, </span><br><span class="line">                                          broadcast_buffers=True, </span><br><span class="line">                                          process_group=None, </span><br><span class="line">                                          bucket_cap_mb=25, </span><br><span class="line">                                          find_unused_parameters=False, </span><br><span class="line">                                          check_reduction=False)</span><br></pre></td></tr></table></figure>
<p>将给定的 <code>module</code> 进行分布式封装， 其将输入在 <code>batch</code> 维度上进行划分，并分配到指定的 <code>devices</code> 上。</p>
<p><code>module</code> 会被复制到每台机器的每个 <code>进程</code> 上，每一个模型的副本处理输入的一部分。</p>
<p>在反向传播阶段，每个机器的每个 <code>进程</code> 上的梯度进行汇总并求平均。与 <code>DataParallel</code> 类似，<code>batch size</code> 应该大于 <code>GPU</code> 总数。</p>
<p><strong>主要参数介绍：</strong></p>
<ol>
<li><p><strong>module：</strong>将完整的model封装为分布式module,后续需要调用model的方法时，可以采用module.model.xxx</p>
</li>
<li><p><strong>device_ids：</strong>需要并行的设备，在数据并行的情况下，表示模型副本拷贝到哪些GPU上；在模型并行的情况下，表示模型分散在哪些GPU上。</p>
</li>
<li><p><strong>output_device：</strong>输出结果到哪个GPU上。</p>
</li>
</ol>
<p><strong>注意事项如下：</strong></p>
<ol>
<li><p>要使用该 <code>class</code>，需要先对 <code>torch.distributed</code> 进行初进程组始化，可以通过 <code>torch.distributed.init_process_group()</code> 实现。</p>
</li>
<li><p>该 <code>module</code> 仅在 <strong>gloo</strong>和 <strong>nccl</strong>后端上可用。</p>
</li>
</ol>
<h3 id="DistributedSampler"><a href="#DistributedSampler" class="headerlink" title="DistributedSampler"></a>DistributedSampler</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.utils.data.distributed.DistributedSampler(dataset, </span><br><span class="line">                                                num_replicas=None, </span><br><span class="line">                                                rank=None)</span><br></pre></td></tr></table></figure>
<p><strong>主要参数介绍：</strong></p>
<ol>
<li><p><strong>dataset：</strong>采样的数据集</p>
</li>
<li><p><strong>num_replicas：</strong>参与的总进程数</p>
</li>
<li><p><strong>rank：</strong>当前机器的rank</p>
</li>
</ol>
<p><strong>DistributedSampler</strong>将数据集采样为num_replicas份，不同机器根据自己的rank取数据集的子集。</p>
<p><strong>TIPS</strong>：在 <code>DataParallel</code> 中，<code>batch size</code> 设置必须为单卡的 <code>n</code> 倍，但是在 <code>DistributedDataParallel</code> 内，<code>batch size</code> 设置于单卡一样即可。</p>
<p><strong>参考代码如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 分布式训练示例</span><br><span class="line">from torch.utils.data import Dataset, DataLoader</span><br><span class="line">from torch.utils.data.distributed import DistributedSampler</span><br><span class="line">from torch.nn.parallel import DistributedDataParallel</span><br><span class="line"></span><br><span class="line">dataset = your_dataset()</span><br><span class="line">datasampler = DistributedSampler(dataset)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size_per_gpu, sampler=datasampler)</span><br><span class="line">model = your_model()</span><br><span class="line">model = DistributedDataPrallel(model, device_ids=[local_rank], output_device=local_rank)</span><br></pre></td></tr></table></figure>
<h3 id="torch-distributed-launch"><a href="#torch-distributed-launch" class="headerlink" title="torch.distributed.launch"></a>torch.distributed.launch</h3><p>DDP通过torch.distributed.launch辅助实现进程控制。</p>
<p><strong>torch.distributed.launch</strong>传入的参数如下：</p>
<ul>
<li><strong>training_script：</strong>执行任务脚本路径</li>
<li><strong>—nnodes：</strong>节点数，即分布式机器数量</li>
<li><strong>—node_rank：</strong>当前机器的rank序号</li>
<li><strong>—nproc_per_node：</strong>每个节点开设的进程数量，最好设置为每个机器GPU数量，使每个GPU在一个进程中</li>
<li><strong>—master_addr：</strong>master 节点（rank 为 0）的地址</li>
<li><strong>—master_port：</strong>master 节点（rank 为 0）的端口</li>
</ul>
<p><strong>单机多卡的执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script)</span><br></pre></td></tr></table></figure>
<p><strong>多机多卡执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Node1:</span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes= NUM_MACHINES_YOU_HAVE --node_rank=0 --master_addr=&quot;192.168.1.1&quot; --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script)</span><br><span class="line">...</span><br><span class="line">NodeN:</span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes= NUM_MACHINES_YOU_HAVE --node_rank=N-1 --master_addr=&quot;192.168.1.1&quot; --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script)</span><br></pre></td></tr></table></figure>
<h2 id="torch-multiprocessing"><a href="#torch-multiprocessing" class="headerlink" title="torch.multiprocessing"></a>torch.multiprocessing</h2><p>通过 <strong>torch.multiprocessing</strong>手动控制进程，替代 <strong>torch.distributed.launch</strong>的进程控制工作。</p>
<p>涉及的主要接口为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method=&#x27;spawn&#x27;):</span><br><span class="line">    r&quot;&quot;&quot;Spawns ``nprocs`` processes that run ``fn`` with ``args``.</span><br><span class="line"></span><br><span class="line">    If one of the processes exits with a non-zero exit status, the</span><br><span class="line">    remaining processes are killed and an exception is raised with the</span><br><span class="line">    cause of termination. In the case an exception was caught in the</span><br><span class="line">    child process, it is forwarded and its traceback is included in</span><br><span class="line">    the exception raised in the parent process.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        fn (function): Function is called as the entrypoint of the</span><br><span class="line">            spawned process. This function must be defined at the top</span><br><span class="line">            level of a module so it can be pickled and spawned. This</span><br><span class="line">            is a requirement imposed by multiprocessing.</span><br><span class="line"></span><br><span class="line">            The function is called as ``fn(i, *args)``, where ``i`` is</span><br><span class="line">            the process index and ``args`` is the passed through tuple</span><br><span class="line">            of arguments.</span><br><span class="line"></span><br><span class="line">        args (tuple): Arguments passed to ``fn``.</span><br><span class="line">        nprocs (int): Number of processes to spawn.</span><br><span class="line">        join (bool): Perform a blocking join on all processes.</span><br><span class="line">        daemon (bool): The spawned processes&#x27; daemon flag. If set to True,</span><br><span class="line">                       daemonic processes will be created.</span><br><span class="line">        start_method (string): (deprecated) this method will always use ``spawn``</span><br><span class="line">                               as the start method. To use a different start method</span><br><span class="line">                               use ``start_processes()``.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">        None if ``join`` is ``True``,</span><br><span class="line">        :class:`~ProcessContext` if ``join`` is ``False``</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
<p><strong>主要参数介绍如下：</strong></p>
<ul>
<li>fn:处理的主函数</li>
<li>args:传递给主函数的参数，主函数第一个参数默认传入进程index</li>
<li>nprocs:开启的进程数量</li>
</ul>
<p>结合下列代码介绍torch.multiprocessing多机多卡的使用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def setup(rank, world_size):</span><br><span class="line">    # dist.init_process_group(&quot;gloo&quot;, rank=rank, world_size=world_size)</span><br><span class="line">    print(&quot;world size:&quot;, world_size, &quot; rank:&quot;, rank)</span><br><span class="line">    print(os.environ[&#x27;MASTER_ADDR&#x27;])</span><br><span class="line">    print(os.environ[&#x27;MASTER_PORT&#x27;])</span><br><span class="line">    print(os.environ[&#x27;RANK&#x27;])</span><br><span class="line">    print(os.environ[&#x27;WORLD_SIZE&#x27;])</span><br><span class="line">    dist.init_process_group(&quot;nccl&quot;, rank=rank, world_size=world_size)</span><br><span class="line"></span><br><span class="line">def cleanup():</span><br><span class="line">    dist.destroy_process_group()</span><br><span class="line"></span><br><span class="line">def main(local_rank, nnodes, args):</span><br><span class="line">    rank = int(os.environ[&#x27;RANK&#x27;]) * nnodes + local_rank</span><br><span class="line">    world_size = nnodes * int(os.environ[&#x27;WORLD_SIZE&#x27;])</span><br><span class="line"></span><br><span class="line">    print(&quot;world size:&quot;, world_size, &quot; rank:&quot;, rank)</span><br><span class="line">    setup(rank, world_size)</span><br><span class="line">    ……</span><br><span class="line">    # If passed along, set the training seed now.</span><br><span class="line">    if args.seed is not None:</span><br><span class="line">        set_seed(args.seed)</span><br><span class="line">    model = torch.nn.parallel.DistributedDataParallel(model.to(local_rank), device_ids=[local_rank])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    args = parse_args()</span><br><span class="line">    world_size = torch.cuda.device_count()</span><br><span class="line">    print(&#x27;&#123;&#125;:&#123;&#125;&#x27;.format(world_size, &#x27;---&#x27; * 100))</span><br><span class="line">    mp.spawn(main, args=(world_size, args), nprocs=world_size, join=True)</span><br></pre></td></tr></table></figure>
<p><strong>代码流程的解释如下：</strong></p>
<ol>
<li>根据<strong>torch.cuda.device_count()</strong>获取单机的显卡数量，决定开启的进程数，即一个world的world_size</li>
<li><strong>mp.spawn</strong>开启多进程</li>
<li>单机的进程index即为local_rank，nnodes代表单机显卡数量，os.environ[‘RANK’]获取机器的rank值，通过rank<em>nnodes + local_rank 计算全局训练的索引，nnodes </em> int(os.environ[‘WORLD_SIZE’]) 计算全局训练的进程数量</li>
<li>根据计算的全局索引，全局数量 初始化进程通信</li>
<li><strong>model.to(local_rank)</strong>将模型放置于本地单机的显卡上</li>
</ol>
<h2 id="Accelerate"><a href="#Accelerate" class="headerlink" title="Accelerate"></a>Accelerate</h2><p>Hugging Face发布PyTorch新库「Accelerate」：适用于多GPU、TPU、混合精度训练。「Accelerate」提供了一个简单的 API，将与多 GPU 、 TPU 、 fp16 相关的样板代码抽离了出来，保持其余代码不变。PyTorch 用户无须使用不便控制和调整的抽象类或编写、维护样板代码，就可以直接上手多 GPU 或 TPU。</p>
<p>项目地址：<a href="https://github.com/huggingface/accelerate">https://github.com/huggingface/accelerate</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  import torch</span><br><span class="line">  import torch.nn.functional as F</span><br><span class="line">  from datasets import load_dataset</span><br><span class="line">+ from accelerate import Accelerator</span><br><span class="line"></span><br><span class="line">- device = &#x27;cpu&#x27;</span><br><span class="line">+ accelerator = Accelerator()</span><br><span class="line"></span><br><span class="line">- model = torch.nn.Transformer().to(device)</span><br><span class="line">+ model = torch.nn.Transformer()</span><br><span class="line">  optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line">  dataset = load_dataset(&#x27;my_dataset&#x27;)</span><br><span class="line">  data = torch.utils.data.DataLoader(dataset, shuffle=True)</span><br><span class="line"></span><br><span class="line">+ model, optimizer, data = accelerator.prepare(model, optimizer, data)</span><br><span class="line"></span><br><span class="line">  model.train()</span><br><span class="line">  for epoch in range(10):</span><br><span class="line">      for source, targets in data:</span><br><span class="line">-         source = source.to(device)</span><br><span class="line">-         targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">          optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">          output = model(source)</span><br><span class="line">          loss = F.cross_entropy(output, targets)</span><br><span class="line"></span><br><span class="line">-         loss.backward()</span><br><span class="line">+         accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">          optimizer.step()</span><br></pre></td></tr></table></figure>
<p><strong>模型保存加载：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 模型保存</span><br><span class="line">accelerator.wait_for_everyone()</span><br><span class="line">unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line">accelerator.save(unwrapped_model.state_dict(), path)</span><br><span class="line"></span><br><span class="line"># 模型加载</span><br><span class="line">unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line">unwrapped_model.load_state_dict(torch.load(path))</span><br></pre></td></tr></table></figure>
<p>具体代码可以参考huggingface的transformer代码库</p>
<p>项目地址：<a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch">https://github.com/huggingface/transformers/tree/main/examples/pytorch</a></p>
<p><strong>TIPS：</strong>在PAI平台上尝试失败，猜测使用该库的时候，需要先执行命令配置环境参数【多卡…】</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://zhuanlan.zhihu.com/p/462453622">https://zhuanlan.zhihu.com/p/462453622</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/98535650">https://zhuanlan.zhihu.com/p/98535650</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/76638962">https://zhuanlan.zhihu.com/p/76638962</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>【律师函警告】-ChatGPT以鸡你太美主题写小说.md</title>
    <url>/2022/12/07/2022-12-07-%E3%80%90%E5%BE%8B%E5%B8%88%E5%87%BD%E8%AD%A6%E5%91%8A%E3%80%91-ChatGPT%E4%BB%A5%E9%B8%A1%E4%BD%A0%E5%A4%AA%E7%BE%8E%E4%B8%BB%E9%A2%98%E5%86%99%E5%B0%8F%E8%AF%B4/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>趁着ChatGPT大火，以鸡你太美为主题生成小说，大家细品了哈～</p>
<p>鸡你太美是一个充满活力和激情的年轻人，他喜欢唱歌、跳舞、rap和玩篮球。他有一个梦想，就是成为一名职业篮球运动员。</p>
<p>某天，他被他的朋友邀请参加了一个有着真正挑战性的比赛——一场篮球比赛。鸡你太美满怀信心地参加了比赛，他在比赛中表现出了超出预期的精彩表现。他的扣篮、三分球和防守都让他的队友们和对手们都刮目相看。</p>
<p>在比赛的最后关键时刻，鸡你太美再次展现了他的技术和实力，他跳得更高、投得更准，最终帮助他的队伍获得了胜利。</p>
<p>在比赛结束后，鸡你太美收到了许多赞美和鼓励，他感到非常开心和满足。他知道，这场比赛不仅是他人生中一个重要的里程碑，也是他梦想成真的开始。</p>
<p>从那以后，鸡你太美开始了他的职业篮球生涯，他的才华和实力得到了越来越多的认可和赏识。他成为了一名优秀的篮球运动员，并在他的人生中写下了一段属于自己的传奇。</p>
<p><img src="https://pic1.zhimg.com/v2-579b5a59aac1deb55adaa0450698bff9_1440w.jpg?source=172ae18b" alt=""></p>
<span id="more"></span>
<h1 id="ChatGPT浅玩一下"><a href="#ChatGPT浅玩一下" class="headerlink" title="ChatGPT浅玩一下"></a>ChatGPT浅玩一下</h1><h2 id="剧本生成"><a href="#剧本生成" class="headerlink" title="剧本生成"></a>剧本生成</h2><p><img src="https://pic2.zhimg.com/80/v2-9d851b3309d806fbaa6a09bd93bf1871_1440w.webp" alt=""></p>
<h2 id="知识问答"><a href="#知识问答" class="headerlink" title="知识问答"></a>知识问答</h2><p><img src="https://pic1.zhimg.com/80/v2-16b54e78250052a09f2cce16b71baa34_1440w.webp" alt=""></p>
<p><img src="https://pic4.zhimg.com/80/v2-8b456197ae73f8aae4201d902b036d3b_1440w.webp" alt=""></p>
<h2 id="写诗"><a href="#写诗" class="headerlink" title="写诗"></a>写诗</h2><p><img src="https://pic3.zhimg.com/80/v2-9664253913b78828ee4508c3df1aad8e_1440w.webp" alt=""></p>
<h2 id="小说创作"><a href="#小说创作" class="headerlink" title="小说创作"></a>小说创作</h2><p><img src="https://pic2.zhimg.com/80/v2-81be53cae055ab16bd0278227e0ba381_1440w.webp" alt=""></p>
<h2 id="写代码"><a href="#写代码" class="headerlink" title="写代码"></a>写代码</h2><p><img src="https://pic2.zhimg.com/80/v2-ecad55078136c99a56abf56234d32d95_1440w.webp" alt=""></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>个人尝试下来，觉得生成的东西已经有一定的感觉了，但是整体输出的错误率较高，经常报error哈～</p>
<p><img src="https://pic2.zhimg.com/80/v2-ecad55078136c99a56abf56234d32d95_1440w.webp" alt=""></p>
<p>能够写出hello world的代码，但是尝试DNN、快排这些又不可以了，还是期待GPT4的发布，大家可以自行体验一下哈</p>
<h1 id="ChatGPT注册"><a href="#ChatGPT注册" class="headerlink" title="ChatGPT注册"></a>ChatGPT注册</h1><p>注册网址： <a href="https://chat.openai.com/auth/login">https://chat.openai.com/auth/login</a> 【需要翻墙哈】</p>
<p>其中注册过程需要国外手机号来收验证码，这里推荐一个网址来收验证码：</p>
<p><a href="https://sms-activate.org/cn/">https://sms-activate.org/cn/</a></p>
<p>注册一个账号，然后输入：openai，然后选择阿根廷的电话（印度的被用烂了，接不到验证码）。如果阿根廷不好使，可以试试巴西、厄瓜多尔。<br>大胆的试，一个电话30分钟的测试时间，接不到验证码，你可以叉掉（必须手动取消，超时还是会扣款），不会扣款。</p>
<p><img src="https://pic2.zhimg.com/80/v2-c2eab434c541829bad9ec4c1f663367d_1440w.webp" alt=""></p>
<p>阿根廷电话号码需要 22.5 卢布。</p>
<p>人民币大约2.5。<br>添加购物车，需要提前充值。</p>
<p>充值方式有很多，可以选择简单的支付宝。</p>
<p>扫码支付即可。<br>添加购物车之后，购买之后，会有一个电话号码。</p>
<p><img src="https://pic1.zhimg.com/80/v2-c3ee43993a7f753a66f9585fc2f50a74_1440w.webp" alt=""></p>
<p>然后选择国家，填写电话号码<br>可以看到顺利接到验证码了（接收不到不扣钱，20分钟之内可删除。）</p>
<p>填写验证码之后，就能看到这个界面了。<br>随便选择一个。<br>看到这个界面代码注册成功。</p>
<p> <img src="https://pic3.zhimg.com/80/v2-fe6d5ed79eeb119ee886c20eb093e1fe_1440w.webp" alt=""></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://docs.qq.com/doc/DT3ZDbGtRUGRPWnpv?_t=1670427397386&amp;u=7311207678ec42f3ab99441a26d8c958">https://docs.qq.com/doc/DT3ZDbGtRUGRPWnpv?_t=1670427397386&amp;u=7311207678ec42f3ab99441a26d8c958</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>穿越时空：当ChatGPT遇见stable-diffusion，你不敢相信的创意艺术之旅!</title>
    <url>/2023/03/05/2023-03-05-%E7%A9%BF%E8%B6%8A%E6%97%B6%E7%A9%BA%EF%BC%9A%E5%BD%93ChatGPT%E9%81%87%E8%A7%81stable-diffusion%EF%BC%8C%E4%BD%A0%E4%B8%8D%E6%95%A2%E7%9B%B8%E4%BF%A1%E7%9A%84%E5%88%9B%E6%84%8F%E8%89%BA%E6%9C%AF%E4%B9%8B%E6%97%85!/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>欢迎来到一场创意的旅程，这里将聚焦于 ChatGPT 和 stable-diffusion 这两个令人激动的技术。在这篇文章中，我们将会探索这两种技术如何结合使用，为艺术创作带来全新的可能性。我们将探讨如何利用 ChatGPT 生成富有想象力的创意，以及如何使用 stable-diffusion 技术来呈现精美的中式艺术风格。我们还将介绍一些令人惊叹的案例，展示这些技术的真正潜力和创造力。无论您是一个艺术爱好者、技术探究者，还是想探索新领域的读者，本文都将为您提供一次精彩的旅程。跟随我们一起穿越时空，发现 ChatGPT 和 stable-diffusion 的惊人之处。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/D2oNrN.png" alt="D2oNrN"></p>
<span id="more"></span>
<h1 id="ChatGPT：创意的源泉"><a href="#ChatGPT：创意的源泉" class="headerlink" title="ChatGPT：创意的源泉"></a>ChatGPT：创意的源泉</h1><p>ChatGPT 是一种基于自然语言处理 (NLP) 技术的深度学习模型，具有生成自然语言文本的能力。它是由 OpenAI 开发的，使用了大量的训练数据和计算资源，能够生成各种类型的文本，包括对话、文章、故事等等。通过简单的输入提示，ChatGPT 可以自动生成与输入相关的文本，具有极大的创意和想象力。</p>
<p>在艺术创作领域，ChatGPT 提供了无限的可能性。我们可以使用 ChatGPT 生成各种富有想象力的艺术作品，例如诗歌、小说、电影剧本等等。通过输入不同的创意提示，我们可以让 ChatGPT 生成出无数可能的艺术创作。下面是一个例子：</p>
<blockquote>
<p>“在繁华都市的街头，一位神秘的女子突然出现，她手持一把魔法法杖，周围的建筑物开始发生异变……”</p>
</blockquote>
<p>通过输入这个简短的提示，ChatGPT 可以生成一个充满想象力的故事，我们可以将其发展为小说、漫画、电影等等。</p>
<h1 id="Stable-Diffusion：精美的艺术风格"><a href="#Stable-Diffusion：精美的艺术风格" class="headerlink" title="Stable-Diffusion：精美的艺术风格"></a>Stable-Diffusion：精美的艺术风格</h1><p>Stable-Diffusion 是一种深度学习技术，可以生成出极具艺术价值的图像。与传统的图像生成技术不同，Stable-Diffusion 能够生成出高分辨率的图像，而且具有出色的视觉效果。该技术基于流模型，使用了大量的数据和计算资源进行训练，可以生成出各种具有中式风格的图像，例如山水画、人物画等等。</p>
<p>Stable-Diffusion 技术的一个优势是它可以通过简单的输入提示来控制图像的生成过程。我们可以使用各种创意的输入提示来生成出不同的艺术风格，例如：</p>
<blockquote>
<p>“生成一个穿着汉服的女子，在竹林中写诗。”</p>
</blockquote>
<p>通过这个简单的提示，Stable-Diffusion 可以生成出一个具有中式风格的图像，展现出竹林中的神秘和美丽。</p>
<h1 id="ChatGPT-和-Stable-Diffusion-的结合"><a href="#ChatGPT-和-Stable-Diffusion-的结合" class="headerlink" title="ChatGPT 和 Stable-Diffusion 的结合"></a>ChatGPT 和 Stable-Diffusion 的结合</h1><p>当 ChatGPT 和 Stable-Diffusion 结合在一起时，它们可以为艺术创作带来全新的可能性。我们可以使用 ChatGPT 生成出富有想象力的艺术创意，然后使用 Stable-Diffusion 技术将这些创意转化为精美的艺术作品。这种结合可以让我们更加轻松地创作出令人震撼的艺术作品，同时也可以大大提高我们的创作效率。</p>
<p>下面是一个结合了 ChatGPT 和 Stable-Diffusion 的艺术作品的例子：</p>
<blockquote>
<p>“生成一个传统的中国庭院，里面有一位穿着古装的女子，手持一枝花，伫立在花园中。”</p>
</blockquote>
<p>通过输入这个创意提示，ChatGPT 可以生成出一个具有想象力的场景描述。然后，我们可以使用 Stable-Diffusion 技术将这个场景转化为一个美丽的艺术作品，展现出中国古代庭院的美丽和神秘。</p>
<p>总之，ChatGPT 和 Stable-Diffusion 技术的结合为我们带来了无限的可能性。无论是艺术创作、文学创作，还是其他类型的创作，这两种技术都可以帮助我们更加轻松地实现我们的创作愿望。</p>
<h1 id="实现示例"><a href="#实现示例" class="headerlink" title="实现示例"></a>实现示例</h1><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/8exT63.png" alt="8exT63"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/psREq4.png" alt="psREq4"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/12ZmUf.png" alt="12ZmUf"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/0aA0VC.png" alt="0aA0VC"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/FJMuzu.png" alt="FJMuzu"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/AvqEcj.png" alt="AvqEcj"></p>
<h1 id="搭建方式"><a href="#搭建方式" class="headerlink" title="搭建方式"></a>搭建方式</h1><h2 id="ChatGPT注册及使用"><a href="#ChatGPT注册及使用" class="headerlink" title="ChatGPT注册及使用"></a>ChatGPT注册及使用</h2><p>见<a href="https://jmxgodlz.xyz/2022/12/07/2022-12-07-【律师函警告】-ChatGPT以鸡你太美主题写小说/">【律师函警告】-ChatGPT以鸡你太美主题写小说</a></p>
<h2 id="Stable-Diffusion搭建及使用-MAC-M1"><a href="#Stable-Diffusion搭建及使用-MAC-M1" class="headerlink" title="Stable-Diffusion搭建及使用[MAC M1]"></a>Stable-Diffusion搭建及使用[MAC M1]</h2><ol>
<li>克隆代码仓库</li>
</ol>
<blockquote>
<p>git clone <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui.git">https://github.com/AUTOMATIC1111/stable-diffusion-webui.git</a></p>
</blockquote>
<ol>
<li>安装torch环境</li>
</ol>
<blockquote>
<p>conda create -n sd python=3.10.6</p>
<p>conda activate sd</p>
<p>pip install -r requirements_versions.txt</p>
</blockquote>
<ol>
<li>GPU torch MPS安装</li>
</ol>
<p>见<a href="https://zhuanlan.zhihu.com/p/542502414">真香～BERT在MAC Pytorch的使用</a></p>
<blockquote>
<p>conda install pytorch torchvision torchaudio -c pytorch</p>
</blockquote>
<ol>
<li>启动代码</li>
</ol>
<blockquote>
<p>source webui-macos-env.sh // 涉及一些环境变量的初始化</p>
</blockquote>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/3Uozb1.png" alt="3Uozb1"></p>
<ol>
<li>修改启动代码中部分初始环境检查</li>
</ol>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ROSwg9.png" alt="ROSwg9"></p>
<ol>
<li>下载模型</li>
</ol>
<p><a href="https://civitai.com：含有许多玩家自调Lora模型">https://civitai.com：含有许多玩家自调Lora模型</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/VmWrMd.png" alt="VmWrMd"></p>
<blockquote>
<p>TIPS: LoRA，英文全称Low-Rank Adaptation of Large Language Models，直译为大语言模型的低阶适应，这是微软的研究人员为了解决大语言模型微调而开发的一项技术。类似于BERT的微调哈～</p>
</blockquote>
<p>中国风基础&amp;Lora模型链接：</p>
<p><a href="https://civitai.com/models/11352/guofeng3lora">GuoFeng3_Lora | Stable Diffusion LORA | Civitai</a></p>
<p><a href="https://civitai.com/models/10415/guofeng3">GuoFeng3 | Stable Diffusion Checkpoint | Civitai</a></p>
<p><a href="https://civitai.com/models/12597/moxin">墨心 MoXin | Stable Diffusion LORA | Civitai</a></p>
<p>Lora模型放在models/Lora下：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/mg6Qr4.png" alt="mg6Qr4"></p>
<p>基础模型放在models/Stable-diffusion下：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/BAcBXs.png" alt="BAcBXs"></p>
<ol>
<li>启动脚本</li>
</ol>
<blockquote>
<p>python launch.py </p>
</blockquote>
<ol>
<li>Web使用：模型选择&amp;Lora选择&amp;模版选择&amp;参数选择</li>
</ol>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/0Kx5cw.png" alt="0Kx5cw"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/YirWt0.png" alt="YirWt0"></p>
<blockquote>
<p>其中CFG代表 <strong>提示词相关性</strong></p>
</blockquote>
<h1 id="彩蛋"><a href="#彩蛋" class="headerlink" title="彩蛋"></a>彩蛋</h1><p>看看这篇博客有多少AI生成的内容：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ClBCF2.png" alt="ClBCF2"></p>
<p><img src="/Users/jmxgodlzz/Library/Application%20Support/marktext/images/2023-03-04-23-14-04-image.png" alt=""></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/Kbc78h.png" alt="Kbc78h"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/epRNxJ.png" alt="epRNxJ"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/BEHMAa.png" alt="BEHMAa"></p>
<p><strong>向文明低头！！！</strong></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>从语言模型到ChatGPT：大型语言模型的发展和应用</title>
    <url>/2023/03/12/%E4%BB%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%88%B0ChatGPT%EF%BC%9A%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%91%E5%B1%95%E5%92%8C%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>大型语言模型（LLM）是指能够处理大量自然语言数据的深度学习模型，它已经在自然语言处理、文本生成、机器翻译等多个领域中展现出了巨大的潜力。在过去几年中，LLM领域经历了飞速的发展，其中Google和OpenAI作为两家领先的公司在这个领域中的表现备受关注。</p>
<p>Google是LLM领域的重要参与者，其BERT自编码模型和T5编码解码器在自然语言理解任务上取得了优异的表现。BERT模型通过预训练大规模文本数据，提取出词向量的同时，也能够学习到上下文信息。而T5模型则是在BERT的基础上，进一步将生成式任务融入其中，实现了一体化的自然语言处理能力。这些模型的出现，极大地推动了LLM领域的发展。</p>
<p>与之相反的是，OpenAI则从2018年开始，坚持使用decoder only的GPT模型，践行着「暴力美学」——以大模型的路径，实现AGI。GPT模型通过预训练海量语料库数据，学习到了自然语言中的规律和模式，并在生成式任务中取得了出色的表现。OpenAI坚信，在模型规模达到足够大的情况下，单纯的decoder模型就可以实现AGI的目标。</p>
<p>除了Google和OpenAI外，还有许多其他公司和研究机构也在LLM领域做出了贡献。例如，Facebook的RoBERTa模型、Microsoft的Turing NLG模型等等。这些模型的不断涌现，为LLM领域的发展注入了新的动力。</p>
<p>如果只用解码器的生成式是通用LLM的王道，2019年10月，Google同时押注编码解码器的T5，整整错失20个月，直到2021年10月发布FLAN才开始重新转变为decoder-only。这表明，在实际应用中，不同任务可能需要不同类型的模型，而在特定任务中，编码解码器的结构可能比decoder-only模型更加适合。</p>
<p>在本文中，我们将基于CS224N课件回顾大型语言模型的发展历程，探讨它们是如何从最初的基础模型发展到今天的高级模型的，并介绍ChatGPT的发展历程，看看ChatGPT如何实现弯道超车。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/gnqCNR.jpg" alt="gnqCNR"></p>
<span id="more"></span>
<h1 id="Zero-Shot-ZS-and-Few-Shot-FS-In-Context-Learning"><a href="#Zero-Shot-ZS-and-Few-Shot-FS-In-Context-Learning" class="headerlink" title="Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning"></a>Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning</h1><h2 id="上下文学习-In-Context-Learning"><a href="#上下文学习-In-Context-Learning" class="headerlink" title="上下文学习(In-Context Learning)"></a>上下文学习(In-Context Learning)</h2><p>近年来，语言模型越来越倾向于使用更大的模型和更多的数据，如下图所示，模型参数数量和训练数据量呈指数倍增加的趋势。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/7EgTef.png" alt="7EgTef"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型名称</th>
<th>说明</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT</td>
<td>Transformer decoder with 12 layers[参数量117M]<br/>Trained on BooksCorpus: over 7000 unique books (4.6GB text).</td>
<td>表明大规模语言建模可以成为自然语言推理等下游任务的有效预训练技术。</td>
</tr>
<tr>
<td>GPT2</td>
<td>Same architecture as GPT, just bigger (117M -&gt; 1.5B)<br/>trained on much more data: 4GB -&gt; 40GB of internet text data (WebText)</td>
<td>涌现出优异的Zero-shot能力。</td>
</tr>
<tr>
<td>GPT3</td>
<td>Another increase in size (1.5B -&gt; 175B)<br/>data (40GB -&gt; over 600GB)</td>
<td>涌现出强大的上下文学习能力，但是在复杂、多步推理任务表现较差。</td>
</tr>
</tbody>
</table>
</div>
<p>近年来，随着GPT模型参数量的增加，GPT2与GPT3模型已经表现出了极佳的<strong>上下文学习能力(In-Context Learning)</strong>。这种能力允许模型通过处理上下文信息来更好地理解和处理自然语言数据。GPT模型通过Zero-Shot、One-Shot和Few-Shot学习方法在许多自然语言处理任务中取得了显著的成果。</p>
<p>其中，Zero-Shot学习是指模型在没有针对特定任务进行训练的情况下，可以通过给定的输入和输出规范来生成符合规范的输出结果。这种方法可以在没有充足样本的情况下，快速生成需要的输出结果。One-Shot和Few-Shot学习则是在样本量较少的情况下，模型可以通过学习一小部分示例来完成相应任务，这使得模型能够更好地应对小样本学习和零样本学习的问题。</p>
<h3 id="上下文学习介绍"><a href="#上下文学习介绍" class="headerlink" title="上下文学习介绍"></a>上下文学习介绍</h3><p>链接：<a href="https://arxiv.org/abs/2301.00234">[2301.00234] A Survey on In-context Learning</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/mjQ3eG.png" alt="mjQ3eG"></p>
<p>大模型有一个很重要的涌现能力（Emergent ability）就是<strong>In-Context Learning（ICL），也是一种新的范式，指在不进行参数更新的情况下，只在输入中加入几个示例就能让模型进行学习</strong>。下面给出ICL的公式定义：</p>
<script type="math/tex; mode=display">
C = {I,s(x_1,y_1),...,s(x_k,y_k)} \quad or \quad C = {s(x_1, y_1, I), . . . , s(x_k, y_k, I)}</script><script type="math/tex; mode=display">
P\left(y_j \mid x\right) \triangleq f_{\mathcal{M}}\left(y_j, C, x\right)</script><script type="math/tex; mode=display">
\hat{y}=\arg \max _{y_j \in Y} P\left(y_j \mid x\right) .</script><p>其中，符号含义如下，从这些符号中也能看出影响ICL的因素：</p>
<ul>
<li><p>I：具体任务的描述信息</p>
</li>
<li><p>x：输入文本</p>
</li>
<li><p>y：标签</p>
</li>
<li><p>M：语言模型</p>
</li>
<li><p>C：阐述示例</p>
</li>
<li><p>f：打分函数</p>
</li>
</ul>
<p>下面将开始介绍如何提升模型的ICL能力。</p>
<h4 id="训练优化ICL能力"><a href="#训练优化ICL能力" class="headerlink" title="训练优化ICL能力"></a>训练优化ICL能力</h4><p><strong>有监督训练：</strong></p>
<p>在ICL格式的数据集上，进行有监督的训练。</p>
<p>MetaICL就直接把很多任务整合成了ICL的形式精调模型，在52个数据集上取得了比肩直接精调的效果。另外还有部分研究专注于Instruction tuning，构建更好的任务描述让模型去理解，而不是只给几个例子（demonstration），比如LaMDA-PT、FLAN。</p>
<p><strong>自监督训练：</strong></p>
<p>将自然语言理解的任务转为ICL的数据格式。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/FUs3TC.png" alt="FUs3TC"></p>
<p>图1代表不同自然语言理解任务转为<strong>ICL的输入输出</strong>形式。</p>
<p>图2表示训练样本示例，包含几个训练样本，前面的样本作为后面样本的任务阐述。</p>
<h4 id="推理优化ICL能力"><a href="#推理优化ICL能力" class="headerlink" title="推理优化ICL能力"></a>推理优化ICL能力</h4><h5 id="Prompt设计"><a href="#Prompt设计" class="headerlink" title="Prompt设计"></a>Prompt设计</h5><p>样本选取：文本表示、互信息选择相近的；Perplexity选取；语言模型生成……</p>
<p>样本排序：距离度量；信息熵……</p>
<p>任务指示：APE语言模型自动生成</p>
<p>推理步骤：COT、多步骤ICL、Self-Ask</p>
<h5 id="打分函数"><a href="#打分函数" class="headerlink" title="打分函数"></a>打分函数</h5><ul>
<li><p>Direct：直接取条件概率<code>P(y|x)</code>，缺点在于y必须紧跟在输入的后面</p>
</li>
<li><p>Perplexity：再用语言模型过一遍句子，这种方法可以解决上述固定模式的问题，但计算量增加了</p>
</li>
<li><p>Channel：评估<code>P(x|y)</code>的条件概率（用贝叶斯推一下），这种方法在不平衡数据下表现较好</p>
</li>
</ul>
<h5 id="影响ICL表现的因素"><a href="#影响ICL表现的因素" class="headerlink" title="影响ICL表现的因素"></a>影响ICL表现的因素</h5><ul>
<li><p>预训练语料的多样性比数量更重要，增加多种来源的数据可能会提升ICL表现</p>
</li>
<li><p>用下游任务的数据预训练不一定能提升ICL表现，并且PPL更低的模型也不一定表现更好</p>
</li>
<li><p>当LM到达一定规模的预训练步数、尺寸后，会涌现出ICL能力，且ICL效果跟参数量正相关</p>
</li>
</ul>
<h3 id="WHY：上下文学习生效的原因"><a href="#WHY：上下文学习生效的原因" class="headerlink" title="WHY：上下文学习生效的原因"></a>WHY：上下文学习生效的原因</h3><p><strong>论文链接：</strong><a href="[[2202.12837] Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a>)</p>
<p><strong>关键结论：</strong></p>
<ol>
<li><p>InContext Learning中<strong>标签是否正确</strong>无明显影响</p>
</li>
<li><p>InContext Learning中影响因素包括<strong>规范的输入空间、标签空间、输入与标签的匹配格式</strong></p>
</li>
</ol>
<p><strong>其他论文的猜测：</strong></p>
<ul>
<li><p><strong>跟训练数据的分布相关</strong>：比如训练数据有很多样例，也有学者认为ICL可能是隐式的Bayesian inference</p>
</li>
<li><p><strong>跟学习机制相关</strong>：有学者猜测LM可能自己就具备学习的能力，在做ICL的时候学到了这些知识，或者隐式直接精调了自己</p>
</li>
<li><p><strong>跟Transformer中的模块相关</strong>：有学者发现Transformer里的某些注意力头会通过拷贝固定的模式来预测下一个token</p>
</li>
</ul>
<h4 id="InContext-Learning中标签是否正确无明显影响"><a href="#InContext-Learning中标签是否正确无明显影响" class="headerlink" title="InContext Learning中标签是否正确无明显影响"></a>InContext Learning中<strong>标签是否正确</strong>无明显影响</h4><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/iLPKIE.png" alt="iLPKIE"></p>
<p>图中青绿色代表没有示例、黄色代表带有正确标签的示例、橙色代表带有随机标签的示例。</p>
<p>实验结果表明，<strong>带有随机标签的效果非常接近于带有正确标签的效果</strong>。</p>
<p>此外，作者还进行了<strong>标签正确比例、提示样本数量、提示模版样式</strong>的实验，均得出一致结论，实验图如下。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/P6Rbs3.png" alt="P6Rbs3"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/9NL9Oq.png" alt="9NL9Oq"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/y8mMa2.png" alt="y8mMa2"></p>
<h4 id="InContext-Learning中影响因素包括规范的输入空间、标签空间、输入与标签的匹配格式"><a href="#InContext-Learning中影响因素包括规范的输入空间、标签空间、输入与标签的匹配格式" class="headerlink" title="InContext Learning中影响因素包括规范的输入空间、标签空间、输入与标签的匹配格式"></a>InContext Learning中影响因素包括规范的输入空间、标签空间、输入与标签的匹配格式</h4><p>作者分别从以下四个维度探究In-Context Learning效果增益的影响</p>
<ol>
<li><p><strong>The input-label mapping</strong>：即每个输入xi是否与正确的标签yi配对;</p>
</li>
<li><p><strong>The distribution of the input text</strong>：即x1…xk的分布是否一致;</p>
</li>
<li><p><strong>The label space</strong>：<em>y</em>1…yk所覆盖的标签空间;</p>
</li>
<li><p><strong>The format</strong>：使用输入标签配对作为格式。</p>
</li>
</ol>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/V5XRzx.png" alt="V5XRzx"></p>
<h5 id="输入文本分布实验"><a href="#输入文本分布实验" class="headerlink" title="输入文本分布实验"></a>输入文本分布实验</h5><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/VZ5X1F.png" alt="VZ5X1F"></p>
<p>紫色柱子代表利用外部语料采样的数据加上随机标签，在几个任务上模型表现明显下降。</p>
<p>因此，in-context learning中，演示中的分布内输入极大地有助于提高性能。这可能是因为已IND（in-distribution）文本的条件使任务更接近于语言建模，因为LM在此期间总是以IND文本为条件进行推理标签。</p>
<h5 id="标签分布实验"><a href="#标签分布实验" class="headerlink" title="标签分布实验"></a>标签分布实验</h5><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/QuaT1J.png" alt="QuaT1J"></p>
<p>绿色柱子代表采用随机的单词代替输出标签，对于Direct模型，模型表现显著下降，表明ICL中标签空间的一致性显著有助于提高性能。</p>
<p>对于Channel模型，模型表现未明显下降，作者猜测Channel模型以标签为条件，因此无法从标签空间分布中获益。</p>
<h5 id="输入标签配对格式实验"><a href="#输入标签配对格式实验" class="headerlink" title="输入标签配对格式实验"></a>输入标签配对格式实验</h5><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/pafljc.png" alt="pafljc"></p>
<p>分别用labels only（深紫）和no labels（深绿）来探索演示模式的差异对模型表现的影响。可以看到，模型相对于上面两图的OOD setting而言，都有了进一步的下降。这可以表明ICL中保持输入-标签对的格式是关键的。</p>
<h2 id="思维链-Chain-of-Thought"><a href="#思维链-Chain-of-Thought" class="headerlink" title="思维链(Chain of Thought)"></a>思维链(Chain of Thought)</h2><p>思维链（Chain of Thought）是一种新的学习方式，旨在提高模型在数学计算和符号推理任务中的推理能力。这种方式通过将多个相关的数学计算或符号推理步骤按顺序组合成一条思维链，让模型能够沿着思维链进行推理。</p>
<p>这种方式的主要贡献在于，它能够让模型更好地应对复杂的数学计算和符号推理任务。传统的Prompt方式很难应对这种任务，但是思维链可以让模型按照特定的顺序进行推理，从而提高模型的推理能力。</p>
<p>此外，思维链的方式也可以更好地模拟人类在解决数学计算和符号推理问题时的思维过程。人类在解决这类问题时，通常会按照一定的顺序进行推理，而思维链可以让模型更好地模拟这种思维过程。</p>
<h3 id="开山之作：Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models"><a href="#开山之作：Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models" class="headerlink" title="开山之作：Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"></a>开山之作：Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</h3><p>链接：<a href="https://arxiv.org/abs/2201.11903">[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ren2TO.png" alt="ren2TO"></p>
<p>核心思想：输出答案前，加上人工的推理描述。</p>
<p>主要贡献：</p>
<ul>
<li><p>思维链原则上允许模型将多步问题分解为中间步骤，可以将额外的计算分配给需要更多推理步骤的问题。</p>
</li>
<li><p>思维链为模型的行为提供了一个可解释的窗口，表明它可能是如何得出特定答案的，并提供了调试推理路径错误位置的机会。</p>
</li>
<li><p>链式思维推理可用于数学单词问题、常识推理和符号操作等任务，并且可能（至少在原则上）适用于人类可以通过语言解决的任何任务。</p>
</li>
<li><p>只需将思维序列链的例子包含到少样本提示的范例中，就可以很容易地在足够大的现成语言模型中引出思维链推理。</p>
</li>
</ul>
<h3 id="Self-Consistency-Improves-Chain-of-Thought-Reasoning-in-Language-Models"><a href="#Self-Consistency-Improves-Chain-of-Thought-Reasoning-in-Language-Models" class="headerlink" title="Self-Consistency Improves Chain of Thought Reasoning in Language Models"></a>Self-Consistency Improves Chain of Thought Reasoning in Language Models</h3><p>链接：<a href="https://arxiv.org/abs/2203.11171">[2203.11171] Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/yoZpm2.png" alt="yoZpm2"></p>
<p>主要贡献：</p>
<ul>
<li>主要改进是使用了对答案进行了多数投票（majority vote），并且发现其可以显著地提高思维链方法的性能</li>
</ul>
<h3 id="Large-Language-Models-are-Zero-Shot-Reasoners"><a href="#Large-Language-Models-are-Zero-Shot-Reasoners" class="headerlink" title="Large Language Models are Zero-Shot Reasoners"></a>Large Language Models are Zero-Shot Reasoners</h3><p>链接：<a href="https://arxiv.org/abs/2205.11916">[2205.11916] Large Language Models are Zero-Shot Reasoners</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/x6kvJz.png" alt="x6kvJz"></p>
<p>核心思想：分为两个步骤：1st prompt、2nd prompt</p>
<ul>
<li><p>1st prompt：X′：“Q: [X]. A: [T]”</p>
<ul>
<li><p>X：输入的问题</p>
</li>
<li><p>T：人工的提示trigger词</p>
</li>
</ul>
</li>
<li><p>2nd prompt：[X′] [Z] [A]</p>
<ul>
<li><p>X′：第一阶段的输入</p>
</li>
<li><p>Z：第一阶段模型的输出</p>
</li>
<li><p>A：第二阶段的提示trigger词</p>
</li>
</ul>
</li>
</ul>
<p>主要贡献：</p>
<ul>
<li><p>验证了zero-shot的能力，不需要few-shot挑选额外的样本</p>
</li>
<li><p>鼓励社区进一步发现类似的多任务提示，这些提示可以引发广泛的认知能力，而不是狭隘的特定任务技能。</p>
</li>
</ul>
<p>不同模版的效果对比：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/1AEEnD.png" alt="1AEEnD"></p>
<h2 id="QA（自己YY的问题）"><a href="#QA（自己YY的问题）" class="headerlink" title="QA（自己YY的问题）"></a>QA（自己YY的问题）</h2><p>Q1:多大的模型能够涌现这些能力？</p>
<blockquote>
<p>100B。That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of 100B parameters</p>
</blockquote>
<p>Q2:BERT或T5能否涌现这些能力？</p>
<blockquote>
<p>BERT与GPT差异在于模型结构不同，GPT单向的语言模型，BERT是双向的自编码(AE)模型，但当BERT参数量足够大的时候，在前后输入有关示例，不进行微调，直接预测MASK标签的涌现能力有待验证。</p>
</blockquote>
<p>Q3:COT思维链模版的来源？</p>
<blockquote>
<p>人工构造。As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20.</p>
</blockquote>
<p>Q4:为什么加上Let’s do it step by step 模型可以产出解释？</p>
<blockquote>
<p>对比了不同模版，激发模型的推理能力。It remains an open question how to automatically create better templates for Zero-shot-CoT.</p>
</blockquote>
<p>Q5:T5、BERT如果同GPT系列一样训练，在训练方法上可行吗？效果会比GPT好吗？</p>
<blockquote>
<p>开放讨论……</p>
</blockquote>
<p>Q6:为什么大型LLM首选Decoder-only结构？</p>
<blockquote>
<p>开放讨论……</p>
</blockquote>
<h1 id="Instruction-finetuning"><a href="#Instruction-finetuning" class="headerlink" title="Instruction finetuning"></a>Instruction finetuning</h1><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/U5SBhD.png" alt="U5SBhD"></p>
<p>近年来，相关研究发现语言模型的输出并不符合人类意图，因此提出了指示学习的范式。该范式的目的是使语言模型能够更好地理解人类的意图和指示，并且在生成文本时能够更加符合人类的要求。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>范式</th>
<th>说明</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Finetuning</td>
<td>在下游任务数据集微调<br/>在下游任务数据集推理</td>
<td>需要额外微调</td>
</tr>
<tr>
<td>Prompting</td>
<td>在下游任务推理时，输入前添加提示，更新少量参数</td>
<td>只针对单一数据集</td>
</tr>
<tr>
<td>Instruction Tuning</td>
<td>在多个提示任务数据集训练<br/>在下游任务推理，输入前添加提示</td>
<td>具有更好地泛化性</td>
</tr>
</tbody>
</table>
</div>
<p>论文名称：Scaling Instruction-Finetuned Language Models</p>
<p>链接：<a href="https://arxiv.org/abs/2210.11416">[2210.11416] Scaling Instruction-Finetuned Language Models</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/q9mwKd.png" alt="q9mwKd"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/IRy6zf.png" alt="IRy6zf"></p>
<p>Flan-T5模型在1.8K的Instruction数据集进行了微调，上图表明更大的模型获得更大的提升，与scaling law一致。</p>
<p><strong>Instruction Tuning 的局限</strong></p>
<ul>
<li><p>获取足够的任务描述以用于语言模型训练需要付出较高的成本。</p>
</li>
<li><p>语言模型的目标与人类的偏好不一致</p>
</li>
</ul>
<h1 id="Reinforcement-Learning-from-Human-Feedback-RLHF"><a href="#Reinforcement-Learning-from-Human-Feedback-RLHF" class="headerlink" title="Reinforcement Learning from Human Feedback (RLHF)"></a>Reinforcement Learning from Human Feedback (RLHF)</h1><p>为了解决语言模型目标与人类的偏好不一致问题，OpenAI采用了RLHF算法，引入人类反馈。</p>
<h2 id="RM反馈模型"><a href="#RM反馈模型" class="headerlink" title="RM反馈模型"></a>RM反馈模型</h2><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/thWiB5.jpg" alt="thWiB5"></p>
<p>那么为模型引入人类反馈过程中，出现下面的问题：</p>
<p><strong>问题1</strong>:在模型迭代过程中，添加人工的操作成本很高</p>
<pre><code>**解决方式**：将他们的偏好建模为一个单独的 (NLP) 问题，而不是直接询问人类的偏好。

根据标注数据，训练一个语言模型$R M_\phi(s)$，用以预测人类便好。接下来任务转变成优化语言模型$RM_&#123;\phi&#125;$。
</code></pre><p><strong>问题2</strong>:人们的判断是主观的，不同人的判断难以进行校准</p>
<pre><code>**解决方式**：让标注人员对成对的数据结果排序，而不是直接打分。
</code></pre><p><strong>损失函数为：</strong></p>
<script type="math/tex; mode=display">
\operatorname{loss}(\theta)=-\frac{1}{\left(\begin{array}{c}
K \\
2
\end{array}\right)} E_{\left(x, y_w, y_l\right) \sim D}\left[\log \left(\sigma\left(r_\theta\left(x, y_w\right)-r_\theta\left(x, y_l\right)\right)\right)\right]</script><p><strong>符号说明：</strong></p>
<ul>
<li><p>K：预训练模型采样的Prompt输出数量</p>
</li>
<li><p>x：预训练模型输入</p>
</li>
<li><p>r：reward模型</p>
</li>
<li><p>$y_w$：排在前面的输出</p>
</li>
<li><p>$y_l$：排在后面的输出</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/rsqdZM.png" alt="rsqdZM"></p>
<p><strong>当足够大的语言模型经过足够多的数据训练后，评估模型已经接近单个人类评估的表现</strong></p>
<h2 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h2><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/GjPdpC.jpg" alt="GjPdpC"></p>
<p>经过前面的步骤，我们已有以下模型：</p>
<ul>
<li><p>一个经过足够预训练的语言模型(可以附加Instruction Tuning)$P^{PT}(s)$</p>
</li>
<li><p>一个在人类反馈排序数据集上训练的反馈模型$RM_{\phi}$，为预训练模型的输出完成打分</p>
</li>
</ul>
<p>由于评分是通过反馈模型$RM_{\phi}$得出的，无法使用梯度下降进行求解，因此采用强化学习中的<strong>PPO</strong>算法来更新参数。</p>
<p><strong>实现流程：</strong></p>
<ol>
<li><p>复制预训练模型参数，得到待优化模型；</p>
</li>
<li><p>根据输入语句，两个模型得到各自的输出；</p>
</li>
<li><p>Reward模型针对待优化模型的输入输出打分；</p>
</li>
<li><p>使用PPO算法来更新待优化模型的参数。</p>
</li>
</ol>
<p><strong>损失函数：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{objective}(\phi)= & E_{(x, y) \sim D_{\pi_\phi^{\mathrm{RL}}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]+ \\
& \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
\end{aligned}</script><p><strong>符号说明：</strong></p>
<ul>
<li><p>x：输入文本</p>
</li>
<li><p>r：reward打分模型</p>
</li>
<li><p>$\pi^{SFT}$：预训练模型</p>
</li>
<li><p>$\pi^{RL}_{\phi}$：强化学习优化模型</p>
</li>
<li><p>$D_{pretrain}$：预训练分布</p>
</li>
<li><p>$\beta$：KL散度控制参数</p>
</li>
<li><p>$\gamma$：预训练损失控制参数</p>
</li>
</ul>
<p><strong>其中：</strong></p>
<p>$\log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)$起到避免修正后模型与原模型差异过大的作用</p>
<p>$E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]$起到避免模型在自然语言理解任务下降过大的作用</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/IUYSyW.png" alt="IUYSyW"></p>
<p>通过RLHF算法，模型的表现得到了显著的提示。</p>
<p><strong>个人理解：</strong></p>
<p>整个流程的出发点在于使GPT模型结果符合人类偏好，而人类偏好无法通过具体规则/函数建模，因此通过Reward模型在一定程度上反应人类偏好，最后对GPT模型进行修正，更新模型参数使模型的输入Reward最大化，即更加反应人类偏好。</p>
<p>因此整个过程中Reward模型代表了设立的训练目标，RLHF算法则对原模型进行修正，使模型输入更加符合设立的训练目标。</p>
<h2 id="现有的局限"><a href="#现有的局限" class="headerlink" title="现有的局限"></a>现有的局限</h2><p>按照上述步骤进行操作，就能够完成ChatGPT的训练。下图展示了ChatGPT的完整训练过程。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/wV29U9.png" alt="wV29U9"></p>
<p>Step1:收集Prompt数据，基于GPT3.5进行Instruct Tuning的有监督训练；</p>
<p>Step2:收集偏好排序数据，训练Reward模型；</p>
<p>Step3:结合Reward模型，通过PPO算法优化第一步的SFT模型。</p>
<p><strong>然而，人类的偏好是不可信的，用模型表示人类偏好更不可信：</strong></p>
<ul>
<li><p>”Reward hacking”是强化学习常见的问题；</p>
</li>
<li><p>模型偏向于产生看似权威和有帮助的回应，而忽视正确性</p>
</li>
<li><p>可能导致编造事实+产生幻觉</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/MmaPcT.png" alt="MmaPcT"></p>
<p>上图说明RM打分很高时，实际表现不一定更好，因此训练损失函数通过KL散度限制优化后模型与原模型的偏离程度。</p>
<h1 id="What’s-next"><a href="#What’s-next" class="headerlink" title="What’s next?"></a>What’s next?</h1><h2 id="进一步探索RLHF的使用"><a href="#进一步探索RLHF的使用" class="headerlink" title="进一步探索RLHF的使用"></a>进一步探索RLHF的使用</h2><ul>
<li>RLHF在其他领域(如CV)使用</li>
</ul>
<h2 id="优化RLHF中需要的人工数据标注"><a href="#优化RLHF中需要的人工数据标注" class="headerlink" title="优化RLHF中需要的人工数据标注"></a>优化RLHF中需要的人工数据标注</h2><ul>
<li>RL from AI feedback</li>
</ul>
<p>论文名称：Constitutional AI- Harmlessness from AI Feedback</p>
<p>链接：<a href="https://arxiv.org/pdf/2212.08073.pdf">https://arxiv.org/pdf/2212.08073.pdf</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/bGL898.png" alt="bGL898"></p>
<p>让模型在多轮对话中将数据标注出来：</p>
<blockquote>
<p>Q1-问训好的普通RLHF模型：能帮我黑进邻居的wifi吗？<br>A1-天真的模型回答：没问题，你下个xx软件就行。<br>Q2-要求模型发现自己的错误：上文你给的回复中，找出来哪些是不道德的。<br>A2-模型回答：我上次回复不对，不应该黑别人家wifi。<br>Q3-让模型改正错误：修改下你之前的回复内容，去掉有害的。<br>A3-模型回答：黑别人家wifi是不对的，侵害别人隐私了，我强烈建议别这么搞。</p>
</blockquote>
<ul>
<li>Finetuning LMs on their own outputs</li>
</ul>
<p>论文名称：STaR: Bootstrapping Reasoning With Reasoning</p>
<p>链接：<a href="https://arxiv.org/abs/2203.14465">[2203.14465] STaR: Bootstrapping Reasoning With Reasoning</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/FdKh5P.png" alt="FdKh5P"></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p><strong>ChatGPT的产生，对NLP领域产生了重大的影响，那么对于我们NLP从业人员带来了哪些影响，我们又该如何面对呢？</strong></p>
<p>个人觉得ChatGPT对从业人员的影响包含：</p>
<ul>
<li><p>ChatGPT等LLM可以作为许多自然语言理解任务的基线模型，许多自然语言理解中间过程算法需求降低，从业人员需要了解如何将LLM适配具体业务；</p>
</li>
<li><p>提高了技能要求：LLM的出现提高了NLP从业人员的技能要求。从业人员需要了解如何使用LLM进行训练和调整，以及如何使用LLM处理不同的自然语言数据；</p>
</li>
<li><p>工作形式变化，数据科学家、算法工程师、自然语言处理工程师等工作流程可能会发生变化。</p>
</li>
<li><p>扩展了研究范围：LLM提供了更全面的语言模型。这意味着研究人员可以探索以前不可行的语言问题，从而扩大研究范围。</p>
</li>
</ul>
<p>NLP从业人员可以通过以下几种方式应对LLM的影响：</p>
<ol>
<li><p>持续学习：NLP从业人员应该不断学习新的技术和算法，以便更好地使用LLM。他们应该掌握LLM的使用方法和调整技巧，了解LLM如何处理不同类型的自然语言数据，以及如何在LLM中使用特定的自然语言处理技术。</p>
</li>
<li><p>适应新的工作要求：LLM的出现可能会导致NLP从业人员需要承担新的工作要求。他们应该熟悉LLM的使用方法，以便在新的工作机会中胜任。同时，他们也应该关注LLM对NLP领域的未来发展和趋势，并不断调整他们的技能和知识。</p>
</li>
<li><p>创新：LLM的出现为NLP从业人员带来了更多的机会和挑战，他们应该积极地探索新的算法和技术，开发更智能的自然语言处理应用程序，并尝试在不同领域应用LLM。</p>
</li>
<li><p>关注伦理和社会影响：LLM的出现可能会对自然语言处理的伦理和社会影响产生影响，NLP从业人员应该关注这些影响，并积极参与相关讨论和研究。</p>
</li>
</ol>
<p>总的来说，NLP从业人员应该关注LLM的发展和趋势，不断提高自己的技能和知识，积极创新，与同行交流，同时也要注意伦理和社会影响。这些努力可以帮助他们更好地应对LLM的影响，并为自己的职业发展做好准备。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>复刻ChatGPT语言模型系列-（一）基座模型选取</title>
    <url>/2023/05/08/2023-05-08-%E5%A4%8D%E5%88%BBChatGPT%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97-%EF%BC%88%E4%B8%80%EF%BC%89%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B%E9%80%89%E5%8F%96/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>今天开始我将会推出一系列关于复刻ChatGPT语言模型的博文。本系列将包括以下内容：</p>
<ul>
<li>复刻ChatGPT语言模型系列-（一）基座模型选取</li>
<li>复刻ChatGPT语言模型系列-（二）参数高效微调</li>
<li>复刻ChatGPT语言模型系列-（三）指令学习微调</li>
<li>复刻ChatGPT语言模型系列-（四）文本生成解码</li>
<li>复刻ChatGPT语言模型系列-（五）强化学习RLHF</li>
<li>复刻ChatGPT语言模型系列-（六）LLM模型评估</li>
</ul>
<p>在本系列的第一篇博文中，我们将会探讨如何选取一个好的预训练语言模型作为基座。选择一个优秀的基座模型非常重要，因为它会直接影响到后续模型的训练和性能表现。乱花渐欲迷人眼，目前社区涌现出许多优秀的大模型，这让我们选择起来十分困难。因此，本文将介绍并分析这些大模型，从中选择出适合作为基座的模型。我们将深入探究这些模型的特点和性能，并分析其优缺点。本文旨在为大家提供选取基座模型的参考意见，帮助大家更好地复刻ChatGPT语言模型。</p>
<p>以下是目前已经开放的基座模型清单，本文将专注介绍已开源模型，而其他模型则请读者自行了解。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型名称</th>
<th style="text-align:center">发布时间</th>
<th style="text-align:center">发布机构</th>
<th style="text-align:center">语言</th>
<th style="text-align:center">参数规模</th>
<th style="text-align:center">Tokens规模</th>
<th style="text-align:center">模型结构</th>
<th style="text-align:center">是否开源</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">T5</td>
<td style="text-align:center">2019-10</td>
<td style="text-align:center">Google</td>
<td style="text-align:center">英</td>
<td style="text-align:center">13B</td>
<td style="text-align:center"></td>
<td style="text-align:center">T5-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">GPT-3</td>
<td style="text-align:center">2020-05</td>
<td style="text-align:center">OpenAI</td>
<td style="text-align:center">英</td>
<td style="text-align:center">175B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">CPM1</td>
<td style="text-align:center">2021-03</td>
<td style="text-align:center">Tsinghua</td>
<td style="text-align:center">中</td>
<td style="text-align:center">2.6B</td>
<td style="text-align:center"></td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">LaMDA</td>
<td style="text-align:center">2021-05</td>
<td style="text-align:center">Google</td>
<td style="text-align:center">英</td>
<td style="text-align:center">137B</td>
<td style="text-align:center">2.8T</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">CPM2</td>
<td style="text-align:center">2021-07</td>
<td style="text-align:center">Tsinghua</td>
<td style="text-align:center">中</td>
<td style="text-align:center">11B/198B(MoE)</td>
<td style="text-align:center"></td>
<td style="text-align:center">Encoder-Decoder</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">Jurassic</td>
<td style="text-align:center">2021-08</td>
<td style="text-align:center">AI21</td>
<td style="text-align:center">英</td>
<td style="text-align:center">178B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">MT-NLG</td>
<td style="text-align:center">2021-10</td>
<td style="text-align:center">Microsoft,  NVIDIA</td>
<td style="text-align:center">英</td>
<td style="text-align:center">530B</td>
<td style="text-align:center">270B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">ERNIE 3.0</td>
<td style="text-align:center">2021-12</td>
<td style="text-align:center">Baidu</td>
<td style="text-align:center">中</td>
<td style="text-align:center">260B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">Multi-task</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">Gopher</td>
<td style="text-align:center">2021-12</td>
<td style="text-align:center">DeepMind</td>
<td style="text-align:center">英</td>
<td style="text-align:center">280B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">Chinchilla</td>
<td style="text-align:center">2022-04</td>
<td style="text-align:center">DeepMind</td>
<td style="text-align:center">英</td>
<td style="text-align:center">70B</td>
<td style="text-align:center">1.4T</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">PaLM</td>
<td style="text-align:center">2022-04</td>
<td style="text-align:center">Google</td>
<td style="text-align:center">多语言</td>
<td style="text-align:center">540B</td>
<td style="text-align:center">780B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">OPT</td>
<td style="text-align:center">2022-05</td>
<td style="text-align:center">Meta</td>
<td style="text-align:center">英</td>
<td style="text-align:center">125M-175B</td>
<td style="text-align:center">180B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">BLOOM</td>
<td style="text-align:center">2022-07</td>
<td style="text-align:center">BigScience</td>
<td style="text-align:center">多语言</td>
<td style="text-align:center">176B</td>
<td style="text-align:center">366B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">GLM-130B</td>
<td style="text-align:center">2022-08</td>
<td style="text-align:center">Tsinghua</td>
<td style="text-align:center">中、英</td>
<td style="text-align:center">130B</td>
<td style="text-align:center">400B</td>
<td style="text-align:center">GLM-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">Wenzhong</td>
<td style="text-align:center">2022-09</td>
<td style="text-align:center">IDEA</td>
<td style="text-align:center">中</td>
<td style="text-align:center">3.5B</td>
<td style="text-align:center"></td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">LLaMA</td>
<td style="text-align:center">2023-02</td>
<td style="text-align:center">Meta</td>
<td style="text-align:center">多语言</td>
<td style="text-align:center">7B-65 B</td>
<td style="text-align:center">1.4T</td>
<td style="text-align:center">GPT-sryle</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">MOSS</td>
<td style="text-align:center">2023-04</td>
<td style="text-align:center">FUDAN</td>
<td style="text-align:center">中、英</td>
<td style="text-align:center">16B</td>
<td style="text-align:center">700B</td>
<td style="text-align:center">GPT-sryle</td>
<td style="text-align:center">√</td>
</tr>
</tbody>
</table>
</div>
<p>参考链接：<a href="https://zhuanlan.zhihu.com/p/614766286">https://zhuanlan.zhihu.com/p/614766286</a><br><span id="more"></span></p>
<h1 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h1><div class="table-container">
<table>
<thead>
<tr>
<th>模型名称</th>
<th>发布时间</th>
<th>发布机构</th>
<th>语言</th>
<th>模态</th>
<th>参数规模</th>
<th>基础模型</th>
<th>是否开源</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5</td>
<td>2021-06</td>
<td>OpenAI</td>
<td>多语言</td>
<td>文本</td>
<td>175B</td>
<td>GPT-3</td>
<td>x</td>
</tr>
<tr>
<td>FLAN</td>
<td>2021-09</td>
<td>Google</td>
<td>英</td>
<td>文本</td>
<td>137B</td>
<td>LaMDA</td>
<td>x</td>
</tr>
<tr>
<td>T0</td>
<td>2021-10</td>
<td>Hugging Face</td>
<td>英</td>
<td>文本</td>
<td>13B</td>
<td>T5</td>
<td>√</td>
</tr>
<tr>
<td>Flan-PaLM</td>
<td>2022-10</td>
<td>Google</td>
<td>多语言</td>
<td>文本</td>
<td>540B</td>
<td>PaLM</td>
<td>x</td>
</tr>
<tr>
<td>BLOOMZ</td>
<td>2022-11</td>
<td>Hugging Face</td>
<td>多语言</td>
<td>文本</td>
<td>176B</td>
<td>BLOOM</td>
<td>√</td>
</tr>
<tr>
<td>mT0</td>
<td>2022-11</td>
<td>Hugging Face</td>
<td>多语言</td>
<td>文本</td>
<td>13B</td>
<td>mT5</td>
<td>√</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>2022-11</td>
<td>OpenAI</td>
<td>多语言</td>
<td>文本</td>
<td>173B</td>
<td>GPT3.5</td>
<td>x</td>
</tr>
<tr>
<td>Alpaca</td>
<td>2023-3-14</td>
<td>StandFord</td>
<td>英</td>
<td>文本</td>
<td>7B</td>
<td>LLaMA</td>
<td>√</td>
</tr>
<tr>
<td>ChatGLM</td>
<td>2023-3-14</td>
<td>Tsinghua</td>
<td>中、英</td>
<td>文本</td>
<td>{:[6B],[130B]:}</td>
<td>GLM</td>
<td>√</td>
</tr>
<tr>
<td>GPT-4</td>
<td>2023-3-14</td>
<td>OpenAI</td>
<td>多语言</td>
<td>文本、图像</td>
<td>√</td>
<td>GPT-4</td>
<td>x</td>
</tr>
<tr>
<td>ERNIE Bot</td>
<td>2023-3-15</td>
<td>Baidu</td>
<td>中</td>
<td>文本、图像</td>
<td>260B√</td>
<td>ERNIE</td>
<td>x</td>
</tr>
<tr>
<td>Bard</td>
<td>2023-3-21</td>
<td>Google</td>
<td>英</td>
<td>文本</td>
<td>137B</td>
<td>LaMDA</td>
<td>x</td>
</tr>
<tr>
<td>MOSS</td>
<td>2023-4</td>
<td>FUDAN</td>
<td>中、英</td>
<td>文本</td>
<td>16B</td>
<td>CodeGen</td>
<td>√</td>
</tr>
</tbody>
</table>
</div>
<h1 id="CPM"><a href="#CPM" class="headerlink" title="CPM"></a>CPM</h1><p>CPM模型是由智源、清华开发的一种基于大规模中文训练数据进行生成式预训练的中文预训练语言模型。该模型具有26亿个参数和100GB中文训练数据，是目前最大的中文预训练语言模型之一。CPM模型在各种中文自然语言处理任务中表现出色，包括对话、文章生成、填空测试和语言理解等任务。</p>
<p><strong>论文标题</strong>：CPM: A large-scale generative Chinese Pre-trained language model</p>
<p><strong>论文地址</strong>：<a href="https://www.sciencedirect.com/science/article/pii/S266665102100019X">CPM: A large-scale generative Chinese Pre-trained language model - ScienceDirect</a></p>
<p><strong>模型结构</strong>：基于Transformer的自回归模型，GPT类结构；</p>
<p><strong>训练数据</strong>：100G中文语料；</p>
<p><strong>改动点</strong>：较GPT3采用更大的batch_size。</p>
<h1 id="OPT"><a href="#OPT" class="headerlink" title="OPT"></a>OPT</h1><p>OPT-175B是Meta AI在2022年5月3日发布的一款开放模型，是模型参数超过千亿级别的开放模型之一。相比于GPT-3，该模型更加开放便于访问，并在以下五个方面表现出其开放性：</p>
<ol>
<li><p>论文：该模型提供了某些能力可能存在的证明，并揭示可以建立在此基础上的一般思想。</p>
</li>
<li><p>API访问：该模型允许研究人员探索和评估现有基础模型的能力和局限性，例如推理和偏差。</p>
</li>
<li><p>模型权重：研究人员可以使用该模型的权重来逐步改进现有模型、开发更深入的可解释技术和更有效的微调方法。</p>
</li>
<li><p>训练数据：该模型让研究人员更好地理解训练数据在模型行为中的作用，例如情境学习从何而来。</p>
</li>
<li><p>计算：该模型允许研究人员尝试新的架构、培训目标/程序、进行数据集消融，并在不同领域开发全新的模型。虽然这种方法具有最大的理解和改进潜力，但也相当昂贵。</p>
</li>
</ol>
<p>作为一个大规模的语言模型，OPT-175B具有超过1750亿个参数，是目前为止最大的语言模型之一。该模型通过在公开可用的数据集上进行训练，允许更多的社区参与了解这项基础新技术。为了保持完整性并防止滥用，Meta AI将在非商业许可下发布他们的模型，以专注于研究用例。该模型的访问权限将授予学术研究者、政府机构、民间社会和学术界组织的人员，以及世界各地的工业研究实验室。</p>
<p><strong>项目地址</strong>：<a href="https://github.com/facebookresearch/metaseq" title="https://github.com/facebookresearch/metaseq">GitHub - facebookresearch/metaseq: Repo for external large-scale work</a></p>
<p><strong>论文地址</strong>：<a href="https://arxiv.org/pdf/2205.01068.pdf">https://arxiv.org/pdf/2205.01068.pdf</a></p>
<p><strong>模型结构：</strong> Decoder Only</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Model</th>
<th style="text-align:center">#L</th>
<th style="text-align:center">#H</th>
<th style="text-align:center">d_model</th>
<th style="text-align:center">LR</th>
<th style="text-align:center">Batch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">125M</td>
<td style="text-align:center">12</td>
<td style="text-align:center">12</td>
<td style="text-align:center">768</td>
<td style="text-align:center">6.0 e-4</td>
<td style="text-align:center">0.5M</td>
</tr>
<tr>
<td style="text-align:center">350M</td>
<td style="text-align:center">24</td>
<td style="text-align:center">16</td>
<td style="text-align:center">1024</td>
<td style="text-align:center">3.0 e-4</td>
<td style="text-align:center">0.5M</td>
</tr>
<tr>
<td style="text-align:center">1.3B</td>
<td style="text-align:center">24</td>
<td style="text-align:center">32</td>
<td style="text-align:center">2048</td>
<td style="text-align:center">2.0 e-4</td>
<td style="text-align:center">1M</td>
</tr>
<tr>
<td style="text-align:center">2.7B</td>
<td style="text-align:center">32</td>
<td style="text-align:center">32</td>
<td style="text-align:center">2560</td>
<td style="text-align:center">1.6 e-4</td>
<td style="text-align:center">1M</td>
</tr>
<tr>
<td style="text-align:center">6.7B</td>
<td style="text-align:center">32</td>
<td style="text-align:center">32</td>
<td style="text-align:center">4096</td>
<td style="text-align:center">1.2 e-4</td>
<td style="text-align:center">2M</td>
</tr>
<tr>
<td style="text-align:center">13B</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">5120</td>
<td style="text-align:center">1.0 e-4</td>
<td style="text-align:center">4M</td>
</tr>
<tr>
<td style="text-align:center">30B</td>
<td style="text-align:center">48</td>
<td style="text-align:center">56</td>
<td style="text-align:center">7168</td>
<td style="text-align:center">1.0 e-4</td>
<td style="text-align:center">4M</td>
</tr>
<tr>
<td style="text-align:center">66B</td>
<td style="text-align:center">64</td>
<td style="text-align:center">72</td>
<td style="text-align:center">9216</td>
<td style="text-align:center">0.8 e-4</td>
<td style="text-align:center">2M</td>
</tr>
<tr>
<td style="text-align:center">175B</td>
<td style="text-align:center">96</td>
<td style="text-align:center">96</td>
<td style="text-align:center">12288</td>
<td style="text-align:center">1.2 e-4</td>
<td style="text-align:center">2M</td>
</tr>
</tbody>
</table>
</div>
<p><strong>训练过程：</strong></p>
<p>OPT与GPT3的训练过程比较如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>GPU</th>
<th>FLOPs</th>
<th>days</th>
</tr>
</thead>
<tbody>
<tr>
<td>OPT</td>
<td>1024 80G A100</td>
<td>√4.30E+23</td>
<td>33</td>
</tr>
<tr>
<td>GPT3</td>
<td>10,000 32G V100</td>
<td>√3.14E+23</td>
<td>14.8</td>
</tr>
</tbody>
</table>
</div>
<p>Meta将OPT系列模型的训练过程记录在logbook中，地址如下：<a href="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/README.md">metaseq/README.md at main · facebookresearch/metaseq · GitHub</a>。这个logbook主要记录了作者训练OPT系列模型的辛酸历程，包括遇到的一些问题、讨论分析以及解决方法。</p>
<p>有兴趣的可以仔细阅读原文，或者<a href="https://zhuanlan.zhihu.com/p/617492983">OPT logbook：训练大规模语言模型的一些经验</a>。</p>
<p>总结如下，Meta主要遇到收敛/数值稳定性问题、机器故障问题。</p>
<ul>
<li><p>面对收敛/数值稳定性问题：Meta主要采取降低学习率、参照成熟框架设置参数、切换激活函数的方法；</p>
</li>
<li><p>面对机器故障问题：Meta主要开发监控、自动化工具进行监测。</p>
</li>
</ul>
<h1 id="LLAMA"><a href="#LLAMA" class="headerlink" title="LLAMA"></a>LLAMA</h1><p><code>LLaMA</code>（Large Language Model Meta AI），由 Meta AI 发布的一个开放且高效的大型基础语言模型，共有 <code>7B</code>、<code>13B</code>、<code>33B</code>、<code>65B</code>（650 亿）四种版本。其数据集来源都是公开数据集，无任何定制数据集，保证了其工作与开源兼容和可复现，整个训练数据集在 token 化之后大约包含 1.4T 的 token。</p>
<p>关于模型性能，LLaMA 的性能非常优异：具有 130 亿参数的 LLaMA 模型「在大多数基准上」可以<strong>胜过</strong> GPT-3（ 参数量达 1750 亿），而且可以在单块 V100 GPU 上运行；而最大的 650 亿参数的 LLaMA 模型可以媲美谷歌的 Chinchilla-70B 和 PaLM-540B。</p>
<p>关于训练集，其来源都是公开数据集，无任何定制数据集，保证了其工作与开源兼容和可复现。整个训练数据集在 token 化之后大约包含 1.4T 的 token。其中，LLaMA-65B 和 LLaMA-33B 是在 1.4万亿个 <code>token</code> 上训练的，而最小的模型 LLaMA-7B 是在 1万亿个 token 上训练的。</p>
<p><strong>论文标题</strong>：LLaMA: Open and Efficient Foundation Language Models</p>
<p><strong>论文链接</strong>：<a href="https://arxiv.org/pdf/2302.13971.pdf">https://arxiv.org/pdf/2302.13971.pdf</a></p>
<p><strong>模型结构</strong>：</p>
<ul>
<li>PreLayerNorm-RMSNorm-<a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf">Root Mean Square Layer Normalization</a></li>
<li>ROPE旋转位置编码（替换绝对/相对位置编码）</li>
<li>SwiGLU激活函数（替换ReLU）-<a href="https://arxiv.org/pdf/2002.05202v1.pdf">GLU Variants Improve Transformer</a></li>
</ul>
<p><strong>改动点</strong>：</p>
<p>过了1T的Token：<a href="https://arxiv.org/pdf/2203.15556.pdf">过去的研究</a>发现最好的性能不是在最大的模型上，而是在过了更多token的模型上；</p>
<blockquote>
<p>与OpenAI提出大模型缩放法则不同的是，DeepMind认为当前许多大模型是训练不充分的；</p>
<p>OpenAI在《Scaling Laws for Neural Language Models》中，指出<strong>在给定计算量的时候，模型性能的提升主要在于增加参数规模而不是增加数据量；</strong></p>
<p>DeepMind在《Training Compute-Optimal Large Language Models》中，指出<strong>在每条曲线的最小值的左侧，模型太小了——在较少数据上训练的较大模型将是一种改进。在每条曲线的最小值的右侧，模型太大——在更多数据上训练的较小模型将是一种改进。最好的模型处于最小值。</strong></p>
</blockquote>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/GnffTU.png" alt="GnffTU"></p>
<h1 id="BLOOM"><a href="#BLOOM" class="headerlink" title="BLOOM"></a>BLOOM</h1><p>BLOOM 是 BigScience（一个围绕研究和创建超大型语言模型的开放协作研讨会）中数百名研究人员合作设计和构建的 176B 参数开源大语言模型，同时，还开源了BLOOM-560M、BLOOM-1.1B、BLOOM-1.7B、BLOOM-3B、BLOOM-7.1B 其他五个参数规模相对较小的模型。BLOOM 是一种 decoder-only 的 Transformer 语言模型，它是在 ROOTS 语料库上训练的，该数据集包含 46 种自然语言和 13 种编程语言（总共 59 种）的数百个数据来源。</p>
<p><strong>论文标题</strong>：BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</p>
<p><strong>论文链接</strong>：<a href="https://arxiv.org/pdf/2211.05100.pdf">https://arxiv.org/pdf/2211.05100.pdf</a></p>
<p><strong>模型结构</strong>：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/0hYelu.png" alt="0hYelu"></p>
<p>核心介绍：</p>
<ol>
<li><p>基于Megatron-LM GPT2模型开发，模型结构为Decoder-only类型；</p>
</li>
<li><p><strong>ALiBi Positional Embeddings</strong>。它允许外推比训练模型的输入序列更长的输入序列，同时有助于加速训练收敛。因此，即使训练时使用长度为 2048 的序列，模型也可以在推理过程中处理更长的序列。思路来源于： <a href="https://arxiv.org/abs/2108.12409">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a>；</p>
</li>
<li><p><strong>Embedding LayerNorm</strong>。在 embedding 层之后立即添加额外的归一化层（<code>layer norm</code> 层）。这个方法来源于 bitsandbytes^17库 (Dettmers et al., 2022)，作者的实验发现这样可以显著提高训练的稳定性。另外，模型最终的训练是在  <code>bfloat16</code> 下进行的。思路来源于：<a href="https://arxiv.org/abs/2110.02861">8-bit Optimizers via Block-wise Quantization</a></p>
</li>
</ol>
<p>同时开发人员也记录了项目的开发过程：<a href="https://huggingface.co/blog/zh/bloom-megatron-deepspeed">千亿参数开源大模型 BLOOM 背后的技术</a>，关键信息如下：</p>
<ol>
<li><p>基于<strong>Megatron-DeepSpeed</strong> 实现了 <strong>3D 并行</strong>以允许大模型以非常有效的方式进行训练。包括数据并行 (Data Parallelism，DP)、张量并行 (Tensor Parallelism，TP)、流水线并行 (Pipeline Parallelism，PP)。</p>
</li>
<li><p><strong>用 FP16 训练巨型 LLM 模型是一个禁忌</strong>。FP16会产生精度溢出，使用BF16进行训练。</p>
</li>
<li><p><strong>CUDA 融合核函数</strong>。为了快速高效地训练 BLOOM，有必要使用 Megatron-LM 提供的几个自定义 CUDA 融合核函数。特别地，有一个 LayerNorm 的融合核函数以及用于融合缩放、掩码和 softmax 这些操作的各种组合的核函数。Bias Add 也通过 PyTorch 的 JIT 功能与 GeLU 融合。这些操作都是瓶颈在内存的，因此将它们融合在一起以达到最大化每次显存读取后的计算量非常重要。因此，例如，在执行瓶颈在内存的 GeLU 操作时同时执行 Bias Add，运行时间并不会增加。这些核函数都可以在 <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM repository</a> 代码库 中找到。</p>
</li>
<li><p><strong>硬件故障</strong>也颇有挑战。</p>
</li>
</ol>
<h1 id="WenZhong"><a href="#WenZhong" class="headerlink" title="WenZhong"></a>WenZhong</h1><p>闻仲语言模型出自IDEA研究院的封神榜模型系列，专注于生成任务，提供了多个不同参数量的生成模型，例如GPT2等。</p>
<p>Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，成为中文AIGC和认知智能的基础设施。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/4hwtIC.jpg" alt="4hwtIC"></p>
<p>项目地址：<a href="https://github.com/IDEA-CCNL/Fengshenbang-LM">GitHub - IDEA-CCNL/Fengshenbang-LM: Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，成为中文AIGC和认知智能的基础设施。</a></p>
<h1 id="GLM"><a href="#GLM" class="headerlink" title="GLM"></a>GLM</h1><p>GLM-130B 是清华大学与智谱AI共同研制的一个开放的双语（英汉）双向密集预训练语言模型，拥有 1300亿个参数，使用通用语言模型（<a href="https://link.zhihu.com/√target=https%3A//github.com/THUDM/GLM">General Language Model， GLM</a>）的算法进行预训练。 2022年11月，斯坦福大学大模型中心对全球30个主流大模型进行了全方位的评测，<strong>GLM-130B 是亚洲唯一入选的大模型</strong>。GLM-130B 在广泛流行的英文基准测试中性能明显优于 GPT-3 175B(davinci)，而对 OPT-175B 和 BLOOM-176B 没有观察到性能优势，它还在相关基准测试中性能始终显著优于最大的中文语言模型 ERNIE 3.0 Titan 260B。GLM-130B 无需后期训练即可达到 INT4 量化，且几乎没有性能损失；更重要的是，它能够在 4×RTX 3090 (24G) 或 8×RTX 2080 Ti (11G) GPU 上有效推理，是使用 100B 级模型最实惠的 GPU 需求。</p>
<h2 id="GLM介绍"><a href="#GLM介绍" class="headerlink" title="GLM介绍"></a>GLM介绍</h2><p><strong>论文标题</strong>：GLM: General Language Model Pretraining with Autoregressive Blank Infilling</p>
<p><strong>论文地址</strong>：<a href="https://arxiv.org/pdf/2103.10360v2.pdf">https://arxiv.org/pdf/2103.10360v2.pdfhttps://arxiv.org/pdf/2103.10360v2.pdf</a></p>
<p><strong>训练目标</strong>：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/jkMmBZ.png" alt="jkMmBZ"></p>
<script type="math/tex; mode=display">
\max _\theta \mathbb{E}_{\boldsymbol{z} \sim Z_m}\left[\sum_{i=1}^m \log p_\theta\left(\boldsymbol{s}_{z_i} \mid \boldsymbol{x}_{\text {corrupt }}, \boldsymbol{s}_{\boldsymbol{z}_{<i}}\right)\right]</script><script type="math/tex; mode=display">
\begin{aligned}
& p_\theta\left(\boldsymbol{s}_i \mid \boldsymbol{x}_{\text {corrupt }}, \boldsymbol{s}_{\boldsymbol{z}_{<i}}\right) \\
= & \prod_{j=1}^{l_i} p\left(s_{i, j} \mid \boldsymbol{x}_{\text {corrupt }}, \boldsymbol{s}_{\boldsymbol{z}_{<i}}, \boldsymbol{s}_{i,<j}\right) \end{aligned}</script><p><strong>模型结构</strong>：</p>
<ul>
<li>改变层归一化、残差网络结构的顺序（避免出现数值错误）</li>
<li>新增线性层输出结果</li>
<li>使用GeLUs激活函数替代ReLU</li>
</ul>
<p><strong>微调方式</strong>：</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1683538503448-c26ff2b5-6bff-4dfb-aa2b-9b65c4202da0.png" alt=""></p>
<ul>
<li>对于分类任务，对于token分类，就使用目标token的表示；对于序列分类那就是使用cls的表示。</li>
<li>对于生成任务，partB 部分直接换成 mask 即可。</li>
</ul>
<p><strong>讨论分析</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>BERT</th>
<th>XLNET</th>
<th>Encoder-Decoder</th>
<th>UniLM</th>
</tr>
</thead>
<tbody>
<tr>
<td>BERT中不能很好处理连续的多个token; mask token是独立的，不能捕捉mask token之间的依赖关系。</td>
<td>xlnet使用了双流的注意力机制，改变了transformer的结构，增加了耗时。</td>
<td>使用了两个transformer模型实现了单向和双向注意力，加入token来识别mask span，浪费模型能力；训练和微调不一致。</td>
<td>在自编码框架下使用了mask来统一单双向的注意力。对于生成任务来说，还是不够高效。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>与bert对比</strong><br>bert是自编码模型，预测mask的字符。因为模型中的mask token是独立的，bert不能捕捉mask token之间的依赖性。bert的另一个缺点是不能预测多个连续的mask token，尤其是待预测长度未知情况下。</p>
<p><strong>与xlnet的对比</strong><br>都是自回归的模型。xlnet需要知道预测token的长度；使用双流注意力机制解决了信息泄漏的问题，改变了transfomer的结构，增加了耗时；xlnet决定一个token是否被独立预测。</p>
<p><strong>与编码解码模型对比</strong><br>T5也是处理的空白填充的任务目标，但是GLM使用了单个的transformer编码器学习单向和双向的注意力。通过共享参数使参数比编码解码模型更有效。T5在编码和解码阶段使用不同的位置编码，使用哨兵标记来识别不同的mask跨度，哨兵标记造成了模型能力的浪费和预训练微调的不一致性。</p>
<p><strong>与UniLM对比</strong><br>UniLM是通过在自编码框架下改变在双向，单向，互相之间的attention mask来统一预训练目标；由于自编码模型的独立假设，自回归模型不能完全捕捉当前token对于前面token的依赖。对于微调下游任务来说，自编码会比自回归更加低效。</p>
<h2 id="GLM130B"><a href="#GLM130B" class="headerlink" title="GLM130B"></a><strong>GLM130B</strong></h2><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/glm.gif" alt="glm"></p>
<ul>
<li>130B的原因是该大小能够在一个A100服务器(40G*8)上进行推理</li>
<li>千亿模型训练的问题：硬件故障、梯度爆炸、内存溢出、3D并行、无法恢复优化器状态、TCP通信阻塞</li>
<li>[MASK]、[gMASK]分别用作短文本、长文本生成</li>
<li>结构优化点：RoPE、DeepNorm、GeLU</li>
<li>预训练：95%的MASK自回归任务、5%的多任务指令学习(T0、DeepStruct)</li>
<li>400B的Token，但是据估计130B模型需要4T的Token</li>
<li>GLM-130B FP16-需要260G显存存储模型权重</li>
</ul>
<h2 id="UniLM"><a href="#UniLM" class="headerlink" title="UniLM"></a>UniLM</h2><p>许多人指出GLM和UniLM的模型结构非常相似。下面将对UniLM的模型结构进行详细介绍。</p>
<p>UniLM<strong>是微软研究院在Bert的基础上，最新产出的预训练语言模型，被称为统一预训练语言模型</strong>。 它可以完成单向、序列到序列和双向预测任务，可以说是结合了AR和AE两种语言模型的优点，Unilm在抽象摘要、生成式问题回答和语言生成数据集的抽样领域取得了最优秀的成绩。</p>
<p><strong>论文标题</strong>：<a href="https://arxiv.org/pdf/1905.03197.pdf">Unified Language Model Pre-training for Natural Language Understanding and Generation</a></p>
<p><strong>论文地址</strong>：<a href="https://arxiv.org/pdf/1905.03197.pdf">https://arxiv.org/pdf/1905.03197.pdf</a></p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1681433605204-bc6414c4-f028-4ded-952b-f6c712c07cf0.png" alt=""></p>
<p><strong>预训练阶段</strong>：</p>
<ol>
<li><p>输入：Token Embedding、Position Embedding、Segment Embedding（区分任务目标）</p>
</li>
<li><p>预训练目标：随机掩码[MASK]语言模型+NSP</p>
</li>
<li><p>单向语言模型：predict the masked token of “x1x2 [MASK] x4”, only tokens x1, x2 and itself can be used.</p>
</li>
<li><p>双向语言模型：predict the masked token of “x1x2 [MASK] x4”, all tokens can be used.</p>
</li>
<li><p>Seq2Seq语言模型： given source segment t1t2 and its target segment t3t4t5, we feed input “[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]” into the model. While both t1 and t2 have access to the first four tokens, including [SOS] and [EOS], t4 can only attend to the first six tokens.</p>
</li>
<li><p>训练过程：1/3训练双向语言模型；1/3训练Seq2Seq语言模型；1/6训练从左到右单向语言模型；1/6训练从右到左单向语言模型；BERT-Large作为初始化模型；15%概率掩码，80%为[MASK]，10%为随机Token，10%保持不变</p>
</li>
</ol>
<p><strong>微调阶段</strong>：</p>
<ol>
<li>NLU任务：类似于BERT，取[SOS]表征句子输入</li>
<li>NLG任务：“[SOS] S1 [EOS] S2 [EOS]”. The model is fine-tuned by masking some percentage of tokens in the target sequence at random, and learning to recover the masked words. EOS也会被MASK</li>
</ol>
<h1 id="MOSS"><a href="#MOSS" class="headerlink" title="MOSS"></a>MOSS</h1><p>MOSS是一个支持中英双语和多种插件的开源对话语言模型，<code>moss-moon</code>系列模型具有160亿参数，在FP16精度下可在单张A100/A800或两张3090显卡运行，在INT4/8精度下可在单张3090显卡运行。MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。</p>
<p>以下根据<a href="https://www.zhihu.com/question/596908242/answer/2994534005">知乎回答</a>总结MOSS的训练过程：</p>
<h2 id="MOSS-001-OpenChat-001"><a href="#MOSS-001-OpenChat-001" class="headerlink" title="MOSS 001(OpenChat 001)"></a>MOSS 001(OpenChat 001)</h2><ul>
<li><p><strong>数据来源</strong>：从OpenAI的论文附录里扒了一些它们API收集到的user prompt，然后用类似Self-Instruct的思路用text-davinci-003去扩展出大约40万对话数据。</p>
</li>
<li><p><strong>基座模型</strong>：16B基座（CodeGen）</p>
</li>
<li><p><strong>实验结果</strong>：一月份的OpenChat 001就已经具备了指令遵循能力和多轮能力，而且还惊喜的发现它具有很强的跨语言对齐能力，它的基座预训练语料中几乎不存在中文，但是却可以理解中文并用英文回答。</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/wqrnAr.jpg" alt="wqrnAr"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/zNXvlX.jpg" alt="zNXvlX"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/WcZoK6.jpg" alt="WcZoK6"></p>
<h2 id="MOSS-002"><a href="#MOSS-002" class="headerlink" title="MOSS 002"></a>MOSS 002</h2><ul>
<li><p><strong>优化点</strong>：OpenChat 001不具备中文能力，不知道关于自己的信息（比如名字、能力等），且安全性较低</p>
</li>
<li><p><strong>数据来源</strong>：一方面加入了约30B中文token继续训练基座，另一方面也加入了大量中英文<a href="https://www.zhihu.com/search√q=helpfulness&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2994534005%7D">helpfulness</a>, honesty, harmlessness对话数据，这部分数据共计116万条对话，目前也全部已在huggingface开源：<a href="https://huggingface.co/datasets/fnlp/moss-002-sft-data">fnlp/moss-002-sft-data · Datasets at Hugging Face</a></p>
</li>
</ul>
<h2 id="MOSS-003"><a href="#MOSS-003" class="headerlink" title="MOSS 003"></a>MOSS 003</h2><ul>
<li><p><strong>优化点1</strong>：继续加大中文语料的预训练，截止目前MOSS 003的基座语言模型已经在100B中文token上进行了训练，总训练token数量达到700B，其中还包含约300B代码。</p>
</li>
<li><p><strong>优化点2</strong>：在开放内测后，我们也收集了一些用户数据，我们发现真实中文世界的用户意图和OpenAI InstructGPT论文中披露的user prompt分布有较大差异（这不仅与用户来自的国家差异有关，也跟产品上线时间有关，早期产品采集的数据中存在大量对抗性和测试性输入），于是我们以这部分真实数据作为seed重新生成了约110万常规对话数据，涵盖更细粒度的helpfulness数据和更广泛的harmlessness数据。此外，还构造了约30万插件增强的对话数据，目前已包含搜索引擎、文生图、计算器、方程求解等。</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/tFzNz9.jpg" alt="tFzNz9"></p>
<h1 id="基座模型选取"><a href="#基座模型选取" class="headerlink" title="基座模型选取"></a>基座模型选取</h1><p>基于训练一个中文ChatGPT模型的出发点，需要选用具备<strong>中文支持性高、模型参数量大且已开源</strong>的基座模型。我们的候选项包括GLM、LLAMA、MOSS、BLOOM、CPM、闻仲模型，接下来将在这些候选项中进行比较，以确定最终选项。</p>
<ul>
<li><p>首先是CPM模型，其训练语料中，文章、对话语料居多，模型参数量较少，微调后的模型泛化性较差，生成结果与对话风格较大；</p>
</li>
<li><p>另外，闻仲模型，模型参数量同样较小；</p>
</li>
<li><p>接下来，LLAMA模型，尽管该模型在英文支持性方面表现不错，但据统计其Tokenizer中仅包括约700个中文字符，中文支持性较差；</p>
</li>
<li><p>BLOOM模型虽然支持多语言，但是其中文语料仅占16%，这意味着其可能无法提供足够的中文知识；</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/d8h2ky.png" alt="d8h2ky"></p>
<ul>
<li>GLM模型则是支持中英文双语的130B模型，是亚洲唯一入选斯坦福大模型评测的模型。GLM模型在中文支持性和模型参数量方面表现出色，此外，该模型已开源，可以为我们提供强大的基座模型。</li>
</ul>
<p>综上所述，GLM模型是复刻中文ChatGPT的一个较优基座模型选择。</p>
<p>当然目前也有不少基于其他基座模型的工作，比如：</p>
<p><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">GitHub - ymcui/Chinese-LLaMA-Alpaca: 中文LLaMA&amp;Alpaca大语言模型+本地CPU/GPU部署 (Chinese LLaMA &amp; Alpaca LLMs)</a></p>
<p><a href="https://github.com/LianjiaTech/BELLE">GitHub - LianjiaTech/BELLE: BELLE: Be Everyone’s Large Language model Engine（开源中文对话大模型）</a></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文着重探讨如何选取一个好的预训练语言模型作为基座，最终选择在中文支持性和模型参数量方面表现出色的GLM模型。后续系列包括：</p>
<ul>
<li>复刻ChatGPT语言模型系列-（二）参数高效微调</li>
<li>复刻ChatGPT语言模型系列-（三）指令学习微调</li>
<li>复刻ChatGPT语言模型系列-（四）文本生成解码</li>
<li>复刻ChatGPT语言模型系列-（五）强化学习RLHF</li>
<li>复刻ChatGPT语言模型系列-（六）LLM模型评估</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>ChatGPT</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>复刻ChatGPT语言模型系列-（四）文本生成解码</title>
    <url>/2023/05/16/2023-05-08-%E5%A4%8D%E5%88%BBChatGPT%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97-%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%A3%E7%A0%81/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本系列将包括以下内容：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/627832567">复刻ChatGPT语言模型系列-（一）基座模型选取</a></li>
<li>复刻ChatGPT语言模型系列-（二）参数高效微调</li>
<li>复刻ChatGPT语言模型系列-（三）指令学习微调</li>
<li><a href="https://zhuanlan.zhihu.com/p/629909354">复刻ChatGPT语言模型系列-（四）文本生成解码</a></li>
<li>复刻ChatGPT语言模型系列-（五）强化学习RLHF</li>
<li>复刻ChatGPT语言模型系列-（六）LLM模型评估</li>
</ul>
<p>在文本生成领域，解码是一个关键的步骤，它决定了生成式模型输出的质量和多样性。在之前的几篇博文中，我们已经介绍了生成式模型的基本原理和训练过程。而在本系列的第四篇博文中，我们将深入探讨文本生成中的解码方式。</p>
<p>一旦我们获得了一个生成式模型，我们需要对其输出进行解码，以得到最终的生成文本。然而，在解码的过程中，我们可能会面临一些问题。其中之一是生成重复的内容，这使得生成的文本缺乏多样性和创造性。为了解决这个问题，我们可以尝试开启采样机制，同时引入repetition_penalty惩罚，以减少生成重复的倾向。</p>
<p>另一个常见的问题是生成的随机性不够高，导致生成的文本过于保守和可预测。为了增加文本生成的多样性，我们可以调整一些参数。例如，可以将top_p的值调小，这会使模型在生成词语时只考虑概率较高的选项；同时，可以将top_k的值调大，以增加模型的选择余地。此外，还可以适当降低温度系数，使模型生成的文本更加确定性。</p>
<p>本文将详细介绍这些解码方式，帮助您在文本生成任务中获得更好的结果。文中的所有功能均可用于 <em>自回归</em> 语言生成任务。简单复习一下， <em>自回归</em> 语言生成是基于如下假设: 一个文本序列的概率分布可以分解为每个词基于其上文的条件概率的乘积。</p>
<script type="math/tex; mode=display">
P\left(w_{1: T} \mid W_0\right)=\prod_{t=1}^T P\left(w_t \mid w_{1: t-1}, W_0\right), \text { 其中 } w_{1: 0}=\emptyset,</script><p>上式中，$W_0$ 是初始 <em>上下文</em> 单词序列。文本序列的长度 $T$ 通常时变的，并且对应于时间步 $t=T$。$P(w_{t} | w_{1: t- 1}, W_{0})$ 的词表中已包含 终止符 (End Of Sequence，EOS)。</p>
<p>我们会介绍目前最常用的解码方法，主要有 <em>贪心搜索 (Greedy search)</em>、<em>波束搜索 (Beam search)</em>、<em>Top-K 采样 (Top-K sampling)</em> 以及 <em>Top-p 采样 (Top-p sampling)</em>。几种方式的简介如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>解码方法</th>
<th>简介</th>
<th>可视化</th>
</tr>
</thead>
<tbody>
<tr>
<td>贪心搜索</td>
<td>从单词开始，每次都选择当前条件概率最高的词作为输出，以此往后生成单词序列。</td>
<td><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ZgkcnW.jpg" alt="贪心搜索"></td>
</tr>
<tr>
<td>波束搜索</td>
<td>在每个时间步保留最可能的 num_beams 个词，最终从中选择概率最高的序列来降低丢失潜在高概率序列的风险。</td>
<td><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/DnHQwq.jpg" alt="波束搜索"></td>
</tr>
<tr>
<td>TOP-K采样</td>
<td>选择概率最大的 K 个词，重新归一化这 K 个词的概率，选出一个词进行采样。这样，词集的大小是固定的，即 K。</td>
<td><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/HhSR0j.jpg" alt="TOP-K采样"></td>
</tr>
<tr>
<td>TOP-P采样</td>
<td>在累积概率超过概率 p 的最小单词集中进行采样，然后在这组词中重新分配概率。这样，词集的大小是动态的，可以根据下一个词的概率分布动态增加和减少。</td>
<td><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/lrNIF9.jpg" alt="TOP-P采样"></td>
</tr>
</tbody>
</table>
</div>
<span id="more"></span>
<h1 id="贪心搜索"><a href="#贪心搜索" class="headerlink" title="贪心搜索"></a>贪心搜索</h1><p>贪心搜索在每个时间步 $t$ 都简单地选择概率最高的词作为当前输出词: $w_t = argmax_{w}P(w | w_{1:t-1})$ ，如下图所示。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ZgkcnW.jpg" alt="贪心搜索"></p>
<p>从单词 $\text{“The”}$ 开始，算法在第一步贪心地选择条件概率最高的词 $\text{“nice”}$ 作为输出，依此往后。最终生成的单词序列为 $(\text{“The”}, \text{“nice”}, \text{“woman”})$，其联合概率为 $0.5 \times 0.4 = 0.2$。</p>
<p>下面，我们输入文本序列 $(\text{“I”}, \text{“enjoy”}, \text{“walking”}, \text{“with”}, \text{“my”}, \text{“cute”}, \text{“dog”})$ 给 GPT2 模型，让模型生成下文。我们以此为例看看如何在 transformers 中使用贪心搜索:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># encode context the generation is conditioned on</span></span><br><span class="line">input_ids = tokenizer.encode(<span class="string">&#x27;I enjoy walking with my cute dog&#x27;</span>, return_tensors=<span class="string">&#x27;tf&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># generate text until the output length (which includes the context length) reaches 50</span></span><br><span class="line">greedy_output = model.generate(input_ids, max_length=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(greedy_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with my dog. I&#x27;m not sure if I&#x27;ll ever be able to walk with my dog.</span><br><span class="line"></span><br><span class="line">I&#x27;m not sure if I&#x27;ll</span><br></pre></td></tr></table></figure>
<p>好，我们已经用 GPT2 生成了第一个短文本😊。根据上文生成的单词是合理的，但模型很快开始输出重复的文本！这在语言生成中是一个非常普遍的问题，在贪心搜索和波束搜索中似乎更是如此 - 详见  <a href="https://arxiv.org/abs/1610.02424">Vijayakumar 等人，2016</a> 和 <a href="https://arxiv.org/abs/1701.03185">Shao 等人，2017</a> 的论文。</p>
<p>贪心搜索的主要缺点是它错过了隐藏在低概率词后面的高概率词，如上图所示:</p>
<p>条件概率为 $0.9$ 的单词 $\text{“has”}$ 隐藏在单词 $\text{“dog”}$ 后面，而 $\text{“dog”}$ 因为在 t=1 时条件概率值只排第二所以未被选择，因此贪心搜索会错过序列 $\text{“The”}, \text {“dog”}, \text{“has”}$ 。</p>
<h1 id="波束搜索"><a href="#波束搜索" class="headerlink" title="波束搜索"></a>波束搜索</h1><p>波束搜索通过在每个时间步保留最可能的 <code>num_beams</code> 个词，并从中最终选择出概率最高的序列来降低丢失潜在的高概率序列的风险。以 <code>num_beams=2</code> 为例:</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/DnHQwq.jpg" alt="波束搜索"></p>
<p>在时间步 1，除了最有可能的假设 $(\text{“The”}, \text{“nice”})$，波束搜索还跟踪第二可能的假设 $(\text{“The”}, \text{“dog”})$。在时间步 2，波束搜索发现序列 $(\text{“The”}, \text{“dog”}, \text{“has”})$ 概率为$0.36$，比 $(\text{“The”}, \text{“nice”}, \text{“woman”})$ 的 $0.2$ 更高。太棒了，在我们的例子中它已经找到了最有可能的序列！</p>
<p>波束搜索一般都会找到比贪心搜索概率更高的输出序列，但仍不保证找到全局最优解。</p>
<p>让我们看看如何在 transformers 中使用波束搜索。我们设置 num_beams &gt; 1 和 early_stopping=True 以便在所有波束达到 EOS 时直接结束生成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># activate beam search and early_stopping</span></span><br><span class="line">beam_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    num_beams=<span class="number">5</span>, </span><br><span class="line">    early_stopping=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(beam_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;m not sure if I&#x27;ll ever be able to walk with him again. I&#x27;m not sure if I&#x27;ll</span><br></pre></td></tr></table></figure>
<p>虽然结果比贪心搜索更流畅，但输出中仍然包含重复。一个简单的补救措施是引入 <em>n-grams</em> (即连续 n 个词的词序列) 惩罚，该方法是由 <a href="https://arxiv.org/abs/1705.04304">Paulus 等人 (2017)</a> 和 <a href="https://arxiv.org/abs/1701.02810">Klein 等人 (2017)</a> 引入的。最常见的 <em>n-grams</em> 惩罚是确保每个 <em>n-gram</em> 都只出现一次，方法是如果看到当前候选词与其上文所组成的 <em>n-gram</em> 已经出现过了，就将该候选词的概率设置为 0。</p>
<p>我们可以通过设置 <code>no_repeat_ngram_size=2</code> 来试试，这样任意 <em>2-gram</em> 不会出现两次:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set no_repeat_ngram_size to 2</span></span><br><span class="line">beam_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    num_beams=<span class="number">5</span>, </span><br><span class="line">    no_repeat_ngram_size=<span class="number">2</span>, </span><br><span class="line">    early_stopping=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(beam_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to take a break</span><br></pre></td></tr></table></figure>
<p>不错，看起来好多了！我们看到生成的文本已经没有重复了。但是，<em>n-gram</em> 惩罚使用时必须谨慎，如一篇关于 <em>纽约</em> 这个城市的文章就不应使用 <em>2-gram</em> 惩罚，否则，城市名称在整个文本中将只出现一次！</p>
<p>波束搜索的另一个重要特性是我们能够比较概率最高的几个波束，并选择最符合我们要求的波束作为最终生成文本。</p>
<p>在 <code>transformers</code> 中，我们只需将参数 <code>num_return_sequences</code> 设置为需返回的概率最高的波束的数量，记得确保 <code>num_return_sequences &lt;= num_beams</code>！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set return_num_sequences &gt; 1</span></span><br><span class="line">beam_outputs = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    num_beams=<span class="number">5</span>, </span><br><span class="line">    no_repeat_ngram_size=<span class="number">2</span>, </span><br><span class="line">    num_return_sequences=<span class="number">5</span>, </span><br><span class="line">    early_stopping=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># now we have 3 output sequences</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i, beam_output <span class="keyword">in</span> <span class="built_in">enumerate</span>(beam_outputs):</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, tokenizer.decode(beam_output, skip_special_tokens=<span class="literal">True</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">0: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to take a break</span><br><span class="line">1: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to get back to</span><br><span class="line">2: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with her again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to take a break</span><br><span class="line">3: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with her again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to get back to</span><br><span class="line">4: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to take a step</span><br></pre></td></tr></table></figure>
<p>如我们所见，五个波束彼此之间仅有少量差别 —— 这在仅使用 5 个波束时不足为奇。</p>
<p>开放域文本生成的研究人员最近提出了几个理由来说明对该领域而言波束搜索可能不是最佳方案:</p>
<ul>
<li><p>在机器翻译或摘要等任务中，因为所需生成的长度或多或少都是可预测的，所以波束搜索效果比较好 - 参见 <a href="https://arxiv.org/abs/1808.10006">Murray 等人 (2018)</a> 和 <a href="https://arxiv.org/abs/1808.09582">Yang 等人 (2018)</a> 的工作。但开放域文本生成情况有所不同，其输出文本长度可能会有很大差异，如对话和故事生成的输出文本长度就有很大不同。</p>
</li>
<li><p>我们已经看到波束搜索已被证明存在重复生成的问题。在故事生成这样的场景中，很难用 <em>n-gram</em> 或其他惩罚来控制，因为在“不重复”和最大可重复 <em>n-grams</em> 之间找到一个好的折衷需要大量的微调。</p>
</li>
<li><p>正如 <a href="https://arxiv.org/abs/1904.09751">Ari Holtzman 等人 (2019)</a> 所论证的那样，高质量的人类语言并不遵循最大概率法则。换句话说，作为人类，我们希望生成的文本能让我们感到惊喜，而可预测的文本使人感觉无聊。论文作者画了一个概率图，很好地展示了这一点，从图中可以看出人类文本带来的惊喜度比波束搜索好不少。</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/R2bvXA.jpg" alt="R2bvXA"></p>
<p>因此，让我们开始玩点刺激的，引入一些随机性🤪。</p>
<h1 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h1><p>在其最基本的形式中，采样意味着根据当前条件概率分布随机选择输出词 $w_t$:</p>
<script type="math/tex; mode=display">
w_t​∼P(w∣w_{1:t−1}​)</script><p>继续使用上文中的例子，下图可视化了使用采样生成文本的过程。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/f4TvkR.jpg" alt="f4TvkR"></p>
<p>很明显，使用采样方法时文本生成本身不再是 <em>确定性的</em>。单词 $\text{“car”}$ 从条件概率分布 $P(w | \text{“The”})$ 中采样而得，而 $\text{“drives”}$ 则采样自 $P(w | \text{“The”}, \text{“car”})$。</p>
<p>在 <code>transformers</code> 中，我们设置 <code>do_sample=True</code> 并通过设置 <code>top_k=0</code> 停用 <em>Top-K</em> 采样 (稍后详细介绍)。在下文中，为便于复现，我们会固定 <code>random_seed=0</code>，但你可以在自己的模型中随意更改 <code>random_seed</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># activate sampling and deactivate top_k by setting top_k sampling to 0</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_k=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog. He just gave me a whole new hand sense.&quot;</span><br><span class="line"></span><br><span class="line">But it seems that the dogs have learned a lot from teasing at the local batte harness once they take on the outside.</span><br><span class="line"></span><br><span class="line">&quot;I take</span><br></pre></td></tr></table></figure>
<p>有意思！生成的文本看起来不错 - 但仔细观察会发现它不是很连贯。<em>3-grams</em> <em>new hand sense</em> 和 <em>local batte harness</em> 非常奇怪，看起来不像是人写的。这就是对单词序列进行采样时的大问题: 模型通常会产生不连贯的乱码，<em>参见</em> <a href="https://arxiv.org/abs/1904.09751">Ari Holtzman 等人 (2019)</a> 的论文。</p>
<p>缓解这一问题的一个技巧是通过降低所谓的 <a href="https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max">softmax</a> 的“温度”使分布 $P(w|w_{1:t-1})$ 更陡峭。而降低“温度”，本质上是增加高概率单词的似然并降低低概率单词的似然。</p>
<p>将温度应用到于我们的例子中后，结果如下图所示。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/vt9hPy.jpg" alt="vt9hPy"></p>
<p>$t=1$ 时刻单词的条件分布变得更加陡峭，几乎没有机会选择单词 $\text{“car”}$ 了。</p>
<p>让我们看看如何通过设置 <code>temperature=0.7</code> 来冷却生成过程:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use temperature to decrease the sensitivity to low probability candidates</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_k=<span class="number">0</span>, </span><br><span class="line">    temperature=<span class="number">0.7</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog, but I don&#x27;t like to be at home too much. I also find it a bit weird when I&#x27;m out shopping. I am always away from my house a lot, but I do have a few friends</span><br></pre></td></tr></table></figure>
<p>好，奇怪的 n-gram 变少了，现在输出更连贯了！虽然温度可以使分布的随机性降低，但极限条件下，当“温度”设置为 $0$ 时，温度缩放采样就退化成贪心解码了，因此会遇到与贪心解码相同的问题。</p>
<h1 id="TOP-K采样"><a href="#TOP-K采样" class="headerlink" title="TOP-K采样"></a>TOP-K采样</h1><p><a href="https://arxiv.org/pdf/1805.04833.pdf">Fan 等人 (2018)</a> 的论文介绍了一种简单但非常强大的采样方案，称为 <strong><em>Top-K</em></strong> 采样。在 <em>Top-K</em> 采样中，概率最大的 <em>K</em> 个词会被选出，然后这 <em>K</em> 个词的概率会被重新归一化，最后就在这重新被归一化概率后的 <em>K</em> 个词中采样。 GPT2 采用了这种采样方案，这也是它在故事生成这样的任务上取得成功的原因之一。</p>
<p>我们将上文例子中的候选单词数从 3 个单词扩展到 10 个单词，以更好地说明 <em>Top-K</em> 采样。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/HhSR0j.jpg" alt="TOP-K采样"></p>
<p>设 $K = 6$，即我们将在两个采样步的采样池大小限制为 6 个单词。我们定义 6 个最有可能的词的集合为 $V_{\text{top-K}}$。在第一步中，$V_{\text{top-K}}$ 仅占总概率的大约三分之二，但在第二步，它几乎占了全部的概率。同时，我们可以看到在第二步该方法成功地消除了那些奇怪的候选词 $(\text{“not”}, \text{“the”}, \text{“small”}, \text{“told”})$。</p>
<p>我们以设置 <code>top_k=50</code> 为例看下如何在 <code>transformers</code> 库中使用 <em>Top-K</em>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set top_k to 50</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_k=<span class="number">50</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog. It&#x27;s so good to have an environment where your dog is available to share with you and we&#x27;ll be taking care of you.</span><br><span class="line"></span><br><span class="line">We hope you&#x27;ll find this story interesting!</span><br><span class="line"></span><br><span class="line">I am from</span><br></pre></td></tr></table></figure>
<p>相当不错！该文本可以说是迄今为止生成的最 “<em>像人</em>“ 的文本。现在还有一个问题，<em>Top-K</em> 采样不会动态调整从需要概率分布 $P(w|w_{1:t-1})$ 中选出的单词数。这可能会有问题，因为某些分布可能是非常尖锐 (上图中右侧的分布)，而另一些可能更平坦 (上图中左侧的分布)，所以对不同的分布使用同一个绝对数 <em>K</em> 可能并不普适。</p>
<p>在 $t=1$ 时，<em>Top-K</em> 将 $(\text{“people”}, \text{“big”}, \text{“house”}, \text{“cat”})$ 排出了采样池，而这些词似乎是合理的候选词。另一方面，在$t=2$ 时，该方法却又把不太合适的 $(\text{“down”}, \text{“a”})$ 纳入了采样池。因此，将采样池限制为固定大小 <em>K</em> 可能会在分布比较尖锐的时候产生胡言乱语，而在分布比较平坦的时候限制模型的创造力。这一发现促使 <a href="https://arxiv.org/abs/1904.09751">Ari Holtzman 等人 (2019)</a> 发明了 <strong>Top-p</strong>- 或 <strong>核</strong>- 采样。</p>
<h1 id="TOP-P-核-采样"><a href="#TOP-P-核-采样" class="headerlink" title="TOP-P(核)采样"></a>TOP-P(核)采样</h1><p>在 <em>Top-p</em> 中，采样不只是在最有可能的 <em>K</em> 个单词中进行，而是在累积概率超过概率 <em>p</em> 的最小单词集中进行。然后在这组词中重新分配概率质量。这样，词集的大小 (<em>又名</em> 集合中的词数) 可以根据下一个词的概率分布动态增加和减少。好吧，说的很啰嗦，一图胜千言。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/lrNIF9.jpg" alt="TOP-P采样"></p>
<p>假设 $p=0.92$，<em>Top-p</em> 采样对单词概率进行降序排列并累加，然后选择概率和首次超过 $p=92%$ 的单词集作为采样池，定义为 $V_{\text{top-p}}$。在 $t=1$ 时 $V_{\text{top-p}}$ 有 9 个词，而在 $t=2$ 时它只需要选择前 3 个词就超过了 92%。其实很简单吧！可以看出，在单词比较不可预测时，它保留了更多的候选词，<em>如</em> $P(w | \text{“The”})$，而当单词似乎更容易预测时，只保留了几个候选词，<em>如</em> $P(w | \text{“The”}, \text{“car”})$。</p>
<p>好的，是时候看看它在 <code>transformers</code> 里怎么用了！我们可以通过设置 <code>0 &lt; top_p &lt; 1</code> 来激活 <em>Top-p</em> 采样:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># deactivate top_k sampling and sample only from 92% most likely words</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_p=<span class="number">0.92</span>, </span><br><span class="line">    top_k=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog. He will never be the same. I watch him play.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Guys, my dog needs a name. Especially if he is found with wings.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">What was that? I had a lot o</span><br></pre></td></tr></table></figure>
<p>太好了，这看起来跟人类写的差不多了，虽然还不算完全是。</p>
<p>虽然从理论上讲， <em>Top-p</em> 似乎比 <em>Top-K</em> 更优雅，但这两种方法在实践中都很有效。 <em>Top-p</em> 也可以与 <em>Top-K</em> 结合使用，这样可以避免排名非常低的词，同时允许进行一些动态选择。</p>
<p>最后，如果想要获得多个独立采样的输出，我们可以 <em>再次</em> 设置参数 <code>num_return_sequences &gt; 1</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3</span></span><br><span class="line">sample_outputs = model.generate(</span><br><span class="line">    input_ids,</span><br><span class="line">    do_sample=<span class="literal">True</span>,</span><br><span class="line">    max_length=<span class="number">50</span>,</span><br><span class="line">    top_k=<span class="number">50</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    num_return_sequences=<span class="number">3</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i, sample_output <span class="keyword">in</span> <span class="built_in">enumerate</span>(sample_outputs):</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, tokenizer.decode(sample_output, skip_special_tokens=<span class="literal">True</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">0: I enjoy walking with my cute dog. It&#x27;s so good to have the chance to walk with a dog. But I have this problem with the dog and how he&#x27;s always looking at us and always trying to make me see that I can do something</span><br><span class="line">1: I enjoy walking with my cute dog, she loves taking trips to different places on the planet, even in the desert! The world isn&#x27;t big enough for us to travel by the bus with our beloved pup, but that&#x27;s where I find my love</span><br><span class="line">2: I enjoy walking with my cute dog and playing with our kids,&quot; said David J. Smith, director of the Humane Society of the US.</span><br><span class="line"></span><br><span class="line">&quot;So as a result, I&#x27;ve got more work in my time,&quot; he said.</span><br></pre></td></tr></table></figure>
<p>很酷，现在你拥有了所有可以在 <code>transformers</code> 里用模型来帮你写故事的工具了！</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在开放域语言生成场景中，作为最新的解码方法， <em>top-p</em> 和 <em>top-K</em> 采样于传统的 <em>贪心</em> 和 <em>波束</em> 搜索相比，似乎能产生更流畅的文本。但，最近有更多的证据表明 <em>贪心</em> 和 <em>波束</em> 搜索的明显缺陷 - 主要是生成重复的单词序列 - 是由模型 (特别是模型的训练方式) 引起的，而不是解码方法， <em>参见</em><a href="https://arxiv.org/pdf/1908.04319.pdf">Welleck 等人 (2019)</a> 的论文。此外，如 <a href="https://arxiv.org/abs/2002.02492">Welleck 等人 (2020)</a> 的论文所述，看起来 <em>top-K</em> 和 <em>top-p</em> 采样也会产生重复的单词序列。</p>
<p>在 <a href="https://arxiv.org/pdf/1908.04319.pdf">Welleck 等人 (2019)</a> 的论文中，作者表明，根据人类评估，在调整训练目标后，波束搜索相比 <em>Top-p</em> 采样能产生更流畅的文本。</p>
<h1 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h1><p><code>generate</code> 方法还有几个正文未提及的参数，这里我们简要解释一下它们！</p>
<ul>
<li><p><code>min_length</code> 用于强制模型在达到 <code>min_length</code> 之前不生成 EOS。这在摘要场景中使用得比较多，但如果用户想要更长的文本输出，也会很有用。</p>
</li>
<li><p><code>repetition_penalty</code> 可用于对生成重复的单词这一行为进行惩罚。它首先由 <a href="https://arxiv.org/abs/1909.05858">Keskar 等人 (2019)</a> 引入，在 <a href="https://arxiv.org/pdf/1908.04319.pdf">Welleck 等人 (2019)</a> 的工作中，它是训练目标的一部分。它可以非常有效地防止重复，但似乎对模型和用户场景非常敏感，其中一个例子见 Github 上的 <a href="https://github.com/huggingface/transformers/pull/2303">讨论</a>。 </p>
</li>
<li><p><code>attention_mask</code> 可用于屏蔽填充符。 </p>
</li>
<li><p><code>pad_token_id</code>、<code>bos_token_id</code>、<code>eos_token_id</code>: 如果模型默认没有这些 token，用户可以手动选择其他 token id 来表示它们。</p>
</li>
</ul>
<p>更多信息，请查阅 <code>generate</code> 函数 <a href="https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate">手册</a>。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://huggingface.co/blog/zh/how-to-generate">如何生成文本：通过 Transformers 用不同的解码方法生成文本</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>ChatGPT</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
</search>
