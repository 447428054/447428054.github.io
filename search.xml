<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>中文情感词典的构建</title>
    <url>/2019/02/28/2019-02-28-%E4%B8%AD%E6%96%87%E6%83%85%E6%84%9F%E8%AF%8D%E5%85%B8%E7%9A%84%E6%9E%84%E5%BB%BA/</url>
    <content><![CDATA[<p>本文介绍了情感词典的构建方式，文章主要分为：通用情感词典的构建、通用情感词典的扩展、领域情感词典的构建</p>
<span id="more"></span>
<blockquote>
<p>首先，国外英文的情感分析已经取得了很好的效果，得益于英文单词自身分析的便捷性与英文大量的数据集 WordNet。但由于中文的多变性，语义的多重性与数据集的缺乏，使得国内的情感分析暂落后于国外。本文将记录博主在项目中构建情感词典的经验，欢迎大家指正。<br>我们首先将情感词典分为通用情感词典与专用情感词典。</p>
</blockquote>
<h1 id="1-通用情感词典的构建"><a href="#1-通用情感词典的构建" class="headerlink" title="1.通用情感词典的构建"></a>1.通用情感词典的构建</h1><p>   通用情感词典的构建主要是通过将目前开源的情感词典整合起来，筛去重复和无用的单词。<br>   目前网上开源的情感词典包含有：知网（HowNet）情感词典、台湾大学（NTSUSD)简体中文情感极性词典、大连理工大学情感词汇本体。<br>   前两个都可以在网上找到，第三个需要到其学校官网申请，说明完用途即可获得。</p>
<h1 id="2-通用情感词典的扩展"><a href="#2-通用情感词典的扩展" class="headerlink" title="2.通用情感词典的扩展"></a>2.通用情感词典的扩展</h1><p>上述情感词典年代都已经比较久远，所以我们可以采取一定方法对其扩展。这里我们采用的方法是将词典的同义词添加到词典里。<br>我们通过使用哈工大整理的同义词词林来获取词典的同义词，需要一提的是第一版的同义词林年代较为久远，现在也有哈工大整理的同义词林扩展版。<br>使用的链接在这里：<a href="https://blog.csdn.net/sinat_33741547/article/details/80016713">哈工大同义词林扩展版</a><br>使用代码编写时也可以利用Python的Synonyms库来获取同义词。<br>其已经开源，链接为：<a href="https://github.com/huyingxi/Synonyms">synonyms</a><br>如：</p>
<pre><code>import synonyms
print(&quot;人脸: %s&quot; % (synonyms.nearby(&quot;人脸&quot;)))
print(&quot;识别: %s&quot; % (synonyms.nearby(&quot;识别&quot;)))
</code></pre><h1 id="3-领域情感词典的构建"><a href="#3-领域情感词典的构建" class="headerlink" title="3.领域情感词典的构建"></a>3.领域情感词典的构建</h1><p>构建特定领域的情感词典大体有两种方法：基于规则的情感词典构建方法、基于统计的情感词典构建方法。</p>
<p>基于规则的情感词典方法一般是用句型固定、句式不多变的情况。通过对语料进行句法分析，词性标注等操作，得到语料中常用的句型并将其抽象出来，根据抽象出来的句型模板来对新的语料进行模板匹配，以此来选择出新的语料中的情感词，将其加入到对应的情感词典中。</p>
<p>基于统计的情感词典构建方法需要利用PMI互信息计算与左右熵来发现所需要的新词。具体方法我们可以添加情感种子词，来计算分好词的语料中各个词语与情感种子词的互信息度与左右熵，再将互信息度与左右熵结合起来，选择出与情感词关联度最高的TopN个词语，将其添加到对应的情感词典。<br>这里可以参考链接<a href="https://www.jianshu.com/p/e9313fd692ef">link</a></p>
<h3 id="互信息度计算"><a href="#互信息度计算" class="headerlink" title="互信息度计算"></a>互信息度计算</h3><p><img src="https://img-blog.csdnimg.cn/20190228172006936.png" alt="互信息度计算"></p>
<ul>
<li>p(x,y)为两个词一起出现的概率</li>
<li>p(x)为词x出现的概率</li>
<li>p(y)为词y出现的概率</li>
</ul>
<hr>
<p>具体例子：4G， 上网卡，4G上网卡;如果4G的词频是2,上网卡的词频是10,4G上网卡的词频是1，那么记单单词的总数有N个，双单词的总数有M个，则有下面的公式<br><img src="https://img-blog.csdnimg.cn/20190228172528100.png" alt="具体例子"></p>
<h3 id="左右熵"><a href="#左右熵" class="headerlink" title="左右熵"></a>左右熵</h3><p>我们这里使用左右熵来衡量主要是想表示预选词的自由程度(4G上网卡为一个预选词），左右熵越大，表示这个词的左边右边的词换的越多，那么它就很有可能是一个单独的词。<br>我们这里的左右熵定义为(以左熵为例):<br><img src="https://img-blog.csdnimg.cn/20190228172807236.png" alt="左熵"><br>这里我们还是举一个具体的例子来理解它<br>假设4G上网卡左右有这么几种搭配<br>[买4G上网卡, 有4G上网卡，有4G上网卡， 丢4G上网卡]<br>那么4G上网卡的左熵为<br><img src="https://img-blog.csdnimg.cn/20190228172830315.png" alt="例子"><br>这里A = [买, 有, 丢]</p>
<blockquote>
<p>后面就是具体的实现了，这里的难点就在如何获得这些概率值，就博主看到的用法有：利用搜索引擎获取词汇共现率即p(x,y)、利用语料库获取各个词出现概率</p>
</blockquote>
<h2 id="最后我们只需要将这三步获得的情感词典进行整合就可以了"><a href="#最后我们只需要将这三步获得的情感词典进行整合就可以了" class="headerlink" title="最后我们只需要将这三步获得的情感词典进行整合就可以了"></a>最后我们只需要将这三步获得的情感词典进行整合就可以了</h2><p>参考文献：<br><a href="https://www.jianshu.com/p/e9313fd692ef">python3实现互信息和左右熵的新词发现</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>情感分析</category>
      </categories>
      <tags>
        <tag>情感分析</tag>
        <tag>情感词典</tag>
      </tags>
  </entry>
  <entry>
    <title>Coursera Machine Learning 学习笔记（二）Linear Regression</title>
    <url>/2019/03/02/2019-03-02-Coursera%20Machine%20Learning%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89Linear%20Regression/</url>
    <content><![CDATA[<p>文章为博主学习Coursera上的Machine Learning课程的笔记，Coursera Machine Learning 学习笔记（二）Linear Regression</p>
<span id="more"></span>
<blockquote>
<p>文章为博主学习Coursera上的Machine Learning课程的笔记，来记录自己的学习过程，欢迎大家一起学习交流</p>
</blockquote>
<h1 id="02：Linear-Regression"><a href="#02：Linear-Regression" class="headerlink" title="02：Linear Regression"></a>02：Linear Regression</h1><p>仍然以房价预测作为示例，具体示例仍需见课程内容。<br>符号含义：</p>
<ol>
<li>m 为数据集的大小</li>
<li>x’s为输入数据</li>
<li>y’s为对应的目标输出结果</li>
<li>(x,y)为所有训练数据</li>
<li>(x<sup>i</sup>, y<sup>i</sup>)为具体第i行数据，第i个训练数据</li>
</ol>
<p>假设函数h(x)，以一元线性回归为例：<br><img src="https://img-blog.csdnimg.cn/20190302112701595.png" alt="假设函数"></p>
<script type="math/tex; mode=display">\theta_0：截距  \theta_1：梯度</script><h5 id="Linear-regression-implementation（损失函数cost-function）"><a href="#Linear-regression-implementation（损失函数cost-function）" class="headerlink" title="Linear regression - implementation（损失函数cost function）"></a>Linear regression - implementation（损失函数cost function）</h5><p>计算由不同θ 取值带来的不同损失函数值，本质上是一个最小化问题：使下式取值最小</p>
<script type="math/tex; mode=display">Minimize ：(h_\theta(x)-y)^2</script><p>即可以看成下述式子：</p>
<script type="math/tex; mode=display">J(\theta_0,\theta_1) = \frac 1 {2m}\sum_1^m(h_\theta(x^{(i)})-y^{(i)})^2</script><script type="math/tex; mode=display">\frac1 m 是求平均\frac1 {2m}是为了数学计算方便</script><p>这个损失函数是均方误差，适用于多类回归问题，当然也可以有其他的损失函数。</p>
<h5 id="梯度下降算法（Gradient-descent-algorithm）"><a href="#梯度下降算法（Gradient-descent-algorithm）" class="headerlink" title="梯度下降算法（Gradient descent algorithm）"></a>梯度下降算法（Gradient descent algorithm）</h5><p><strong>目的：</strong> 使损失函数J最小</p>
<h6 id="工作方式"><a href="#工作方式" class="headerlink" title="工作方式"></a>工作方式</h6><ul>
<li>从随机初始化开始</li>
</ul>
<ol>
<li>对θ 随机赋值，可以为任何值</li>
<li>每次一点点改变θ，使J(θ)减小</li>
</ol>
<ul>
<li>每次沿着梯度下降最大的方向</li>
<li>重复上述操作直到达到局部最小值</li>
<li>从哪里开始的可能决定你到达哪个局部最优解<br><img src="https://img-blog.csdnimg.cn/2019030217133153.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjc2MDMz,size_16,color_FFFFFF,t_70" alt="梯度下降"><br><strong>一个更正规的定义：</strong><br>做下述操作直到收敛：<br><img src="https://img-blog.csdnimg.cn/20190302171951356.png" alt="参数更新"><br>符号解释：</li>
</ul>
<ol>
<li><strong>:=</strong><br>表示赋值<br>NB a = b 是一个正确的断言</li>
<li>α (alpha)<br>学习率，控制参数更新的步长<ol>
<li>如果学习率过大，可能无法得到最优解，误差会增大</li>
<li>如果学习率过小，那么达到最优解需要非常多步，耗费很长时间</li>
</ol>
</li>
</ol>
<p>注： 参数的更新必须<strong>同步</strong>即需要一个中间变量保存之前的值，原因是二者的式子中包含了对方，一方的更新会导致第二方式子内值的变化。<br><img src="https://img-blog.csdnimg.cn/20190302173355106.png" alt="在这里插入图片描述"><br><strong>当我们达到局部最优解时：</strong></p>
<ol>
<li>后面部分的梯度为0</li>
<li>各参数值就保持不变了 </li>
</ol>
<h5 id="使用梯度下降的线性回归算法"><a href="#使用梯度下降的线性回归算法" class="headerlink" title="使用梯度下降的线性回归算法"></a>使用梯度下降的线性回归算法</h5><ul>
<li>将梯度下降算法应用到最小化损失函数J(θ)上</li>
</ul>
<script type="math/tex; mode=display">\frac{\partial} {\partial\theta_j}=\frac 1 {2m}\sum_1^m(h_\theta(x^{(i)})-y^{(i)})^2=\frac{\partial} {\partial\theta_j}\frac 1 {2m}\sum_1^m(\theta_0+\theta_1x^{(i)}-y^{(i)})^2</script><p>按照求导公式可以推出：</p>
<script type="math/tex; mode=display">j=0:\frac{\partial} {\partial\theta_0}J(\theta_0,\theta_1)=\frac1 m\sum_1^m(h_\theta(x^{(i)})-y^{(i)})</script><script type="math/tex; mode=display">j=1:\frac{\partial} {\partial\theta_1}J(\theta_0,\theta_1)=\frac1 m\sum_1^m(h_\theta(x^{(i)})-y^{(i)})*x^{(i)}</script><p>注：因为线性回归是一个凸函数，是一个碗形的图，所以会趋于局部最优解</p>
<ul>
<li>现在的这种梯度下降算法又叫Batch Gradient Descent 原因是每一次都遍历了整个数据集，后面会提到取数据集中的部分进行的Gradient Descent</li>
<li>线性回归也有正规方程求解，但其中矩阵运算当数据集过大时不宜使用，这时就可以使用梯度下降</li>
</ul>
<h5 id="数值求解的正规方程法"><a href="#数值求解的正规方程法" class="headerlink" title="数值求解的正规方程法"></a>数值求解的正规方程法</h5><ul>
<li>直接通过数值求解来避免繁琐的迭代过程，从数学上求解出min(J(θ))<br><strong>正规方程的优缺点：</strong><br><strong>优点：</strong><br>1.不需要学习率这个参数<br>2.对某些问题可以很快的解决<br><strong>缺点：</strong><br>会很复杂</li>
</ul>
<h5 id="面对数据量很大的时候"><a href="#面对数据量很大的时候" class="headerlink" title="面对数据量很大的时候"></a>面对数据量很大的时候</h5><p>这个时候就需要<strong>将数据向量化</strong>利用线性代数中矩阵运算来完成计算</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Coursera Machine Learning 学习笔记（一）Introduction</title>
    <url>/2019/03/02/2019-03-02-Coursera%20Machine%20Learning%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89Introduction/</url>
    <content><![CDATA[<p>文章为博主学习Coursera上的Machine Learning课程的笔记，Coursera Machine Learning 学习笔记（一）Introduction</p>
<span id="more"></span>
<blockquote>
<p>文章为博主学习Coursera上的Machine Learning课程的笔记，来记录自己的学习过程，欢迎大家一起学习交流</p>
</blockquote>
<h1 id="01-Introduction"><a href="#01-Introduction" class="headerlink" title="01:Introduction"></a>01:Introduction</h1><h2 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><ul>
<li><p>Arthur Samuel(1959)<br>  <strong>Machine Learning:</strong>“Field of study that gives computers the ability to learn without being explicitly programmed”</p>
</li>
<li><p>Tom Michel(1999)<br>  <strong>Well posed learning problem:</strong>“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”<br>  在下棋的例子中：</p>
</li>
</ul>
<ol>
<li>E经验为1000场下棋游戏</li>
<li>T任务为下棋</li>
<li>P评价准则为是否获胜</li>
</ol>
<ul>
<li><p>学习算法的类型<br>  。 有监督学习（Supervised learning）</p>
<pre><code>      Teach the computer how to do something, then let it use it;s new found knowledge to do it

      注：一般是有标注的训练集
</code></pre><p>  。无监督学习（Unsupervised learning）<br>  Let the computer learn how to do something, and use this to determine structure and patterns in data</p>
<pre><code>      注：一般是无标注训练集，使机器从中提取特征，常见算法为聚类
</code></pre><p>  。强化学习（Reinforcement learning）</p>
<p>  。推荐系统（Recommender systems）</p>
</li>
</ul>
<h3 id="有监督学习介绍"><a href="#有监督学习介绍" class="headerlink" title="有监督学习介绍"></a>有监督学习介绍</h3><h5 id="问题分类："><a href="#问题分类：" class="headerlink" title="问题分类："></a>问题分类：</h5><ul>
<li>预测问题<br>课程内拿房价预测作为示例：具体可以看课程内容<br>预测问题也叫回归问题，具有以下特征：</li>
</ul>
<ol>
<li>预测连续的输出</li>
<li>没有明显得离散划分</li>
</ol>
<ul>
<li>分类问题<br>课程内以肿瘤划分作为示例</li>
</ul>
<h3 id="无监督学习介绍"><a href="#无监督学习介绍" class="headerlink" title="无监督学习介绍"></a>无监督学习介绍</h3><p>在无监督学习里我们获得的是没有标注的数据，将这些数据划分成不同的数据簇</p>
<h5 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h5><p>具体应用示例：</p>
<ol>
<li>新闻划分</li>
<li>基因组排序</li>
<li>分布式计算机集群划分</li>
<li>社交网络划分</li>
<li>天文数据分析</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Introduction</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据分析-数据可视化</title>
    <url>/2020/01/03/2020-01-03-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<p>本文将介绍使用Matplotlib工具，完成Python数据分析-数据可视化</p>
<span id="more"></span>
<h1 id="一-Matplotlib-基本概念"><a href="#一-Matplotlib-基本概念" class="headerlink" title="一. Matplotlib 基本概念"></a>一. Matplotlib 基本概念</h1><p>Matplotlib是python的一个数据可视化工具库。</p>
<p>特点：专门用于开发2D图表(包括3D图表)， 操作简单。</p>
<p>可视化是在整个数据挖掘的关键辅助工具，可以清晰的理解数据，从而调整我们的分析方法。</p>
<h1 id="二-Matplotlib三层结构"><a href="#二-Matplotlib三层结构" class="headerlink" title="二. Matplotlib三层结构"></a>二. Matplotlib三层结构</h1><p><img src="https://img-blog.csdnimg.cn/20190313235406342.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="三层结构"></p>
<h1 id="三-Matplotlib-基本使用"><a href="#三-Matplotlib-基本使用" class="headerlink" title="三. Matplotlib 基本使用"></a>三. Matplotlib 基本使用</h1><h2 id="1-折线图"><a href="#1-折线图" class="headerlink" title="1. 折线图"></a>1. 折线图</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"># 图形显示设置</span><br><span class="line">%matplotlib inline   </span><br><span class="line"> </span><br><span class="line"># 绘制画布-容器层  figsize: 画布长宽属性   dpi: 图象的清晰度</span><br><span class="line">plt.figure(figsize=(16,8), dpi=60)</span><br><span class="line"> </span><br><span class="line"># 绘制折线图-图象层</span><br><span class="line">plt.plot([1,2,3,4,5,6], [22,19,18,25,27,19])</span><br><span class="line"> </span><br><span class="line"># 显示图象</span><br><span class="line"># plt.show()</span><br><span class="line"> </span><br><span class="line"># 保存图象 -注：plt.show()会释放figure资源，保存图片需要将plt.show()注释掉</span><br><span class="line"># 图片的保存路径 -- </span><br><span class="line">plt.savefig(&quot;plot.png&quot;)</span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdn.net/20180917191105231?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"></p>
<h2 id="2-绘制多条折线图"><a href="#2-绘制多条折线图" class="headerlink" title="2. 绘制多条折线图"></a>2. 绘制多条折线图</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 中文显示问题-- 下载中文字体，安装字体-修改配置文件下面手动修改配置</span><br><span class="line"># from pylab import mpl</span><br><span class="line"># mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line"># mpl.rcParams[&quot;axes.unicode_minus&quot;] = False # 解决保存图像是负号&#x27;-&#x27;显示为方块的问题</span><br><span class="line"> </span><br><span class="line"># 准备数据</span><br><span class="line">x = range(60)</span><br><span class="line">y_sh = [random.uniform(26,31) for i in x]</span><br><span class="line">y_bj = [random.uniform(27, 35) for i in x]</span><br><span class="line"> </span><br><span class="line"># 创建画布</span><br><span class="line">plt.figure(figsize=(16,8), dpi=60)</span><br><span class="line"> </span><br><span class="line"># 同一坐标内--绘制多条折线图  （新增）</span><br><span class="line">plt.plot(x, y_sh, label=&quot;sh&quot;)</span><br><span class="line">plt.plot(x, y_bj, label=&quot;bj&quot;, linestyle=&quot;--&quot;, color=&quot;y&quot;)  # 线条颜色，线条样式设置 见下图</span><br><span class="line"> </span><br><span class="line"># 自定义x, y轴 刻度 &amp; 刻度标签 (新增)</span><br><span class="line">x_ticks = range(0, 60, 5)</span><br><span class="line">y_ticks = range(20, 40, 5)</span><br><span class="line"> </span><br><span class="line">x_ticks_label = [&quot;11点&#123;&#125;分&quot;.format(i) for i in x_ticks]</span><br><span class="line"> </span><br><span class="line">plt.xticks(x_ticks, x_ticks_label)</span><br><span class="line">plt.yticks(y_ticks)</span><br><span class="line"> </span><br><span class="line"># 添加辅助描述信息-- x，y轴标签 &amp; 图形标题</span><br><span class="line">plt.xlabel(&quot;时间&quot;)</span><br><span class="line">plt.ylabel(&quot;温度&quot;)</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;两地同一时间温度变化图&quot;)</span><br><span class="line"> </span><br><span class="line"># 添加网格线 - alpha:透明度   （新增）</span><br><span class="line">plt.grid(True, linestyle=&quot;--&quot;, alpha=0.6)</span><br><span class="line"> </span><br><span class="line"># 显示图例 -- loc：位置设置,详见下图  （新增）</span><br><span class="line">plt.legend(loc=&quot;best&quot;)</span><br><span class="line"> </span><br><span class="line"># 显示图象</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdn.net/20180917235044993?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br><strong>附参数表:</strong><br><img src="https://img-blog.csdn.net/20180917235125313?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190310015842874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="3-绘制多个坐标系-—-plt-subplots"><a href="#3-绘制多个坐标系-—-plt-subplots" class="headerlink" title="3. 绘制多个坐标系 — plt.subplots"></a>3. 绘制多个坐标系 — plt.subplots</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 中文显示问题--下面手动修改配置 </span><br><span class="line">from pylab import mpl</span><br><span class="line">mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line">mpl.rcParams[&quot;axes.unicode_minus&quot;] = False</span><br><span class="line"> </span><br><span class="line"># 准备x,y轴数据</span><br><span class="line">x = range(60)</span><br><span class="line">y_sh = [random.uniform(15, 18) for i in x ]</span><br><span class="line">y_bj = [random.uniform(5, 12) for i in x ]</span><br><span class="line"> </span><br><span class="line"># 创建画布--多个坐标轴, 绘制折线图</span><br><span class="line">fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), dpi=100)</span><br><span class="line"># Returns:  fig: 图对象    ax: 坐标轴对象列表</span><br><span class="line">axes[0].plot(x, y_sh, label=&#x27;上海&#x27;)</span><br><span class="line">axes[1].plot(x, y_bj, label=&#x27;北京&#x27;, color=&#x27;r&#x27;, linestyle=&#x27;--&#x27;)</span><br><span class="line"> </span><br><span class="line"># 显示图例/坐标轴刻度/网格线</span><br><span class="line">axes[0].legend()</span><br><span class="line">axes[1].legend()</span><br><span class="line"> </span><br><span class="line">x_ticks_label = [&#x27;11点&#123;&#125;分&#x27;.format(i) for i in x]</span><br><span class="line">y_ticks = range(40)</span><br><span class="line">axes[0].set_xticks(x[::5], x_ticks_label[::5])</span><br><span class="line">axes[0].set_yticks(y_ticks[::5])</span><br><span class="line">axes[1].set_xticks(x[::5], x_ticks_label[::5])</span><br><span class="line">axes[1].set_yticks(y_ticks[::5])</span><br><span class="line"> </span><br><span class="line">axes[0].grid(True, linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line">axes[1].grid(True, linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line"> </span><br><span class="line"># 添加 标题/坐标轴描述信息</span><br><span class="line">axes[0].set_title(&#x27;上海11点0分到12点之间的温度变化图&#x27;)</span><br><span class="line">axes[0].set_xlabel(&quot;时间&quot;)</span><br><span class="line">axes[0].set_ylabel(&quot;温度&quot;)</span><br><span class="line"> </span><br><span class="line">axes[1].set_title(&#x27;北京11点0分到12点之间的温度变化图&#x27;)</span><br><span class="line">axes[1].set_xlabel(&#x27;时间&#x27;)</span><br><span class="line">axes[1].set_ylabel(&#x27;温度&#x27;)</span><br><span class="line"> </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="4-绘制sin-函数图像-—-plot"><a href="#4-绘制sin-函数图像-—-plot" class="headerlink" title="4. 绘制sin()函数图像 — plot"></a>4. 绘制sin()函数图像 — plot</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">import numpy as np</span><br><span class="line">x = np.linspace(-10, 10, 1000)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制图像，显示图像</span><br><span class="line">plt.figure(figsize=(10, 1), dpi=100)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.grid(linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2019031201165850.png" alt="在这里插入图片描述"></p>
<h2 id="5-散点图-—-scatter"><a href="#5-散点图-—-scatter" class="headerlink" title="5. 散点图 — scatter"></a>5. 散点图 — scatter</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># -- 案例： 探究房屋面积和房屋价格的关系</span><br><span class="line">from pylab import mpl          # 中文显示问题--下面手动修改配置 </span><br><span class="line">mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line">mpl.rcParams[&quot;axes.unicode_minus&quot;] = False</span><br><span class="line"># 准备数据</span><br><span class="line">x = [225.98, 247.07, 253.14, 457.85, 241.58, 301.01,  20.67, 288.64,</span><br><span class="line">       163.56, 120.06, 207.83, 342.75, 147.9 ,  53.06, 224.72,  29.51,</span><br><span class="line">        21.61, 483.21, 245.25, 399.25, 343.35]</span><br><span class="line">y = [196.63, 203.88, 210.75, 372.74, 202.41, 247.61,  24.9 , 239.34,</span><br><span class="line">       140.32, 104.15, 176.84, 288.23, 128.79,  49.64, 191.74,  33.1 ,</span><br><span class="line">        30.74, 400.02, 205.35, 330.64, 283.45]</span><br><span class="line"> </span><br><span class="line"># 创建画布  -- 绘制散点图 -- 显示图像</span><br><span class="line">plt.figure(figsize=(20,8), dpi=100)</span><br><span class="line">plt.scatter(x, y)</span><br><span class="line"> </span><br><span class="line">plt.title(&#x27;房屋面积和房屋价格的关系--案例测试&#x27;)</span><br><span class="line">plt.xlabel(&#x27;面积&#x27;)</span><br><span class="line">plt.ylabel(&#x27;价格&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190312012238645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="6-柱状图-—-bar"><a href="#6-柱状图-—-bar" class="headerlink" title="6. 柱状图 — bar"></a>6. 柱状图 — bar</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;东方快车谋杀案&#x27;,&#x27;寻梦环游记&#x27;,&#x27;全球风暴&#x27;,&#x27;降魔传&#x27;,&#x27;追捕&#x27;,&#x27;七十七天&#x27;,&#x27;密战&#x27;,&#x27;狂兽&#x27;,&#x27;其它&#x27;]</span><br><span class="line">y = [73853,57767,22354,15969,14839,8725,8716,8318,7916,6764,52222]</span><br><span class="line">x = range(len(movie_name))</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制柱状图，添加标题和格线，显示图像</span><br><span class="line">plt.figure(figsize=(18, 6), dpi=80)</span><br><span class="line">plt.bar(x, y, width=0.5, color=[&#x27;b&#x27;,&#x27;r&#x27;,&#x27;g&#x27;,&#x27;y&#x27;,&#x27;c&#x27;,&#x27;m&#x27;,&#x27;y&#x27;,&#x27;k&#x27;,&#x27;c&#x27;,&#x27;g&#x27;,&#x27;m&#x27;])</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影票房收入对比&quot;)</span><br><span class="line">plt.xticks(x, movie_name)</span><br><span class="line">plt.grid(linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190312014545814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="7-柱状图-—-多个指标对比"><a href="#7-柱状图-—-多个指标对比" class="headerlink" title="7. 柱状图 — 多个指标对比"></a>7. 柱状图 — 多个指标对比</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;寻梦环游记&#x27;]</span><br><span class="line">first_day = [10587.6,10062.5,1275.7]</span><br><span class="line">first_weekend=[36224.9,34479.6,11830]</span><br><span class="line">x = range(len(movie_name))</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制柱状图，添加标题/坐标轴刻度标签/网格线/示例， 显示图像</span><br><span class="line">plt.figure(figsize=(10, 5), dpi=80)</span><br><span class="line">plt.bar(x, first_day, width=0.2, label=&quot;首日票房&quot;)</span><br><span class="line">plt.bar([i+0.2 for i in x], first_weekend, width=0.2, label=&quot;首周票房&quot;)</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影首日和首周的票房对比&quot;)</span><br><span class="line">plt.xticks([i+0.1 for i in x], movie_name)     # 修改x轴刻度显示</span><br><span class="line">plt.grid(linestyle=&quot;--&quot;, alpha=0.5)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190312020011216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="8-直方图-—-hist"><a href="#8-直方图-—-hist" class="headerlink" title="8 直方图 — hist"></a>8 直方图 — hist</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">time = [131,  98, 125, 131, 124, 139, 131, 117, 128, 108, 135, 138, 131, 102, 107, 114, 119, 128, 121, 142, 127, 130, 124, 101, 110, 116, 117, 110, 128, 128, 115,  99, 136, 126, 134,  95, 138, 117, 111,78, 132, 124, 113, 150, 110, 117,  86,  95, 144, 105, 126, 130,126, 130, 126, 116, 123, 106, 112, 138, 123,  86, 101,  99, 136,123, 117, 119, 105, 137, 123, 128, 125, 104, 109, 134, 125, 127,105, 120, 107, 129, 116, 108, 132, 103, 136, 118, 102, 120, 114,105, 115, 132, 145, 119, 121, 112, 139, 125, 138, 109, 132, 134,156, 106, 117, 127, 144, 139, 139, 119, 140,  83, 110, 102,123,107, 143, 115, 136, 118, 139, 123, 112, 118, 125, 109, 119, 133,112, 114, 122, 109, 106, 123, 116, 131, 127, 115, 118, 112, 135,115, 146, 137, 116, 103, 144,  83, 123, 111, 110, 111, 100, 154,136, 100, 118, 119, 133, 134, 106, 129, 126, 110, 111, 109, 141,120, 117, 106, 149, 122, 122, 110, 118, 127, 121, 114, 125, 126,114, 140, 103, 130, 141, 117, 106, 114, 121, 114, 133, 137,  92,121, 112, 146,  97, 137, 105,  98, 117, 112,  81,  97, 139, 113,134, 106, 144, 110, 137, 137, 111, 104, 117, 100, 111, 101, 110,105, 129, 137, 112, 120, 113, 133, 112,  83,  94, 146, 133, 101,131, 116, 111,  84, 137, 115, 122, 106, 144, 109, 123, 116, 111,111, 133, 150]</span><br><span class="line"> </span><br><span class="line"># 创建画布， 绘制直方图， 添加标题/坐标轴刻度标签/网格线</span><br><span class="line">plt.figure(figsize=(20, 8), dpi=80)</span><br><span class="line">distance = 2</span><br><span class="line">group_num = int((max(time)-min(time)) / distance)</span><br><span class="line">plt.hist(time, bins=group_num)</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影时长分布状况&quot;)</span><br><span class="line">plt.xticks(range(min(time), max(time))[::2])</span><br><span class="line">plt.xlabel(&quot;电影时长&quot;)</span><br><span class="line">plt.ylabel(&quot;数量&quot;)</span><br><span class="line">plt.grid(linestyle=&quot;--&quot;, alpha=0.5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190313233637810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="9-饼图-—-pie"><a href="#9-饼图-—-pie" class="headerlink" title="9 饼图 — pie"></a>9 饼图 — pie</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据   -- 案例：电影的排片占比</span><br><span class="line">movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;东方快车谋杀案&#x27;,&#x27;寻梦环游记&#x27;,&#x27;全球风暴&#x27;,&#x27;降魔传&#x27;,&#x27;追捕&#x27;,&#x27;七十七天&#x27;,&#x27;密战&#x27;,&#x27;狂兽&#x27;,&#x27;其它&#x27;]</span><br><span class="line">place_count = [60605,54546,45819,28243,13270,9945,7679,6799,6101,4621,20105]</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制饼图，添加标题/坐标轴刻度标签</span><br><span class="line">plt.figure(figsize=(18, 6), dpi=80)</span><br><span class="line">plt.pie(place_count, labels=movie_name, autopct=&quot;%1.2f%%&quot;)</span><br><span class="line">plt.axis(&quot;equal&quot;)          #  坐标轴长宽相等，保证饼图成圆形</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影的排片占比&quot;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190313235128167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="10-VENN-韦恩图"><a href="#10-VENN-韦恩图" class="headerlink" title="10.VENN 韦恩图"></a>10.VENN 韦恩图</h2><p><strong>需要先下载matplotlib_venn</strong></p>
<h3 id="10-1-具有2个分组的基本的维恩图"><a href="#10-1-具有2个分组的基本的维恩图" class="headerlink" title="10.1 具有2个分组的基本的维恩图"></a>10.1 具有2个分组的基本的维恩图</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn2</span><br><span class="line"></span><br><span class="line"># 第一种方法，10，5为两组的大小，2为两组交叉大小;</span><br><span class="line"># set_labels为组名</span><br><span class="line"># venn2(subsets = (10, 5, 2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;))</span><br><span class="line"># 设置两组数据为ABCD和DEF</span><br><span class="line">venn2([set([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;]), set([&#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;])])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzRfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<h3 id="10-2-具有3个组的基本维恩图"><a href="#10-2-具有3个组的基本维恩图" class="headerlink" title="10.2 具有3个组的基本维恩图"></a>10.2 具有3个组的基本维恩图</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line"># Import the library</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn3</span><br><span class="line"> </span><br><span class="line"># Make the diagram</span><br><span class="line">venn3(subsets = (10, 8, 22, 6,9,4,2)) # 通过直接设置各部分数据来配置韦恩图</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzZfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 设置三组ABCD、DEF、ADG</span><br><span class="line">venn3([set([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;]), set([&#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;]), set([&#x27;A&#x27;, &#x27;D&#x27;, &#x27;G&#x27;,&#x27;F&#x27;])]) # 设置数据来配置韦恩图</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzdfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<h3 id="10-3-自定义维恩图"><a href="#10-3-自定义维恩图" class="headerlink" title="10.3 自定义维恩图"></a>10.3 自定义维恩图</h3><ol>
<li>自定义标签</li>
<li>自定义维恩图上圆的线条</li>
<li>自定义维恩图上的圆</li>
</ol>
<p><strong>自定义标签</strong></p>
<ul>
<li>get_label_by_id 可查看其源代码 表示分类里不同部分<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">## Venn上的自定义标签 Custom label on Venn</span><br><span class="line"># Import the library</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn3</span><br><span class="line">from matplotlib_venn import venn3_circles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Custom text labels: change the label of group A</span><br><span class="line">v=venn3(subsets = (10, 8, 22, 6,9,4,2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;, &#x27;Group C&#x27;))</span><br><span class="line"># 单独改变A的标签</span><br><span class="line">v.get_label_by_id(&#x27;A&#x27;).set_text(&#x27;My Favourite group!&#x27;)</span><br></pre></td></tr></table></figure>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzlfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"><br><strong>自定义维恩图上圆的线条</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">## 自定义维恩图上圆的线条 Custom Circles lines on Venn</span><br><span class="line"># Line style: can be &#x27;dashed&#x27; or &#x27;dotted&#x27; for example</span><br><span class="line"># 设置维恩图</span><br><span class="line">v = venn3(subsets = (10, 8, 22, 6,9,4,2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;, &#x27;Group C&#x27;))</span><br><span class="line"># 画圆，linestyle线条类型，linewith线宽，color线条颜色</span><br><span class="line">c = venn3_circles(subsets = (10, 8, 22, 6,9,4,2), linestyle=&#x27;dashed&#x27;, linewidth=1, color=&quot;grey&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzEwXzAucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"><br><strong>自定义维恩图上的圆</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">## 自定义维恩图上的圆 Custom a circle on Venn</span><br><span class="line"># Change one group only</span><br><span class="line">v=venn3(subsets = (10, 8, 22, 6,9,4,2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;, &#x27;Group C&#x27;))</span><br><span class="line">c=venn3_circles(subsets = (10, 8, 22, 6,9,4,2), linestyle=&#x27;dashed&#x27;, linewidth=1, color=&quot;grey&quot;)</span><br><span class="line"># 设置第一个圆的线宽</span><br><span class="line">c[0].set_lw(8.0)</span><br><span class="line"># 设置第一个圆的线形</span><br><span class="line">c[0].set_ls(&#x27;dotted&#x27;)</span><br><span class="line"># 设置第一个圆的填充颜色</span><br><span class="line">c[0].set_color(&#x27;skyblue&#x27;)</span><br><span class="line"> </span><br><span class="line"># Color</span><br><span class="line"># id号</span><br><span class="line"># 如ABC三个簇，010代表非A和B和非C,100代表A和非B和非C</span><br><span class="line"># 设置透明度</span><br><span class="line">v.get_patch_by_id(&#x27;011&#x27;).set_alpha(1.0)</span><br><span class="line"># 设置颜色</span><br><span class="line">v.get_patch_by_id(&#x27;011&#x27;).set_color(&#x27;red&#x27;)</span><br><span class="line"># 打印id号</span><br><span class="line">#v.id2idx</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzExXzAucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<h3 id="10-4-修改韦恩图数值"><a href="#10-4-修改韦恩图数值" class="headerlink" title="10.4 修改韦恩图数值"></a>10.4 修改韦恩图数值</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn3, venn3_circles</span><br><span class="line">import matplotlib.font_manager as fm</span><br><span class="line">from pathlib import Path</span><br><span class="line">def venn(train_filename=&#x27;biaozhuData/generate_data/train/pkuverb.txt&#x27;, val_filename=&#x27;biaozhuData/generate_data_2/valid/pkuverb.txt&#x27;, test_filename=&#x27;biaozhuData/generate_data_2/test/pkuverb.txt&#x27;):</span><br><span class="line">	&#x27;&#x27;&#x27;</span><br><span class="line">	训练集、验证集、测试集 数据分布的韦恩图</span><br><span class="line">	:param train_filename:</span><br><span class="line">	:param val_filename:</span><br><span class="line">	:param test_filename:</span><br><span class="line">	:return:</span><br><span class="line">	&#x27;&#x27;&#x27;</span><br><span class="line">	path = Path(train_filename)</span><br><span class="line">	set1 = set(open(test_filename, encoding=&#x27;utf-8&#x27;).readlines())</span><br><span class="line">	set2 = set(open(val_filename, encoding=&#x27;utf-8&#x27;).readlines())</span><br><span class="line">	set3 = set(open(train_filename, encoding=&#x27;utf-8&#x27;).readlines())</span><br><span class="line"></span><br><span class="line">	set_len1 = len(set1)</span><br><span class="line">	set_len2 = len(set2)</span><br><span class="line">	set_len3 = len(set3)</span><br><span class="line"></span><br><span class="line">	v = venn3([set1, set2, set3], (&#x27;test&#x27;, &#x27;valid&#x27;, &#x27;train&#x27;))</span><br><span class="line">	a = v.get_label_by_id(&#x27;100&#x27;).get_text()</span><br><span class="line">	b = v.get_label_by_id(&#x27;010&#x27;).get_text()</span><br><span class="line">	c = v.get_label_by_id(&#x27;001&#x27;).get_text()</span><br><span class="line"></span><br><span class="line">	a = str(round(int(a) / set_len1, 3)) + &#x27; &#x27; + a</span><br><span class="line">	b = str(round(int(b) / set_len2, 3)) + &#x27; &#x27; + b</span><br><span class="line">	c = str(round(int(c) / set_len3, 3)) + &#x27; &#x27; + c</span><br><span class="line"></span><br><span class="line">	v.get_label_by_id(&#x27;100&#x27;).set_text(a)</span><br><span class="line">	v.get_label_by_id(&#x27;010&#x27;).set_text(b)</span><br><span class="line">	v.get_label_by_id(&#x27;001&#x27;).set_text(c)</span><br><span class="line">	plt.title(name[path.stem]+&#x27;集合关系&#x27;, fontproperties=myfont)</span><br><span class="line">	sfname = path.name.replace(path.suffix, &#x27;.png&#x27;)</span><br><span class="line">	savepath = Path(venn_savedir)/sfname</span><br><span class="line">	#plt.show()</span><br><span class="line">	plt.savefig(savepath)</span><br><span class="line">	plt.close()</span><br></pre></td></tr></table></figure>
<h1 id="四-Matplotlib-中文无法显示问题"><a href="#四-Matplotlib-中文无法显示问题" class="headerlink" title="四. Matplotlib 中文无法显示问题"></a>四. Matplotlib 中文无法显示问题</h1><ol>
<li>windows下：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.rcParams[&#x27;font.sans-serif&#x27;]=[&#x27;SimHei&#x27;] #用来正常显示中文标签</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 中文显示问题--下面手动修改配置 </span><br><span class="line">from pylab import mpl</span><br><span class="line">mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line">mpl.rcParams[&quot;axes.unicode_minus&quot;] = False</span><br></pre></td></tr></table></figure>
<ol>
<li>Ubuntu/LInux下</li>
</ol>
<p><a href="https://www.cnblogs.com/panlq/p/9270826.html">Ubuntu解决matplotlib中文无法显示</a></p>
<p>其实这些乱码问题，只是路径等配置问题，上述只是修改默认的配置，可以通过指定字体路径等来解决。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">myfont = fm.FontProperties(fname=r&#x27;1031Competition/font/simsun.ttc&#x27;)  # 设置字体</span><br><span class="line">……</span><br><span class="line">plt.xlabel(&#x27;短语类型&#x27;, fontproperties=myfont)</span><br><span class="line">plt.ylabel(&#x27;比例&#x27;, fontproperties=myfont)</span><br><span class="line">plt.legend(loc=&#x27;upper right&#x27;)</span><br><span class="line">plt.savefig(&#x27;test.png&#x27;)</span><br></pre></td></tr></table></figure>
<p><strong>参考文章</strong><br><a href="https://blog.csdn.net/refrain__wg/article/details/82747254">Matplotlib 数据可视化-基本使用教程</a><br><a href="https://blog.csdn.net/LuohenYJ/article/details/103091081">python基于matplotlib_venn实现维恩图的绘制</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>数据分析</category>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch 多卡训练</title>
    <url>/2021/09/12/2021-09-12-Pytorch%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>本文将介绍Pytorch 多卡训练的原理及实现。多卡训练流程一般如下：</p>
<ol>
<li>指定主机节点</li>
<li>主机节点划分数据，一个batch数据平均分到每个机器上</li>
<li>模型从主机拷贝到各个机器</li>
<li>每个机器进行前向传播</li>
<li>每个机器计算loss损失</li>
<li>主机收集所有loss结果，进行参数更新</li>
<li>将更新后参数模型拷贝给各个机器</li>
</ol>
<span id="more"></span>
<h1 id="Pytorch-多卡训练"><a href="#Pytorch-多卡训练" class="headerlink" title="Pytorch 多卡训练"></a>Pytorch 多卡训练</h1><h2 id="一、多卡训练原理"><a href="#一、多卡训练原理" class="headerlink" title="一、多卡训练原理"></a>一、多卡训练原理</h2><p>多卡训练流程一般如下：</p>
<ol>
<li>指定主机节点</li>
<li>主机节点划分数据，一个batch数据平均分到每个机器上</li>
<li>模型从主机拷贝到各个机器</li>
<li>每个机器进行前向传播</li>
<li>每个机器计算loss损失</li>
<li>主机收集所有loss结果，进行参数更新</li>
<li>将更新后参数模型拷贝给各个机器</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/img_convert/1665788acfe3757d493b3d82422035c1.png" alt="image"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/bffacf11040884149347a428d6f63260.png" alt="image"></p>
<h2 id="二、单机多卡训练"><a href="#二、单机多卡训练" class="headerlink" title="二、单机多卡训练"></a>二、单机多卡训练</h2><p>使用<strong>torch.nn.DataParallel</strong>(module, device_ids)模块，module为模型，device_ids为并行的GPU id列表</p>
<p>使用方式：将模型调用该接口执行操作</p>
<p><code>model = torch.nn.DataParallel(model)</code></p>
<p>示例：我们假设模型输入为(32, input_dim)，这里的 32 表示batch_size，模型输出为(32, output_dim)，使用 4 个GPU训练。nn.DataParallel起到的作用是将这 32 个样本拆成 4 份，发送给 4 个GPU 分别做 forward，然后生成 4 个大小为(8, output_dim)的输出，然后再将这 4 个输出都收集到cuda:0上并合并成(32, output_dim)。</p>
<p>可以看出，nn.DataParallel没有改变模型的输入输出，因此其他部分的代码不需要做任何更改，非常方便。但弊端是，后续的loss计算只会在cuda:0上进行，没法并行，因此会导致负载不均衡的问题。</p>
<p>通过在模型内置loss计算可以解决上述负载不均衡的情况，最后所得loss进行取平均。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Net:</span><br><span class="line">    def __init__(self,...):</span><br><span class="line">        # code</span><br><span class="line">    </span><br><span class="line">    def forward(self, inputs, labels=None)</span><br><span class="line">        # outputs = fct(inputs)</span><br><span class="line">        # loss_fct = ...</span><br><span class="line">        if labels is not None:</span><br><span class="line">            loss = loss_fct(outputs, labels)  # 在训练模型时直接将labels传入模型，在forward过程中计算loss</span><br><span class="line">            return loss</span><br><span class="line">        else:</span><br><span class="line">            return outputs</span><br></pre></td></tr></table></figure>
<p>按照我们上面提到的模型并行逻辑，在每个GPU上会计算出一个loss，这些loss会被收集到cuda:0上并合并成长度为 4 的张量。这个时候在做backward的之前，必须对将这个loss张量合并成一个标量，一般直接取mean就可以。这在Pytorch官方文档nn.DataParallel函数中有提到：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</span><br></pre></td></tr></table></figure>
<h2 id="三、多机多卡训练"><a href="#三、多机多卡训练" class="headerlink" title="三、多机多卡训练"></a>三、多机多卡训练</h2><p><strong>该方式也可以实现单机多卡</strong></p>
<p>使用<strong>torch.nn.parallel.DistributedDataParallel</strong>和<strong>torch.utils.data.distributed.DistributedSampler</strong>结合多进程实现。</p>
<ol>
<li><p>从一开始就会启动多个进程(进程数小于等于GPU数)，每个进程独享一个GPU，每个进程都会独立地执行代码。这意味着每个进程都独立地初始化模型、训练，当然，在每次迭代过程中会通过进程间通信共享梯度，整合梯度，然后独立地更新参数。</p>
</li>
<li><p>每个进程都会初始化一份训练数据集，当然它们会使用数据集中的不同记录做训练，这相当于同样的模型喂进去不同的数据做训练，也就是所谓的数据并行。这是通过<strong>torch.utils.data.distributed.DistributedSampler</strong>函数实现的，不过逻辑上也不难想到，只要做一下数据partition，不同进程拿到不同的parition就可以了，官方有一个简单的demo，感兴趣的可以看一下代码实现：Distributed Training</p>
</li>
<li><p>进程通过local_rank变量来标识自己，local_rank为0的为master，其他是slave。这个变量是torch.distributed包帮我们创建的，使用方法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import argparse  # 必须引入 argparse 包</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&quot;--local_rank&quot;, type=int, default=-1)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure>
<p>必须以如下方式运行代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 train.py</span><br></pre></td></tr></table></figure>
<p>这样的话，torch.distributed.launch就以命令行参数的方式将args.local_rank变量注入到每个进程中，每个进程得到的变量值都不相同。比如使用 4 个GPU的话，则 4 个进程获得的args.local_rank值分别为0、1、2、3。</p>
</li>
</ol>
<p>上述命令行参数nproc_per_node表示每个节点需要创建多少个进程(使用几个GPU就创建几个)；nnodes表示使用几个节点，做单机多核训练设为1。</p>
<ol>
<li>因为每个进程都会初始化一份模型，为保证模型初始化过程中生成的随机权重相同，需要设置随机种子。方法如下：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def set_seed(seed):</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>使用方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from torch.utils.data.distributed import DistributedSampler  # 负责分布式dataloader创建，也就是实现上面提到的partition。</span><br><span class="line"></span><br><span class="line"># 负责创建 args.local_rank 变量，并接受 torch.distributed.launch 注入的值</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&quot;--local_rank&quot;, type=int, default=-1)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"># 每个进程根据自己的local_rank设置应该使用的GPU</span><br><span class="line">torch.cuda.set_device(args.local_rank)</span><br><span class="line">device = torch.device(&#x27;cuda&#x27;, args.local_rank)</span><br><span class="line"></span><br><span class="line"># 初始化分布式环境，主要用来帮助进程间通信</span><br><span class="line">torch.distributed.init_process_group(backend=&#x27;nccl&#x27;)</span><br><span class="line"></span><br><span class="line"># 固定随机种子</span><br><span class="line">seed = 42</span><br><span class="line">random.seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line">torch.cuda.manual_seed_all(seed)</span><br><span class="line"></span><br><span class="line"># 初始化模型</span><br><span class="line">model = Net()</span><br><span class="line">model.to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.1)</span><br><span class="line"></span><br><span class="line"># 只 master 进程做 logging，否则输出会很乱</span><br><span class="line">if args.local_rank == 0:</span><br><span class="line">    tb_writer = SummaryWriter(comment=&#x27;ddp-training&#x27;)</span><br><span class="line"></span><br><span class="line"># 分布式数据集</span><br><span class="line">train_sampler = DistributedSampler(train_dataset)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)  # 注意这里的batch_size是每个GPU上的batch_size</span><br><span class="line"></span><br><span class="line"># 分布式模型</span><br><span class="line">model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)</span><br></pre></td></tr></table></figure>
<p><strong>torch.distributed.init_process_group</strong>()包含四个常用的参数：</p>
<ul>
<li>backend: 后端, 实际上是多个机器之间交换数据的协议</li>
<li>init_method: 机器之间交换数据, 需要指定一个主节点, 而这个参数就是指定主节点的</li>
<li>world_size: 介绍都是说是进程, 实际就是机器的个数, 例如两台机器一起训练的话, world_size就设置为2</li>
<li>rank: 区分主节点和从节点的, 主节点为0, 剩余的为了1-(N-1), N为要使用的机器的数量, 也就是world_size</li>
</ul>
<h3 id="后端初始化"><a href="#后端初始化" class="headerlink" title="后端初始化"></a>后端初始化</h3><p>pytorch提供下列常用后端：</p>
<p><img src="https://pic2.zhimg.com/80/v2-b4d4a27387dde5cbd043d883948def09_720w.jpg" alt="image"></p>
<h3 id="初始化init-method"><a href="#初始化init-method" class="headerlink" title="初始化init_method"></a>初始化init_method</h3><ol>
<li>TCP初始化</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line"></span><br><span class="line">dist.init_process_group(backend, init_method=&#x27;tcp://10.1.1.20:23456&#x27;,</span><br><span class="line">                        rank=rank, world_size=world_size)</span><br></pre></td></tr></table></figure>
<p>注意这里使用格式为tcp://ip:端口号, 首先ip地址是你的主节点的ip地址, 也就是rank参数为0的那个主机的ip地址, 然后再选择一个空闲的端口号, 这样就可以初始化init_method了.</p>
<ol>
<li>共享文件系统初始化</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line"></span><br><span class="line">dist.init_process_group(backend, init_method=&#x27;file:///mnt/nfs/sharedfile&#x27;,</span><br><span class="line">                        rank=rank, world_size=world_size)</span><br></pre></td></tr></table></figure>
<h3 id="初始化rank和world-size"><a href="#初始化rank和world-size" class="headerlink" title="初始化rank和world_size"></a>初始化rank和world_size</h3><p>这里其实没有多难, 你需要确保, 不同机器的rank值不同, 但是主机的rank必须为0, 而且使用init_method的ip一定是rank为0的主机, 其次world_size是你的主机数量, 你不能随便设置这个数值, 你的参与训练的主机数量达不到world_size的设置值时, 代码是不会执行的.</p>
<h2 id="四、模型保存"><a href="#四、模型保存" class="headerlink" title="四、模型保存"></a>四、模型保存</h2><p>模型的保存与加载，与单GPU的方式有所不同。这里通通将参数以cpu的方式save进存储, 因为如果是保存的GPU上参数，pth文件中会记录参数属于的GPU号，则加载时会加载到相应的GPU上，这样就会导致如果你GPU数目不够时会在加载模型时报错</p>
<p>或者模型保存时控制进程，只在主进程进行保存。<br>模型保存都是一致的，不过分布式运行有多个进程在同时跑，所以会保存多个模型到存储上，如果使用共享存储就要注意文件名的问题，当然一般只在rank0进程上保存参数即可，因为所有进程的模型参数是同步的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.save(model.module.cpu().state_dict(), &quot;model.pth&quot;)</span><br></pre></td></tr></table></figure>
<p>参数加载：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">param=torch.load(&quot;model.pth&quot;)</span><br></pre></td></tr></table></figure>
<p>以下是huggingface/transformers代码中用到的模型保存代码<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if torch.distributed.get_rank() == 0:</span><br><span class="line">    model_to_save = model.module if hasattr(model, &quot;module&quot;) else model  # Take care of distributed/parallel training</span><br><span class="line">    model_to_save.save_pretrained(args.output_dir)</span><br><span class="line">    tokenizer.save_pretrained(args.output_dir)</span><br></pre></td></tr></table></figure></p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://zhuanlan.zhihu.com/p/86441879">pytorch多gpu并行训练</a></p>
<p><a href="https://github.com/jia-zhuang/pytorch-multi-gpu-training">PyTorch 单机多GPU 训练方法与原理整理</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>Nvidia-Docker配置python3与pytorch环境</title>
    <url>/2021/09/28/2021-09-28-Nvidia-Docker%E9%85%8D%E7%BD%AEpython3%E4%B8%8Epytorch%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<p>本文将介绍Nvidia-Docker配置python3与pytorch环境，完成docker容器内安装GPU深度学习环境。<br>TIPS：为了避免下载源过慢，建议添加中科大源/清华源<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">2. sudo sed -i &#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27; /etc/apt/sources.list</span><br><span class="line">3. sudo apt update</span><br></pre></td></tr></table></figure></p>
<span id="more"></span>
<h1 id="一、Docker与Nvidia-docker安装"><a href="#一、Docker与Nvidia-docker安装" class="headerlink" title="一、Docker与Nvidia-docker安装"></a>一、Docker与Nvidia-docker安装</h1><h2 id="TIPS：为了避免下载源过慢，建议添加中科大源-清华源"><a href="#TIPS：为了避免下载源过慢，建议添加中科大源-清华源" class="headerlink" title="TIPS：为了避免下载源过慢，建议添加中科大源/清华源"></a>TIPS：为了避免下载源过慢，建议添加中科大源/清华源</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">2. sudo sed -i &#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27; /etc/apt/sources.list</span><br><span class="line">3. sudo apt update</span><br></pre></td></tr></table></figure>
<h2 id="1-Docker安装"><a href="#1-Docker安装" class="headerlink" title="1. Docker安装"></a>1. Docker安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. Update the apt package index and install packages to allow apt to use a repository over HTTPS:</span><br><span class="line"></span><br><span class="line"> sudo apt-get update</span><br><span class="line"> sudo apt-get install \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    gnupg \</span><br><span class="line">    lsb-release</span><br><span class="line">    </span><br><span class="line">2. Add Docker’s official GPG key:</span><br><span class="line"></span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg</span><br><span class="line"></span><br><span class="line">3. Use the following command to set up the stable repository. To add the nightly or test repository, add the word nightly or test (or both) after the word stable in the commands below. Learn about nightly and test channels.</span><br><span class="line"></span><br><span class="line"> echo \</span><br><span class="line">  &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \</span><br><span class="line">  $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br><span class="line">  </span><br><span class="line">4. Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version:</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install docker-ce docker-ce-cli containerd.io</span><br><span class="line"></span><br><span class="line">5. To install a specific version of Docker Engine, list the available versions in the repo, then select and install:</span><br><span class="line"></span><br><span class="line">a. List the versions available in your repo:</span><br><span class="line"></span><br><span class="line">apt-cache madison docker-ce</span><br><span class="line"></span><br><span class="line">b. Install a specific version using the version string from the second column, for example, 5:18.09.1~3-0~ubuntu-xenial.</span><br><span class="line"></span><br><span class="line">sudo apt-get install docker-ce=&lt;VERSION_STRING&gt; docker-ce-cli=&lt;VERSION_STRING&gt; containerd.io</span><br><span class="line"></span><br><span class="line">6. Verify that Docker Engine is installed correctly by running the hello-world image.</span><br><span class="line"></span><br><span class="line">sudo docker run hello-world</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-Nvidia-Docker-安装"><a href="#2-Nvidia-Docker-安装" class="headerlink" title="2. Nvidia-Docker 安装"></a>2. Nvidia-Docker 安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. Setup the stable repository and the GPG key:</span><br><span class="line"></span><br><span class="line">distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line"></span><br><span class="line">2. To get access to experimental features such as CUDA on WSL or the new MIG capability on A100, you may want to add the experimental branch to the repository listing:</span><br><span class="line"></span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list</span><br><span class="line"></span><br><span class="line">3. Install the nvidia-docker2 package (and dependencies) after updating the package listing:</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y nvidia-docker2</span><br><span class="line"></span><br><span class="line">4. Restart the Docker daemon to complete the installation after setting the default runtime:</span><br><span class="line"></span><br><span class="line">sudo systemctl restart docker</span><br><span class="line"></span><br><span class="line">5. At this point, a working setup can be tested by running a base CUDA container:</span><br><span class="line">sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi</span><br></pre></td></tr></table></figure>
<h1 id="二、docker内安装python与pytorch环境"><a href="#二、docker内安装python与pytorch环境" class="headerlink" title="二、docker内安装python与pytorch环境"></a>二、docker内安装python与pytorch环境</h1><blockquote>
<p>拉取nvidia包含cuda的基础镜像。拟安装环境：python3.7, pytorch1.6</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 宿主机：提前在宿主机上下载好安装pip3.7要用到的包</span><br><span class="line">curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py</span><br><span class="line"></span><br><span class="line"># 宿主机与容器传输文件</span><br><span class="line">docker cp a.txt containerid:/path</span><br><span class="line"></span><br><span class="line"># 宿主机：运行ubuntu:18.04容器</span><br><span class="line">docker run -it -d --name=lz-ubuntu -v /root/get-pip.py:/root/get-pip.py ubuntu:18.04</span><br><span class="line"></span><br><span class="line"># 宿主机：进入到容器</span><br><span class="line">docker exec -it lz-ubuntu bash</span><br><span class="line"></span><br><span class="line"># 容器内：可选-安装vim</span><br><span class="line">apt-get update</span><br><span class="line">apt-get install vim -y</span><br><span class="line"></span><br><span class="line"># 容器内：配置pip源，用以加速安装</span><br><span class="line">sudo mkdir ~/.pip</span><br><span class="line">sudo vim ~/.pip/pip.conf</span><br><span class="line"></span><br><span class="line">添加以下内容：</span><br><span class="line">[global]</span><br><span class="line">index-url=https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host=mirrors.aliyun.com</span><br><span class="line"></span><br><span class="line">国内源：</span><br><span class="line">清华：</span><br><span class="line">https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">阿里云：</span><br><span class="line">http://mirrors.aliyun.com/pypi/simple/</span><br><span class="line">中国科技大学 </span><br><span class="line">https://pypi.mirrors.ustc.edu.cn/simple/</span><br><span class="line">华中理工大学：</span><br><span class="line">http://pypi.hustunique.com/</span><br><span class="line">山东理工大学：</span><br><span class="line">http://pypi.sdutlinux.org/</span><br><span class="line">豆瓣：</span><br><span class="line">http://pypi.douban.com/simple/</span><br><span class="line"></span><br><span class="line"># 容器内：可选-配置apt源</span><br><span class="line">mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">vim /etc/apt/sources.list</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line"># 容器内：更新软件包列表</span><br><span class="line">apt-get update</span><br><span class="line"></span><br><span class="line"># 容器内：可选-安装调试工具</span><br><span class="line">apt-get install iputils-ping net-tools curl</span><br><span class="line"></span><br><span class="line"># 容器内：安装最主要的python包</span><br><span class="line">apt-get install python3.7 python3.7-dev</span><br><span class="line"></span><br><span class="line"># 容器内：安装pip3.7</span><br><span class="line">apt install python3-distutils</span><br><span class="line">python3.7 get-pip.py</span><br><span class="line"></span><br><span class="line"># 容器内：安装pytorch</span><br><span class="line"># CUDA 10.1</span><br><span class="line">pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line"># 安装其他python包</span><br><span class="line">pip install transformers==2.10.0</span><br><span class="line">pip install pytorch-crf==0.7.2</span><br><span class="line">pip install sklearn</span><br><span class="line">pip install seqeval==1.2.2</span><br><span class="line">pip install pandas</span><br><span class="line"></span><br><span class="line"># 时区设置</span><br><span class="line"># 宿主机：从宿主机中拷贝时区文件到容器内，/usr/share/zoneinfo/UCT这个文件是通过软链追溯到的，时区是亚洲/上海</span><br><span class="line">docker cp /usr/share/zoneinfo/UCT  lyz-ubuntu:/etc/</span><br><span class="line"># 容器内：然后在容器内将其改名为/etc/localtime</span><br><span class="line">mv /etc/UCT /etc/localtime</span><br><span class="line"></span><br><span class="line"># 容器内：清理无用的包</span><br><span class="line">apt-get clean</span><br><span class="line">apt-get autoclean</span><br><span class="line">du -sh /var/cache/apt/</span><br><span class="line">rm -rf /var/cache/apt/archives</span><br><span class="line"></span><br><span class="line"># 容器内：清理pip缓存</span><br><span class="line">rm -rf ~/.cache/pip</span><br><span class="line"></span><br><span class="line"># 容器内：清理命令日志</span><br><span class="line">history -c</span><br><span class="line"></span><br><span class="line"># 宿主机：打包镜像</span><br><span class="line">docker commit -a &#x27;提交人&#x27; -m &#x27;描述&#x27; &lt;容器名/ID&gt; &lt;镜像名称&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="三、nvidia-docker-运行"><a href="#三、nvidia-docker-运行" class="headerlink" title="三、nvidia-docker 运行"></a>三、nvidia-docker 运行</h1><blockquote>
<p>nvidia-docker2版本下nvidia-docker run与docker run —runtime=nvidia效果无太大差异</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># pytorch gpu 可运行</span><br><span class="line">docker run -itd --gpus all --name 容器名 -eNVIDIA_DRIVER_CAPABILITIES=compute,utility -e NVIDIA_VISIBLE_DEVICES=all 镜像名</span><br><span class="line"></span><br><span class="line">多出来的东西其实就是这个家伙：NVIDIA_DRIVER_CAPABILITIES=compute,utility</span><br><span class="line">　　</span><br><span class="line">也就是说，如果你不改这个环境变量，宿主机的nvidia driver在容器内是仅作为utility存在的，如果加上compute，宿主机的英伟达driver将对容器提供计算支持（所谓的计算支持也就是cuda支持）。</span><br><span class="line"></span><br><span class="line"># nvidia-docker2验证</span><br><span class="line">nvidia-docker --version</span><br><span class="line"></span><br><span class="line"># nvidia-docker2 测试</span><br><span class="line">docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi</span><br><span class="line"></span><br><span class="line"># nvidia-docker2启动</span><br><span class="line">启动nvidia-docker：</span><br><span class="line">      $: systemctl start nvidia-docker</span><br><span class="line"> 查看docker服务是否启动：</span><br><span class="line">      $: systemctl status nvidia-docker</span><br></pre></td></tr></table></figure>
<h2 id="1-docker-run-参数介绍"><a href="#1-docker-run-参数介绍" class="headerlink" title="1. docker run 参数介绍"></a>1. docker run 参数介绍</h2><p><strong>docker run 常用参数介绍：</strong></p>
<ul>
<li><p>—rm选项，这样在容器退出时就能够自动清理容器内部的文件系统。</p>
</li>
<li><p>—i选项，打开STDIN，用于控制台交互</p>
</li>
<li><p>—t选项，分配tty设备，该可以支持终端登录，默认为false</p>
</li>
<li><p>—d选项，指定容器运行于前台还是后台，默认为false</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">其他参数介绍：</span><br><span class="line">  -u, --user=&quot;&quot;              指定容器的用户  </span><br><span class="line">  -a, --attach=[]            登录容器（必须是以docker run -d启动的容器）</span><br><span class="line">  -w, --workdir=&quot;&quot;           指定容器的工作目录 </span><br><span class="line">  -c, --cpu-shares=0        设置容器CPU权重，在CPU共享场景使用  </span><br><span class="line">  -e, --env=[]               指定环境变量，容器中可以使用该环境变量  </span><br><span class="line">  -m, --memory=&quot;&quot;            指定容器的内存上限  </span><br><span class="line">  -P, --publish-all=false    指定容器暴露的端口  </span><br><span class="line">  -p, --publish=[]           指定容器暴露的端口 </span><br><span class="line">  -h, --hostname=&quot;&quot;          指定容器的主机名  </span><br><span class="line">  -v, --volume=[]            给容器挂载存储卷，挂载到容器的某个目录  </span><br><span class="line">  --volumes-from=[]          给容器挂载其他容器上的卷，挂载到容器的某个目录</span><br><span class="line">  --cap-add=[]               添加权限，权限清单详见：http://linux.die.net/man/7/capabilities  </span><br><span class="line">  --cap-drop=[]              删除权限，权限清单详见：http://linux.die.net/man/7/capabilities  </span><br><span class="line">  --cidfile=&quot;&quot;               运行容器后，在指定文件中写入容器PID值，一种典型的监控系统用法  </span><br><span class="line">  --cpuset=&quot;&quot;                设置容器可以使用哪些CPU，此参数可以用来容器独占CPU  </span><br><span class="line">  --device=[]                添加主机设备给容器，相当于设备直通  </span><br><span class="line">  --dns=[]                   指定容器的dns服务器  </span><br><span class="line">  --dns-search=[]            指定容器的dns搜索域名，写入到容器的/etc/resolv.conf文件  </span><br><span class="line">  --entrypoint=&quot;&quot;            覆盖image的入口点  </span><br><span class="line">  --env-file=[]              指定环境变量文件，文件格式为每行一个环境变量  </span><br><span class="line">  --expose=[]                指定容器暴露的端口，即修改镜像的暴露端口  </span><br><span class="line">  --link=[]                  指定容器间的关联，使用其他容器的IP、env等信息  </span><br><span class="line">  --lxc-conf=[]              指定容器的配置文件，只有在指定--exec-driver=lxc时使用  </span><br><span class="line">  --name=&quot;&quot;                  指定容器名字，后续可以通过名字进行容器管理，links特性需要使用名字  </span><br><span class="line">  --net=&quot;bridge&quot;             容器网络设置:</span><br><span class="line">				                bridge 使用docker daemon指定的网桥     </span><br><span class="line">				                host 	//容器使用主机的网络  </span><br><span class="line">				                container:NAME_or_ID  &gt;//使用其他容器的网路，共享IP和PORT等网络资源  </span><br><span class="line">				                none 容器使用自己的网络（类似--net=bridge），但是不进行配置 </span><br><span class="line">  --privileged=false         指定容器是否为特权容器，特权容器拥有所有的capabilities  </span><br><span class="line">  --restart=&quot;no&quot;             指定容器停止后的重启策略:</span><br><span class="line">				                no：容器退出时不重启  </span><br><span class="line">				                on-failure：容器故障退出（返回值非零）时重启 </span><br><span class="line">				                always：容器退出时总是重启  </span><br><span class="line">  --rm=false                 指定容器停止后自动删除容器(不支持以docker run -d启动的容器)  </span><br><span class="line">  --sig-proxy=true           设置由代理接受并处理信号，但是SIGCHLD、SIGSTOP和SIGKILL不能被代理  </span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<h2 id="2-docker-常用命令"><a href="#2-docker-常用命令" class="headerlink" title="2. docker 常用命令"></a>2. docker 常用命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> ### 显示版本信息 (与python, nvcc相比少了两个‘--’）</span><br><span class="line">$ docker version</span><br><span class="line"></span><br><span class="line">### 了解当前Docker的使用状态（当前容器，镜像数目信息，存储空间占用信息，</span><br><span class="line"># OS内核版本， 发行版本， 硬件资源等）</span><br><span class="line">$ docker info</span><br><span class="line"></span><br><span class="line">### 拉去一个镜像 ( xxxx 表示某个镜像名字，）</span><br><span class="line">$ docker pull xxxx</span><br><span class="line"># e.g.</span><br><span class="line"># docker pull ubuntu</span><br><span class="line"></span><br><span class="line">### 查看系统中已有的镜像(images要带‘s&#x27;)</span><br><span class="line">$ docker images</span><br><span class="line"># e.g.:</span><br><span class="line"># REPOSITORY  TAG    IMAGES ID   CREATED VIRTUAL SIZE</span><br><span class="line"># ubuntu      latest 4ef6axxxxx   5 day ago  84.0M</span><br><span class="line"></span><br><span class="line">### 从镜像创建docker容器</span><br><span class="line">$ docker run -i -t ubuntu /bin/bash </span><br><span class="line"># or</span><br><span class="line">$ docker run -it 4ef /bin/bash</span><br><span class="line"># 其中 -i, 交互模式，让输入输出都在标准控制台进行；-d，则进入后台</span><br><span class="line"># -t, 为新创建的容器分配一个伪终端</span><br><span class="line"># ubuntu, 用于创建容器的镜像名，可用ID来代替（前3位足够）</span><br><span class="line"># /bin/bash， 在新建容器中运行的命令，可以为任意Linux命令</span><br><span class="line"></span><br><span class="line">### 离开当前容器,返回宿主机终端，使用组合键 &quot;Ctrl+P&quot; 和 &quot;Ctrl+Q&quot;</span><br><span class="line"></span><br><span class="line">### 查看当前活动的容器</span><br><span class="line">$ docker ps</span><br><span class="line"># CONTAINER ID  IMAGE  COMMAND  CREATED   STATUS   PORTS NAME</span><br><span class="line"># 610xxxx  ubuntu:latest  &quot;/bin/bash&quot; 1 minute ago Up 1 minute ago prickly_wilson</span><br><span class="line"></span><br><span class="line">### 宿主机终端与某个容器建立连接</span><br><span class="line">$ docker attach 610</span><br><span class="line"></span><br><span class="line">### 从容器创建Docker镜像</span><br><span class="line">$ docker commit -m &quot;hhahaha&quot; 610 ubuntu:hhh</span><br><span class="line"># -m, 新镜像说明</span><br><span class="line"># 610， 某个容器的ID</span><br><span class="line"># ubuntu:hhh， 命名最好不要这么随意</span><br><span class="line"># 那么接下来可以查看新生成的镜像，命令 docker images</span><br><span class="line"></span><br><span class="line">### 基于新的镜像创建一个新的容器(一样的)</span><br><span class="line">$ docker run -it ubuntu:hhh /bin/bash</span><br><span class="line"></span><br><span class="line">### 给镜像重命名(方便记忆)</span><br><span class="line">$ docker tag IMAGEID(image id) REPOSITORY:TAG</span><br><span class="line"></span><br><span class="line">### 给容器重命名</span><br><span class="line">$ docker rename old-container-name new-container-name</span><br></pre></td></tr></table></figure>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://blog.csdn.net/qq_39698985/article/details/109524762">https://blog.csdn.net/qq_39698985/article/details/109524762</a></p>
<p><a href="https://blog.csdn.net/mumoDM/article/details/82503022">https://blog.csdn.net/mumoDM/article/details/82503022</a></p>
<p><a href="https://blog.csdn.net/qq_29518275/article/details/107486028">https://blog.csdn.net/qq_29518275/article/details/107486028</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>Nvidia-Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习基础-神经网络权重初始化</title>
    <url>/2021/09/29/2021-09-29-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
    <content><![CDATA[<p>本文将介绍神经网络权重初始化的原理与实现。首先解决了两个问题：1. 全零初始化是否可以、2. 参数全部相同初始化是否可以</p>
<p>然后介绍了初始化的方式：</p>
<ol>
<li>预训练初始化</li>
<li>随机初始化</li>
<li>固定初始化</li>
</ol>
<span id="more"></span>
<h1 id="一、两个问题"><a href="#一、两个问题" class="headerlink" title="一、两个问题"></a>一、两个问题</h1><blockquote>
<p>假设3层神经网络,输入节点v0，第一层节点v1,v2,v3 第二层节点v4,v5 第三层节点v6。其中vi=f(ai),i=4,5,6 f为激活函数。</p>
</blockquote>
<p><strong>前向传播：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/83cc1293388e439ab8a1d33e6c67396f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASk1YR09ETFo=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h2 id="1-全零初始化是否可以"><a href="#1-全零初始化是否可以" class="headerlink" title="1. 全零初始化是否可以"></a>1. 全零初始化是否可以</h2><p>一般情况不可以。当全零参数初始化时，除输入节点所有节点值均为0,根据上式除第一层梯度与输入值有关其余均为0.</p>
<p>LR等一层网络可以全零初始化， 网络梯度与输入值有关。仅全零初始化一层也不影响训练，但涉及两层及以上，从涉及层到输入层的梯度都为0,参数无法更新。</p>
<h2 id="2-参数全部相同初始化是否可以"><a href="#2-参数全部相同初始化是否可以" class="headerlink" title="2. 参数全部相同初始化是否可以"></a>2. 参数全部相同初始化是否可以</h2><p>不可以。若初始化为相同的参数，隐藏层所有节点输出相同，梯度也是相同的。相当于输入经过一个节点。</p>
<h1 id="二、参数初始化方式"><a href="#二、参数初始化方式" class="headerlink" title="二、参数初始化方式"></a>二、参数初始化方式</h1><h2 id="1-预训练初始化"><a href="#1-预训练初始化" class="headerlink" title="1. 预训练初始化"></a>1. 预训练初始化</h2><p><strong>pretraining + finetuning</strong>加载已训练好的模型参数，进行下游任务的模型训练。</p>
<h2 id="2-随机初始化"><a href="#2-随机初始化" class="headerlink" title="2. 随机初始化"></a>2. 随机初始化</h2><h3 id="2-1-random-initialization"><a href="#2-1-random-initialization" class="headerlink" title="2.1 random initialization"></a>2.1 random initialization</h3><p><code>random initialization: np.random.randn(m,n)</code></p>
<p>随机产生符合正态分布的m×n维向量</p>
<p>弊端：随机会产生梯度消失，随着网络层次加深，由于链式求导法则，输出越来越接近0</p>
<h3 id="2-2-Xavier-initialization"><a href="#2-2-Xavier-initialization" class="headerlink" title="2.2 Xavier initialization"></a>2.2 Xavier initialization</h3><p><code>tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in)</code></p>
<p><img src="https://pic3.zhimg.com/80/v2-6302a7093b93e1376e54e95033c58086_720w.jpg" alt="image"></p>
<p>M 代表着输入输出维度即node_in,node_out</p>
<p>保证输入输出方差一致</p>
<h3 id="2-3-He-initialization"><a href="#2-3-He-initialization" class="headerlink" title="2.3 He initialization"></a>2.3 He initialization</h3><p><code>tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in/2)</code></p>
<p>适用于RELU激活函数，只有半区有效</p>
<h2 id="3-固定初始化"><a href="#3-固定初始化" class="headerlink" title="3. 固定初始化"></a>3. 固定初始化</h2><p>比如对于偏置（bias）通常用0初始化，LSTM遗忘门偏置通常为1或2，使时序上的梯度变大，对于ReLU神经元，偏置设为0.01，使得训练初期更容易激活。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://note.youdao.com/">https://www.leiphone.com/category/ai/3qMp45aQtbxTdzmK.html</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>权重初始化</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention可视化</title>
    <url>/2021/12/24/2021-12-24-Attention%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<p>本文介绍了Attention可视化的方式，包含两种热力图可视化、文本可视化。</p>
<ol>
<li>使用matplotlib与seaborn完成attention矩阵的热力图绘制</li>
<li>根据注意力矩阵，得到html颜色深浅代表相关性强弱</li>
</ol>
<span id="more"></span>
<h1 id="热力图可视化"><a href="#热力图可视化" class="headerlink" title="热力图可视化"></a>热力图可视化</h1><blockquote>
<p>使用matplotlib与seaborn完成attention矩阵的热力图绘制</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"># author:   LZ</span><br><span class="line"># create time:  2021/11/15 14:54</span><br><span class="line"># file: genpic.py</span><br><span class="line"># IDE:  PyCharm</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.ticker as ticker</span><br><span class="line">import seaborn as sns</span><br><span class="line">import matplotlib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gperes = &#123;</span><br><span class="line">     &#x27;武&#x27;: &#123;&#x27;武&#x27;: 0.1, &#x27;汉&#x27;: 0.9, &#x27;市&#x27;: 1, &#x27;长&#x27;: 0.05, &#x27;江&#x27;: 0.09, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.2&#125;,</span><br><span class="line">     &#x27;汉&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0.2, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.03, &#x27;江&#x27;: 0.04, &#x27;大&#x27;: 0.02, &#x27;桥&#x27;: 0.07&#125;,</span><br><span class="line">     &#x27;市&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.01, &#x27;江&#x27;: 0.02, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.05&#125;,</span><br><span class="line">     &#x27;长&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0.25, &#x27;江&#x27;: 0.1, &#x27;大&#x27;: 0.08, &#x27;桥&#x27;: 0.09&#125;,</span><br><span class="line">     &#x27;江&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0.15, &#x27;大&#x27;: 0.04, &#x27;桥&#x27;: 0.015&#125;,</span><br><span class="line">     &#x27;大&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0.14, &#x27;桥&#x27;: 0.15&#125;,</span><br><span class="line">     &#x27;桥&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0, &#x27;桥&#x27;: 0.12&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">newgperes = &#123;&#125;</span><br><span class="line">for k, v in gperes.items():</span><br><span class="line">     for w, a in v.items():</span><br><span class="line">          newgperes.setdefault(w, &#123;&#125;)</span><br><span class="line">          newgperes[w][k] = a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.rcParams[&#x27;font.sans-serif&#x27;]=[&#x27;Arial Unicode MS&#x27;]</span><br><span class="line">plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False</span><br><span class="line">gpedata = pd.DataFrame(newgperes)</span><br><span class="line">ax = sns.heatmap(gpedata, cmap=&quot;YlOrRd&quot;)</span><br><span class="line">ax.xaxis.tick_top()</span><br><span class="line">ax.set_yticklabels(ax.get_yticklabels(), rotation=0)</span><br><span class="line">plt.savefig(&#x27;loctest.png&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://s3.bmp.ovh/imgs/2021/12/4100fe169b8b9921.png" alt="gpe.png"></p>
<h1 id="NLP文本注意力可视化"><a href="#NLP文本注意力可视化" class="headerlink" title="NLP文本注意力可视化"></a>NLP文本注意力可视化</h1><blockquote>
<p>根据注意力矩阵，得到html颜色深浅代表相关性强弱</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Credits to Lin Zhouhan(@hantek) for the complete visualization code</span><br><span class="line">import random, os, numpy, scipy</span><br><span class="line">from codecs import open</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def createHTML(texts, weights, savepath):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Creates a html file with text heat.</span><br><span class="line">	weights: attention weights for visualizing</span><br><span class="line">	texts: text on which attention weights are to be visualized</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    fOut = open(savepath, &quot;w&quot;, encoding=&quot;utf-8&quot;)</span><br><span class="line">    part1 = &quot;&quot;&quot;</span><br><span class="line">    &lt;html lang=&quot;en&quot;&gt;</span><br><span class="line">    &lt;head&gt;</span><br><span class="line">    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;</span><br><span class="line">    &lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">    font-family: Sans-Serif;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;/style&gt;</span><br><span class="line">    &lt;/head&gt;</span><br><span class="line">    &lt;body&gt;</span><br><span class="line">    &lt;h3&gt;</span><br><span class="line">    Heatmaps</span><br><span class="line">    &lt;/h3&gt;</span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">    &lt;script&gt;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    part2 = &quot;&quot;&quot;</span><br><span class="line">    var color = &quot;255,0,0&quot;;</span><br><span class="line">    var ngram_length = 3;</span><br><span class="line">    var half_ngram = 1;</span><br><span class="line">    for (var k=0; k &lt; any_text.length; k++) &#123;</span><br><span class="line">    var tokens = any_text[k].split(&quot; &quot;);</span><br><span class="line">    var intensity = new Array(tokens.length);</span><br><span class="line">    var max_intensity = Number.MIN_SAFE_INTEGER;</span><br><span class="line">    var min_intensity = Number.MAX_SAFE_INTEGER;</span><br><span class="line">    for (var i = 0; i &lt; intensity.length; i++) &#123;</span><br><span class="line">    intensity[i] = 0.0;</span><br><span class="line">    for (var j = -half_ngram; j &lt; ngram_length-half_ngram; j++) &#123;</span><br><span class="line">    if (i+j &lt; intensity.length &amp;&amp; i+j &gt; -1) &#123;</span><br><span class="line">    intensity[i] += trigram_weights[k][i + j];</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if (i == 0 || i == intensity.length-1) &#123;</span><br><span class="line">    intensity[i] /= 2.0;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">    intensity[i] /= 3.0;</span><br><span class="line">    &#125;</span><br><span class="line">    if (intensity[i] &gt; max_intensity) &#123;</span><br><span class="line">    max_intensity = intensity[i];</span><br><span class="line">    &#125;</span><br><span class="line">    if (intensity[i] &lt; min_intensity) &#123;</span><br><span class="line">    min_intensity = intensity[i];</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    var denominator = max_intensity - min_intensity;</span><br><span class="line">    for (var i = 0; i &lt; intensity.length; i++) &#123;</span><br><span class="line">    intensity[i] = (intensity[i] - min_intensity) / denominator;</span><br><span class="line">    &#125;</span><br><span class="line">    if (k%2 == 0) &#123;</span><br><span class="line">    var heat_text = &quot;&lt;p&gt;&lt;br&gt;&lt;b&gt;Example:&lt;/b&gt;&lt;br&gt;&quot;;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">    var heat_text = &quot;&lt;b&gt;Example:&lt;/b&gt;&lt;br&gt;&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    var space = &quot;&quot;;</span><br><span class="line">    for (var i = 0; i &lt; tokens.length; i++) &#123;</span><br><span class="line">    heat_text += &quot;&lt;span style=&#x27;background-color:rgba(&quot; + color + &quot;,&quot; + intensity[i] + &quot;)&#x27;&gt;&quot; + space + tokens[i] + &quot;&lt;/span&gt;&quot;;</span><br><span class="line">    if (space == &quot;&quot;) &#123;</span><br><span class="line">    space = &quot; &quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //heat_text += &quot;&lt;p&gt;&quot;;</span><br><span class="line">    document.body.innerHTML += heat_text;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;/script&gt;</span><br><span class="line">    &lt;/html&gt;&quot;&quot;&quot;</span><br><span class="line">    putQuote = lambda x: &quot;\&quot;%s\&quot;&quot; % x</span><br><span class="line">    textsString = &quot;var any_text = [%s];\n&quot; % (&quot;,&quot;.join(map(putQuote, texts)))</span><br><span class="line">    weightsString = &quot;var trigram_weights = [%s];\n&quot; % (&quot;,&quot;.join(map(str, weights)))</span><br><span class="line">    fOut.write(part1)</span><br><span class="line">    fOut.write(textsString)</span><br><span class="line">    fOut.write(weightsString)</span><br><span class="line">    fOut.write(part2)</span><br><span class="line">    fOut.close()</span><br><span class="line"></span><br><span class="line">    return</span><br><span class="line"></span><br><span class="line">gperes = &#123;</span><br><span class="line">     &#x27;武&#x27;: &#123;&#x27;武&#x27;: 0.1, &#x27;汉&#x27;: 0.9, &#x27;市&#x27;: 1, &#x27;长&#x27;: 0.05, &#x27;江&#x27;: 0.09, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.2&#125;,</span><br><span class="line">     &#x27;汉&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0.2, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.03, &#x27;江&#x27;: 0.04, &#x27;大&#x27;: 0.02, &#x27;桥&#x27;: 0.07&#125;,</span><br><span class="line">     &#x27;市&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.01, &#x27;江&#x27;: 0.02, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.05&#125;,</span><br><span class="line">     &#x27;长&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0.25, &#x27;江&#x27;: 0.1, &#x27;大&#x27;: 0.08, &#x27;桥&#x27;: 0.09&#125;,</span><br><span class="line">     &#x27;江&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0.15, &#x27;大&#x27;: 0.04, &#x27;桥&#x27;: 0.015&#125;,</span><br><span class="line">     &#x27;大&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0.14, &#x27;桥&#x27;: 0.15&#125;,</span><br><span class="line">     &#x27;桥&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0, &#x27;桥&#x27;: 0.12&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">matrix = []</span><br><span class="line"># 获取矩阵表示</span><br><span class="line">for k, v in gperes.items():</span><br><span class="line">    matrix.append(list(v.values()))</span><br><span class="line">str_ = &#x27;武 汉 市 长 江 大 桥&#x27;</span><br><span class="line">createHTML([str_] * 7,</span><br><span class="line">           matrix,</span><br><span class="line">           &#x27;./visualization/test.html&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.bmp.ovh/imgs/2021/12/04573a3a1f1fb2b6.png" alt="wx20211211-193035@2x.png"></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://blog.csdn.net/qq_38607066/article/details/101345282#t17">https://blog.csdn.net/qq_38607066/article/details/101345282#t17</a></p>
<p><a href="https://blog.csdn.net/u010490755/article/details/89574847">https://blog.csdn.net/u010490755/article/details/89574847</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>2022预训练的下一步是什么</title>
    <url>/2021/12/31/2021-12-31-2022%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5%E6%98%AF%E4%BB%80%E4%B9%88/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文内容为自己对2021年自身算法经历的回顾，同时展望了未来研究的方向。如有理解不对的地方，欢迎指正批评。</p>
<p>2021年研究热点</p>
<ul>
<li>大规模预训练</li>
<li>对比学习</li>
<li>prompt</li>
</ul>
<p>展望未来</p>
<p>回顾自身算法经历</p>
<ol>
<li>需求分析</li>
<li>模型选型及设计</li>
<li>数据分析</li>
<li>模型训练及优化</li>
<li>分析负例<ol>
<li>检查数据质量是否过差</li>
<li>根据指标进行分析<ul>
<li>recall低</li>
<li>precision低</li>
</ul>
</li>
</ol>
</li>
</ol>
<span id="more"></span>
<blockquote>
<p>该内容为自己对2021年自身算法经历的回顾，同时展望了未来研究的方向。如有理解不对的地方，欢迎指针批评。</p>
</blockquote>
<h1 id="2021年研究热点"><a href="#2021年研究热点" class="headerlink" title="2021年研究热点"></a>2021年研究热点</h1><h2 id="大规模预训练"><a href="#大规模预训练" class="headerlink" title="大规模预训练"></a>大规模预训练</h2><p>预训练+微调的做法，在多个下游领域取得优异的表现。而在过去的一年里，预训练模型更是在往<strong>大而深</strong>的方向发展。</p>
<p>目前，国内已有智源研究院、鹏城实验室、中科院自动化所、阿里、百度、华为、浪潮等科研院所和企业研相继发出“悟道”、“盘古”、“紫东 · 太初”、M6、PLUG、ERNIE 3.0 等大模型。</p>
<p>但是模型在往大而深方向发展的同时，也存在如下亟待解决的问题：</p>
<ul>
<li>如何解释预训练模型的理论基础（如大模型智能的参数规模极限存在吗）</li>
<li>如何将大模型高效、低成本的应用于实际系统</li>
<li>如何克服构建大模型的数据质量、训练效率、算力消耗、模型交付等诸多障碍</li>
<li>如何解决目前大部分大模型普遍缺乏认知能力的问题</li>
</ul>
<h2 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h2><p><strong>对比学习的出发点在于避免模型坍塌，理想的模型应该符合alignment和uniformity，即语义相近的句子彼此聚集，语义无关的句子均匀分布。</strong></p>
<p>如果仅仅通过数据增强构建正例，随机句子作为负例，并为其打上0，1标签，存在以下问题：</p>
<ul>
<li>数据增强生成正例的变化有限</li>
<li>随机搭配成负例，含有除正例组合外其他组合全部为0的诱导</li>
<li>0，1标签的赋予太过绝对，对相似性表述不够准确</li>
</ul>
<p>因此对比学习的核心思想转变为：</p>
<script type="math/tex; mode=display">score(X,X^{'}) >> score(X,Y)</script><p>其中，X代表原样本，$X^{‘c}$代表数据增强的正样本，Y代表随机选择的负样本。</p>
<p>根据该思想，对比学习采用InfoNCE损失函数：</p>
<script type="math/tex; mode=display">loss = -log \frac{exp(score(X,X^{'}))}{score(X,X^{'})+\sum_{i=1}^{N}score(X,Y_i)}</script><p>通过该损失函数实现正例拉近，负例推远的效果。</p>
<h2 id="prompt"><a href="#prompt" class="headerlink" title="prompt"></a>prompt</h2><p>prompt被誉为NLP领域的新范式，与预训练+微调的范式相比，其过程分为：”pre-train, prompt, and predict”。</p>
<p><strong>prompt的出发点在于以更轻量化的方式利用预训练模型，避免微调与预训练之间存在的差异。</strong></p>
<p>prompt通过构建模版的方式，将下游任务转为与预训练相似的MLM任务，以该方式充分发挥预训练模型的性能。</p>
<p>以文本情感分类任务中，”I love this movie.”句子为例，prompt按照以下方式进行处理：</p>
<ol>
<li>生成prompt句子</li>
</ol>
<p>该步骤完成输入句子到模型输入的映射：</p>
<script type="math/tex; mode=display">x^{'}=f_{prompt}(x)</script><p>其中，$x^{‘}$为生成的prompt句子，x为输入句子，$f_{prompt}$为prompt函数。</p>
<p>在本例中，使用的模版为： “ [X] Overall, it was a [Z] movie.”</p>
<p>因此，得到的，$x^{‘}$为”I love this movie. Overall it was a [Z] movie.”</p>
<ol>
<li>模型预测</li>
</ol>
<p>该步骤将$x^{‘}$输入模型，模型完成模版空白位置的词语预测。</p>
<p>在本例中，模型可能预测为：”excellent”, “great”, “wonderful” 等词语</p>
<ol>
<li>结果映射</li>
</ol>
<p>通常模型预测的词语与任务输出存在一定差距，因此我们需要完成词语到输出结果的映射。</p>
<script type="math/tex; mode=display">y = f(x^{'})</script><p>在本例中，”excellent”, “great”, “wonderful” 等词语映射为标签 “++”</p>
<h1 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a>展望未来</h1><p>首先我认为当前基于数据驱动方法存在如下的问题：</p>
<ol>
<li>长尾效应：自然界中的数据分布就是长尾的，在学习的过程中，模型容易发生过拟合，泛化性较差。</li>
<li>数据噪声：有标签的数据，在标注过程中就不可避免的存在噪声。尤其是多位人员一起标注时，不同标注人员根据自身的理解完成数据的标注，但不同的人自身理解存在偏差，因此标注结果极易存在误差。归根到底：标注的规范难以确定，无法统一大家的知识库。</li>
</ol>
<p>当前我遇到的一些问题分享：模型仍无法很好地处理下述问题：</p>
<blockquote>
<p>太阳有几只眼睛？</p>
<p>姚明与奥尼尔身高谁比较高？</p>
<p>猫咪可以吃生蛋黄吗？猫咪是可以吃蛋黄的。这里特定煮熟的白水蛋，猫咪不能吃生鸡蛋，因为生鸡蛋中有细菌。</p>
<p>物质都是由分子构成的吗？物质都是由分子构成的，分子又由原子构成-错的！因为有些物质是不含分子的。</p>
</blockquote>
<p>这些问题，我总结为两方面的困难：</p>
<ol>
<li>缺乏知识，由于预训练与微调领域存在偏差，模型在下游任务中缺乏特定知识，同时模型在一些常识问题上表现较差。</li>
<li>缺乏深度语义的理解，模型表现的更像通过字面匹配完成任务，推理的成分更弱。</li>
</ol>
<p>当前研究热点仍然在于挖掘预训练模型的能力，但在基于常识性知识与逻辑推理的问题上，这种基于数据驱动的方式从底层就存在问题。引用一下大咖们对2022年的展望。</p>
<blockquote>
<p>大模型一方面在不少问题上取得了以往难以预期的成功，另一方面其巨大的训练能耗和碳排放是不能忽视的问题。个人以为，大模型未来会在一些事关国计民生的重大任务上发挥作用，而在其他一些场景下或许会通过类似集成学习的手段来利用小模型，尤其是通过很少量训练来 “复用” 和集成已有的小模型来达到不错的性能。</p>
<p>我们提出了一个叫做 “学件” 的思路，目前在做一些这方面的探索。大致思想是，假设很多人已经做了模型并且乐意放到某个市场去共享，市场通过建立规约来组织和管理学件，以后的人再做新应用时，就可以不用从头收集数据训练模型，可以先利用规约去市场里找找看是否有比较接近需求的模型，然后拿回家用自己的数据稍微打磨就能用。这其中还有一些技术挑战需要解决，我们正在研究这个方向。</p>
<p>另一方面，有可能通过利用人类的常识和专业领域知识，使模型得以精简，这就要结合逻辑推理和机器学习。逻辑推理比较善于利用人类知识，机器学习比较善于利用数据事实，如何对两者进行有机结合一直是人工智能中的重大挑战问题。麻烦的是逻辑推理是严密的基于数理逻辑的 “从一般到特殊”的演绎过程，机器学习是不那么严密的概率近似正确的 “从特殊到一般”的归纳过程，在方法论上就非常不一样。已经有的探索大体上是以其中某一方为倚重，引入另一方的某些成分，我们最近在探索双方相对均衡互促利用的方式。</p>
</blockquote>
<p>谈谈自己的理解，<strong>预训练模型的方式归根到底仍然属于数据驱动的任务，其通过在大规模数据上学习，推断未知数据的概率。如果说数据中存在表述不准确、表述有歧义或者词汇本身就有多个含义的话，以概率的方式难以解决这些问题。</strong> 而人脑在未知问题上，推理成分居多，以一词多义为例，人类会考虑该词汇有几种用法，考虑在这种上下文语境下使用哪一种用法，所以是否可以建立一套类似于标准公理的语言规范，以该规范为基础，对未知句子进行拆解推理，理解句子的完整含义。通过了解模型的推理过程，模型的可解释性增强。当预测错误时，我们可以进行溯源分析，对模型依赖的知识进行调整，或者让模型学习的更充分。</p>
<p>接下来对自己2022年的期望：</p>
<ol>
<li>自身学习更多模型结构变化的同时，更多地理解业务的架构，明白模型在业务中起的作用。</li>
<li>在算法研究上能够研究的更加深入，希望能够找到解决上述困难的方法。</li>
</ol>
<h1 id="回顾自身算法经历"><a href="#回顾自身算法经历" class="headerlink" title="回顾自身算法经历"></a>回顾自身算法经历</h1><p>2021年自身的算法经历主要分为：实习、算法比赛、项目、论文四部分。在这些经历里面主要接触分类、阅读理解、信息抽取三种任务，评估方式均采用精确率、召回率及F1值。下面将以这些经历为基础，介绍我处理这些任务的方式。</p>
<h2 id="1-需求分析"><a href="#1-需求分析" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h2><p>开展算法工作之前，首先要搞清楚算法需要满足什么样的需求。包括：</p>
<ul>
<li>业务属于什么样的任务</li>
<li>算法需要侧重的方向</li>
<li>训练数据及线上数据的情况</li>
<li>线上的指标</li>
<li>线下的评估方式</li>
<li>……</li>
</ul>
<p><strong>需求分析的目的在于了解业务的需求与算法在业务中起到的作用。</strong></p>
<h2 id="2-模型选型及设计"><a href="#2-模型选型及设计" class="headerlink" title="2. 模型选型及设计"></a>2. 模型选型及设计</h2><p>在明白需求之后，需要根据任务类型选择模型，并根据需求的不同，对模型结构进行调整。如阅读理解任务下：针对多答案、无答案的情况，我们需要调整模型的结构。</p>
<p><strong>模型选型及设计的目的在于选择或设计能够很好地满足业务需求的模型。</strong></p>
<h2 id="3-数据分析"><a href="#3-数据分析" class="headerlink" title="3. 数据分析"></a>3. 数据分析</h2><p><strong>数据分析这一步是最重要的一步，当前模型主要还是以数据驱动，数据对模型的影响很大。</strong> </p>
<p>我主要从以下角度进行分析：</p>
<ul>
<li>数据是否存在噪声：标点、大小写、特殊符号等</li>
<li>训练集测试集分布是否存在差异，测试集能否反映模型在具体业务下的表现</li>
<li>数据存在哪些特征，通过引入额外的特征，模型可以表现地更好</li>
<li>训练集分布：标签分布、长度分布等，是否会给模型带来类别不均衡、长文本等问题</li>
<li>数据量大小，数据量足够时可以继续预训练</li>
</ul>
<p><strong>数据分析的目的在于数据能否充分发挥模型性能，能否得到符合业务需求的模型</strong></p>
<h2 id="4-模型训练及优化"><a href="#4-模型训练及优化" class="headerlink" title="4. 模型训练及优化"></a>4. 模型训练及优化</h2><p><strong>模型进行训练，开始炼丹【调参】。</strong></p>
<ul>
<li>设置合适的超参数【可以通过一些超参数搜索算法】</li>
<li>选择合适的优化器【adam/adamw/sgd】</li>
<li>学习率调整的策略</li>
</ul>
<p><strong>进阶版：</strong></p>
<ul>
<li>对抗训练</li>
<li>对比学习</li>
<li>UDA等数据增强方式</li>
<li>继续预训练</li>
<li>多任务学习</li>
<li>伪标签</li>
<li>SWA</li>
<li>……</li>
</ul>
<h2 id="5-分析负例"><a href="#5-分析负例" class="headerlink" title="5. 分析负例"></a>5. 分析负例</h2><p>该过程同样重要，我们需要了解模型在测试数据上的表现情况，在什么数据表现较差，如何优化这些负例。</p>
<p><strong>在优化过程中，建议记录每一次优化信息，分析模型的提升/降低是否符合自己预期，充分利用每一次实验</strong></p>
<p>下面总结了我在优化过程常用的分析方式：</p>
<h3 id="1-检查数据质量是否过差"><a href="#1-检查数据质量是否过差" class="headerlink" title="1. 检查数据质量是否过差"></a>1. 检查数据质量是否过差</h3><p>这种情况通常表现为数据质量较差，模型在原始数据上表现不佳，精确率与召回率都很低。针对这种情况，需要对数据做必要的预处理，让模型能够更好地学习。</p>
<h3 id="2-根据指标进行分析"><a href="#2-根据指标进行分析" class="headerlink" title="2. 根据指标进行分析"></a>2. 根据指标进行分析</h3><h4 id="recall低"><a href="#recall低" class="headerlink" title="recall低"></a>recall低</h4><p>召回率表示召回的数量，测试集数据未召回较多，则从下列角度检查数据：</p>
<ol>
<li>训练集测试集数据差异是否较大，即训练集中是否存在类似数据，若不存在则引入更多数据或者对该数据进行数据增强。<strong>这种情况，常见原因为数据分布不均衡-少数数据训练不充分；训练集、测试集分布差异较大导致</strong></li>
<li>训练集中存在类似数据，检查训练集中该种情况有无标注错误：漏标、错标。</li>
</ol>
<h4 id="precision低"><a href="#precision低" class="headerlink" title="precision低"></a>precision低</h4><p>精确率表示预测出的准确率，测试集数据分错的较多：</p>
<ol>
<li>检查数据分布，是否数据分布不均衡。<strong>数据不均衡导致模型倾向于预测数量较多的数据，精确率下降</strong></li>
<li>标签定义是否准确，是否存在两类标签混淆的情况。<strong>这种情况，需要考虑对标签进行融合</strong></li>
</ol>
<p>类别不均衡常用解决方式：</p>
<ul>
<li>数据增强</li>
<li>resample</li>
<li>reweight</li>
<li>集成学习</li>
</ul>
<p>数据错误常用解决方式：</p>
<ul>
<li>交叉验证</li>
<li>置信学习</li>
<li>聚类分析</li>
</ul>
<p>接下来的过程则是迭代分析，直到模型性能符合业务需求。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://mp.weixin.qq.com/s/RqkQzeR5BOVpU7tj_zUgqQ">https://mp.weixin.qq.com/s/RqkQzeR5BOVpU7tj_zUgqQ</a></p>
<p><a href="https://www.zhihu.com/question/480187938/answer/2103245373">https://www.zhihu.com/question/480187938/answer/2103245373</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/399295895">https://zhuanlan.zhihu.com/p/399295895</a></p>
]]></content>
      <categories>
        <category>年度总结</category>
        <category>2021</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>年度总结</tag>
        <tag>NLP</tag>
        <tag>预训练</tag>
        <tag>对比学习</tag>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据分析-数据可视化(二)</title>
    <url>/2022/01/09/2022-01-09-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96(%E4%BA%8C)/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>看到有些论文插图十分简洁美观，于是便摸索一下如何美化一下折线图绘图。本文将在前文<a href="https://jmxgodlz.xyz/2020/01/03/2020-01-03-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/#more">Python数据分析-数据可视化</a>的基础上，介绍折线图格式的调整。</p>
<p>本文使用的画图工具为matplotlib，相关API可访问<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html">python matplotlib文档</a>。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gy7pemcxsnj30m80ecdh1.jpg" alt=""></p>
<span id="more"></span>
<h1 id="Matplotlib-折线图格式调整"><a href="#Matplotlib-折线图格式调整" class="headerlink" title="Matplotlib 折线图格式调整"></a>Matplotlib 折线图格式调整</h1><p>首先，贴一下文档中折线图绘制的附加参数表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>agg_filter</td>
<td>a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array</td>
</tr>
<tr>
<td>alpha</td>
<td>scalar or None</td>
</tr>
<tr>
<td>animated</td>
<td>bool</td>
</tr>
<tr>
<td>antialiased or aa</td>
<td>bool</td>
</tr>
<tr>
<td>clip_box</td>
<td>Bbox</td>
</tr>
<tr>
<td>clip_on</td>
<td>bool</td>
</tr>
<tr>
<td>clip_path</td>
<td>Patch or (Path, Transform) or None</td>
</tr>
<tr>
<td>color or c</td>
<td>color</td>
</tr>
<tr>
<td>dash_capstyle</td>
<td>CapStyle or {‘butt’, ‘projecting’, ‘round’}</td>
</tr>
<tr>
<td>dash_joinstyle</td>
<td>JoinStyle or {‘miter’, ‘round’, ‘bevel’}</td>
</tr>
<tr>
<td>dashes</td>
<td>sequence of floats (on/off ink in points) or (None, None)</td>
</tr>
<tr>
<td>data</td>
<td>(2, N) array or two 1D arrays</td>
</tr>
<tr>
<td>drawstyle or ds</td>
<td>{‘default’, ‘steps’, ‘steps-pre’, ‘steps-mid’, ‘steps-post’}, default: ‘default’</td>
</tr>
<tr>
<td>figure</td>
<td>Figure</td>
</tr>
<tr>
<td>fillstyle</td>
<td>{‘full’, ‘left’, ‘right’, ‘bottom’, ‘top’, ‘none’}</td>
</tr>
<tr>
<td>gid</td>
<td>str</td>
</tr>
<tr>
<td>in_layout</td>
<td>bool</td>
</tr>
<tr>
<td>label</td>
<td>object</td>
</tr>
<tr>
<td>linestyle or ls</td>
<td>{‘-‘, ‘—‘, ‘-.’, ‘:’, ‘’, (offset, on-off-seq), …}</td>
</tr>
<tr>
<td>linewidth or lw</td>
<td>float</td>
</tr>
<tr>
<td>marker</td>
<td>marker style string, Path or MarkerStyle</td>
</tr>
<tr>
<td>markeredgecolor or mec</td>
<td>color</td>
</tr>
<tr>
<td>markeredgewidth or mew</td>
<td>float</td>
</tr>
<tr>
<td>markerfacecolor or mfc</td>
<td>color</td>
</tr>
<tr>
<td>markerfacecoloralt or mfcalt</td>
<td>color</td>
</tr>
<tr>
<td>markersize or ms</td>
<td>float</td>
</tr>
<tr>
<td>markevery</td>
<td>None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]</td>
</tr>
<tr>
<td>path_effects</td>
<td>AbstractPathEffect</td>
</tr>
<tr>
<td>picker</td>
<td>float or callable[[Artist, Event], tuple[bool, dict]]</td>
</tr>
<tr>
<td>pickradius</td>
<td>float</td>
</tr>
<tr>
<td>rasterized</td>
<td>bool</td>
</tr>
<tr>
<td>sketch_params</td>
<td>(scale: float, length: float, randomness: float)</td>
</tr>
<tr>
<td>snap</td>
<td>bool or None</td>
</tr>
<tr>
<td>solid_capstyle</td>
<td>CapStyle or {‘butt’, ‘projecting’, ‘round’}</td>
</tr>
<tr>
<td>solid_joinstyle</td>
<td>JoinStyle or {‘miter’, ‘round’, ‘bevel’}</td>
</tr>
<tr>
<td>transform</td>
<td>unknown</td>
</tr>
<tr>
<td>url</td>
<td>str</td>
</tr>
<tr>
<td>visible</td>
<td>bool</td>
</tr>
<tr>
<td>xdata</td>
<td>1D array</td>
</tr>
<tr>
<td>ydata</td>
<td>1D array</td>
</tr>
<tr>
<td>zorder</td>
<td>float</td>
</tr>
</tbody>
</table>
</div>
<p><strong>接下来，我将挑选几个常用的附加参数介绍使用方式与效果。</strong></p>
<h2 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h2><ol>
<li>附加参数名：label</li>
<li>功能：为绘制曲线命名，该名称会在图例显示</li>
<li>使用方式：plt.plot(x,y,label=’example’)</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic3.zhimg.com/80/v2-b76ae257686d07ce9c469d0053286e92_1440w.webp" alt=""></p>
<h2 id="线条颜色"><a href="#线条颜色" class="headerlink" title="线条颜色"></a>线条颜色</h2><ol>
<li>附加参数名：color</li>
<li>功能：选择绘制线条的颜色</li>
<li>使用方式：plt.plot(x,y,color=’r’)</li>
<li>颜色选取方式分为三种：</li>
</ol>
<ul>
<li>用全名或简称 ，如blue或b</li>
<li>16进制 ，如FF00FF</li>
<li>(r, g, b) 或 (r, g, b, a)，如（1,0,1,1） ，其中 r g b a 取均为[0, 1]之间，[0, 1]之间的浮点数的字符串形式，表示灰度值。0表示黑色，1表示白色</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic3.zhimg.com/80/v2-a1256f476d9e062a706f81f2f38dd67e_1440w.webp" alt=""></p>
<h2 id="线条形状"><a href="#线条形状" class="headerlink" title="线条形状"></a>线条形状</h2><ol>
<li>附加参数名：linestyle(或ls)</li>
<li>功能：选择绘制线条的形状</li>
<li>使用方式：plt.plot(x,y,linestyle=’:’)或者plt.plot(x,y,ls=’:’)</li>
<li>常用形状：</li>
</ol>
<ul>
<li>-      实线(solid)</li>
<li>—     短线(dashed)</li>
<li>-.     短点相间线(dashdot)</li>
<li>：    虚点线(dotted)</li>
<li>‘’, ‘ ‘, None</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;, linestyle=&#x27;-&#x27;)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;, ls=&#x27;--&#x27;)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;, ls=&#x27;:&#x27;)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6), ls=&#x27;&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic3.zhimg.com/80/v2-ea4768baded601f29bd81d25a6220e72_1440w.webp" alt=""></p>
<h2 id="折点样式"><a href="#折点样式" class="headerlink" title="折点样式"></a>折点样式</h2><ol>
<li>附加参数名：<br>(1)marker — 折点形状</li>
</ol>
<p>(2)markeredgecolor 或 mec — 折点外边颜色</p>
<p>(3)markeredgewidth 或 mew — 折点线宽</p>
<p>(4)markerfacecolor 或 mfc —折点实心颜色</p>
<p>(5)markerfacecoloralt 或 mfcalt</p>
<p>(6)markersize 或 ms —折点大小</p>
<p>折点形状选择如下表:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>character</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&#39;-&#39;</code></td>
<td>solid line style</td>
</tr>
<tr>
<td><code>&#39;--&#39;</code></td>
<td>dashed line style</td>
</tr>
<tr>
<td><code>&#39;-.&#39;</code></td>
<td>dash-dot line style</td>
</tr>
<tr>
<td><code>&#39;:&#39;</code></td>
<td>dotted line style</td>
</tr>
<tr>
<td><code>&#39;.&#39;</code></td>
<td>point marker</td>
</tr>
<tr>
<td><code>&#39;,&#39;</code></td>
<td>pixel marker</td>
</tr>
<tr>
<td><code>&#39;o&#39;</code></td>
<td>circle marker</td>
</tr>
<tr>
<td><code>&#39;v&#39;</code></td>
<td>triangle_down marker</td>
</tr>
<tr>
<td><code>&#39;^&#39;</code></td>
<td>triangle_up marker</td>
</tr>
<tr>
<td><code>&#39;&lt;&#39;</code></td>
<td>triangle_left marker</td>
</tr>
<tr>
<td><code>&#39;&gt;&#39;</code></td>
<td>triangle_right marker</td>
</tr>
<tr>
<td><code>&#39;1&#39;</code></td>
<td>tri_down marker</td>
</tr>
<tr>
<td><code>&#39;2&#39;</code></td>
<td>tri_up marker</td>
</tr>
<tr>
<td><code>&#39;3&#39;</code></td>
<td>tri_left marker</td>
</tr>
<tr>
<td><code>&#39;4&#39;</code></td>
<td>tri_right marker</td>
</tr>
<tr>
<td><code>&#39;s&#39;</code></td>
<td>square marker</td>
</tr>
<tr>
<td><code>&#39;p&#39;</code></td>
<td>pentagon marker</td>
</tr>
<tr>
<td><code>&#39;*&#39;</code></td>
<td>star marker</td>
</tr>
<tr>
<td><code>&#39;h&#39;</code></td>
<td>hexagon1 marker</td>
</tr>
<tr>
<td><code>&#39;H&#39;</code></td>
<td>hexagon2 marker</td>
</tr>
<tr>
<td><code>&#39;+&#39;</code></td>
<td>plus marker</td>
</tr>
<tr>
<td><code>&#39;x&#39;</code></td>
<td>x marker</td>
</tr>
<tr>
<td><code>&#39;D&#39;</code></td>
<td>diamond marker</td>
</tr>
<tr>
<td><code>&#39;d&#39;</code></td>
<td>thin_diamond marker</td>
</tr>
<tr>
<td>``’</td>
<td>‘``</td>
<td>vline marker</td>
</tr>
<tr>
<td><code>&#39;_&#39;</code></td>
<td>hline marker</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;, linestyle=&#x27;-&#x27;, marker=&#x27;o&#x27;)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;, ls=&#x27;--&#x27;, marker=&#x27;1&#x27;)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;, ls=&#x27;:&#x27;, marker=&#x27;2&#x27;)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6), marker=&#x27;3&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic1.zhimg.com/80/v2-00fdeaa9405065aa4cd35ff740b99754_1440w.webp" alt=""></p>
<h2 id="线条透明度"><a href="#线条透明度" class="headerlink" title="线条透明度"></a>线条透明度</h2><ol>
<li>附加参数名：alpha,值在[0,1]之间</li>
<li>功能：选择绘制线条的透明度</li>
<li>使用方式：plt.plot(x,y,alpha=’0.9’)</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;, linestyle=&#x27;-&#x27;, alpha=0.3)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;, ls=&#x27;--&#x27;, alpha=0.1)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;, ls=&#x27;:&#x27;, alpha=0.5)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6), ls=&#x27;&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://pic4.zhimg.com/80/v2-b17579efda53b64ef09cc794ea0fdc27_1440w.webp" alt=""></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>数据分析</category>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络调参-warmup and decay</title>
    <url>/2022/01/25/2022-01-25-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82-warmup%20and%20decay/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文将介绍神经网络调参技巧：warmup，decay。反向传播主要完成参数更新：$\theta_t=\theta_{t-1}-\alpha * g_t$，其中$\alpha$为学习率，$g_t$为梯度更新量，而warmup、decay就是调整$\alpha$的方式，优化器决定梯度更新的方式即$g_t$的计算方式。衰减方式如下图所示：</p>
<p><img src="https://pic2.zhimg.com/80/v2-e33b13a40632425c6e9ec680d13bcf29_1440w.jpg" alt=""><br><span id="more"></span></p>
<h1 id="warmup-and-decay"><a href="#warmup-and-decay" class="headerlink" title="warmup and decay"></a>warmup and decay</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Warmup and Decay是模型训练过程中，一种学习率（learning rate）的调整策略。</p>
<p>Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，训练了一些epoches或者steps(比如4个epoches,10000steps),再修改为预先设置的学习来进行训练。</p>
<p>同理，Decay是学习率衰减方法，它指定在训练到一定epoches或者steps后，按照线性或者余弦函数等方式，将学习率降低至指定值。一般，使用Warmup and Decay，学习率会遵循从小到大，再减小的规律。</p>
<h2 id="为什么要warmup"><a href="#为什么要warmup" class="headerlink" title="为什么要warmup"></a>为什么要warmup</h2><p>这里引用知乎：<a href="https://www.zhihu.com/question/338066667/answer/771252708的讨论：">https://www.zhihu.com/question/338066667/answer/771252708的讨论：</a><br><strong>SGD训练中常见的方式是初始较大的学习率，然后衰减为小的学习率，而warmup是先以较小的学习率上升到初始学习率，然后再衰减到小的学习率上，那么为什么warmup有效。</strong></p>
<h3 id="直观上解释"><a href="#直观上解释" class="headerlink" title="直观上解释"></a>直观上解释</h3><p>深层网络随机初始化差异较大，如果一开始以较大的学习率，初始学习带来的偏差在后续学习过程中难以纠正。</p>
<p>训练刚开始时梯度更新较大，若学习率设置较大则更新的幅度较大，该类型与传统学习率先大后小方式不同的原因在于起初浅层网络幅度大的更新并不会导致方向错误。</p>
<h3 id="理论上解释"><a href="#理论上解释" class="headerlink" title="理论上解释"></a>理论上解释</h3><p><strong>warmup带来的优点包含：</strong></p>
<ul>
<li>缓解模型在初期对mini-batch过拟合的现象</li>
<li>保持模型深层的稳定性</li>
</ul>
<p><strong>给出三个论文中的结论：</strong></p>
<ol>
<li>当batch大小增加时，学习率也可以成倍增加</li>
<li>限制大batch训练的是高学习率带来的训练不稳定性</li>
<li>warmup主要限制深层的权重变化，并且冻结深层权重的变化可以取得相似的效果</li>
</ol>
<h4 id="batch与学习率大小的关系"><a href="#batch与学习率大小的关系" class="headerlink" title="batch与学习率大小的关系"></a>batch与学习率大小的关系</h4><p>假设现在模型已经train到第t步，权重为$w_t$，我们有k个mini-batch，每个mini-batch大小为n，记为$\mathcal{B}_{1:k}$ 。下面我们来看，以学习率 $\eta$训k次 $\mathcal{B}_{1:k}$ 和以学习率 $\hat{\eta}$ 一次训练$\mathcal{B}$时学习率的关系。</p>
<p>假设我们用的是SGD，那么训k次后我们可以得到：</p>
<script type="math/tex; mode=display">
w_{t+k}=w_{t}-\eta \frac{1}{n} \sum_{j<k} \sum_{x \in \mathcal{B}_{j}} \nabla l\left(x, w_{t+j}\right)</script><p>如果我们一次训练就可以得到：</p>
<script type="math/tex; mode=display">
\hat{w}_{t+1}=w_{t}-\hat{\eta} \frac{1}{k n} \sum_{j<k} \sum_{x \in \mathcal{B}_{j}} \nabla l\left(x, w_{t}\right)</script><p>其中$w_{t+k}$与$\hat{w}_{t+1}$代表按上述方式训练k次与1次，完成参数更新后的参数。显然，这两个是不一样的。但如果我们假设$\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)$，那么令$\hat{\eta}=k\eta $就可以保证 <script type="math/tex">\hat{w}_{t+1} \approx w_{t+k}</script> 。那么，在什么时候 $\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)$ 可能不成立呢？[1]告诉我们有两种情况：</p>
<ul>
<li>在训练的开始阶段，模型权重迅速改变</li>
<li>Mini-batch 大小较小，样本方差较大</li>
</ul>
<p>第一种情况，模型初始参数分布取决于初始化方式，初始数据对于模型都是初次修正，所以梯度更新较大，若一开始以较大的学习率学习，易对数据造成过拟合，需要经过之后更多轮的训练进行修正。</p>
<p>第二种情况，在训练的过程中，如果有mini-batch内的数据分布方差特别大，这会导致模型学习剧烈波动，使其学得的权重很不稳定，这在训练初期最为明显，最后期较为缓解。</p>
<p>针对上述两种情况，并不能简单的成倍增长学习率$\hat{\eta}=k\eta$,因为此时不符合$\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)$假设。此时要么更改学习率增长方式[warmup]，要么解决这两种情况[数据预处理以减小样本方差]。</p>
<h4 id="warmup与模型学习的稳定性"><a href="#warmup与模型学习的稳定性" class="headerlink" title="warmup与模型学习的稳定性"></a>warmup与模型学习的稳定性</h4><p>该部分通过一些论文实验结果，推断有了warmup之后模型能够学习的更稳定。</p>
<p><img src="https://pic3.zhimg.com/80/v2-5d6ffb22059ddf3da93bac83c02482a6_1440w.webp" alt=""></p>
<p><strong>上图表示有了warmup之后，模型能够学习的更加稳定。</strong></p>
<p><img src="https://pic3.zhimg.com/80/v2-9440892a277639a91c2468e66ad65072_1440w.webp" alt=""></p>
<p><strong>上图b，c表示有了warmup之后，模型最后几层的相似性增加，避免模型不稳定的改变。</strong></p>
<h1 id="学习率衰减策略"><a href="#学习率衰减策略" class="headerlink" title="学习率衰减策略"></a>学习率衰减策略</h1><h2 id="可视化代码"><a href="#可视化代码" class="headerlink" title="可视化代码"></a>可视化代码</h2><p><strong>下列各种学习率衰减策略均采用warmup，为了图片反应的更加直观：起始学习率设置为1，warmup 步数为20，总步数为100。通常warmup步数可以设置为总步数的10%，参照BERT的经验策略。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># author： JMXGODLZZ</span><br><span class="line"># datetime： 2022/1/23 下午7:10 </span><br><span class="line"># ide： PyCharm</span><br><span class="line">import keras</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">from learningrateSchedules import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_cosine_with_hard_restarts_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_polynomial_decay_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_step_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_exp_schedule_with_warmup</span><br><span class="line">init_lr = 1</span><br><span class="line">warmupsteps = 20</span><br><span class="line">totalsteps = 100</span><br><span class="line"></span><br><span class="line">lrs = get_linear_schedule_with_warmup(1, warmupsteps, totalsteps)</span><br><span class="line">cos_warm_lrs = get_cosine_schedule_with_warmup(1, warmupsteps, totalsteps)</span><br><span class="line">cos_hard_warm_lrs = get_cosine_with_hard_restarts_schedule_with_warmup(1, warmupsteps, totalsteps, 2)</span><br><span class="line">poly_warm_lrs = get_polynomial_decay_schedule_with_warmup(1, warmupsteps, totalsteps, 0, 5)</span><br><span class="line">step_warm_lrs = get_step_schedule_with_warmup(1, warmupsteps, totalsteps)</span><br><span class="line">exp_warm_lrs = get_exp_schedule_with_warmup(1, warmupsteps, totalsteps, 0.9)</span><br><span class="line">x = list(range(totalsteps))</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, lrs, label=&#x27;linear_warmup&#x27;, color=&#x27;k&#x27;)</span><br><span class="line">plt.plot(x, cos_warm_lrs, label=&#x27;cosine_warmup&#x27;, color=&#x27;b&#x27;)</span><br><span class="line">plt.plot(x, cos_hard_warm_lrs, label=&#x27;cosine_cy2_warmup&#x27;, color=&#x27;g&#x27;)</span><br><span class="line">plt.plot(x, poly_warm_lrs, label=&#x27;polynomial_warmup_pw5&#x27;, color=&#x27;r&#x27;)</span><br><span class="line">plt.plot(x, step_warm_lrs, label=&#x27;step_warmup&#x27;, color=&#x27;purple&#x27;)</span><br><span class="line">plt.plot(x, exp_warm_lrs, label=&#x27;exp_warmup&#x27;, color=&#x27;orange&#x27;)</span><br><span class="line">plt.xlabel(&#x27;steps&#x27;)</span><br><span class="line">plt.ylabel(&#x27;learning rate&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://pic2.zhimg.com/80/v2-e33b13a40632425c6e9ec680d13bcf29_1440w.webp" alt=""></p>
<h2 id="指数衰减学习率"><a href="#指数衰减学习率" class="headerlink" title="指数衰减学习率"></a>指数衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_exp_schedule_with_warmup(learning_rate, num_warmup_steps, num_training_steps, gamma, last_epoch=-1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span><br><span class="line">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        stepmi = (current_step - num_warmup_steps)</span><br><span class="line">        return pow(gamma, stepmi)</span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="余弦衰减学习率"><a href="#余弦衰减学习率" class="headerlink" title="余弦衰减学习率"></a>余弦衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_cosine_schedule_with_warmup(</span><br><span class="line">    learning_rate, num_warmup_steps: int, num_training_steps: int, num_cycles: float = 0.5, last_epoch: int = -1</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases following the values of the cosine function between the</span><br><span class="line">    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the</span><br><span class="line">    initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        num_cycles (:obj:`float`, `optional`, defaults to 0.5):</span><br><span class="line">            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0</span><br><span class="line">            following a half-cosine).</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))</span><br><span class="line">        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))</span><br><span class="line"></span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="线性衰减学习率"><a href="#线性衰减学习率" class="headerlink" title="线性衰减学习率"></a>线性衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_linear_schedule_with_warmup(learning_rate, num_warmup_steps, num_training_steps, last_epoch=-1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span><br><span class="line">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        return max(</span><br><span class="line">            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))</span><br><span class="line">        )</span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="阶梯衰减学习率"><a href="#阶梯衰减学习率" class="headerlink" title="阶梯衰减学习率"></a>阶梯衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_step_schedule_with_warmup(learning_rate, num_warmup_steps, num_training_steps, last_epoch=-1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span><br><span class="line">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        stepmi = (current_step - num_warmup_steps) // 20 + 1</span><br><span class="line">        return pow(0.5, stepmi)</span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="多项式衰减学习率"><a href="#多项式衰减学习率" class="headerlink" title="多项式衰减学习率"></a>多项式衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_polynomial_decay_schedule_with_warmup(</span><br><span class="line">    learning_rate, num_warmup_steps, num_training_steps, lr_end=1e-7, power=1.0, last_epoch=-1</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the</span><br><span class="line">    optimizer to end lr defined by `lr_end`, after a warmup period during which it increases linearly from 0 to the</span><br><span class="line">    initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        lr_end (:obj:`float`, `optional`, defaults to 1e-7):</span><br><span class="line">            The end LR.</span><br><span class="line">        power (:obj:`float`, `optional`, defaults to 1.0):</span><br><span class="line">            Power factor.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Note: `power` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT</span><br><span class="line">    implementation at</span><br><span class="line">    https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    lr_init = learning_rate</span><br><span class="line">    if not (lr_init &gt; lr_end):</span><br><span class="line">        raise ValueError(f&quot;lr_end (&#123;lr_end&#125;) must be be smaller than initial lr (&#123;lr_init&#125;)&quot;)</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        elif current_step &gt; num_training_steps:</span><br><span class="line">            return lr_end / lr_init  # as LambdaLR multiplies by lr_init</span><br><span class="line">        else:</span><br><span class="line">            lr_range = lr_init - lr_end</span><br><span class="line">            decay_steps = num_training_steps - num_warmup_steps</span><br><span class="line">            pct_remaining = 1 - (current_step - num_warmup_steps) / decay_steps</span><br><span class="line">            decay = lr_range * pct_remaining ** power + lr_end</span><br><span class="line">            return decay / lr_init  # as LambdaLR multiplies by lr_init</span><br><span class="line"></span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="余弦循环衰减学习率"><a href="#余弦循环衰减学习率" class="headerlink" title="余弦循环衰减学习率"></a>余弦循环衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_cosine_with_hard_restarts_schedule_with_warmup(</span><br><span class="line">    learning_rate, num_warmup_steps: int, num_training_steps: int, num_cycles: int = 1, last_epoch: int = -1</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases following the values of the cosine function between the</span><br><span class="line">    initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases</span><br><span class="line">    linearly between 0 and the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        num_cycles (:obj:`int`, `optional`, defaults to 1):</span><br><span class="line">            The number of hard restarts to use.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))</span><br><span class="line">        if progress &gt;= 1.0:</span><br><span class="line">            return 0.0</span><br><span class="line">        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))</span><br><span class="line"></span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h1 id="学习率衰减实现"><a href="#学习率衰减实现" class="headerlink" title="学习率衰减实现"></a>学习率衰减实现</h1><h2 id="Pytorch学习率策略"><a href="#Pytorch学习率策略" class="headerlink" title="Pytorch学习率策略"></a>Pytorch学习率策略</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if args.scheduler == &quot;constant_schedule&quot;:</span><br><span class="line">    scheduler = get_constant_schedule(optimizer)</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;constant_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_constant_schedule_with_warmup(</span><br><span class="line">        optimizer, num_warmup_steps=args.warmup_steps</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;linear_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;cosine_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_cosine_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">        num_cycles=args.cosine_schedule_num_cycles,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;cosine_with_hard_restarts_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">        num_cycles=args.cosine_schedule_num_cycles,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;polynomial_decay_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_polynomial_decay_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">        lr_end=args.polynomial_decay_schedule_lr_end,</span><br><span class="line">        power=args.polynomial_decay_schedule_power,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">else:</span><br><span class="line">    raise ValueError(&quot;&#123;&#125; is not a valid scheduler.&quot;.format(args.scheduler))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="keras学习率策略"><a href="#keras学习率策略" class="headerlink" title="keras学习率策略"></a>keras学习率策略</h2><ul>
<li>Keras提供了四种衰减策略分别是ExponentialDecay(指数衰减)、 PiecewiseConstantDecay(分段常数衰减) 、 PolynomialDecay(多项式衰减)和InverseTimeDecay(逆时间衰减)。只要在Optimizer中指定衰减策略，一行代码就能实现，在以下方法一中详细介绍。</li>
<li>如果想要自定义学习率的衰减，有第二种方法，更加灵活，需要使用callbacks来实现动态、自定义学习率衰减策略，方法二中将详细介绍。</li>
<li>如果两种方法同时使用，默认优先使用第二种，第一种方法将被忽略。</li>
</ul>
<h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><h4 id="指数衰减"><a href="#指数衰减" class="headerlink" title="指数衰减"></a>指数衰减</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(</span><br><span class="line">    initial_learning_rate=1e-2,</span><br><span class="line">    decay_steps=10000,</span><br><span class="line">    decay_rate=0.96)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h4 id="分段衰减"><a href="#分段衰减" class="headerlink" title="分段衰减"></a>分段衰减</h4><p>[0~1000]的steps，学习率为1.0,[10001～9000]的steps，学习率为0.5，其他steps，学习率为0.1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">step = tf.Variable(0, trainable=False)</span><br><span class="line">boundaries = [1000, 10000]</span><br><span class="line">values = [1.0, 0.5, 0.1]</span><br><span class="line">learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)</span><br><span class="line">lr_scheduler = learning_rate_fn(step)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h4 id="多项式衰减"><a href="#多项式衰减" class="headerlink" title="多项式衰减"></a>多项式衰减</h4><p>在10000步中从0.1衰减到0.001，使用开根式( power=0.5)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start_lr = 0.1</span><br><span class="line">end_lr = 0.001</span><br><span class="line">decay_steps = 10000</span><br><span class="line">lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(</span><br><span class="line">    start_lr,</span><br><span class="line">    decay_steps,</span><br><span class="line">    end_lr,</span><br><span class="line">    power=0.5)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h4 id="逆时间衰减"><a href="#逆时间衰减" class="headerlink" title="逆时间衰减"></a>逆时间衰减</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">initial_lr = 0.1</span><br><span class="line">decay_steps = 1.0</span><br><span class="line">decay_rate = 0.5</span><br><span class="line">lr_scheduler = keras.optimizers.schedules.InverseTimeDecay(</span><br><span class="line">  initial_lr, decay_steps, decay_rate)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><h4 id="自定义指数衰减"><a href="#自定义指数衰减" class="headerlink" title="自定义指数衰减"></a>自定义指数衰减</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 第一步：自定义指数衰减策略</span><br><span class="line">def step_decay(epoch):</span><br><span class="line">    init_lr = 0.1</span><br><span class="line">    drop=0.5</span><br><span class="line">    epochs_drop=10</span><br><span class="line">    if epoch&lt;100:</span><br><span class="line">        return init_lr</span><br><span class="line">    else:</span><br><span class="line">        return init_lr*pow(drop,floor(1+epoch)/epochs_drop)</span><br><span class="line">        </span><br><span class="line"># ……</span><br><span class="line"># 第二步：用LearningRateScheduler封装学习率衰减策略</span><br><span class="line">lr_callback = LearningRateScheduler(step_decay)</span><br><span class="line"># 第三步：加入callbacks</span><br><span class="line">model = KerasClassifier(build_fn = create_model,epochs=200,batch_size=5,verbose=1,callbacks=[checkpoint,lr_callback])</span><br><span class="line">model.fit(X,Y)</span><br></pre></td></tr></table></figure>
<h4 id="动态修改学习率"><a href="#动态修改学习率" class="headerlink" title="动态修改学习率"></a>动态修改学习率</h4><p>ReduceLROnPlateau(monitor=’val_acc’, mode=’max’,min_delta=0.1,factor=0.2,patience=5, min_lr=0.001)</p>
<p>训练集连续patience个epochs的val_acc小于min_delta时，学习率将会乘以factor。mode可以选择max或者min，根据monitor的选择而灵活设定。min_lr是学习率的最低值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 第一步：ReduceLROnPlateau定义学习动态变化策略</span><br><span class="line">reduce_lr_callback = ReduceLROnPlateau(monitor=&#x27;val_acc&#x27;, factor=0.2,patience=5, min_lr=0.001)</span><br><span class="line"># 第二步：加入callbacks</span><br><span class="line">model = KerasClassifier(build_fn = create_model,epochs=200,batch_size=5,verbose=1,callbacks=[checkpoint,reduce_lr_callback])</span><br><span class="line">model.fit(X,Y)</span><br></pre></td></tr></table></figure>
<h3 id="Keras学习率回显代码"><a href="#Keras学习率回显代码" class="headerlink" title="Keras学习率回显代码"></a>Keras学习率回显代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_lr_metric(optimizer):</span><br><span class="line">    def lr(y_true, y_pred):</span><br><span class="line">        return optimizer.lr</span><br><span class="line">    return lr</span><br><span class="line"> </span><br><span class="line">x = Input((50,))</span><br><span class="line">out = Dense(1, activation=&#x27;sigmoid&#x27;)(x)</span><br><span class="line">model = Model(x, out)</span><br><span class="line"> </span><br><span class="line">optimizer = Adam(lr=0.001)</span><br><span class="line">lr_metric = get_lr_metric(optimizer)</span><br><span class="line">model.compile(loss=&#x27;binary_crossentropy&#x27;, optimizer=optimizer, metrics=[&#x27;acc&#x27;, lr_metric])</span><br><span class="line"> </span><br><span class="line"># reducing the learning rate by half every 2 epochs</span><br><span class="line">cbks = [LerningRateScheduler(lambda epoch: 0.001 * 0.5 ** (epoch // 2)),</span><br><span class="line">        TensorBoard(write_graph=False)]</span><br><span class="line">X = np.random.rand(1000, 50)</span><br><span class="line">Y = np.random.randint(2, size=1000)</span><br><span class="line">model.fit(X, Y, epochs=10, callbacks=cbks)</span><br></pre></td></tr></table></figure>
<h1 id="分层学习率设置"><a href="#分层学习率设置" class="headerlink" title="分层学习率设置"></a>分层学习率设置</h1><p>有时候我们需要为模型中不同层设置不同学习率大小，比如微调预训练模型时，预训练层数设置较小的学习率进行学习，而其他层以正常大小进行学习。这里给出苏神给出的keras实现，其通过参数变换实现调整学习率的目的：</p>
<p>梯度下降公式如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{\theta}_{n+1}=\boldsymbol{\theta}_{n}-\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\label{eq:sgd-1}</script><p>考虑变换$\boldsymbol{\theta}=\lambda \boldsymbol{\phi}$,其中λ是一个固定的标量，<strong>ϕ</strong>也是参数。现在来优化<strong>ϕ</strong>，相应的更新公式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}\boldsymbol{\phi}_{n+1}=&\boldsymbol{\phi}_{n}-\alpha \frac{\partial L(\lambda\boldsymbol{\phi}_{n})}{\partial \boldsymbol{\phi}_n}\\ 
=&\boldsymbol{\phi}_{n}-\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\frac{\partial \boldsymbol{\theta}_{n}}{\partial \boldsymbol{\phi}_n}\\ 
=&\boldsymbol{\phi}_{n}-\lambda\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\end{aligned}</script><p>然后通过链式求导法则，再上述等式两边同时乘以λ：</p>
<script type="math/tex; mode=display">
\lambda\boldsymbol{\phi}_{n+1}=\lambda\boldsymbol{\phi}_{n}-\lambda^2\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\quad\Rightarrow\quad\boldsymbol{\theta}_{n+1}=\boldsymbol{\theta}_{n}-\lambda^2\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\label{eq:sgd-2}</script><blockquote>
<p>在SGD优化器中，如果做参数变换<strong>θ</strong>=λ<strong>ϕ</strong>，那么等价的结果是学习率从α变成了$\lambda^2\alpha$。</p>
<p>不过，在自适应学习率优化器（比如RMSprop、Adam等），情况有点不一样，因为自适应学习率使用梯度（作为分母）来调整了学习率，抵消了一个λ</p>
<p>在RMSprop、Adam等自适应学习率优化器中，如果做参数变换<strong>θ</strong>=λ<strong>ϕ</strong>，那么等价的结果是学习率从α变成了λα。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import keras.backend as K</span><br><span class="line"></span><br><span class="line">class SetLearningRate:</span><br><span class="line">    &quot;&quot;&quot;层的一个包装，用来设置当前层的学习率</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, layer, lamb, is_ada=False):</span><br><span class="line">        self.layer = layer</span><br><span class="line">        self.lamb = lamb # 学习率比例</span><br><span class="line">        self.is_ada = is_ada # 是否自适应学习率优化器</span><br><span class="line"></span><br><span class="line">    def __call__(self, inputs):</span><br><span class="line">        with K.name_scope(self.layer.name):</span><br><span class="line">            if not self.layer.built:</span><br><span class="line">                input_shape = K.int_shape(inputs)</span><br><span class="line">                self.layer.build(input_shape)</span><br><span class="line">                self.layer.built = True</span><br><span class="line">                if self.layer._initial_weights is not None:</span><br><span class="line">                    self.layer.set_weights(self.layer._initial_weights)</span><br><span class="line">        for key in [&#x27;kernel&#x27;, &#x27;bias&#x27;, &#x27;embeddings&#x27;, &#x27;depthwise_kernel&#x27;, &#x27;pointwise_kernel&#x27;, &#x27;recurrent_kernel&#x27;, &#x27;gamma&#x27;, &#x27;beta&#x27;]:</span><br><span class="line">            if hasattr(self.layer, key):</span><br><span class="line">                weight = getattr(self.layer, key)</span><br><span class="line">                if self.is_ada:</span><br><span class="line">                    lamb = self.lamb # 自适应学习率优化器直接保持lamb比例</span><br><span class="line">                else:</span><br><span class="line">                    lamb = self.lamb**0.5 # SGD（包括动量加速），lamb要开平方</span><br><span class="line">                K.set_value(weight, K.eval(weight) / lamb) # 更改初始化</span><br><span class="line">                setattr(self.layer, key, weight * lamb) # 按比例替换</span><br><span class="line">        return self.layer(inputs)</span><br><span class="line"> </span><br><span class="line">x_in = Input(shape=(None,))</span><br><span class="line">x = x_in</span><br><span class="line"></span><br><span class="line"># 默认情况下是x = Embedding(100, 1000, weights=[word_vecs])(x)</span><br><span class="line"># 下面这一句表示：后面将会用自适应学习率优化器，并且Embedding层以总体的十分之一的学习率更新。</span><br><span class="line"># word_vecs是预训练好的词向量</span><br><span class="line">x = SetLearningRate(Embedding(100, 1000, weights=[word_vecs]), 0.1, True)(x)</span><br><span class="line"></span><br><span class="line"># 后面部分自己想象了～</span><br><span class="line">x = LSTM(100)(x)</span><br><span class="line"></span><br><span class="line">model = Model(x_in, x)</span><br><span class="line">model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;) # 用自适应学习率优化器优化</span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://jishuin.proginn.com/p/763bfbd51f6b">https://jishuin.proginn.com/p/763bfbd51f6b</a></p>
<p><a href="https://www.zhihu.com/question/338066667/answer/771252708">https://www.zhihu.com/question/338066667/answer/771252708</a></p>
<p><a href="https://kexue.fm/archives/6418">https://kexue.fm/archives/6418</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>warmup</tag>
        <tag>decay</tag>
        <tag>学习率</tag>
      </tags>
  </entry>
  <entry>
    <title>解析NLP竞赛中的提分点-对抗训练</title>
    <url>/2022/01/20/2022-01-20-%E8%A7%A3%E6%9E%90NLP%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在NLP比赛中，对抗训练是常见的提分手段。本文将详细介绍对抗训练的场景、作用、类型、具体实现以及未来的展望。</p>
<p><img src="https://pic2.zhimg.com/80/v2-efaa81e59a5af299791913e5e8f519cd_1440w.jpg" alt=""></p>
<span id="more"></span>
<h1 id="对抗训练应用场景"><a href="#对抗训练应用场景" class="headerlink" title="对抗训练应用场景"></a>对抗训练应用场景</h1><p>Szegedy在14年的ICLR中提出了对抗样本的概念。对抗样本可以用来攻击和防御，而对抗训练其实是“对抗”家族中防御的一种方式，其基本原理为：通过添加扰动构建对抗样本，喂入模型一同训练，提高模型遇到对抗样本时的鲁棒性，同时一定程度也能提高模型的表现和泛化能力。</p>
<p>对抗样本一般需要具有两个特点：</p>
<ol>
<li>相对于原始输入，所添加的扰动是微小的；</li>
<li>能使模型犯错。</li>
</ol>
<p>对抗训练的公式如下：</p>
<script type="math/tex; mode=display">
\min _{\theta} \mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\max _{r_{a d v} \in \mathcal{S}} L\left(\theta, x+r_{a d v}, y\right)\right]</script><p>该过程可以分为两步：</p>
<ol>
<li>内部的max过程：寻找让模型犯错最大的扰动</li>
<li>外部的min过程：寻找整体损失最小的参数</li>
</ol>
<p>在图像领域，扰动可以为图像上的噪点，但是在NLP中，如果直接在词编码上加上扰动，输入会偏离原先的语义。由于向量空间中语义相近的词语相互接近，在向量空间中添加微小的扰动的方式并不会对语义带来较大的破坏，因此当前NLP中的对抗训练均针对embedding做扰动。</p>
<h1 id="对抗训练的作用"><a href="#对抗训练的作用" class="headerlink" title="对抗训练的作用"></a>对抗训练的作用</h1><ol>
<li>提高模型应对恶意对抗样本时的鲁棒性。</li>
<li>作为一种正则化方式(regularization)，减少过拟合(overfitting)，提高泛化能力。</li>
</ol>
<p>在NLP任务中，对抗训练的角色不再是为了防御基于梯度的恶意攻击，更多的是作为一种正则化方式(regularization)，提高模型的泛化能力。</p>
<h1 id="对抗训练具体方式FGM-PGD-FreeLB"><a href="#对抗训练具体方式FGM-PGD-FreeLB" class="headerlink" title="对抗训练具体方式FGM/PGD/FreeLB"></a>对抗训练具体方式FGM/PGD/FreeLB</h1><h2 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h2><p>在介绍对抗训练的具体实现之前，本文先介绍下面Pytorch代码中常见的函数：</p>
<p><strong>一般优化流程：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># zero the parameter gradients</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"># forward + backward + optimize</span><br><span class="line">outputs = net(inputs)</span><br><span class="line">loss = criterion(outputs, labels)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p><strong>具体展开流程：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># gradient descent</span><br><span class="line">weights = [0] * n</span><br><span class="line">alpha = 0.0001</span><br><span class="line">max_Iter = 50000</span><br><span class="line">for i in range(max_Iter):</span><br><span class="line">    loss = 0</span><br><span class="line">    d_weights = [0] * n</span><br><span class="line">    for k in range(m):</span><br><span class="line">        h = dot(input[k], weights)</span><br><span class="line">        d_weights = [d_weights[j] + (label[k] - h) * input[k][j] for j in range(n)] # 梯度下降优化</span><br><span class="line">        loss += (label[k] - h) * (label[k] - h) / 2 # 梯度下降优化</span><br><span class="line">    d_weights = [d_weights[k]/m for k in range(n)]</span><br><span class="line">    weights = [weights[k] + alpha * d_weights[k] for k in range(n)]</span><br><span class="line">    if i%10000 == 0:</span><br><span class="line">        print &quot;Iteration %d loss: %f&quot;%(i, loss/m)</span><br><span class="line">        print weights</span><br></pre></td></tr></table></figure>
<p>可以发现它们实际上是一一对应的：</p>
<ul>
<li><strong>optimizer.zero_grad()对应d_weights = [0] * n</strong></li>
</ul>
<p>该步骤将梯度初始化为零（因为一个batch的loss关于weight的导数是所有sample的loss关于weight的导数的累加和）</p>
<ul>
<li><strong>outputs = net(inputs)对应h = dot(input[k], weights)</strong></li>
</ul>
<p>该步骤即前向传播求出预测的值</p>
<ul>
<li><strong>loss = criterion(outputs, labels)对应loss += (label[k] - h) * (label[k] - h) / 2</strong></li>
</ul>
<p>该步骤为求当前具体loss值</p>
<ul>
<li><strong>loss.backward()对应d_weights = [d_weights[j] + (label[k] - h) * input[k][j] for j in range(n)]</strong></li>
</ul>
<p>该步骤即反向传播求梯度</p>
<ul>
<li><strong>optimizer.step()对应weights = [weights[k] + alpha * d_weights[k] for k in range(n)]</strong></li>
</ul>
<p>该步骤即更新所有参数</p>
<h2 id="FGSM-FGM"><a href="#FGSM-FGM" class="headerlink" title="FGSM/FGM"></a>FGSM/FGM</h2><p>该方式的思想为沿着梯度上升方向对扰动可以对模型带来最大的破坏。</p>
<p>FGSM：采用Sign函数对梯度采取max归一化，max归一化是是说如果梯度某个维度上的值为正，则设为1；如果为负，则设为-1；如果为0，则设为0</p>
<p>FGM：采用L2归一化，L2归一化则将梯度的每个维度的值除以梯度的L2范数。 理论上L2归一化更严格的保留了梯度的方向，但是max归一化则不一定和原始梯度的方向相同。</p>
<script type="math/tex; mode=display">
FGSM：\delta=\epsilon Sign(g)</script><script type="math/tex; mode=display">
FGM: \delta = \epsilon (g/||g_2||)</script><script type="math/tex; mode=display">
其中g为梯度g=\nabla x(L(f_{\theta}(X), y))</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FGM</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attack</span>(<span class="params">self, epsilon=<span class="number">1.</span>, emb_name=<span class="string">&#x27;emb.&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name:</span><br><span class="line">                self.backup[name] = param.data.clone()</span><br><span class="line">                norm = torch.norm(param.grad)</span><br><span class="line">                <span class="keyword">if</span> norm != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> torch.isnan(norm):</span><br><span class="line">                    r_at = epsilon * param.grad / norm</span><br><span class="line">                    param.data.add_(r_at)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore</span>(<span class="params">self, emb_name=<span class="string">&#x27;emb.&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name: </span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.backup</span><br><span class="line">                param.data = self.backup[name]</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">fgm = FGM(model)</span><br><span class="line"><span class="keyword">for</span> batch_input, batch_label <span class="keyword">in</span> data:</span><br><span class="line">    <span class="comment"># 正常训练</span></span><br><span class="line">    loss = model(batch_input, batch_label)</span><br><span class="line">    loss.backward() <span class="comment"># 反向传播，得到正常的grad</span></span><br><span class="line">    <span class="comment"># 对抗训练</span></span><br><span class="line">    fgm.attack() <span class="comment"># 在embedding上添加对抗扰动</span></span><br><span class="line">    loss_adv = model(batch_input, batch_label)</span><br><span class="line">    loss_adv.backward() <span class="comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span></span><br><span class="line">    fgm.restore() <span class="comment"># 恢复embedding参数</span></span><br><span class="line">    <span class="comment"># 梯度下降，更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<h3 id="FGM-FGSM-流程总结"><a href="#FGM-FGSM-流程总结" class="headerlink" title="FGM/FGSM 流程总结"></a>FGM/FGSM 流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>执行对抗训练-根据当前梯度对参数值添加扰动；前向传播得到损失与最终梯度</li>
<li>恢复embedding参数</li>
<li>更新本次迭代的参数</li>
</ol>
<p>根据min-max公式可以看出，对抗训练主要完成内部max的过程。FGM/FGSM思想就是沿着梯度上升的方向，找寻最优解。但是<strong>FGM/FGSM 有假设：损失函数是线性或者局部线性。如果不是线性，那梯度提升方向不一定是最优方向。</strong></p>
<h2 id="PGD"><a href="#PGD" class="headerlink" title="PGD"></a>PGD</h2><p>为了解决FGM中线性假设问题，PGD分多次迭代，若扰动超出范围将扰动映射到规定范围内。</p>
<script type="math/tex; mode=display">
X_{t + 1}=\prod _{X+S}(X_t + \epsilon(g_t/||g_t||))</script><script type="math/tex; mode=display">
其中g为梯度g_t=\nabla x_t(L(f_{\theta}(X_t), y))</script><p>虽然PGD很有效，但效率并不高，若经过m次迭代，PGD需要迭代m*(K + 1)次。其代码展示如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PGD</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.emb_backup = &#123;&#125;</span><br><span class="line">        self.grad_backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attack</span>(<span class="params">self, epsilon=<span class="number">1.</span>, alpha=<span class="number">0.3</span>, emb_name=<span class="string">&#x27;emb.&#x27;</span>, is_first_attack=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">if</span> is_first_attack:</span><br><span class="line">                    self.emb_backup[name] = param.data.clone()</span><br><span class="line">                norm = torch.norm(param.grad)</span><br><span class="line">                <span class="keyword">if</span> norm != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> torch.isnan(norm):</span><br><span class="line">                    r_at = alpha * param.grad / norm</span><br><span class="line">                    param.data.add_(r_at)</span><br><span class="line">                    param.data = self.project(name, param.data, epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore</span>(<span class="params">self, emb_name=<span class="string">&#x27;emb.&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name: </span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.emb_backup</span><br><span class="line">                param.data = self.emb_backup[name]</span><br><span class="line">        self.emb_backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">project</span>(<span class="params">self, param_name, param_data, epsilon</span>):</span></span><br><span class="line">        r = param_data - self.emb_backup[param_name]</span><br><span class="line">        <span class="keyword">if</span> torch.norm(r) &gt; epsilon:</span><br><span class="line">            r = epsilon * r / torch.norm(r)</span><br><span class="line">        <span class="keyword">return</span> self.emb_backup[param_name] + r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backup_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                self.grad_backup[name] = param.grad.clone()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                param.grad = self.grad_backup[name]</span><br><span class="line">                </span><br><span class="line">pgd = PGD(model)</span><br><span class="line">K = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> batch_input, batch_label <span class="keyword">in</span> data:</span><br><span class="line">    <span class="comment"># 正常训练</span></span><br><span class="line">    loss = model(batch_input, batch_label)</span><br><span class="line">    loss.backward() <span class="comment"># 反向传播，得到正常的grad</span></span><br><span class="line">    pgd.backup_grad()</span><br><span class="line">    <span class="comment"># 对抗训练</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        pgd.attack(is_first_attack=(t==<span class="number">0</span>)) <span class="comment"># 在embedding上添加对抗扰动, first attack时备份param.data</span></span><br><span class="line">        <span class="keyword">if</span> t != K-<span class="number">1</span>:</span><br><span class="line">            model.zero_grad()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pgd.restore_grad()</span><br><span class="line">        loss_adv = model(batch_input, batch_label)</span><br><span class="line">        loss_adv.backward() <span class="comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span></span><br><span class="line">    pgd.restore() <span class="comment"># 恢复embedding参数</span></span><br><span class="line">    <span class="comment"># 梯度下降，更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<h3 id="PGD流程总结"><a href="#PGD流程总结" class="headerlink" title="PGD流程总结"></a>PGD流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>备份正常的梯度</li>
<li>执行K次对抗训练<ol>
<li>若t=0，备份参数；梯度清零；前向传播，计算梯度与loss值</li>
<li>若t=K-1；恢复第1步的梯度；前向传播，计算梯度与loss值</li>
</ol>
</li>
<li>恢复3.1中的embedding参数</li>
<li>更新本次迭代的参数</li>
</ol>
<p>PGD执行K次目的为分多步获取<strong>内部max的扰动-扰动表现在参数上</strong>，每一步梯度归零，但是参数值得到了累加:$x^{‘}=x+\sum_{t=0}^{K} r_t$,最后根据参数$x^{‘}$以及初始梯度前向传播计算loss和最终梯度，最后，恢复初始参数，根据最终梯度完成参数更新。</p>
<h2 id="FreeAT"><a href="#FreeAT" class="headerlink" title="FreeAT"></a>FreeAT</h2><p>PGD中进行m次反向传播，m *（K + 1） 次前向传播效率不高</p>
<p>FreeAT把前向传播计算出的梯度也进行回传</p>
<p>对比图为：</p>
<p><img src="https://pic4.zhimg.com/80/v2-b54c33d95d0e8123091297d3c4b89a2f_720w.jpg" alt="image"></p>
<p>进行（m/k）*k=m 次反向传播，（m/k）* k = m次前向传播</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">初始化r=0</span><br><span class="line">对于epoch=1...N/m:</span><br><span class="line">  对于每个x:</span><br><span class="line">    对于每步m:</span><br><span class="line">      1.利用上一步的r，计算x+r的前后向，得到梯度</span><br><span class="line">      2.根据梯度更新参数</span><br><span class="line">      3.根据梯度更新r</span><br></pre></td></tr></table></figure>
<h3 id="FreeAT流程总结"><a href="#FreeAT流程总结" class="headerlink" title="FreeAT流程总结"></a>FreeAT流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>备份正常的梯度</li>
<li>执行K次对抗训练<ol>
<li>前向传播得到梯度与loss值</li>
<li>根据梯度更新参数</li>
<li>根据梯度更新扰动</li>
</ol>
</li>
</ol>
<p><strong>缺点：FreeLB指出，FreeAT的问题在于每次的r对于当前的参数都是次优的（无法最大化loss），因为当前r是由$ r_{t-1} $和$\theta_{t-1}$计算出来的，是对于$\theta_{ t-1 }$的最优。</strong></p>
<h2 id="FreeLB"><a href="#FreeLB" class="headerlink" title="FreeLB"></a>FreeLB</h2><p>FreeLB认为，FreeAT和YOPO对于获得最优r (inner max)的计算都存在问题，因此提出了一种类似PGD的方法。只不过PGD只使用了最后一步x+r输出的梯度，而FreeLB取了每次迭代r输出梯度的平均值，相当于把输入看作一个K倍大的虚拟batch，由[X+r1, X+r2, …, X+rk]拼接而成。具体的公式为：</p>
<script type="math/tex; mode=display">
min_{\theta} E(Z,y) - D(\frac{1}{K} \sum_{t=0}^{K-1}max_{r_t \in L_t} L(f_{\theta}(X+r_t),y))</script><p>PGD公式为:</p>
<script type="math/tex; mode=display">
min_{\theta} E(Z,y) - D(max_{||r|| \le\epsilon} L(f_{\theta}(X+r_t),y))</script><p><strong>FreeLB与PGD区别如下：</strong></p>
<ol>
<li>PGD是迭代K次r后取最后一次扰动的梯度更新参数，FreeLB是取K次迭代中的平均梯度</li>
<li>PGD的扰动范围都在epsilon内，因为流程第3步将梯度归0了，每次投影都会回到以第1步x为圆心，半径是epsilon的圆内，而FreeLB每次的x都会迭代，所以r的范围更加灵活，更可能接近局部最优</li>
</ol>
<p>伪代码为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于每个x:</span><br><span class="line">  1.通过均匀分布初始化r，梯度g为0</span><br><span class="line">  对于每步t=1...K:</span><br><span class="line">    2.根据x+r计算前后向，累计梯度g</span><br><span class="line">    3.更新r</span><br><span class="line">  4.根据g/K更新梯度</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class FreeLB(object):</span><br><span class="line">    def __init__(self, adv_K, adv_lr, adv_init_mag, adv_max_norm=0., adv_norm_type=&#x27;l2&#x27;, base_model=&#x27;bert&#x27;):</span><br><span class="line">        self.adv_K = adv_K</span><br><span class="line">        self.adv_lr = adv_lr</span><br><span class="line">        self.adv_max_norm = adv_max_norm</span><br><span class="line">        self.adv_init_mag = adv_init_mag    # adv-training initialize with what magnitude, 即我们用多大的数值初始化delta</span><br><span class="line">        self.adv_norm_type = adv_norm_type</span><br><span class="line">        self.base_model = base_model</span><br><span class="line">    def attack(self, model, inputs, gradient_accumulation_steps=1):</span><br><span class="line">        input_ids = inputs[&#x27;input_ids&#x27;]</span><br><span class="line">        if isinstance(model, torch.nn.DataParallel):</span><br><span class="line">            embeds_init = getattr(model.module, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">        else:</span><br><span class="line">            embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">        if self.adv_init_mag &gt; 0:   # 影响attack首步是基于原始梯度(delta=0)，还是对抗梯度(delta!=0)</span><br><span class="line">            input_mask = inputs[&#x27;attention_mask&#x27;].to(embeds_init)</span><br><span class="line">            input_lengths = torch.sum(input_mask, 1)</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                delta = torch.zeros_like(embeds_init).uniform_(-1, 1) * input_mask.unsqueeze(2)</span><br><span class="line">                dims = input_lengths * embeds_init.size(-1)</span><br><span class="line">                mag = self.adv_init_mag / torch.sqrt(dims)</span><br><span class="line">                delta = (delta * mag.view(-1, 1, 1)).detach()</span><br><span class="line">            elif self.adv_norm_type == &quot;linf&quot;:</span><br><span class="line">                delta = torch.zeros_like(embeds_init).uniform_(-self.adv_init_mag, self.adv_init_mag)</span><br><span class="line">                delta = delta * input_mask.unsqueeze(2)</span><br><span class="line">        else:</span><br><span class="line">            delta = torch.zeros_like(embeds_init)  # 扰动初始化</span><br><span class="line">        loss, logits = None, None</span><br><span class="line">        for astep in range(self.adv_K):</span><br><span class="line">            delta.requires_grad_()</span><br><span class="line">            inputs[&#x27;inputs_embeds&#x27;] = delta + embeds_init  # 累积一次扰动delta</span><br><span class="line">            inputs[&#x27;input_ids&#x27;] = None</span><br><span class="line">            outputs = model(**inputs)</span><br><span class="line">            loss, logits = outputs[:2]  # model outputs are always tuple in transformers (see doc)</span><br><span class="line">            loss = loss.mean()  # mean() to average on multi-gpu parallel training</span><br><span class="line">            loss = loss / gradient_accumulation_steps</span><br><span class="line">            loss.backward()</span><br><span class="line">            delta_grad = delta.grad.clone().detach()  # 备份扰动的grad</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1).view(-1, 1, 1)</span><br><span class="line">                denorm = torch.clamp(denorm, min=1e-8)</span><br><span class="line">                delta = (delta + self.adv_lr * delta_grad / denorm).detach()</span><br><span class="line">                if self.adv_max_norm &gt; 0:</span><br><span class="line">                    delta_norm = torch.norm(delta.view(delta.size(0), -1).float(), p=2, dim=1).detach()</span><br><span class="line">                    exceed_mask = (delta_norm &gt; self.adv_max_norm).to(embeds_init)</span><br><span class="line">                    reweights = (self.adv_max_norm / delta_norm * exceed_mask + (1 - exceed_mask)).view(-1, 1, 1)</span><br><span class="line">                    delta = (delta * reweights).detach()</span><br><span class="line">            elif self.adv_norm_type == &quot;linf&quot;:</span><br><span class="line">                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1, p=float(&quot;inf&quot;)).view(-1, 1, 1)  # p=&#x27;inf&#x27;,无穷范数，获取绝对值最大者</span><br><span class="line">                denorm = torch.clamp(denorm, min=1e-8)  # 类似np.clip，将数值夹逼到(min, max)之间</span><br><span class="line">                delta = (delta + self.adv_lr * delta_grad / denorm).detach()  # 计算该步的delta，然后累加到原delta值上(梯度上升)</span><br><span class="line">                if self.adv_max_norm &gt; 0:</span><br><span class="line">                    delta = torch.clamp(delta, -self.adv_max_norm, self.adv_max_norm).detach()</span><br><span class="line">            else:</span><br><span class="line">                raise ValueError(&quot;Norm type &#123;&#125; not specified.&quot;.format(self.adv_norm_type))</span><br><span class="line">            if isinstance(model, torch.nn.DataParallel):  </span><br><span class="line">                embeds_init = getattr(model.module, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">            else:</span><br><span class="line">                embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">        return loss, logits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if args.do_adv:</span><br><span class="line">    inputs = &#123;</span><br><span class="line">        &quot;input_ids&quot;: input_ids,</span><br><span class="line">        &quot;bbox&quot;: layout,</span><br><span class="line">        &quot;token_type_ids&quot;: segment_ids,</span><br><span class="line">        &quot;attention_mask&quot;: input_mask,</span><br><span class="line">        &quot;masked_lm_labels&quot;: lm_label_ids</span><br><span class="line">    &#125;</span><br><span class="line">    loss, prediction_scores = freelb.attack(model, inputs)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">scheduler.step()</span><br><span class="line">model.zero_grad()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class FreeLB():</span><br><span class="line">    def __init__(self, model, args, optimizer, base_model=&#x27;xlm-roberta&#x27;):</span><br><span class="line">        self.args = args</span><br><span class="line">        self.model = model</span><br><span class="line">        self.adv_K = self.args.adv_K</span><br><span class="line">        self.adv_lr = self.args.adv_lr</span><br><span class="line">        self.adv_max_norm = self.args.adv_max_norm</span><br><span class="line">        self.adv_init_mag = self.args.adv_init_mag  # adv-training initialize with what magnitude, 即我们用多大的数值初始化delta</span><br><span class="line">        self.adv_norm_type = self.args.adv_norm_type</span><br><span class="line">        self.base_model = base_model</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line"></span><br><span class="line">    def attack(self, model, inputs):</span><br><span class="line">        args = self.args</span><br><span class="line">        input_ids = inputs[&#x27;input_ids&#x27;]</span><br><span class="line">        #获取初始化时的embedding</span><br><span class="line">        embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids.to(args.device))</span><br><span class="line"></span><br><span class="line">        if self.adv_init_mag &gt; 0:   # 影响attack首步是基于原始梯度(delta=0)，还是对抗梯度(delta!=0)</span><br><span class="line">            input_mask = inputs[&#x27;attention_mask&#x27;].to(embeds_init)</span><br><span class="line">            input_lengths = torch.sum(input_mask, 1)</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                delta = torch.zeros_like(embeds_init).uniform_(-1, 1) * input_mask.unsqueeze(2)</span><br><span class="line">                dims = input_lengths * embeds_init.size(-1)</span><br><span class="line">                mag = self.adv_init_mag / torch.sqrt(dims)</span><br><span class="line">                delta = (delta * mag.view(-1, 1, 1)).detach()</span><br><span class="line">        else:</span><br><span class="line">            delta = torch.zeros_like(embeds_init)  # 扰动初始化</span><br><span class="line">        # loss, logits = None, None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        for astep in range(self.adv_K):</span><br><span class="line">            delta.requires_grad_()</span><br><span class="line">            inputs[&#x27;inputs_embeds&#x27;] = delta + embeds_init  # 累积一次扰动delta</span><br><span class="line">            # inputs[&#x27;input_ids&#x27;] = None</span><br><span class="line">            loss, _ = model(input_ids=None,</span><br><span class="line">                            attention_mask=inputs[&quot;attention_mask&quot;].to(args.device),</span><br><span class="line">                             token_type_ids=inputs[&quot;token_type_ids&quot;].to(args.device),</span><br><span class="line">                             labels=inputs[&quot;sl_labels&quot;].to(args.device),</span><br><span class="line">                            inputs_embeds=inputs[&quot;inputs_embeds&quot;].to(args.device))</span><br><span class="line"></span><br><span class="line">            loss = loss / self.adv_K # 求平均的梯度</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            if astep == self.adv_K - 1:</span><br><span class="line">                # further updates on delta</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">            delta_grad = delta.grad.clone().detach()  # 备份扰动的grad</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1).view(-1, 1, 1)</span><br><span class="line">                denorm = torch.clamp(denorm, min=1e-8)</span><br><span class="line">                delta = (delta + self.adv_lr * delta_grad / denorm).detach()</span><br><span class="line">                if self.adv_max_norm &gt; 0:</span><br><span class="line">                    delta_norm = torch.norm(delta.view(delta.size(0), -1).float(), p=2, dim=1).detach()</span><br><span class="line">                    exceed_mask = (delta_norm &gt; self.adv_max_norm).to(embeds_init)</span><br><span class="line">                    reweights = (self.adv_max_norm / delta_norm * exceed_mask + (1 - exceed_mask)).view(-1, 1, 1)</span><br><span class="line">                    delta = (delta * reweights).detach()</span><br><span class="line">            else:</span><br><span class="line">                raise ValueError(&quot;Norm type &#123;&#125; not specified.&quot;.format(self.adv_norm_type))</span><br><span class="line"></span><br><span class="line">            embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids.to(args.device))</span><br><span class="line">        return loss</span><br><span class="line"></span><br><span class="line">for batch_input, batch_label in data:</span><br><span class="line">    # 正常训练</span><br><span class="line">    loss = model(batch_input, batch_label)</span><br><span class="line">    loss.backward() # 反向传播，得到正常的grad</span><br><span class="line">    # 对抗训练</span><br><span class="line">    freelb = FreeLB( model, args, optimizer, base_model)</span><br><span class="line">    loss_adv = freelb.attack(model, batch_input)</span><br><span class="line">    loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span><br><span class="line">    # 梯度下降，更新参数</span><br><span class="line">    optimizer.step()</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<h3 id="FreeLB流程总结"><a href="#FreeLB流程总结" class="headerlink" title="FreeLB流程总结"></a>FreeLB流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>备份正常的梯度</li>
<li>执行K次对抗训练<ol>
<li>前向传播，计算梯度与loss值</li>
<li>梯度累加</li>
<li>根据梯度计算扰动</li>
</ol>
</li>
<li>恢复初始embedding参数</li>
<li>更新本次迭代的参数</li>
</ol>
<p>该方法与FreeAT一样都想高效的利用两种梯度。<strong>不同的是，该方法并不是每次都进行更新，而是将参数梯度累积起来，用累积的梯度对参数更新</strong>。</p>
<h2 id="通用范式"><a href="#通用范式" class="headerlink" title="通用范式"></a>通用范式</h2><p>通过对上述几种对抗训练方式的学习，不难看出对抗训练的目的为完成<strong>内部max的任务，找出最大扰动的最优解</strong>。具体表现为：求解最大扰动更新参数；根据参数进行前向传播得到loss与最终梯度；恢复最初的参数值；利用最终的梯度对最初的参数值进行更新。所以通用流程表示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 正常的前向传播-得到梯度与loss值</span><br><span class="line">2. 备份正常的参数</span><br><span class="line">3. 求解扰动最优值，更新参数</span><br><span class="line">4. 根据更新后参数以及最初梯度，前向传播得到最终梯度</span><br><span class="line">5. 恢复最初的参数</span><br><span class="line">6. 根据最初的参数与最终梯度，完成参数的更新</span><br></pre></td></tr></table></figure>
<p><strong>不同对抗训练方式体现为求解扰动最优值的方式不同：</strong></p>
<ul>
<li>FGM/FGSM最优值求解方式为：一步到位，根据最初的梯度与参数值，得到扰动值</li>
<li>PGD最优值求解方式为：多步走，每一步根据上一步的参数获取扰动并更新参数，最终得到多步累加的扰动值</li>
<li>FreeAT最优值求解方式为：与PGD一样分多步，但是该方法相当于：每一步根据上一步参数和梯度获取的扰动值就是最终扰动值</li>
<li>FreeLB最优值求解方式为：与PGD一样分多步，但是在最后进行梯度更新的时候，最终梯度为初始梯度加上每一步梯度的平均值</li>
</ul>
<h1 id="对抗训练展望"><a href="#对抗训练展望" class="headerlink" title="对抗训练展望"></a>对抗训练展望</h1><h2 id="虚拟对抗训练"><a href="#虚拟对抗训练" class="headerlink" title="虚拟对抗训练"></a>虚拟对抗训练</h2><p><strong>那什么是虚拟对抗训练(VAT)呢</strong>？</p>
<blockquote>
<p>VAT不需要标签信息，可应用于无监督学习，其梯度上升的方向是能使预测的输出分布偏离现状的方向，而传统对抗训练课找的是使模型预测最大地偏离label的方向。因此，VAT不使用真实label，而是“虚拟”label——当前模型的预测结果。</p>
</blockquote>
<p>该部分可以查看JayJay的博客：<a href="https://zhuanlan.zhihu.com/p/345264876">虚拟对抗训练：让预训练模型再次强大！</a></p>
<h1 id="延伸思考"><a href="#延伸思考" class="headerlink" title="延伸思考"></a>延伸思考</h1><h2 id="对抗训练与梯度惩罚"><a href="#对抗训练与梯度惩罚" class="headerlink" title="对抗训练与梯度惩罚"></a>对抗训练与梯度惩罚</h2><p>该内容为苏神在博客<a href="https://kexue.fm/archives/7234">对抗训练浅谈：意义、方法和思考（附Keras实现）</a>中所提及：</p>
<p>假设已经得到对抗扰动Δx，那么我们在更新θ时，考虑对$L(x+Δx,y;θ)$的展开：</p>
<script type="math/tex; mode=display">
\min_{\theta}\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[L(x+\Delta x, y;\theta)\right]\\ 
\approx\, \min_{\theta}\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[L(x, y;\theta)+\langle\nabla_x L(x, y;\theta), \Delta x\rangle\right]</script><p>对应的θ的梯度为：</p>
<script type="math/tex; mode=display">
\nabla_{\theta}L(x, y;\theta)+\langle\nabla_{\theta}\nabla_x L(x, y;\theta), \Delta x\rangle</script><p>代入$\Delta x=\epsilon \nabla_x L(x, y;\theta)$，得到</p>
<script type="math/tex; mode=display">
\nabla_{\theta}L(x, y;\theta)+\epsilon\langle\nabla_{\theta}\nabla_x L(x, y;\theta), \nabla_x L(x, y;\theta)\rangle\\ 
=\,\nabla_{\theta}\left(L(x, y;\theta)+\frac{1}{2}\epsilon\left\Vert\nabla_x L(x, y;\theta)\right\Vert^2\right)</script><p>这个结果表示，对输入样本施加$\epsilon \nabla_x L(x, y;\theta)$的对抗扰动，一定程度上等价于往loss里边加入“梯度惩罚”</p>
<script type="math/tex; mode=display">
\frac{1}{2}\epsilon\left\Vert\nabla_x L(x, y;\theta)\right\Vert^2</script><p>如果对抗扰动是$\nabla_x L(x, y;\theta)/\Vert \nabla_x L(x, y;\theta)\Vert$，那么对应的梯度惩罚项则是$\epsilon\left\Vert\nabla_x L(x, y;\theta)\right\Vert$（少了个1/2，也少了个2次方）。</p>
<p>事实上，这个结果不是新的，它首先出现论文<a href="https://arxiv.org/abs/1711.09404">《Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients》</a>里。只不过这篇文章不容易搜到，因为你一旦搜索“adversarial training gradient penalty”等关键词，出来的结果几乎都是WGAN-GP相关的东西。</p>
<h2 id="词向量空间"><a href="#词向量空间" class="headerlink" title="词向量空间"></a>词向量空间</h2><p>NLP中对抗训练目前的方式均是对embedding向量空间添加扰动，那么向量空间究竟什么样呢？在对比学习的研究中<Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere>，同样提出一个好的对比学习系统应该具体两个特点：</p>
<ul>
<li><strong>Alignment：</strong>指的是相似的例子，也就是正例，映射到单位超球面后，应该有接近的特征，也即是说，在超球面上距离比较近</li>
<li><strong>Uniformity：</strong>指的是系统应该倾向在特征里保留尽可能多的信息，这等价于使得映射到单位超球面的特征，尽可能均匀地分布在球面上，分布得越均匀，意味着保留的信息越充分。分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分。</li>
</ul>
<p><img src="https://pic4.zhimg.com/80/v2-da1e6a090490028ab91e49bbe1484443_1440w.webp" alt=""></p>
<p>极端情况下会出现模型塌缩的情况，即所有特征映射到同一点：</p>
<p><img src="https://pic3.zhimg.com/80/v2-a75ca4a6fcbc48588c3cd643efd6d7fe_1440w.webp" alt=""></p>
<p>笔者认为，对抗训练在词向量层添加扰动，与对比学习类似，实现相似的例子在向量空间中相接近的目的，完成输入发生微小改变，输出改变幅度也不大的任务。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>对抗训练</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>对抗训练</tag>
      </tags>
  </entry>
  <entry>
    <title>不要停止预训练实战(二)-一日看尽MLM</title>
    <url>/2022/05/30/2022-05-30-%E4%B8%8D%E8%A6%81%E5%81%9C%E6%AD%A2%E9%A2%84%E8%AE%AD%E7%BB%83%E5%AE%9E%E6%88%98(%E4%BA%8C)-%E4%B8%80%E6%97%A5%E7%9C%8B%E5%B0%BDMLM/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><img src="https://picx.zhimg.com/v2-79c0d08709031f923a7fb3305ae8ff59_1440w.jpg?source=172ae18b" alt=""></p>
<p>本文在上文<a href="https://jmxgodlz.xyz/2022/03/20/2022-03-20-预训练实战/#more">不要停止预训练实战-Roberta与Albert</a>的基础上，进一步完成以下内容：</p>
<ul>
<li>keras预训练</li>
<li>N-gram掩码任务 </li>
<li>Span掩码任务 </li>
</ul>
<span id="more"></span>
<h1 id="掩码任务"><a href="#掩码任务" class="headerlink" title="掩码任务"></a>掩码任务</h1><p>BERT等预训练模型中掩码任务主要涉及下列要素：</p>
<ul>
<li>掩码比例</li>
<li>替换策略</li>
<li>掩码方式</li>
</ul>
<h2 id="掩码比例"><a href="#掩码比例" class="headerlink" title="掩码比例"></a>掩码比例</h2><p>常用掩码比例设置为15%，该比例经过许多研究，已证明该比例能够取得很好的效果。</p>
<p>从理论上来说，笔者从网上找到的说法为：“当取15%时，恰好大概7个词mask一个，正好就是CBOW中，长度为7的滑动窗口的中心词，因此会有比较好的效果”</p>
<p>而近日丹琦大佬等人的论文<a href="https://arxiv.org/abs/2202.08005">Should You Mask 15% in Masked Language Modeling?</a>表明掩码40%能够取得与15%差不多的效果。</p>
<p>该论文表明<strong>“所谓的optimal masking rate并不是一个一成不变的神奇数字，而是一个随着模型大小、mask策略、训练recipe、下游任务变化的函数。”</strong></p>
<h2 id="替换策略"><a href="#替换策略" class="headerlink" title="替换策略"></a>替换策略</h2><p>常用的替换策略如下：</p>
<ul>
<li>80%词语替换为[MASK]</li>
<li>10%词语保持不变</li>
<li>10%词语随机替换为其他词语</li>
</ul>
<p>这样做的目的在于强迫模型学习词语上下文的语义信息。任何一个词语都有可能被替换，不仅靠当前词语，还需要利用上下文的信息预测当前词语。</p>
<p>但是[MASK]标签并未出现在下游任务中，因此<strong>存在预训练与微调的不一致问题。</strong></p>
<p>MacBERT提出<strong>MLM as correction</strong>的方法，替换策略如下：</p>
<ul>
<li>80%词语替换为同义词</li>
<li>10%词语保持不变</li>
<li>10%词语随机替换为其他词语</li>
</ul>
<p>MacBERT论文中与下列替换策略进行对比，对比结果如图所示：</p>
<ul>
<li>MacBERT：80%替换为同义词，10%替换为随机词语，10%保持不变；</li>
<li>Random Replace：90%替换为随机词语，10%保持不变；</li>
<li>Partial Mask：同原生的BERT一样，80%替换为[MASK]，10%替换为随机词语，10%保持不变；</li>
<li>ALL Mask：90%替换为[MASK]，10%保持不变。</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-11ceecc7dceba69da862e6bebac0f4c1_1440w.webp" alt="ss"></p>
<p>图中横坐标代表训练步数，纵坐标代表EM值。第一幅图是CMRC数据集结果，第二幅图是DRCD数据集结果。</p>
<h2 id="掩码方式"><a href="#掩码方式" class="headerlink" title="掩码方式"></a>掩码方式</h2><p>目前的掩码方式主要分为以下几种：</p>
<ul>
<li>单词掩码</li>
<li>全词掩码</li>
<li>实体掩码</li>
<li>N-gram掩码</li>
<li>Span掩码</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">中文</th>
<th style="text-align:center">英文</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">原句</td>
<td style="text-align:center">使用语言模型来预测下一个词的概率。</td>
<td style="text-align:center">we use a language model to predict the probability of the next word.</td>
</tr>
<tr>
<td style="text-align:center">分词</td>
<td style="text-align:center">使用 语言 模型 来 预测 下 一个 词 的 概率 。</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">BERT Tokenizer</td>
<td style="text-align:center">使 用 语 言 模 型 来 预 测 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a language <strong>model</strong> to <strong>pre ##di ##ct</strong> the <strong>pro ##ba ##bility</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">单词掩码</td>
<td style="text-align:center">使 用 语 言 <strong>[M]</strong> 型 来 <strong>[M]</strong> 测 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a language <strong>[M]</strong> to <strong>[M] ##di ##ct</strong> the <strong>pro [M] ##bility</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">全词掩码</td>
<td style="text-align:center">使 用 语 言 <strong>[M] [M]</strong> 来 <strong>[M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a language <strong>[M]</strong> to <strong>[M] [M] [M]</strong> the <strong>[M] [M] [M]</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">实体掩码</td>
<td style="text-align:center">使 用 <strong>[M] [M] [M] [M]</strong> 来 <strong>[M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a <strong>[M] [M]</strong> to <strong>[M] [M] [M]</strong> the <strong>[M] [M] [M]</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">N-gram掩码</td>
<td style="text-align:center">使 用 <strong>[M] [M] [M] [M]</strong> 来 <strong>[M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a <strong>[M] [M]</strong> to <strong>[M] [M] [M]</strong> the <strong>[M] [M] [M] [M] [M]</strong> next word.</td>
</tr>
<tr>
<td style="text-align:center">Span掩码</td>
<td style="text-align:center">使 用 <strong>[M] [M] [M] [M] [M] [M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a <strong>[M] [M] [M] [M] [M] [M]</strong> the <strong>[M] [M] [M] [M] [M]</strong> next word.</td>
</tr>
<tr>
<td style="text-align:center">MAC掩码</td>
<td style="text-align:center">使 用 语 法 建 模 来 预 见 下 一 个 词 的 几 率 。</td>
<td style="text-align:center">we use a <strong>text system</strong> to <strong>ca ##lc ##ulate</strong> the <strong>po ##si ##bility</strong> of the next word.</td>
</tr>
</tbody>
</table>
</div>
<h3 id="全词掩码"><a href="#全词掩码" class="headerlink" title="全词掩码"></a>全词掩码</h3><p>以分词结果为最小粒度，完成掩码任务。</p>
<h3 id="N-gram掩码"><a href="#N-gram掩码" class="headerlink" title="N-gram掩码"></a>N-gram掩码</h3><p>同样以分词结果为最小粒度，以n-gram取词语进行掩码。</p>
<p>例如MacBERT采用基于分词的n-gram masking，1-gram~4gram Masking的概率分别是40%、30%、20%、10%。</p>
<h3 id="实体掩码"><a href="#实体掩码" class="headerlink" title="实体掩码"></a>实体掩码</h3><p>代表模型为：<strong>ERNIE</strong></p>
<p>引入命名实体信息，将实体作为最小粒度，进行掩码。</p>
<h3 id="Span掩码"><a href="#Span掩码" class="headerlink" title="Span掩码"></a>Span掩码</h3><p>代表模型为：<strong>SpanBERT</strong></p>
<p>以上做法让人认为，或许必须得引入类似词边界信息才能帮助训练。但前不久的 MASS 模型，却表明可能并不需要，随机遮盖可能效果也很好，于是就有SpanBERT的 idea：</p>
<p>根据<strong>几何分布</strong>，先随机选择一段（span）的<strong>长度</strong>，之后再根据均匀分布随机选择这一段的<strong>起始位置</strong>，最后按照长度遮盖。文中使用几何分布取 <em>p=0.2</em>，最大长度只能是 10，利用此方案获得平均采样长度分布。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>相关代码实现可见：</p>
<p><a href="https://github.com/447428054/Pretrain/tree/master/KerasExample/pretraining">https://github.com/447428054/Pretrain/tree/master/KerasExample/pretraining</a></p>
<p>Span掩码核心代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def __init__(</span><br><span class="line">    self, tokenizer, word_segment, lower=1, upper=10, p=0.3, mask_rate=0.15, sequence_length=512</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;参数说明：</span><br><span class="line">        tokenizer必须是bert4keras自带的tokenizer类；</span><br><span class="line">        word_segment是任意分词函数。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    super(TrainingDatasetRoBERTa, self).__init__(tokenizer, sequence_length)</span><br><span class="line">    self.word_segment = word_segment</span><br><span class="line">    self.mask_rate = mask_rate</span><br><span class="line"></span><br><span class="line">    self.lower = lower</span><br><span class="line">    self.upper = upper</span><br><span class="line">    self.p = p</span><br><span class="line"></span><br><span class="line">    self.lens = list(range(self.lower, self.upper + 1))</span><br><span class="line">    self.len_distrib = [self.p * (1-self.p)**(i - self.lower) for i in range(self.lower, self.upper + 1)] if self.p &gt;= 0 else None</span><br><span class="line">    self.len_distrib = [x / (sum(self.len_distrib)) for x in self.len_distrib]</span><br><span class="line">    print(self.len_distrib, self.lens)</span><br><span class="line"></span><br><span class="line">def sentence_process(self, text):</span><br><span class="line">    &quot;&quot;&quot;单个文本的处理函数</span><br><span class="line">    流程：分词，然后转id，按照mask_rate构建全词mask的序列</span><br><span class="line">          来指定哪些token是否要被mask</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    word_tokens = self.tokenizer.tokenize(text=text)[1:-1]</span><br><span class="line">    word_token_ids = self.tokenizer.tokens_to_ids(word_tokens)</span><br><span class="line"></span><br><span class="line">    sent_length = len(word_tokens)</span><br><span class="line">    mask_num = math.ceil(sent_length * self.mask_rate)</span><br><span class="line">    mask = set()</span><br><span class="line">    spans = []</span><br><span class="line"></span><br><span class="line">    while len(mask) &lt; mask_num:</span><br><span class="line">        span_len = np.random.choice(self.lens, p=self.len_distrib) # 随机选择span长度</span><br><span class="line"></span><br><span class="line">        anchor = np.random.choice(sent_length)</span><br><span class="line">        if anchor in mask: # 随机生成起点</span><br><span class="line">            continue</span><br><span class="line">        left1 = anchor</span><br><span class="line">        spans.append([left1, left1])</span><br><span class="line">        right1 = min(anchor + span_len, sent_length)</span><br><span class="line">        for i in range(left1, right1):</span><br><span class="line">            if len(mask) &gt;= mask_num:</span><br><span class="line">                break</span><br><span class="line">            mask.add(i)</span><br><span class="line">            spans[-1][-1] = i</span><br><span class="line"></span><br><span class="line">    spans = merge_intervals(spans)</span><br><span class="line">    word_mask_ids = [0] * len(word_tokens)</span><br><span class="line">    for (st, ed) in spans:</span><br><span class="line">        for idx in range(st, ed + 1):</span><br><span class="line">            wid = word_token_ids[idx]</span><br><span class="line">            word_mask_ids[idx] = self.token_process(wid) + 1</span><br><span class="line"></span><br><span class="line">    return [word_token_ids, word_mask_ids]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>预训练</category>
      </categories>
      <tags>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>不要停止预训练实战-Roberta与Albert</title>
    <url>/2022/03/20/2022-03-20-%E9%A2%84%E8%AE%AD%E7%BB%83%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文在LCQMC数据集上，再次对roberta、albert模型进行预训练，详细介绍了预训练的过程并对比了预训练前后的结果。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">roberta</td>
<td style="text-align:center">0.88503</td>
<td style="text-align:center">0.86344</td>
</tr>
<tr>
<td style="text-align:center">albert</td>
<td style="text-align:center">0.85662</td>
<td style="text-align:center">0.84960</td>
</tr>
<tr>
<td style="text-align:center">预训练后roberta</td>
<td style="text-align:center"><strong>0.89343</strong></td>
<td style="text-align:center">0.85328</td>
</tr>
<tr>
<td style="text-align:center">预训练后albert</td>
<td style="text-align:center">0.84958</td>
<td style="text-align:center"><strong>0.85224</strong></td>
</tr>
</tbody>
</table>
</div>
<span id="more"></span>
<h1 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h1><p>根据选取数据集，转为预训练格式数据，完成roberta、albert的预训练，并对比在该数据集上，预训练前后的具体任务指标。</p>
<h1 id="任务数据集"><a href="#任务数据集" class="headerlink" title="任务数据集"></a>任务数据集</h1><p>LCQMC数据集</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">训练集</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">238766</td>
<td style="text-align:center">8802</td>
<td style="text-align:center">12500</td>
</tr>
</tbody>
</table>
</div>
<p>LCQMC数据集的长度分布如下：</p>
<p><img src="https://pic4.zhimg.com/80/v2-c5971c7538e7972a3ad872b3e4bea8bb_1440w.jpg" alt=""></p>
<h1 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h1><p>代码链接：<a href="https://github.com/447428054/Pretrain/tree/master/LcqmcExample">https://github.com/447428054/Pretrain/tree/master/LcqmcExample</a></p>
<p>预训练环境：tensorflow1.14</p>
<p>预训练代码执行顺序：</p>
<ol>
<li>bash create_pretrain_data_lz.sh</li>
<li>bash pretrain_lz.sh</li>
</ol>
<p>LCQMC微调代码:</p>
<ol>
<li>python task_sentence_similarity_lcqmc_roberta.py</li>
<li>python task_sentence_similarity_lcqmc_albert.py</li>
</ol>
<p>TIPS:</p>
<p>记得修改文件路径</p>
<h1 id="预训练数据生成"><a href="#预训练数据生成" class="headerlink" title="预训练数据生成"></a>预训练数据生成</h1><p>预训练代码读取生成的record，数据处理代码首先读取不同文件，每个文件格式为：<strong>每一行存放一个句子，不同文档之间以空行分割</strong></p>
<p>我们将LCQMC中相似的句子作为一个文档，不相似的分开</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">谁有狂三这张高清的</span><br><span class="line"></span><br><span class="line">这张高清图，谁有</span><br><span class="line"></span><br><span class="line">英雄联盟什么英雄最好</span><br><span class="line">英雄联盟最好英雄是什么</span><br><span class="line"></span><br><span class="line">这是什么意思，被蹭网吗</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="roberta的预训练数据处理"><a href="#roberta的预训练数据处理" class="headerlink" title="roberta的预训练数据处理"></a>roberta的预训练数据处理</h2><ol>
<li><p>每个文件中，一个sentence占一行，不同document之间加一个空行分割<br>[[‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘？’], [‘有’, ‘人’, ‘知’, ‘道’, ‘名’, ‘字’, ‘吗’]]</p>
</li>
<li><p>从一个文档中连续的获得文本，直到达到最大长度。如果是从下一个文档中获得，那么加上一个分隔符.将长度限制修改了，因为lcqmc句子都偏短<br>[‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘？’, ‘有’, ‘人’, ‘知’, ‘道’, ‘名’, ‘字’, ‘吗’]</p>
</li>
<li><p>对于获取之后的文本，进行全词分词： 判断每个字符起始长度3以内的，是否在分词里面，在的话添加##标记<br>[‘有’, ‘##人’, ‘知’, ‘##道’, ‘叫’, ‘什’, ‘##么’, ‘名’, ‘##字’, ‘吗’, ‘[UNK]’, ‘？’, ‘有’, ‘##人’, ‘知’, ‘##道’, ‘名’, ‘##字’, ‘吗’]</p>
</li>
<li><p>对获得的token序列，进行掩码:返回 掩码结果，掩码的位置，掩码的标签<br>[‘[CLS]’, ‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘[MASK]’, ‘有’, ‘人’, ‘[MASK]’, ‘[MASK]’, ‘名’, ‘字’, ‘吗’, ‘[SEP]’]<br>[12, 15, 16]<br>[‘？’, ‘知’, ‘##道’]</p>
</li>
</ol>
<p>run_pretraining.py需要注释TPU的引用<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># *tpu_cluster_resolver* = tf.contrib.cluster_resolver.TPUClusterResolver( # TODO</span><br><span class="line">#       tpu=FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)</span><br></pre></td></tr></table></figure></p>
<h2 id="albert的预训练数据处理"><a href="#albert的预训练数据处理" class="headerlink" title="albert的预训练数据处理"></a>albert的预训练数据处理</h2><ol>
<li><p>每个文件中，一个sentence占一行，不同document之间加一个空行分割<br>[[‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘？’], [‘有’, ‘人’, ‘知’, ‘道’, ‘名’, ‘字’, ‘吗’]]</p>
</li>
<li><p>从一个文档中获取sentence,sentece进行全词分词，当长度达到最大长度或者遍历完整个文档了，A[SEP]B 随机分割句子，50%概率交换顺序，得到SOP标签<br>tokenA:[‘有’, ‘##人’, ‘知’, ‘##道’, ‘叫’, ‘什’, ‘##么’, ‘名’, ‘##字’, ‘吗’, ‘[UNK]’, ‘？’]<br>tokenB:[‘有’, ‘##人’, ‘知’, ‘##道’, ‘名’, ‘##字’, ‘吗’]</p>
</li>
</ol>
<p>只有一句话,构不成SOP任务的就continue</p>
<ol>
<li>对获得的token序列，进行掩码:返回 掩码结果，掩码的位置，掩码的标签<br>tokens:[‘[CLS]’, ‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘[MASK]’, ‘吗’, ‘[UNK]’, ‘？’, ‘[SEP]’, ‘[MASK]’, ‘人’, ‘知’, ‘[MASK]’, ‘名’, ‘字’, ‘吗’, ‘[SEP]’]<br>masked_lm_positions:[9, 14, 17]<br>masked_lm_labels:[‘##字’, ‘有’, ‘##道’]<br>is_random_next:False</li>
</ol>
<h1 id="预训练代码"><a href="#预训练代码" class="headerlink" title="预训练代码"></a>预训练代码</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="Roberta"><a href="#Roberta" class="headerlink" title="Roberta"></a>Roberta</h3><p><strong>整个模型结构与BERT相同，整体流程如下：</strong></p>
<ol>
<li><strong>输入的token 经过embedding</strong></li>
<li><strong>再加上token type id 与position embedding</strong></li>
<li><strong>进入transfomer层，每一个transformer又由多头attention、层归一化、残差结构、前馈神经网络构成</strong></li>
<li><strong>获取CLS输出与整个句子的输出</strong></li>
</ol>
<p><strong>整个代码结构跟流程相同：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">with tf.variable_scope(scope, default_name=&quot;bert&quot;):</span><br><span class="line">  with tf.variable_scope(&quot;embeddings&quot;):</span><br><span class="line">    # Perform embedding lookup on the word ids.</span><br><span class="line">    (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        vocab_size=config.vocab_size,</span><br><span class="line">        embedding_size=config.hidden_size,</span><br><span class="line">        initializer_range=config.initializer_range,</span><br><span class="line">        word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    # Add positional embeddings and token type embeddings, then layer</span><br><span class="line">    # normalize and perform dropout.</span><br><span class="line">    self.embedding_output = embedding_postprocessor(</span><br><span class="line">        input_tensor=self.embedding_output,</span><br><span class="line">        use_token_type=True,</span><br><span class="line">        token_type_ids=token_type_ids,</span><br><span class="line">        token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">        token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">        use_position_embeddings=True,</span><br><span class="line">        position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">        initializer_range=config.initializer_range,</span><br><span class="line">        max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">        dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">  with tf.variable_scope(&quot;encoder&quot;):</span><br><span class="line">    # This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span><br><span class="line">    # mask of shape [batch_size, seq_length, seq_length] which is used</span><br><span class="line">    # for the attention scores.</span><br><span class="line">    attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">        input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">    # Run the stacked transformer.</span><br><span class="line">    # `sequence_output` shape = [batch_size, seq_length, hidden_size].</span><br><span class="line">    self.all_encoder_layers = transformer_model(</span><br><span class="line">        input_tensor=self.embedding_output,</span><br><span class="line">        attention_mask=attention_mask,</span><br><span class="line">        hidden_size=config.hidden_size,</span><br><span class="line">        num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">        num_attention_heads=config.num_attention_heads,</span><br><span class="line">        intermediate_size=config.intermediate_size,</span><br><span class="line">        intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">        hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">        attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">        initializer_range=config.initializer_range,</span><br><span class="line">        do_return_all_layers=True)</span><br><span class="line"></span><br><span class="line">  self.sequence_output = self.all_encoder_layers[-1] # [batch_size, seq_length, hidden_size]</span><br><span class="line">  # The &quot;pooler&quot; converts the encoded sequence tensor of shape</span><br><span class="line">  # [batch_size, seq_length, hidden_size] to a tensor of shape</span><br><span class="line">  # [batch_size, hidden_size]. This is necessary for segment-level</span><br><span class="line">  # (or segment-pair-level) classification tasks where we need a fixed</span><br><span class="line">  # dimensional representation of the segment.</span><br><span class="line">  with tf.variable_scope(&quot;pooler&quot;):</span><br><span class="line">    # We &quot;pool&quot; the model by simply taking the hidden state corresponding</span><br><span class="line">    # to the first token. We assume that this has been pre-trained</span><br><span class="line">    first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)</span><br><span class="line">    self.pooled_output = tf.layers.dense(</span><br><span class="line">        first_token_tensor,</span><br><span class="line">        config.hidden_size,</span><br><span class="line">        activation=tf.tanh,</span><br><span class="line">        kernel_initializer=create_initializer(config.initializer_range))</span><br></pre></td></tr></table></figure>
<h4 id="embedding-lookup"><a href="#embedding-lookup" class="headerlink" title="embedding_lookup"></a>embedding_lookup</h4><p><strong>与常规神经网络中词嵌入类似</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def embedding_lookup(input_ids,</span><br><span class="line">                     vocab_size,</span><br><span class="line">                     embedding_size=128,</span><br><span class="line">                     initializer_range=0.02,</span><br><span class="line">                     word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">                     use_one_hot_embeddings=False):</span><br><span class="line">  &quot;&quot;&quot;Looks up words embeddings for id tensor.</span><br><span class="line">  Args:</span><br><span class="line">    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span><br><span class="line">      ids.</span><br><span class="line">    vocab_size: int. Size of the embedding vocabulary.</span><br><span class="line">    embedding_size: int. Width of the word embeddings.</span><br><span class="line">    initializer_range: float. Embedding initialization range.</span><br><span class="line">    word_embedding_name: string. Name of the embedding table.</span><br><span class="line">    use_one_hot_embeddings: bool. If True, use one-hot method for word</span><br><span class="line">      embeddings. If False, use `tf.gather()`.</span><br><span class="line">  Returns:</span><br><span class="line">    float Tensor of shape [batch_size, seq_length, embedding_size].</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # This function assumes that the input is of shape [batch_size, seq_length,</span><br><span class="line">  # num_inputs].</span><br><span class="line">  #</span><br><span class="line">  # If the input is a 2D tensor of shape [batch_size, seq_length], we</span><br><span class="line">  # reshape to [batch_size, seq_length, 1].</span><br><span class="line">  if input_ids.shape.ndims == 2:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[-1])</span><br><span class="line"></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [-1])</span><br><span class="line">  if use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  else:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[0:-1] + [input_shape[-1] * embedding_size])</span><br><span class="line">  return (output, embedding_table)</span><br></pre></td></tr></table></figure>
<h4 id="embedding-postprocessor"><a href="#embedding-postprocessor" class="headerlink" title="embedding_postprocessor"></a>embedding_postprocessor</h4><p><strong>加上token type id与可学习的position id 词嵌入，postprocessor指在embedding之后进行层归一化与dropout</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def embedding_postprocessor(input_tensor,</span><br><span class="line">                            use_token_type=False,</span><br><span class="line">                            token_type_ids=None,</span><br><span class="line">                            token_type_vocab_size=16,</span><br><span class="line">                            token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">                            use_position_embeddings=True,</span><br><span class="line">                            position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">                            initializer_range=0.02,</span><br><span class="line">                            max_position_embeddings=512,</span><br><span class="line">                            dropout_prob=0.1):</span><br><span class="line">  &quot;&quot;&quot;Performs various post-processing on a word embedding tensor.</span><br><span class="line">  Args:</span><br><span class="line">    input_tensor: float Tensor of shape [batch_size, seq_length,</span><br><span class="line">      embedding_size].</span><br><span class="line">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span><br><span class="line">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span><br><span class="line">      Must be specified if `use_token_type` is True.</span><br><span class="line">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span><br><span class="line">    token_type_embedding_name: string. The name of the embedding table variable</span><br><span class="line">      for token type ids.</span><br><span class="line">    use_position_embeddings: bool. Whether to add position embeddings for the</span><br><span class="line">      position of each token in the sequence.</span><br><span class="line">    position_embedding_name: string. The name of the embedding table variable</span><br><span class="line">      for positional embeddings.</span><br><span class="line">    initializer_range: float. Range of the weight initialization.</span><br><span class="line">    max_position_embeddings: int. Maximum sequence length that might ever be</span><br><span class="line">      used with this model. This can be longer than the sequence length of</span><br><span class="line">      input_tensor, but cannot be shorter.</span><br><span class="line">    dropout_prob: float. Dropout probability applied to the final output tensor.</span><br><span class="line">  Returns:</span><br><span class="line">    float tensor with same shape as `input_tensor`.</span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: One of the tensor shapes or input values is invalid.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">  batch_size = input_shape[0]</span><br><span class="line">  seq_length = input_shape[1]</span><br><span class="line">  width = input_shape[2]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  if use_token_type:</span><br><span class="line">    if token_type_ids is None:</span><br><span class="line">      raise ValueError(&quot;`token_type_ids` must be specified if&quot;</span><br><span class="line">                       &quot;`use_token_type` is True.&quot;)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    # This vocab will be small so we always do one-hot here, since it is always</span><br><span class="line">    # faster for a small vocabulary.</span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [-1])</span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  if use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    with tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      # Since the position embedding table is a learned variable, we create it</span><br><span class="line">      # using a (long) sequence length `max_position_embeddings`. The actual</span><br><span class="line">      # sequence length might be shorter than this, for faster training of</span><br><span class="line">      # tasks that do not have long sequences.</span><br><span class="line">      #</span><br><span class="line">      # So `full_position_embeddings` is effectively an embedding table</span><br><span class="line">      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span><br><span class="line">      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span><br><span class="line">      # perform a slice.</span><br><span class="line">      position_embeddings = tf.slice(full_position_embeddings, [0, 0],</span><br><span class="line">                                     [seq_length, -1])</span><br><span class="line">      num_dims = len(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      # Only the last two dimensions are relevant (`seq_length` and `width`), so</span><br><span class="line">      # we broadcast among the first dimensions, which is typically just</span><br><span class="line">      # the batch size.</span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      for _ in range(num_dims - 2):</span><br><span class="line">        position_broadcast_shape.append(1)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  return </span><br></pre></td></tr></table></figure>
<h4 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h4><p><strong>对于每一个Transformer结构如下：</strong></p>
<ol>
<li><strong>多头attention</strong></li>
<li><strong>拼接多头输出，经过隐藏层映射</strong></li>
<li><strong>经过dropout+残差+层归一化</strong></li>
<li><strong>前馈神经网络</strong></li>
<li><strong>经过dropout+残差+层归一化</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def transformer_model(input_tensor,</span><br><span class="line">                      attention_mask=None,</span><br><span class="line">                      hidden_size=768,</span><br><span class="line">                      num_hidden_layers=12,</span><br><span class="line">                      num_attention_heads=12,</span><br><span class="line">                      intermediate_size=3072,</span><br><span class="line">                      intermediate_act_fn=gelu,</span><br><span class="line">                      hidden_dropout_prob=0.1,</span><br><span class="line">                      attention_probs_dropout_prob=0.1,</span><br><span class="line">                      initializer_range=0.02,</span><br><span class="line">                      do_return_all_layers=False):</span><br><span class="line">  &quot;&quot;&quot;Multi-headed, multi-layer Transformer from &quot;Attention is All You Need&quot;.</span><br><span class="line">  This is almost an exact implementation of the original Transformer encoder.</span><br><span class="line">  See the original paper:</span><br><span class="line">  https://arxiv.org/abs/1706.03762</span><br><span class="line">  Also see:</span><br><span class="line">  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span><br><span class="line">  Args:</span><br><span class="line">    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span><br><span class="line">    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span><br><span class="line">      seq_length], with 1 for positions that can be attended to and 0 in</span><br><span class="line">      positions that should not be.</span><br><span class="line">    hidden_size: int. Hidden size of the Transformer.</span><br><span class="line">    num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span><br><span class="line">    num_attention_heads: int. Number of attention heads in the Transformer.</span><br><span class="line">    intermediate_size: int. The size of the &quot;intermediate&quot; (a.k.a., feed</span><br><span class="line">      forward) layer.</span><br><span class="line">    intermediate_act_fn: function. The non-linear activation function to apply</span><br><span class="line">      to the output of the intermediate/feed-forward layer.</span><br><span class="line">    hidden_dropout_prob: float. Dropout probability for the hidden layers.</span><br><span class="line">    attention_probs_dropout_prob: float. Dropout probability of the attention</span><br><span class="line">      probabilities.</span><br><span class="line">    initializer_range: float. Range of the initializer (stddev of truncated</span><br><span class="line">      normal).</span><br><span class="line">    do_return_all_layers: Whether to also return all layers or just the final</span><br><span class="line">      layer.</span><br><span class="line">  Returns:</span><br><span class="line">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span><br><span class="line">    hidden layer of the Transformer.</span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: A Tensor shape or parameter is invalid.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  if hidden_size % num_attention_heads != 0:</span><br><span class="line">    raise ValueError(</span><br><span class="line">        &quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">        &quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">  batch_size = input_shape[0]</span><br><span class="line">  seq_length = input_shape[1]</span><br><span class="line">  input_width = input_shape[2]</span><br><span class="line"></span><br><span class="line">  # The Transformer performs sum residuals on all layers so the input needs</span><br><span class="line">  # to be the same as the hidden size.</span><br><span class="line">  if input_width != hidden_size:</span><br><span class="line">    raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  # We keep the representation as a 2D tensor to avoid re-shaping it back and</span><br><span class="line">  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span><br><span class="line">  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span><br><span class="line">  # help the optimizer.</span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  for layer_idx in range(num_hidden_layers):</span><br><span class="line">    with tf.variable_scope(&quot;layer_%d&quot; % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      with tf.variable_scope(&quot;attention&quot;):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=True,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = None</span><br><span class="line">        if len(attention_heads) == 1:</span><br><span class="line">          attention_output = attention_heads[0]</span><br><span class="line">        else:</span><br><span class="line">          # In the case where we have other sequences, we just concatenate</span><br><span class="line">          # them to the self-attention head before the projection.</span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=-1)</span><br><span class="line"></span><br><span class="line">        # Run a linear projection of `hidden_size` then add a residual</span><br><span class="line">        # with `layer_input`.</span><br><span class="line">        with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      # The activation is only applied to the &quot;intermediate&quot; hidden layer.</span><br><span class="line">      with tf.variable_scope(&quot;intermediate&quot;):</span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      # Down-project back to `hidden_size` then add the residual.</span><br><span class="line">      with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  if do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    for layer_output in all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    return final_outputs</span><br><span class="line">  else:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    return final_output</span><br></pre></td></tr></table></figure>
<p><strong>其中，多头attention结构如下：</strong></p>
<ol>
<li><p><strong>针对输入向量与输出向量，生成Q、K、V向量，隐藏层维度为num heads * head size</strong></p>
<p><strong>self-attention中 输入输出来源相同。Q来源于输入向量，V来源于输出向量。“目的在于计算输入向量 对于 不同输出的 权重</strong>”</p>
<p><strong>若需要对注意力进行掩码，对得分减去很大的值，最终softmax之后得到的影响就非常小</strong></p>
</li>
<li><p><strong>针对Q，K 进行缩放点积计算，再经过softmax</strong></p>
</li>
<li><p><strong>将第二步结果与V相乘得到上下文向量</strong></p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def attention_layer(from_tensor,</span><br><span class="line">                    to_tensor,</span><br><span class="line">                    attention_mask=None,</span><br><span class="line">                    num_attention_heads=1,</span><br><span class="line">                    size_per_head=512,</span><br><span class="line">                    query_act=None,</span><br><span class="line">                    key_act=None,</span><br><span class="line">                    value_act=None,</span><br><span class="line">                    attention_probs_dropout_prob=0.0,</span><br><span class="line">                    initializer_range=0.02,</span><br><span class="line">                    do_return_2d_tensor=False,</span><br><span class="line">                    batch_size=None,</span><br><span class="line">                    from_seq_length=None,</span><br><span class="line">                    to_seq_length=None):</span><br><span class="line">  &quot;&quot;&quot;Performs multi-headed attention from `from_tensor` to `to_tensor`.</span><br><span class="line">  This is an implementation of multi-headed attention based on &quot;Attention</span><br><span class="line">  is all you Need&quot;. If `from_tensor` and `to_tensor` are the same, then</span><br><span class="line">  this is self-attention. Each timestep in `from_tensor` attends to the</span><br><span class="line">  corresponding sequence in `to_tensor`, and returns a fixed-with vector.</span><br><span class="line">  This function first projects `from_tensor` into a &quot;query&quot; tensor and</span><br><span class="line">  `to_tensor` into &quot;key&quot; and &quot;value&quot; tensors. These are (effectively) a list</span><br><span class="line">  of tensors of length `num_attention_heads`, where each tensor is of shape</span><br><span class="line">  [batch_size, seq_length, size_per_head].</span><br><span class="line">  Then, the query and key tensors are dot-producted and scaled. These are</span><br><span class="line">  softmaxed to obtain attention probabilities. The value tensors are then</span><br><span class="line">  interpolated by these probabilities, then concatenated back to a single</span><br><span class="line">  tensor and returned.</span><br><span class="line">  In practice, the multi-headed attention are done with transposes and</span><br><span class="line">  reshapes rather than actual separate tensors.</span><br><span class="line">  Args:</span><br><span class="line">    from_tensor: float Tensor of shape [batch_size, from_seq_length,</span><br><span class="line">      from_width].</span><br><span class="line">    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].</span><br><span class="line">    attention_mask: (optional) int32 Tensor of shape [batch_size,</span><br><span class="line">      from_seq_length, to_seq_length]. The values should be 1 or 0. The</span><br><span class="line">      attention scores will effectively be set to -infinity for any positions in</span><br><span class="line">      the mask that are 0, and will be unchanged for positions that are 1.</span><br><span class="line">    num_attention_heads: int. Number of attention heads.</span><br><span class="line">    size_per_head: int. Size of each attention head.</span><br><span class="line">    query_act: (optional) Activation function for the query transform.</span><br><span class="line">    key_act: (optional) Activation function for the key transform.</span><br><span class="line">    value_act: (optional) Activation function for the value transform.</span><br><span class="line">    attention_probs_dropout_prob: (optional) float. Dropout probability of the</span><br><span class="line">      attention probabilities.</span><br><span class="line">    initializer_range: float. Range of the weight initializer.</span><br><span class="line">    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size</span><br><span class="line">      * from_seq_length, num_attention_heads * size_per_head]. If False, the</span><br><span class="line">      output will be of shape [batch_size, from_seq_length, num_attention_heads</span><br><span class="line">      * size_per_head].</span><br><span class="line">    batch_size: (Optional) int. If the input is 2D, this might be the batch size</span><br><span class="line">      of the 3D version of the `from_tensor` and `to_tensor`.</span><br><span class="line">    from_seq_length: (Optional) If the input is 2D, this might be the seq length</span><br><span class="line">      of the 3D version of the `from_tensor`.</span><br><span class="line">    to_seq_length: (Optional) If the input is 2D, this might be the seq length</span><br><span class="line">      of the 3D version of the `to_tensor`.</span><br><span class="line">  Returns:</span><br><span class="line">    float Tensor of shape [batch_size, from_seq_length,</span><br><span class="line">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span><br><span class="line">      true, this will be of shape [batch_size * from_seq_length,</span><br><span class="line">      num_attention_heads * size_per_head]).</span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: Any of the arguments or tensor shapes are invalid.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,</span><br><span class="line">                           seq_length, width):</span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])</span><br><span class="line">    return output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])</span><br><span class="line"></span><br><span class="line">  if len(from_shape) != len(to_shape):</span><br><span class="line">    raise ValueError(</span><br><span class="line">        &quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;)</span><br><span class="line"></span><br><span class="line">  if len(from_shape) == 3:</span><br><span class="line">    batch_size = from_shape[0]</span><br><span class="line">    from_seq_length = from_shape[1]</span><br><span class="line">    to_seq_length = to_shape[1]</span><br><span class="line">  elif len(from_shape) == 2:</span><br><span class="line">    if (batch_size is None or from_seq_length is None or to_seq_length is None):</span><br><span class="line">      raise ValueError(</span><br><span class="line">          &quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span><br><span class="line">          &quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot;</span><br><span class="line">          &quot;must all be specified.&quot;)</span><br><span class="line"></span><br><span class="line">  # Scalar dimensions referenced here:</span><br><span class="line">  #   B = batch size (number of sequences)</span><br><span class="line">  #   F = `from_tensor` sequence length</span><br><span class="line">  #   T = `to_tensor` sequence length</span><br><span class="line">  #   N = `num_attention_heads`</span><br><span class="line">  #   H = `size_per_head`</span><br><span class="line"></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">  # `query_layer` = [B*F, N*H]</span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=&quot;query&quot;,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # `key_layer` = [B*T, N*H]</span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=&quot;key&quot;,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # `value_layer` = [B*T, N*H]</span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=&quot;value&quot;,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # `query_layer` = [B, N, F, H]</span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  # `key_layer` = [B, N, T, H]</span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw</span><br><span class="line">  # attention scores.</span><br><span class="line">  # `attention_scores` = [B, N, F, T]</span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)</span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 1.0 / math.sqrt(float(size_per_head)))</span><br><span class="line"></span><br><span class="line">  if attention_mask is not None:</span><br><span class="line">    # `attention_mask` = [B, 1, F, T]</span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[1])</span><br><span class="line"></span><br><span class="line">    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span><br><span class="line">    # masked positions, this operation will create a tensor which is 0.0 for</span><br><span class="line">    # positions we want to attend and -10000.0 for masked positions.</span><br><span class="line">    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0</span><br><span class="line"></span><br><span class="line">    # Since we are adding it to the raw scores before the softmax, this is</span><br><span class="line">    # effectively the same as removing these entirely.</span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  # Normalize the attention scores to probabilities.</span><br><span class="line">  # `attention_probs` = [B, N, F, T]</span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  # This is actually dropping out entire tokens to attend to, which might</span><br><span class="line">  # seem a bit unusual, but is taken from the original Transformer paper.</span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  # `value_layer` = [B, T, N, H]</span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  # `value_layer` = [B, N, T, H]</span><br><span class="line">  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])</span><br><span class="line"></span><br><span class="line">  # `context_layer` = [B, N, F, H]</span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  # `context_layer` = [B, F, N, H]</span><br><span class="line">  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])</span><br><span class="line"></span><br><span class="line">  if do_return_2d_tensor:</span><br><span class="line">    # `context_layer` = [B*F, N*H]</span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  else:</span><br><span class="line">    # `context_layer` = [B, F, N*H]</span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  return context_layer</span><br></pre></td></tr></table></figure>
<h3 id="Albert"><a href="#Albert" class="headerlink" title="Albert"></a>Albert</h3><p><strong>Albert整体结构与BERT相似，改动有三点：</strong></p>
<ol>
<li><p><strong>词嵌入层由Vocab <em> Hidden 分解为 Vocab </em> Embedding + Embedding * Hidden</strong></p>
</li>
<li><p><strong>跨层参数共享，主要是全连接层与注意力层的共享</strong></p>
<p><strong>Tensorflow 中 通过get variable 与 变量域Variable Scope完成参数共享</strong></p>
</li>
<li><p><strong>段落连续的SOP任务替换原先NSP任务，SOP任务中文档连续语句为正例，调换顺序后为负例</strong></p>
</li>
</ol>
<p><strong>代码中同时更新了层归一化的顺序：pre-Layer Normalization can converge fast and better. check paper: ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE</strong></p>
<p><strong>模型结构改动主要涉及前两点，接下来我们从代码层面来看这些改动：</strong></p>
<h4 id="embedding-lookup-factorized"><a href="#embedding-lookup-factorized" class="headerlink" title="embedding_lookup_factorized"></a>embedding_lookup_factorized</h4><p><strong>主要拆分为两次矩阵运算，embedding size在中间过渡</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def embedding_lookup_factorized(input_ids, # Factorized embedding parameterization provide by albert</span><br><span class="line">                     vocab_size,</span><br><span class="line">                     hidden_size,</span><br><span class="line">                     embedding_size=128,</span><br><span class="line">                     initializer_range=0.02,</span><br><span class="line">                     word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">                     use_one_hot_embeddings=False):</span><br><span class="line">    &quot;&quot;&quot;Looks up words embeddings for id tensor, but in a factorized style followed by albert. it is used to reduce much percentage of parameters previous exists.</span><br><span class="line">       Check &quot;Factorized embedding parameterization&quot; session in the paper.</span><br><span class="line">     Args:</span><br><span class="line">       input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span><br><span class="line">         ids.</span><br><span class="line">       vocab_size: int. Size of the embedding vocabulary.</span><br><span class="line">       embedding_size: int. Width of the word embeddings.</span><br><span class="line">       initializer_range: float. Embedding initialization range.</span><br><span class="line">       word_embedding_name: string. Name of the embedding table.</span><br><span class="line">       use_one_hot_embeddings: bool. If True, use one-hot method for word</span><br><span class="line">         embeddings. If False, use `tf.gather()`.</span><br><span class="line">     Returns:</span><br><span class="line">       float Tensor of shape [batch_size, seq_length, embedding_size].</span><br><span class="line">     &quot;&quot;&quot;</span><br><span class="line">    # This function assumes that the input is of shape [batch_size, seq_length,</span><br><span class="line">    # num_inputs].</span><br><span class="line">    #</span><br><span class="line">    # If the input is a 2D tensor of shape [batch_size, seq_length], we</span><br><span class="line">    # reshape to [batch_size, seq_length, 1].</span><br><span class="line"></span><br><span class="line">    # 1.first project one-hot vectors into a lower dimensional embedding space of size E</span><br><span class="line">    print(&quot;embedding_lookup_factorized. factorized embedding parameterization is used.&quot;)</span><br><span class="line">    if input_ids.shape.ndims == 2:</span><br><span class="line">        input_ids = tf.expand_dims(input_ids, axis=[-1])  # shape of input_ids is:[ batch_size, seq_length, 1]</span><br><span class="line"></span><br><span class="line">    embedding_table = tf.get_variable(  # [vocab_size, embedding_size]</span><br><span class="line">        name=word_embedding_name,</span><br><span class="line">        shape=[vocab_size, embedding_size],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">    flat_input_ids = tf.reshape(input_ids, [-1])  # one rank. shape as (batch_size * sequence_length,)</span><br><span class="line">    if use_one_hot_embeddings:</span><br><span class="line">        one_hot_input_ids = tf.one_hot(flat_input_ids,depth=vocab_size)  # one_hot_input_ids=[batch_size * sequence_length,vocab_size]</span><br><span class="line">        output_middle = tf.matmul(one_hot_input_ids, embedding_table)  # output=[batch_size * sequence_length,embedding_size]</span><br><span class="line">    else:</span><br><span class="line">        output_middle = tf.gather(embedding_table,flat_input_ids)  # [vocab_size, embedding_size]*[batch_size * sequence_length,]---&gt;[batch_size * sequence_length,embedding_size]</span><br><span class="line"></span><br><span class="line">    # 2. project vector(output_middle) to the hidden space</span><br><span class="line">    project_variable = tf.get_variable(  # [embedding_size, hidden_size]</span><br><span class="line">        name=word_embedding_name+&quot;_2&quot;,</span><br><span class="line">        shape=[embedding_size, hidden_size],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    output = tf.matmul(output_middle, project_variable) # ([batch_size * sequence_length, embedding_size] * [embedding_size, hidden_size])---&gt;[batch_size * sequence_length, hidden_size]</span><br><span class="line">    # reshape back to 3 rank</span><br><span class="line">    input_shape = get_shape_list(input_ids)  # input_shape=[ batch_size, seq_length, 1]</span><br><span class="line">    batch_size, sequene_length, _=input_shape</span><br><span class="line">    output = tf.reshape(output, (batch_size,sequene_length,hidden_size))  # output=[batch_size, sequence_length, hidden_size]</span><br><span class="line">    return (output, embedding_table, project_variable)</span><br></pre></td></tr></table></figure>
<h4 id="prelln-transformer-model"><a href="#prelln-transformer-model" class="headerlink" title="prelln_transformer_model"></a>prelln_transformer_model</h4><p>将Layer Norm放在Attention前面，使训练过程收敛的更快更好。使用Tensorflow的变量域，完成参数共享。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def prelln_transformer_model(input_tensor,</span><br><span class="line">						attention_mask=None,</span><br><span class="line">						hidden_size=768,</span><br><span class="line">						num_hidden_layers=12,</span><br><span class="line">						num_attention_heads=12,</span><br><span class="line">						intermediate_size=3072,</span><br><span class="line">						intermediate_act_fn=gelu,</span><br><span class="line">						hidden_dropout_prob=0.1,</span><br><span class="line">						attention_probs_dropout_prob=0.1,</span><br><span class="line">						initializer_range=0.02,</span><br><span class="line">						do_return_all_layers=False,</span><br><span class="line">						shared_type=&#x27;all&#x27;, # None,</span><br><span class="line">						adapter_fn=None):</span><br><span class="line">	&quot;&quot;&quot;Multi-headed, multi-layer Transformer from &quot;Attention is All You Need&quot;.</span><br><span class="line">	This is almost an exact implementation of the original Transformer encoder.</span><br><span class="line">	See the original paper:</span><br><span class="line">	https://arxiv.org/abs/1706.03762</span><br><span class="line">	Also see:</span><br><span class="line">	https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span><br><span class="line">	Args:</span><br><span class="line">		input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span><br><span class="line">		attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span><br><span class="line">			seq_length], with 1 for positions that can be attended to and 0 in</span><br><span class="line">			positions that should not be.</span><br><span class="line">		hidden_size: int. Hidden size of the Transformer.</span><br><span class="line">		num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span><br><span class="line">		num_attention_heads: int. Number of attention heads in the Transformer.</span><br><span class="line">		intermediate_size: int. The size of the &quot;intermediate&quot; (a.k.a., feed</span><br><span class="line">			forward) layer.</span><br><span class="line">		intermediate_act_fn: function. The non-linear activation function to apply</span><br><span class="line">			to the output of the intermediate/feed-forward layer.</span><br><span class="line">		hidden_dropout_prob: float. Dropout probability for the hidden layers.</span><br><span class="line">		attention_probs_dropout_prob: float. Dropout probability of the attention</span><br><span class="line">			probabilities.</span><br><span class="line">		initializer_range: float. Range of the initializer (stddev of truncated</span><br><span class="line">			normal).</span><br><span class="line">		do_return_all_layers: Whether to also return all layers or just the final</span><br><span class="line">			layer.</span><br><span class="line">	Returns:</span><br><span class="line">		float Tensor of shape [batch_size, seq_length, hidden_size], the final</span><br><span class="line">		hidden layer of the Transformer.</span><br><span class="line">	Raises:</span><br><span class="line">		ValueError: A Tensor shape or parameter is invalid.</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	if hidden_size % num_attention_heads != 0:</span><br><span class="line">		raise ValueError(</span><br><span class="line">				&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">				&quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">	attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line"></span><br><span class="line">	input_shape = bert_utils.get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">	batch_size = input_shape[0]</span><br><span class="line">	seq_length = input_shape[1]</span><br><span class="line">	input_width = input_shape[2]</span><br><span class="line"></span><br><span class="line">	# The Transformer performs sum residuals on all layers so the input needs</span><br><span class="line">	# to be the same as the hidden size.</span><br><span class="line">	if input_width != hidden_size:</span><br><span class="line">		raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %</span><br><span class="line">										 (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">	# We keep the representation as a 2D tensor to avoid re-shaping it back and</span><br><span class="line">	# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span><br><span class="line">	# the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span><br><span class="line">	# help the optimizer.</span><br><span class="line">	prev_output = bert_utils.reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">	all_layer_outputs = []</span><br><span class="line"></span><br><span class="line">	def layer_scope(idx, shared_type):</span><br><span class="line">		if shared_type == &#x27;all&#x27;:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_shared&quot;,</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention&#x27;,</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate&#x27;,</span><br><span class="line">				&#x27;output&#x27;:&#x27;output&#x27;</span><br><span class="line">			&#125;</span><br><span class="line">		elif shared_type == &#x27;attention&#x27;:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_shared&quot;,</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention&#x27;,</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate_&#123;&#125;&#x27;.format(idx),</span><br><span class="line">				&#x27;output&#x27;:&#x27;output_&#123;&#125;&#x27;.format(idx)</span><br><span class="line">			&#125;</span><br><span class="line">		elif shared_type == &#x27;ffn&#x27;:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_shared&quot;,</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention_&#123;&#125;&#x27;.format(idx),</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate&#x27;,</span><br><span class="line">				&#x27;output&#x27;:&#x27;output&#x27;</span><br><span class="line">			&#125;</span><br><span class="line">		else:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_&#123;&#125;&quot;.format(idx),</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention&#x27;,</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate&#x27;,</span><br><span class="line">				&#x27;output&#x27;:&#x27;output&#x27;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">		return tmp</span><br><span class="line"></span><br><span class="line">	all_layer_outputs = []</span><br><span class="line"></span><br><span class="line">	for layer_idx in range(num_hidden_layers):</span><br><span class="line"></span><br><span class="line">		idx_scope = layer_scope(layer_idx, shared_type)</span><br><span class="line"></span><br><span class="line">		with tf.variable_scope(idx_scope[&#x27;layer&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">			layer_input = prev_output</span><br><span class="line"></span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;attention&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				attention_heads = []</span><br><span class="line"></span><br><span class="line">				with tf.variable_scope(&quot;output&quot;, reuse=tf.AUTO_REUSE):</span><br><span class="line">					layer_input_pre = layer_norm(layer_input)</span><br><span class="line"></span><br><span class="line">				with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">					attention_head = attention_layer(</span><br><span class="line">							from_tensor=layer_input_pre,</span><br><span class="line">							to_tensor=layer_input_pre,</span><br><span class="line">							attention_mask=attention_mask,</span><br><span class="line">							num_attention_heads=num_attention_heads,</span><br><span class="line">							size_per_head=attention_head_size,</span><br><span class="line">							attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">							initializer_range=initializer_range,</span><br><span class="line">							do_return_2d_tensor=True,</span><br><span class="line">							batch_size=batch_size,</span><br><span class="line">							from_seq_length=seq_length,</span><br><span class="line">							to_seq_length=seq_length)</span><br><span class="line">					attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">				attention_output = None</span><br><span class="line">				if len(attention_heads) == 1:</span><br><span class="line">					attention_output = attention_heads[0]</span><br><span class="line">				else:</span><br><span class="line">					# In the case where we have other sequences, we just concatenate</span><br><span class="line">					# them to the self-attention head before the projection.</span><br><span class="line">					attention_output = tf.concat(attention_heads, axis=-1)</span><br><span class="line"></span><br><span class="line">				# Run a linear projection of `hidden_size` then add a residual</span><br><span class="line">				# with `layer_input`.</span><br><span class="line">				with tf.variable_scope(&quot;output&quot;, reuse=tf.AUTO_REUSE):</span><br><span class="line">					attention_output = tf.layers.dense(</span><br><span class="line">							attention_output,</span><br><span class="line">							hidden_size,</span><br><span class="line">							kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">					attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">					# attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line">					attention_output = attention_output + layer_input</span><br><span class="line"></span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;output&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				attention_output_pre = layer_norm(attention_output)</span><br><span class="line"></span><br><span class="line">			# The activation is only applied to the &quot;intermediate&quot; hidden layer.</span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;intermediate&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				intermediate_output = tf.layers.dense(</span><br><span class="line">						attention_output_pre,</span><br><span class="line">						intermediate_size,</span><br><span class="line">						activation=intermediate_act_fn,</span><br><span class="line">						kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">			# Down-project back to `hidden_size` then add the residual.</span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;output&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				layer_output = tf.layers.dense(</span><br><span class="line">						intermediate_output,</span><br><span class="line">						hidden_size,</span><br><span class="line">						kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">				layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">				# layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">				layer_output = layer_output + attention_output</span><br><span class="line">				prev_output = layer_output</span><br><span class="line">				all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">	if do_return_all_layers:</span><br><span class="line">		final_outputs = []</span><br><span class="line">		for layer_output in all_layer_outputs:</span><br><span class="line">			final_output = bert_utils.reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">			final_outputs.append(final_output)</span><br><span class="line">		return final_outputs</span><br><span class="line">	else:</span><br><span class="line">		final_output = bert_utils.reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">		return final_output</span><br></pre></td></tr></table></figure>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="MLM"><a href="#MLM" class="headerlink" title="MLM"></a>MLM</h3><p><strong>主要流程为：</strong></p>
<ol>
<li><strong>提取模型输出中mask position位置的向量</strong></li>
<li><strong>经过变换 每一个输出vocab size大小</strong></li>
<li><strong>与标签计算交叉熵损失</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_masked_lm_output(albert_config, input_tensor, output_weights, positions,</span><br><span class="line">                         label_ids, label_weights):</span><br><span class="line">  &quot;&quot;&quot;Get loss and log probs for the masked LM.&quot;&quot;&quot;</span><br><span class="line">  input_tensor = gather_indexes(input_tensor, positions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  with tf.variable_scope(&quot;cls/predictions&quot;):</span><br><span class="line">    # We apply one more non-linear transformation before the output layer.</span><br><span class="line">    # This matrix is not used after pre-training.</span><br><span class="line">    with tf.variable_scope(&quot;transform&quot;):</span><br><span class="line">      input_tensor = tf.layers.dense(</span><br><span class="line">          input_tensor,</span><br><span class="line">          units=albert_config.embedding_size,</span><br><span class="line">          activation=modeling.get_activation(albert_config.hidden_act),</span><br><span class="line">          kernel_initializer=modeling.create_initializer(</span><br><span class="line">              albert_config.initializer_range))</span><br><span class="line">      input_tensor = modeling.layer_norm(input_tensor)</span><br><span class="line"></span><br><span class="line">    # The output weights are the same as the input embeddings, but there is</span><br><span class="line">    # an output-only bias for each token.</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        &quot;output_bias&quot;,</span><br><span class="line">        shape=[albert_config.vocab_size],</span><br><span class="line">        initializer=tf.zeros_initializer())</span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=-1)</span><br><span class="line"></span><br><span class="line">    label_ids = tf.reshape(label_ids, [-1])</span><br><span class="line">    label_weights = tf.reshape(label_weights, [-1])</span><br><span class="line"></span><br><span class="line">    one_hot_labels = tf.one_hot(</span><br><span class="line">        label_ids, depth=albert_config.vocab_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    # The `positions` tensor might be zero-padded (if the sequence is too</span><br><span class="line">    # short to have the maximum number of predictions). The `label_weights`</span><br><span class="line">    # tensor has a value of 1.0 for every real prediction and 0.0 for the</span><br><span class="line">    # padding predictions.</span><br><span class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])</span><br><span class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</span><br><span class="line">    denominator = tf.reduce_sum(label_weights) + 1e-5</span><br><span class="line">    loss = numerator / denominator</span><br><span class="line"></span><br><span class="line">  return (loss, per_example_loss, log_probs)</span><br></pre></td></tr></table></figure>
<h3 id="SOP"><a href="#SOP" class="headerlink" title="SOP"></a>SOP</h3><p>主要流程为：</p>
<ol>
<li>模型输出向量 转换为 输出为2维的向量</li>
<li>与标签计算交叉熵损失</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_sentence_order_output(albert_config, input_tensor, labels):</span><br><span class="line">  &quot;&quot;&quot;Get loss and log probs for the next sentence prediction.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  # Simple binary classification. Note that 0 is &quot;next sentence&quot; and 1 is</span><br><span class="line">  # &quot;random sentence&quot;. This weight matrix is not used after pre-training.</span><br><span class="line">  with tf.variable_scope(&quot;cls/seq_relationship&quot;):</span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        &quot;output_weights&quot;,</span><br><span class="line">        shape=[2, albert_config.hidden_size],</span><br><span class="line">        initializer=modeling.create_initializer(</span><br><span class="line">            albert_config.initializer_range))</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        &quot;output_bias&quot;, shape=[2], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=-1)</span><br><span class="line">    labels = tf.reshape(labels, [-1])</span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)</span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)</span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line">    return (loss, per_example_loss, log_probs)</span><br></pre></td></tr></table></figure>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">roberta</td>
<td style="text-align:center">0.88503</td>
<td style="text-align:center">0.86344</td>
</tr>
<tr>
<td style="text-align:center">albert</td>
<td style="text-align:center">0.85662</td>
<td style="text-align:center">0.84960</td>
</tr>
<tr>
<td style="text-align:center">预训练后roberta</td>
<td style="text-align:center"><strong>0.89343</strong></td>
<td style="text-align:center">0.85328</td>
</tr>
<tr>
<td style="text-align:center">预训练后albert</td>
<td style="text-align:center">0.84958</td>
<td style="text-align:center"><strong>0.85224</strong></td>
</tr>
</tbody>
</table>
</div>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>模型根据验证集结果保存最优模型，因此测试集上表现不一定是最优的，我们主要看在验证集上的表现。以上模型在相同参数下只跑了一次，因此结果会略有浮动。</p>
<ol>
<li>roberta在预训练后效果取得提升，经过再次预训练，模型领域与微调领域更加接近，效果更好</li>
<li>Albert预训练后效果下降，可能与我们构建数据的方式有关，构建的数据与SOP任务并不符合，可以尝试更符合要求的数据进行测试。</li>
</ol>
<h1 id="TO-DO"><a href="#TO-DO" class="headerlink" title="TO DO"></a>TO DO</h1><ul>
<li>MACBert-20220319暂未开源预训练代码</li>
<li>根据SpanBert改为n-gram 掩码与SBO任务</li>
<li>Pytorch与keras的预训练</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>预训练</category>
      </categories>
      <tags>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP技能树学习路线-（一）路线总览.md</title>
    <url>/2022/06/25/2022-06-25-NLP%E6%8A%80%E8%83%BD%E6%A0%91%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF-%EF%BC%88%E4%B8%80%EF%BC%89%E8%B7%AF%E7%BA%BF%E6%80%BB%E8%A7%88/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>偶然发现一位韩国小哥整理了NLP的学习路线：<a href="https://github.com/graykode/nlp-roadmap">nlp-roadmap</a>，其中知识点覆盖很全面。由于里面内容都是英文的，笔者重新用xmind绘制学习路线图，并结合自己的理解，将一些名词翻译为中文名词。</p>
<p>笔者打算根据该学习路线，对自己的知识体系进行查漏补缺，并在该系列文章记录自己学习过程。本部分内容旨在贴出各部分学习路线图，总览待学习内容。</p>
<p><img src="https://picx.zhimg.com/v2-c39bf0e8abf032430c470985e1aae7b1_1440w.jpg?source=172ae18b" alt=""></p>
<span id="more"></span>
<h1 id="概率统计"><a href="#概率统计" class="headerlink" title="概率统计"></a>概率统计</h1><p><img src="https://pic4.zhimg.com/80/v2-a20a27c1c86bbe6d940e95d4e636a6bf_1440w.jpg" alt=""></p>
<p>概率统计是人工智能算法的基础，图中分为贝叶斯、信息理论、模型、采样、基础五部分。笔者知识体系在“基础、采样、贝叶斯部分”存在缺漏，将在后面对该部分内容展开学习。</p>
<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><p><img src="https://pic3.zhimg.com/80/v2-76d9137fd2807eef14d1da51242719b2_1440w.webp" alt=""></p>
<p>机器学习算法也是NLP的基石，含有许多基础概念，图中分为训练、降维、聚类、非概率、线性回归、逻辑回归、正则化七部分。笔者知识体系在“降维、聚类”存在缺漏，将在后面对该部分内容展开学习。</p>
<h1 id="NLP下的数据挖掘"><a href="#NLP下的数据挖掘" class="headerlink" title="NLP下的数据挖掘"></a>NLP下的数据挖掘</h1><p><img src="https://pic4.zhimg.com/80/v2-38e9b7bb97abc054eb3275ef1cf43c5b_1440w.webp" alt=""></p>
<p>NLP下的数据挖掘包含常见NLP数据处理操作，图中分为基础流程、序列标注、词嵌入、NLP基本假设、图、文档六部分。笔者知识体系在“主题模型、NLP基本假设、图”存在缺漏，将在后面对该部分内容展开学习。</p>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><p><img src="https://pic4.zhimg.com/80/v2-8ca2aec0135b78a750a6fef2f2858683_1440w.webp" alt=""></p>
<p>该部分正式介绍NLP的学习路线，分为基础学习、分布式特征、具体任务、语言模型四部分。笔者知识体系在一些具体模型存在缺漏，将在后面对该部分内容展开学习。</p>
<p>若大家需要思维导图源文件可以私我，有兴趣一起学习也可以私我加好友～</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>学习路线</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>真香～BERT在MAC Pytorch的使用.md</title>
    <url>/2022/07/16/2022-07-16-%E7%9C%9F%E9%A6%99%EF%BD%9EBERT%E5%9C%A8MAC%20Pytorch%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>终于，Pytorch也支持MAC的硬件加速，两个字评价一下感受：真香～</p>
<p>周末笔者在自己机器上完成环境安装，笔者机器环境如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-6ac54749fc06a77883b3095e62d36a60_1440w.jpg" alt=""></p>
<p>接着，笔者在该文用卷积、BERT模型对比了有无MAC硬件加速的模型运行时间</p>
<span id="more"></span>
<h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><p>按照官网给出的命令，即可完成安装MAC硬件加速版pytorch。</p>
<p><a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p>
<blockquote>
<p>conda install pytorch torchvision torchaudio -c pytorch</p>
</blockquote>
<h1 id="简单测试"><a href="#简单测试" class="headerlink" title="简单测试"></a>简单测试</h1><p>利用卷积操作，测试有无硬件加速的效果。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev = &#x27;mps:0&#x27;</span><br><span class="line"></span><br><span class="line">conv = torch.nn.Conv2d(10, 10, 3).to(dev)</span><br><span class="line"></span><br><span class="line">img = torch.randn(64, 10, 64, 64).to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line"></span><br><span class="line">for i in range(1000):</span><br><span class="line"></span><br><span class="line">    conv(img)</span><br><span class="line"></span><br><span class="line">t1 = time.time()</span><br><span class="line"></span><br><span class="line">print(&#x27;Use mps, time:&#123;&#125;&#x27;.format(t1-t0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev = &#x27;cpu&#x27;</span><br><span class="line"></span><br><span class="line">conv = torch.nn.Conv2d(10, 10, 3).to(dev)</span><br><span class="line"></span><br><span class="line">img = torch.randn(64, 10, 64, 64).to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line"></span><br><span class="line">for i in range(1000):</span><br><span class="line"></span><br><span class="line">    conv(img)</span><br><span class="line"></span><br><span class="line">t1 = time.time()</span><br><span class="line"></span><br><span class="line">print(&#x27;Use cpu, time:&#123;&#125;&#x27;.format(t1-t0))</span><br></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="https://pic2.zhimg.com/80/v2-371933b35c5a6dc4f9c4c9cd18805f1d_1440w.webp" alt=""></p>
<h1 id="BERT测试"><a href="#BERT测试" class="headerlink" title="BERT测试"></a>BERT测试</h1><p>使用huggingface的glue代码作示例。</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>运行下述代码完成数据下载工作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;&#x27;&#x27; Script for downloading all GLUE data.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Note: for legal reasons, we are unable to host MRPC.</span><br><span class="line"></span><br><span class="line">You can either use the version hosted by the SentEval team, which is already tokenized,</span><br><span class="line"></span><br><span class="line">or you can download the original data from (https://download.microsoft.com/download/D/4/6/D46FF87A-F6B9-4252-AA8B-3604ED519838/MSRParaphraseCorpus.msi) and extract the data from it manually.</span><br><span class="line"></span><br><span class="line">For Windows users, you can run the .msi file. For Mac and Linux users, consider an external library such as &#x27;cabextract&#x27; (see below for an example).</span><br><span class="line"></span><br><span class="line">You should then rename and place specific files in a folder (see below for an example).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mkdir MRPC</span><br><span class="line"></span><br><span class="line">cabextract MSRParaphraseCorpus.msi -d MRPC</span><br><span class="line"></span><br><span class="line">cat MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D | tr -d $&#x27;\r&#x27; &gt; MRPC/msr_paraphrase_train.txt</span><br><span class="line"></span><br><span class="line">cat MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61 | tr -d $&#x27;\r&#x27; &gt; MRPC/msr_paraphrase_test.txt</span><br><span class="line"></span><br><span class="line">rm MRPC/_*</span><br><span class="line"></span><br><span class="line">rm MSRParaphraseCorpus.msi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1/30/19: It looks like SentEval is no longer hosting their extracted and tokenized MRPC data, so you&#x27;ll need to download the data from the original source for now.</span><br><span class="line"></span><br><span class="line">2/11/19: It looks like SentEval actually *is* hosting the extracted data. Hooray!</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">import tempfile</span><br><span class="line"></span><br><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">import zipfile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASKS = [&quot;CoLA&quot;, &quot;SST&quot;, &quot;MRPC&quot;, &quot;QQP&quot;, &quot;STS&quot;, &quot;MNLI&quot;, &quot;QNLI&quot;, &quot;RTE&quot;, &quot;WNLI&quot;, &quot;diagnostic&quot;]</span><br><span class="line"></span><br><span class="line">TASK2PATH = &#123;&quot;CoLA&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/CoLA.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;SST&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/SST-2.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;QQP&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;STS&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/STS-B.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;MNLI&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/MNLI.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;QNLI&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;RTE&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/RTE.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;WNLI&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/WNLI.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;diagnostic&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/AX.tsv&#x27;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MRPC_TRAIN = &#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt&#x27;</span><br><span class="line"></span><br><span class="line">MRPC_TEST = &#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def download_and_extract(task, data_dir):</span><br><span class="line"></span><br><span class="line">    print(&quot;Downloading and extracting %s...&quot; % task)</span><br><span class="line"></span><br><span class="line">    if task == &quot;MNLI&quot;:</span><br><span class="line"></span><br><span class="line">        print(</span><br><span class="line"></span><br><span class="line">            &quot;\tNote (12/10/20): This script no longer downloads SNLI. You will need to manually download and format the data to use SNLI.&quot;)</span><br><span class="line"></span><br><span class="line">    data_file = &quot;%s.zip&quot; % task</span><br><span class="line"></span><br><span class="line">    urllib.request.urlretrieve(TASK2PATH[task], data_file)</span><br><span class="line"></span><br><span class="line">    with zipfile.ZipFile(data_file) as zip_ref:</span><br><span class="line"></span><br><span class="line">        zip_ref.extractall(data_dir)</span><br><span class="line"></span><br><span class="line">    os.remove(data_file)</span><br><span class="line"></span><br><span class="line">    print(&quot;\tCompleted!&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def format_mrpc(data_dir, path_to_data):</span><br><span class="line"></span><br><span class="line">    print(&quot;Processing MRPC...&quot;)</span><br><span class="line"></span><br><span class="line">    mrpc_dir = os.path.join(data_dir, &quot;MRPC&quot;)</span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(mrpc_dir):</span><br><span class="line"></span><br><span class="line">        os.mkdir(mrpc_dir)</span><br><span class="line"></span><br><span class="line">    if path_to_data:</span><br><span class="line"></span><br><span class="line">        mrpc_train_file = os.path.join(path_to_data, &quot;msr_paraphrase_train.txt&quot;)</span><br><span class="line"></span><br><span class="line">        mrpc_test_file = os.path.join(path_to_data, &quot;msr_paraphrase_test.txt&quot;)</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line"></span><br><span class="line">            mrpc_train_file = os.path.join(mrpc_dir, &quot;msr_paraphrase_train.txt&quot;)</span><br><span class="line"></span><br><span class="line">            mrpc_test_file = os.path.join(mrpc_dir, &quot;msr_paraphrase_test.txt&quot;)</span><br><span class="line"></span><br><span class="line">            URLLIB.urlretrieve(MRPC_TRAIN, mrpc_train_file)</span><br><span class="line"></span><br><span class="line">            URLLIB.urlretrieve(MRPC_TEST, mrpc_test_file)</span><br><span class="line"></span><br><span class="line">        except urllib.error.HTTPError:</span><br><span class="line"></span><br><span class="line">            print(&quot;Error downloading MRPC&quot;)</span><br><span class="line"></span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">    assert os.path.isfile(mrpc_train_file), &quot;Train data not found at %s&quot; % mrpc_train_file</span><br><span class="line"></span><br><span class="line">    assert os.path.isfile(mrpc_test_file), &quot;Test data not found at %s&quot; % mrpc_test_file</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    with io.open(mrpc_test_file, encoding=&#x27;utf-8&#x27;) as data_fh, \</span><br><span class="line"></span><br><span class="line">            io.open(os.path.join(mrpc_dir, &quot;test.tsv&quot;), &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as test_fh:</span><br><span class="line"></span><br><span class="line">        header = data_fh.readline()</span><br><span class="line"></span><br><span class="line">        test_fh.write(&quot;index\t#1 ID\t#2 ID\t#1 String\t#2 String\n&quot;)</span><br><span class="line"></span><br><span class="line">        for idx, row in enumerate(data_fh):</span><br><span class="line"></span><br><span class="line">            label, id1, id2, s1, s2 = row.strip().split(&#x27;\t&#x27;)</span><br><span class="line"></span><br><span class="line">            test_fh.write(&quot;%d\t%s\t%s\t%s\t%s\n&quot; % (idx, id1, id2, s1, s2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line"></span><br><span class="line">        URLLIB.urlretrieve(TASK2PATH[&quot;MRPC&quot;], os.path.join(mrpc_dir, &quot;dev_ids.tsv&quot;))</span><br><span class="line"></span><br><span class="line">    except KeyError or urllib.error.HTTPError:</span><br><span class="line"></span><br><span class="line">        print(&quot;\tError downloading standard development IDs for MRPC. You will need to manually split your data.&quot;)</span><br><span class="line"></span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    dev_ids = []</span><br><span class="line"></span><br><span class="line">    with io.open(os.path.join(mrpc_dir, &quot;dev_ids.tsv&quot;), encoding=&#x27;utf-8&#x27;) as ids_fh:</span><br><span class="line"></span><br><span class="line">        for row in ids_fh:</span><br><span class="line"></span><br><span class="line">            dev_ids.append(row.strip().split(&#x27;\t&#x27;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    with io.open(mrpc_train_file, encoding=&#x27;utf-8&#x27;) as data_fh, \</span><br><span class="line"></span><br><span class="line">            io.open(os.path.join(mrpc_dir, &quot;train.tsv&quot;), &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as train_fh, \</span><br><span class="line"></span><br><span class="line">            io.open(os.path.join(mrpc_dir, &quot;dev.tsv&quot;), &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as dev_fh:</span><br><span class="line"></span><br><span class="line">        header = data_fh.readline()</span><br><span class="line"></span><br><span class="line">        train_fh.write(header)</span><br><span class="line"></span><br><span class="line">        dev_fh.write(header)</span><br><span class="line"></span><br><span class="line">        for row in data_fh:</span><br><span class="line"></span><br><span class="line">            label, id1, id2, s1, s2 = row.strip().split(&#x27;\t&#x27;)</span><br><span class="line"></span><br><span class="line">            if [id1, id2] in dev_ids:</span><br><span class="line"></span><br><span class="line">                dev_fh.write(&quot;%s\t%s\t%s\t%s\t%s\n&quot; % (label, id1, id2, s1, s2))</span><br><span class="line"></span><br><span class="line">            else:</span><br><span class="line"></span><br><span class="line">                train_fh.write(&quot;%s\t%s\t%s\t%s\t%s\n&quot; % (label, id1, id2, s1, s2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(&quot;\tCompleted!&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def download_diagnostic(data_dir):</span><br><span class="line"></span><br><span class="line">    print(&quot;Downloading and extracting diagnostic...&quot;)</span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(os.path.join(data_dir, &quot;diagnostic&quot;)):</span><br><span class="line"></span><br><span class="line">        os.mkdir(os.path.join(data_dir, &quot;diagnostic&quot;))</span><br><span class="line"></span><br><span class="line">    data_file = os.path.join(data_dir, &quot;diagnostic&quot;, &quot;diagnostic.tsv&quot;)</span><br><span class="line"></span><br><span class="line">    urllib.request.urlretrieve(TASK2PATH[&quot;diagnostic&quot;], data_file)</span><br><span class="line"></span><br><span class="line">    print(&quot;\tCompleted!&quot;)</span><br><span class="line"></span><br><span class="line">    return</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tasks(task_names):</span><br><span class="line"></span><br><span class="line">    task_names = task_names.split(&#x27;,&#x27;)</span><br><span class="line"></span><br><span class="line">    if &quot;all&quot; in task_names:</span><br><span class="line"></span><br><span class="line">        tasks = TASKS</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line"></span><br><span class="line">        tasks = []</span><br><span class="line"></span><br><span class="line">        for task_name in task_names:</span><br><span class="line"></span><br><span class="line">            assert task_name in TASKS, &quot;Task %s not found!&quot; % task_name</span><br><span class="line"></span><br><span class="line">            tasks.append(task_name)</span><br><span class="line"></span><br><span class="line">    return tasks</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(arguments):</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">    parser.add_argument(&#x27;--data_dir&#x27;, help=&#x27;directory to save data to&#x27;, type=str, default=&#x27;glue_data&#x27;)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(&#x27;--tasks&#x27;, help=&#x27;tasks to download data for as a comma separated string&#x27;,</span><br><span class="line"></span><br><span class="line">                        type=str, default=&#x27;all&#x27;)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(&#x27;--path_to_mrpc&#x27;,</span><br><span class="line"></span><br><span class="line">                        help=&#x27;path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt&#x27;,</span><br><span class="line"></span><br><span class="line">                        type=str, default=&#x27;&#x27;)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args(arguments)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(args.data_dir):</span><br><span class="line"></span><br><span class="line">        os.mkdir(args.data_dir)</span><br><span class="line"></span><br><span class="line">    tasks = get_tasks(args.tasks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    for task in tasks:</span><br><span class="line"></span><br><span class="line">        if task == &#x27;MRPC&#x27;:</span><br><span class="line"></span><br><span class="line">            format_mrpc(args.data_dir, args.path_to_mrpc)</span><br><span class="line"></span><br><span class="line">        elif task == &#x27;diagnostic&#x27;:</span><br><span class="line"></span><br><span class="line">            download_diagnostic(args.data_dir)</span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line"></span><br><span class="line">            download_and_extract(task, args.data_dir)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">    sys.exit(main(sys.argv[1:]))</span><br></pre></td></tr></table></figure>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>requirements内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">accelerate</span><br><span class="line"></span><br><span class="line">datasets &gt;= 1.8.0</span><br><span class="line"></span><br><span class="line">sentencepiece != 0.1.92</span><br><span class="line"></span><br><span class="line">scipy</span><br><span class="line"></span><br><span class="line">scikit-learn</span><br><span class="line"></span><br><span class="line">protobuf</span><br><span class="line"></span><br><span class="line">numpy==1.17.3</span><br><span class="line"></span><br><span class="line">#torch &gt;= 1.3</span><br></pre></td></tr></table></figure>
<h2 id="代码准备"><a href="#代码准备" class="headerlink" title="代码准备"></a>代码准备</h2><p>利用huggingface的<strong>run_glue_no_trainer.py</strong>。</p>
<p>运行脚本如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export TASK_NAME=mrpc</span><br><span class="line"></span><br><span class="line">python run_glue_no_trainer.py \</span><br><span class="line">  --model_name_or_path Pretrained_LMs/bert-base-cased \</span><br><span class="line">  --task_name $TASK_NAME \</span><br><span class="line">  --max_length 128 \</span><br><span class="line">  --per_device_train_batch_size 32 \</span><br><span class="line">  --learning_rate 2e-5 \</span><br><span class="line">  --num_train_epochs 3 \</span><br><span class="line">  --output_dir ./output/$TASK_NAME/</span><br></pre></td></tr></table></figure>
<p>在代码中修改运行设备方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    accelerator.state.device = &#x27;mps&#x27;</span><br><span class="line"></span><br><span class="line">    print(&#x27;-&#x27; * 100)</span><br><span class="line"></span><br><span class="line">    print(accelerator.state.device)</span><br><span class="line"></span><br><span class="line">    print(&#x27;-&#x27; * 100)</span><br></pre></td></tr></table></figure>
<h2 id="运行结果-1"><a href="#运行结果-1" class="headerlink" title="运行结果"></a>运行结果</h2><p>CPU下运行时间约1h：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Num processes: 1</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: cpu</span><br><span class="line">...</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ - ***** Running training *****</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Num examples = 3668</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Num Epochs = 3</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Instantaneous batch size per device = 32</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Total train batch size (w. parallel, distributed &amp; accumulation) = 32</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Gradient Accumulation steps = 1</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Total optimization steps = 345</span><br><span class="line">  2%|███▌                                                                                                                                                                                                       | 6/345 [01:06&lt;1:03:49, 11.30s/it]</span><br></pre></td></tr></table></figure>
<p>硬件加速下运行时间约20min：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Num processes: 1</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: mps</span><br><span class="line">...</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ - ***** Running training *****</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Num examples = 3668</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Num Epochs = 3</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Instantaneous batch size per device = 32</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Total train batch size (w. parallel, distributed &amp; accumulation) = 32</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Gradient Accumulation steps = 1</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Total optimization steps = 345</span><br><span class="line">  5%|██████████▋                                                                                                                                                                                                 | 18/345 [01:03&lt;20:14,  3.71s/it]</span><br></pre></td></tr></table></figure>
<p>观察MAC活动监视器，可以看到程序确实有用到GPU硬件加速。</p>
<p><img src="https://pic3.zhimg.com/80/v2-105dfbe9e73e8dc800994e94e9a08e2e_1440w.webp" alt=""></p>
<h2 id="bug-fix"><a href="#bug-fix" class="headerlink" title="bug fix"></a>bug fix</h2><p>在运行过程中出现如下错误：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OMP: Error #15: Initializing libomp.dylib, but found libiomp5.dylib already initialize异常</span><br></pre></td></tr></table></figure>
<p>参照该<a href="http://t.zoukankan.com/yxym2016-p-13900887.html">链接</a>解决了问题，<strong>如果Python是基本于Conda安装的，则Conda上的numpy包中的mkl很容易与系统内库发生冲突，可选择update numpy package in Conda或者设置为系统库。</strong></p>
<p><strong>解决方案</strong>:降低numpy的版本，此处笔者将版本降低到1.17.3</p>
<blockquote>
<p>pip install numpy==1.17.3</p>
</blockquote>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>一文梳理NLP主要模型发展脉络.md</title>
    <url>/2022/07/31/2022-07-31-%E4%B8%80%E6%96%87%E6%A2%B3%E7%90%86NLP%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文根据笔者所学知识，对NLP主要模型的发展脉络作梳理，目的在于了解主流技术的前世今生，如有理解错误的地方，麻烦指正～</p>
<p><img src="https://pic1.zhimg.com/80/v2-4f13c1d7a7fda98d177c406106177c98_1440w.jpg" alt=""></p>
<p>下面将依次介绍RNN、LSTM、GRU、Encoder-Deocder、Transformer、BERT设计的出发点，模型结构不作详细介绍。</p>
<span id="more"></span>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>自然语言处理的数据类型多为文本类型，文本数据的上下文关系具有较强的序列特征。同时，RNN模型具有“上一时刻输出作为下一时刻的输入”的特征，该特征能够很好的处理序列数据。因此，RNN模型相较于其他模型，更适合处理自然语言处理任务。</p>
<p>当待处理序列长度较长时，RNN模型在反向传播的过程中，受链式求导法则的影响，当求导过程导数过小或过大时，会导致梯度消失或梯度爆炸。</p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>RNN模型的权重矩阵在时间维度上是共享的。LSTM相较于RNN模型，通过引入门控机制，缓解梯度消失，那么<strong>LSTM如何避免梯度消失？</strong></p>
<p>这里给出几个关键结论，详细分析见<a href="https://jmxgodlz.xyz/2022/08/07/2022-08-07-详解LSTM与梯度消失/#more">详解LSTM与梯度消失</a>。</p>
<ul>
<li><p>RNN模型在时间维度共享参数矩阵，因此RNN模型总的梯度等于各时间的梯度之和，$g=\sum{g_t}$。</p>
</li>
<li><p>RNN中总的梯度不会消失，只是远距离梯度消失，梯度被近距离梯度主导，无法捕获远距离特征。</p>
</li>
<li><p>梯度消失的本质：由于RNN模型在时间维度共享参数矩阵，导致针对隐藏状态h求导时，循环计算矩阵乘法，最终梯度上出现了参数矩阵的累乘。</p>
</li>
<li><p>LSTM缓解梯度消失的本质：引入门控机制，将矩阵乘法转为逐元素相乘的哈达马积:$c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tanh \left(W_{c}\left[h_{t-1}, x_{t}\right]+b_{c}\right)$</p>
</li>
</ul>
<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>GRU与LSTM模型相同，引入门控机制，避免梯度消失。区别在于，GRU只用了重置门与更新门两个门结构，参数量较LSTM少，训练速度更快。</p>
<h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p>RNN模型“上一时刻输出作为下一时刻的输入”的特征，也存在模型输入输出一直是等长的问题。Encoder-Decoder模型通过编码器与解码器两个部分，解决了输入输出定长的问题。其中Encoder端负责文本序列的特征表示获取，Decoder端根据特征向量解码输出序列。</p>
<p>但Encoder-Decoder模型仍然存在以下问题：</p>
<ul>
<li><p>文本序列的特征表示向量选取</p>
</li>
<li><p>特征表示向量包含特征的有限性</p>
</li>
<li><p>OOV问题</p>
</li>
</ul>
<p>第一个与第二个问题通过<strong>注意力机制</strong>解决，利用注意力机制有选择的关注文本序列重要的特征。</p>
<p>第三个问题则通过<strong>拷贝机制</strong>以及<strong>Subword编码</strong>解决。</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer模型主要包含多头自注意力模块、前馈神经网络、残差结构与Dropout，其中核心模块为<strong>多头自注意力模块</strong>，各组件的功能如下：</p>
<ul>
<li><p><strong>自注意力机制</strong>在编码器端有选择的关注文本序列重要的特征，解决文本序列的特征表示向量选取及该向量包含特征的有限性问题。</p>
</li>
<li><p><strong>多头机制</strong>中每一头映射到不同空间，得到不同侧重点的特征表示，使得特征表示的更充分。</p>
</li>
<li><p><strong>残差结构</strong>有效避免梯度消失。</p>
</li>
<li><p><strong>Dropout</strong>有效避免过拟合。</p>
</li>
<li><p><strong>前馈神经网络</strong>完成隐含层到输出空间的映射</p>
</li>
</ul>
<p>接下来将重点介绍<strong>Transformer</strong>模型的<strong>优点</strong>。</p>
<h2 id="1-Transformer能够实现长距离依赖"><a href="#1-Transformer能够实现长距离依赖" class="headerlink" title="1. Transformer能够实现长距离依赖"></a>1. Transformer能够实现长距离依赖</h2><p>在自注意力机制中，每个字符能够与其他所有字符计算注意力得分。这种计算方式未考虑时序特征，能够捕获长距离依赖。</p>
<p>但该方式缺点在于，注意力得分计算的时间复杂度为$O(n^2)$，当输入序列较长时，时间复杂度过高，因此不适合处理过长数据。</p>
<h2 id="2-Transformer能够实现并行化"><a href="#2-Transformer能够实现并行化" class="headerlink" title="2. Transformer能够实现并行化"></a>2. Transformer能够实现并行化</h2><p>假设输入序列为（a,b,c,d）</p>
<p>传统RNN需要计算a的embedding向量得到$e_a$,再经过特征抽取得到$h_a$,然后以相同方式计算b,c,d。</p>
<p>Transformer通过self-attention机制，每个词都可以与全部序列交互，应此模型可以同时处理整个序列，得到$e_a,e_b,e_c,e_d$，然后再一起计算$h_a,h_b,h_c,h_d$。</p>
<h2 id="3-Transformer适合预训练"><a href="#3-Transformer适合预训练" class="headerlink" title="3. Transformer适合预训练"></a>3. Transformer适合预训练</h2><ul>
<li><p>RNN模型作出输入数据具有时序性的假设，将前一时刻的输出作为下一时刻的输入。</p>
</li>
<li><p>CNN模型基于输入数据为图像的假设，在结构中加入一些特质【如卷积生成特征】，使前向传播更加高效，降低网络的参数量。</p>
</li>
</ul>
<p>与CNN、RNN模型不同， Transformer 模型是一种灵活的架构，对输入数据的结构无限制，因此适合在大规模数据上进行预训练，但该点也带来了Transformer模型在小规模数据集上泛化性差的问题。改进方法包括引入结构偏差或正则化，对大规模未标记数据进行预训练等。</p>
<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>首先，BERT模型参照GPT模型，采样预训练-微调两阶段的训练方式。但是，与GPT使用Transformer解码器部分不同，BERT为了充分利用上下文信息，使用Transformer编码器部分作为模型结构。</p>
<h3 id="训练任务"><a href="#训练任务" class="headerlink" title="训练任务"></a>训练任务</h3><p>如果BERT与GPT同样使用语言模型作为学习任务，则模型存在标签泄露的问题【一个词的上下文包含了另一个词的预测目标】。因此为了利用上下文信息，BERT提出MLM掩码语言模型任务，通过上下文预测遮盖词，MLM有关介绍可见-<a href="https://jmxgodlz.xyz/2022/05/30/2022-05-30-不要停止预训练实战(二">不要停止预训练实战(二)-一日看尽MLM</a>-一日看尽MLM/#more)。</p>
<h3 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h3><p>针对Transformer结构及预训练方式，BERT模型仍存在以下改进点：</p>
<ul>
<li><p>训练方式：针对掩码方式与多任务训练方式进行改进，调整NSP训练任务与掩码的方式</p>
</li>
<li><p>模型结构调整：针对Transformer $O(n^2)$的时间复杂度以及输入结构无假设的两点，调整模型结构</p>
</li>
<li><p>架构调整：轻量化结构、加强跨块连接、自适应计算时间、分治策略的Transformer</p>
</li>
<li><p>预训练：使用完整Encoder模型，如T5，BART模型</p>
</li>
<li><p>多模态等下游任务应用</p>
</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-b90f773588e3119ca7c8ff505ed52785_1440w.webp" alt=""></p>
<p>BERT模型未来发展方向主要包含：更大更深的模型、多模态、跨语言、小样本、模型蒸馏，有关讨论可见-<a href="https://jmxgodlz.xyz/2021/12/31/2021-12-31-2022预训练的下一步是什么/">2022预训练的下一步是什么</a></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://arxiv.org/pdf/2106.04554.pdf">https://arxiv.org/pdf/2106.04554.pdf</a></p>
<p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>公式向-完美解释梯度消失与LSTM.md</title>
    <url>/2022/08/07/2022-08-07-%E8%AF%A6%E8%A7%A3LSTM%E4%B8%8E%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>首先抛出<strong>关键性结论：</strong></p>
<ol>
<li><p><strong>RNN模型在时间维度共享参数矩阵，因此RNN模型总的梯度等于各时间的梯度之和</strong>，$g=\sum{g_t}$。</p>
</li>
<li><p><strong>RNN中总的梯度不会消失，只是远距离梯度消失，梯度被近距离梯度主导，无法捕获远距离特征。</strong></p>
</li>
<li><p><strong>梯度消失的本质：由于RNN模型在时间维度共享参数矩阵，导致针对隐藏状态h求导时，循环计算矩阵乘法，最终梯度上出现了参数矩阵的累乘。</strong></p>
</li>
<li><p><strong>LSTM缓解梯度消失的本质：引入门控机制，将矩阵乘法转为逐元素相乘的哈达马积:</strong>$c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tanh \left(W_{c}\left[h_{t-1}, x_{t}\right]+b_{c}\right)$</p>
</li>
</ol>
<span id="more"></span>
<h1 id="梯度消失分析"><a href="#梯度消失分析" class="headerlink" title="梯度消失分析"></a>梯度消失分析</h1><h2 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h2><p>RNN的状态更新公式如下：</p>
<script type="math/tex; mode=display">
\begin{equation}
h_t=f(Wh_{t-1}+Ux_t) \tag{1}
\end{equation}</script><script type="math/tex; mode=display">
y_t=f(Vh_t) \tag{2}</script><p>其中，h表示隐藏状态，f表示激活函数，W、U表示参数矩阵，x表示输入。从该式中可以看出<strong>不同时间维度的参数矩阵是共享的</strong>。</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>我们在反向传播过程进行梯度求导，以对参数矩阵U求导为例</p>
<script type="math/tex; mode=display">
\frac{\partial y_{t}}{\partial U}=\frac{\partial y_{t}}{\partial h_{t}}\frac{\partial h_{t}}{\partial U} \tag{3}</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{equation}
\frac{\partial h_{t}}{\partial U}=\sum_{s=0}^{t}\frac{\partial h_{t}}{\partial h_s}\frac{\partial h_{s}}{\partial U} \tag{4}
\end{equation}</script><script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{s}}=\frac{\partial h_{t}}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial h_{t-2}} \ldots \frac{\partial h_{s+1}}{\partial h_{s}} \tag{5}</script><p>从式（1）（2）中可以递推到$h_0$，每一个隐藏状态h均与参数矩阵有关，最后的梯度$y_t$<strong>依赖于每一个隐藏状态</strong>。</p>
<p>$以y_2为例,y_2=f(Vh_2)=f(Vf(Wh_1+Ux_1))=f(Vf(Wf(Wh_0+Ux_0)+Ux_1))$,通过全微分求导，$h_t$对$U,h_{t-1}$求偏导数</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\frac{\partial y_{2}}{\partial U}&=\frac{\partial y_{2}}{\partial h_{2}} \frac{\partial h_{2}}{\partial U} 
\\ &= \frac{\partial y_{2}}{\partial h_{2}} (\frac{\partial h_{2}}{\partial h_{1}} * \frac{\partial h_{1}}{\partial U} + \frac{\partial h_{2}}{\partial U})
\\ &=  \frac{\partial y_{2}}{\partial h_{2}} (\frac{\partial h_{2}}{\partial h_{1}} * (\frac{\partial h_{1}}{\partial h_{0}} * \frac{\partial h_{0}}{\partial U} + \frac{\partial h_{1}}{\partial U}) + \frac{\partial h_{2}}{\partial U})
\\ &=\frac{\partial y_{2}}{\partial h_{2}} (\frac{\partial h_{2}}{\partial U} + \frac{\partial h_{2}}{\partial h_1} \frac{\partial h_{1}}{\partial U} + \frac{\partial h_{2}}{\partial h_1} \frac{\partial h_{1}}{\partial h_0} \frac{\partial h_{0}}{\partial U})
\end{aligned}
\tag{6}</script><script type="math/tex; mode=display">
设:z_t=Wh_{t-1}+Ux_t \tag{7}</script><p>$z_t$代表未经过激活函数的神经网络输出，式（1）转化为：</p>
<script type="math/tex; mode=display">
h_t=f(z_t) \tag{8}</script><script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{t-1}}=\frac{\partial h_{t}}{\partial z_{t}} \frac{\partial z_{t}}{\partial h_{t-1}} \tag{9}</script><p>式（8）可以拆分为两部分：</p>
<script type="math/tex; mode=display">
\frac{\partial z_{t}}{\partial h_{t-1}} = W \tag{10}</script><script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial z_{t}}=\left(\begin{array}{cccc}\frac{\partial h_{t, 1}}{\partial z_{t, 1}} & \frac{\partial h_{t, 1}}{\partial z_{t, 2}} & \cdots & \frac{\partial h_{t, 1}}{\partial z_{t, n}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial h_{t, n}}{\partial z_{t, 1}} & \frac{\partial h_{t, n}}{\partial z_{t, 2}} & \cdots & \frac{\partial h_{t, n}}{\partial z_{t, n}}\end{array}\right) \tag{11}</script><p>其中，$h_t$元素由$z_t$逐元素激活得到，因此两者对应元素才具有依赖关系，未对应元素无依赖关系，导数为0，式（10）成为一个对角矩阵.</p>
<script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial z{t}}=\left(\begin{array}{cccc}f^{\prime}\left(\text { z}_{t, 1}\right) & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & f^{\prime}\left(z_{t, n}\right)\end{array}\right)=diag[f^{\prime}(z_t)] \tag{12}</script><p>根据式（9）（11），式（5）求解得到：</p>
<script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{s}}=\prod_{k=s+1}^{t} W^{T} \operatorname{diag}\left[f^{\prime}\left(\operatorname{z}_{k}\right)\right] \tag{13}</script><p>在式（12）中已经出现了<strong>矩阵的连乘</strong>，根据矩阵的相容性”$||X Y|| \le ||X|| ||Y||$”</p>
<script type="math/tex; mode=display">
\begin{aligned} 
||\frac{\partial h_{t}}{\partial h_{s}}||&=\prod_{k=s+1}^{t} || W^{T} \operatorname{diag}\left[f^{\prime}\left(\operatorname{z}_{k}\right)\right]|| 
\\ &\le \prod_{k=s+1}^{t} ||W^{T}|| ||\operatorname{diag}\left[f^{\prime}\left(\operatorname{z}_{k}\right)\right]||  
\\ &\le \prod_{k=s+1}^{t} \sigma_{max} \gamma = (\sigma_{max} \gamma)^{t-s}   
\end{aligned} 
\tag{14}</script><p>其中,<strong>$\sigma$ 代表矩阵W的最大奇异解，$\gamma$代表激活函数f的上界</strong>，例如双曲正切函数的上界为$||tanh^{‘}(x)|| \le 1$，sigmoid函数的上界为$||sigmoid^{‘}(x) \le \frac{1}{4}||$。<br>因此在远距离依赖，即t-s较大的情况下，<strong>当$\sigma_{max} \gamma \lt 1$时，会发生梯度消失;当$\sigma_{max} \gamma \gt 1$时，会发生梯度爆炸。</strong></p>
<p><strong>TIPS：</strong>这里只是不等式情况，因此即使不等式右边远大于1，也有可能发生梯度消失。但是，<strong>在实际情况下，矩阵范数的约束与实际值相当接近。</strong></p>
<h3 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h3><p>对于传统RNN模型，在训练初期避免梯度消失与参数矩阵的初始化，即最大奇异解$\sigma$值有关。</p>
<p><strong>避免梯度消失的矩阵最小初始化方式如下：</strong></p>
<p>以双曲正切函数为例，双曲正切函数的$\gamma = 1$，为了使$\sigma_{max} \gamma = 1$，即$\sigma=1$。</p>
<p>为了使不等式置信度更高，将矩阵W的所有奇异解设置为1.</p>
<p>对于每一列而言，$\Sigma_{i} w_{i j}^{2}=1$，其中j代表第j列，矩阵中每个元素是一个n维向量，i代表矩阵第i行，w代表列向量。</p>
<p>$n \mathbb{E}\left(w^{2}\right)=1$，</p>
<p>我们假设w服从均匀分布，区间为$[-R,R]$,均匀分布的均值为0，方差$\mathbb{E}\left(w^{2}\right)=\frac{R^2}{3}$【均匀分布方差为$\frac{(b-a)^2}{12}$】。</p>
<p>代入得到<script type="math/tex">n\frac{R^2}{3}=1 \tag{15}</script></p>
<p>即<script type="math/tex">R=\frac{\sqrt{3}}{\sqrt{n}} \tag{16}</script></p>
<p>因此w符合的分布为$[-\frac{\sqrt{3}}{\sqrt{n}}, \frac{\sqrt{3}}{\sqrt{n}}]$，当矩阵是方阵时的Xavier-Glorot initialization分布；当矩阵行列不同时，Xavier-Glorot initialization分布为$\left[-\frac{\sqrt{6}}{\sqrt{m+n}}, \frac{\sqrt{6}}{\sqrt{m+n}}\right]$。</p>
<h1 id="LSTM提出"><a href="#LSTM提出" class="headerlink" title="LSTM提出"></a>LSTM提出</h1><p>关于RNN反向传播的一些评论：</p>
<ol>
<li>RNN模型在时间维度共享参数矩阵</li>
<li>权重更新的频率与梯度的准确性需要权衡，越少的更新次数，梯度准确性越高，但训练速度也下降了。【由反向传递时，使用上一时刻状态做近似导致】</li>
<li>梯度消失带来不稳定的梯度流；共享参数带来对最新更新的过度敏感</li>
<li>针对上述三点，进行错误传播过程的梯度截断是有必要的</li>
<li>传播梯度分量也是可以的</li>
</ol>
<p>LSTM缓解梯度消失的根本方法：<strong>write it down</strong>【将状态记录下来】，但是如果无限制的写入也会带来问题，因此升级为有选择的读写，这样带来了LSTM的三个关键机制：</p>
<ol>
<li>有选择的写入，写入关键信息</li>
<li>有选择的读取信息</li>
<li>有选择的遗忘信息</li>
</ol>
<p>我们可以通过门机制实现选择性，但要如何将这三个机制结合起来呢？</p>
<h2 id="LSTM原型"><a href="#LSTM原型" class="headerlink" title="LSTM原型"></a>LSTM原型</h2><p>首先，提出一个LSTM原型，本着先读取状态，再写入的原则，每一次更新状态的增量$\tilde{s}_{t}$，由$o_t$选择性读取上一个状态的内容，$i_t$为选择写入，$f_t$选择性遗忘上一状态的内容：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i} s_{t-1}+U_{i} x_{t}+b_{i}\right) \\ o_{t} &=\sigma\left(W_{o} s_{t-1}+U_{o} x_{t}+b_{o}\right) \\ f_{t} &=\sigma\left(W_{f} s_{t-1}+U_{f} x_{t}+b_{f}\right) \\ \tilde{s_{t}} &=\phi\left(W\left(o_{t} \odot s_{t-1}\right)+U x_{t}+b\right) \\ s_{t} &=f_{t} \odot s_{t-1}+i_{t} \odot \tilde{s}_{t} \end{aligned} \tag{17}</script><p><img src="https://pic1.zhimg.com/80/v2-a2898c8e0c7c45f763256ec8f60b8844_1440w.webp" alt=""></p>
<h2 id="三个起效果的改进版本"><a href="#三个起效果的改进版本" class="headerlink" title="三个起效果的改进版本"></a>三个起效果的改进版本</h2><p>按理说上述LSTM原型能够起效果，但事与愿违，<strong>选择性读取与选择性写入未能很好的协调，导致状态值非常大，紧接着门机制变得饱和。</strong>这种情况源于的$s_t$是无界的，会变得非常大从而导致门机制饱和，因此接下来的三个改进的生效版本均是约束$s_t$的大小，将其约束成有界。</p>
<h3 id="归一化LSTM原型"><a href="#归一化LSTM原型" class="headerlink" title="归一化LSTM原型"></a>归一化LSTM原型</h3><p>针对$s_t$进行正态归一化，$s_t = \frac{s_t-mean(s_t)}{\sqrt{Var(s_t) + 1}}$，也可以类似于层归一化等方式添加缩放与平移分量。</p>
<p>归一化后的$s_t$从无界成为有界。</p>
<h3 id="GRU：将写入与遗忘强绑定"><a href="#GRU：将写入与遗忘强绑定" class="headerlink" title="GRU：将写入与遗忘强绑定"></a>GRU：将写入与遗忘强绑定</h3><script type="math/tex; mode=display">s_{t}=\left(1-i_{t}\right) \odot s_{t-1}+i_{t} \odot \tilde{s}_{t} \tag{18}</script><p>GRU将写入门与遗忘门绑定起来，使之加和为1。将$s_t$变成$s_{t-1}$与$\tilde{s}_{t}$的element-wise加权平均，当两者均有界时，$s_t$也有界。</p>
<p>以下给出GRU的计算公式，与原理图：</p>
<script type="math/tex; mode=display">
\begin{aligned} r_{t} &=\sigma\left(W_{r} s_{t-1}+U_{r} x_{t}+b_{r}\right) \\ z_{t} &=\sigma\left(W_{z} s_{t-1}+U_{z} x_{t}+b_{z}\right) \\ \tilde{s_{t}} &=\phi\left(W\left(r_{t} \odot s_{t-1}\right)+U x_{t}+b\right) \\ s_{t} &=z_{t} \odot s_{t-1}+\left(1-z_{t}\right) \odot \tilde{s}_{t} \end{aligned} \tag{19}</script><p><img src="https://pic2.zhimg.com/80/v2-29885f66462e4f48db0aba5722798aa5_1440w.webp" alt=""></p>
<h3 id="伪LSTM：通过激活函数约束"><a href="#伪LSTM：通过激活函数约束" class="headerlink" title="伪LSTM：通过激活函数约束"></a>伪LSTM：通过激活函数约束</h3><p>通过激活函数，将$s_t$限制到激活函数的上界内。</p>
<p>只有在更新写入时，为了避免信息的变化，未使用激活函数约束。</p>
<p>以下给出伪LSTM的计算公式，与原理图：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i}\left(\phi\left(s_{t-1}\right)\right)+U_{i} x_{t}+b_{i}\right) \\ o_{t} &=\sigma\left(W_{o}\left(\phi\left(s_{t-1}\right)\right)+U_{o} x_{t}+b_{o}\right) \\ f_{t} &=\sigma\left(W_{f}\left(\phi\left(s_{t-1}\right)\right)+U_{f} x_{t}+b_{f}\right) \\ \tilde{s}_{t} &=\phi\left(W\left(o_{t} \odot \phi\left(s_{t-1}\right)\right)+U x_{t}+b\right) \\ s_{t} &=f_{t} \odot s_{t-1}+i_{t} \odot \tilde{s}_{t} \\ \mathbf{r n n}_{\text {out }} &=\phi\left(s_{t}\right) \end{aligned} \tag{20}</script><p><img src="https://pic2.zhimg.com/80/v2-62d798b3d85004c5ea63b44dbf9312d5_1440w.webp" alt=""></p>
<h2 id="LSTM提出-1"><a href="#LSTM提出-1" class="headerlink" title="LSTM提出"></a>LSTM提出</h2><p>LSTM与伪LSTM的几点关键区别如下：</p>
<ol>
<li><p>LSTM是先写后读，因此添加了一个“影子”状态，Hochreiter and Schmidhuber等人认为状态s与剩余的RNN cell是独立的。</p>
</li>
<li><p>使用门控影子状态$h_{t-1}=o_{t-1}\odot \phi(c_{t-1})$计算门结构，替换激活后的$\phi(c_{t-1})$。这样隐藏状态均是当前时间下的信息，与读取信息时$(o_t \odot s_{t-1})$利用前一时刻信息不同。</p>
</li>
<li><p>使用门控影子状态作为RNN cell的输出$h_{t}=o_{t}\odot \phi(c_{t})$，替代$\phi(c_t)$。</p>
</li>
</ol>
<p>这样一来，LSTM的输入就是上一时刻的$c_{t-1},h_{t-1}$，输出为$c_t,h_t$。</p>
<h3 id="基础LSTM"><a href="#基础LSTM" class="headerlink" title="基础LSTM"></a>基础LSTM</h3><p>基础LSTM单元的公式与原理图如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i} h_{t-1}+U_{i} x_{t}+b_{i}\right) \\ o_{t} &=\sigma\left(W_{o} h_{t-1}+U_{o} x_{t}+b_{o}\right) \\ f_{t} &=\sigma\left(W_{f} h_{t-1}+U_{f} x_{t}+b_{f}\right) \\ \tilde{c}_{t} &=\phi\left(W h_{t-1}+U x_{t}+b\right) \\ c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t} \\ h_{t} &=o_{t} \odot \phi\left(c_{t}\right) \\ \mathrm{rnn}_{\text {out }} &=h_{t} \end{aligned} \tag{21}</script><p><img src="https://pic2.zhimg.com/80/v2-74080caa295667886f47324200e7bc39_1440w.webp" alt=""></p>
<h3 id="The-LSTM-with-peepholes"><a href="#The-LSTM-with-peepholes" class="headerlink" title="The LSTM with peepholes"></a>The LSTM with peepholes</h3><p>利用前一状态$c_{t-1}$来进行门控机制的计算，但输出利用实时信息$c_t$。</p>
<p>计算公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned} i_{t} &=\sigma\left(W_{i} h_{t-1}+U_{i} x_{t}+P_{i} c_{t-1}+b_{i}\right) \\ f_{t} &=\sigma\left(W_{f} h_{t-1}+U_{f} x_{t}+P_{f} c_{t-1}+b_{f}\right) \\ \tilde{c}_{t} &=\phi\left(W h_{t-1}+U x_{t}+b\right) \\ c_{t} &=f_{t} \odot c_{t-1}+i_{t} \odot \tilde{c}_{t} \\ o_{t} &=\sigma\left(W_{o} h_{t-1}+U_{o} x_{t}+P_{o} c_{t}+b_{o}\right) \\ h_{t} &=o_{t} \odot \phi\left(c_{t}\right) \\ \operatorname{rnn}_{\text {out }} &=h_{t} \end{aligned} \tag{22}</script><h1 id="LSTM如何解决梯度消失"><a href="#LSTM如何解决梯度消失" class="headerlink" title="LSTM如何解决梯度消失"></a>LSTM如何解决梯度消失</h1><p>从上述分析可以得到，梯度消失中最大原因是需要计算$\frac{\partial h_{t}}{\partial h_{s}}$，如果这个值不随着层数的增加，趋于0或者无穷大，那么就能够捕获到长距离依赖信息。</p>
<p>LSTM将状态与其他部分分开，状态更新部分变成:</p>
<script type="math/tex; mode=display">c_t = f_t \odot c_{t-1} + i_t * \tilde c_t = c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tanh \left(W_{c}\left[h_{t-1}, x_{t}\right]+b_{c}\right) \tag{23}</script><p>针对状态进行求导，同时$c_t$与$c_{t-1},\tilde c_{t-1},f_t,i_t$有关，因此进行全微分求导</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial C_{t}}{\partial C_{t-1}} &= \frac{\partial C_{t}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial \tilde C_{t}} * \frac{\partial \tilde C_{t}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial i_{t}} * \frac{\partial i_{t}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial f_{t}} * \frac{\partial f_{t}}{\partial C_{t-1}} 
\\ &=\frac{\partial C_{t}}{\partial C_{t-1}} +  \frac{\partial C_{t}}{\partial \tilde C_{t}} * \frac{\partial \tilde C_{t}}{\partial h_{t-1}} * \frac{\partial h_{t-1}}{\partial C_{t-1}} 
\\ &+ \frac{\partial C_{t}}{\partial i_{t}} * \frac{\partial i_{t}}{\partial h_{t-1}} * \frac{\partial h_{t-1}}{\partial C_{t-1}} + \frac{\partial C_{t}}{\partial f_{t}} * \frac{\partial f_{t}}{\partial h_{t-1}} * \frac{\partial h_{t-1}}{\partial C_{t-1}}
\\ &= f_t + i_t * tanh^{'}(*)W_c * o_{t-1} tanh^{'}(C_{t-1}) 
\\ &+ \tilde C_t * \sigma^{'}(*)W_i * o_{t-1} tanh^{'}(C_{t-1}) + C_{t-1} * \sigma^{'}(*)W_f * o_{t-1} tanh^{'}(C_{t-1})
\end{aligned}
\tag{24}</script><p>从上式可以得到，$\frac{\partial C_{t}}{\partial C_{t-1}}$成为上述4部分的加和，在连乘的任意时刻可能是$[0, +\infty)$的范围，并不会一直趋于0，或者$\infty$。同时$f_t,i_t,o_{t-1},\tilde C_t$都是网络学习的值，也就是说由网络自己学习哪些梯度保留，哪些梯度剔除。</p>
<p>在这些机制的帮助下，LSTM很好的<strong>缓解了</strong>梯度消失问题。</p>
<h1 id="LSTM延伸"><a href="#LSTM延伸" class="headerlink" title="LSTM延伸"></a>LSTM延伸</h1><p>Highway网络和residual网络同样包含了LSTM最基本的思想：与原先一层网络输出$x_{t + 1} = Net(x_t)$的计算方式相比，计算增量$x_{t + 1} = x_t + \Delta x_{t + 1}$。</p>
<p>因此这两种方式，同样会遇到LSTM的问题：读写的不协调。</p>
<p>关于这两者的介绍，再后续有时间展开进行介绍。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
<p><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html">https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html</a></p>
<p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/109519044">https://zhuanlan.zhihu.com/p/109519044</a></p>
<p><a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1088177386838749184">https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=1088177386838749184</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch多机多卡的多种打开方式</title>
    <url>/2022/09/04/2022-09-04-Pytorch%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E7%9A%84%E5%A4%9A%E7%A7%8D%E6%89%93%E5%BC%80%E6%96%B9%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在上一篇介绍<a href="https://jmxgodlz.xyz/2021/09/12/2021-09-12-Pytorch多卡训练原理/#more">多卡训练原理</a>的基础上，本篇主要介绍Pytorch多机多卡的几种实现方式：<strong>DDP、multiprocessing、Accelerate</strong>。</p>
<p>在介绍具体实现之前，torch.distributed 涉及的分布式概念如下：</p>
<ul>
<li><strong>group：</strong>进程组，通常一个job只有一个组，即一个world，使用多机时，一个group产生了多个world。</li>
<li><strong>world_size：</strong>一个job的全局进程数量</li>
<li><strong>rank：</strong>进程的序号，一般设置rank=0的主机为master节点。</li>
<li><strong>local_rank：</strong>进程内部的GPU序号。</li>
</ul>
<p>比如，有两台8卡机器，这时具有一个group，2个world，每个world_size为8，第一个主机rank=0，显卡编号依次为0,…,7，第二个主机rank=1，显卡编号依次为0,…,7。</p>
<p>在多机多卡的分布式训练过程中，为每个进程的模型、数据配置好这些参数至关重要。</p>
<span id="more"></span>
<h2 id="DDP"><a href="#DDP" class="headerlink" title="DDP"></a>DDP</h2><p>Pytorch分布式执行流程如下：</p>
<ol>
<li><strong>init_process_group</strong> 初始化进程组，同时初始化 distributed 包。</li>
<li>创建分布式模型<strong>model = DDP(model)</strong></li>
<li>创建分布式数据采样的<strong>datasampler</strong></li>
<li>利用<strong>torch.distributed.launch</strong>控制进程训练</li>
<li><strong>destory_process_group</strong>销毁进程组</li>
</ol>
<h3 id="进程组初始化"><a href="#进程组初始化" class="headerlink" title="进程组初始化"></a>进程组初始化</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">init_process_group(backend, </span><br><span class="line">                   init_method=None, </span><br><span class="line">                   timeout=datetime.timedelta(0, 1800), </span><br><span class="line">                   world_size=-1, </span><br><span class="line">                   rank=-1, </span><br><span class="line">                   store=None)</span><br></pre></td></tr></table></figure>
<h4 id="TCP初始化"><a href="#TCP初始化" class="headerlink" title="TCP初始化"></a>TCP初始化</h4><p>使用<strong>TCP初始化</strong>时，需要指定下列<strong>参数</strong>：</p>
<ol>
<li><code>rank</code> 为当前进程的进程号</li>
<li><code>word_size</code> 为当前 <code>job</code> 的总进程数</li>
<li><code>init_method</code> 内指定 <code>tcp</code> 模式，且所有进程的 <code>ip:port</code> 必须一致，设定为主进程的 <code>ip:port</code></li>
</ol>
<p>初始化时，<strong>需要注意下列事项：</strong></p>
<ol>
<li><p>在 <code>rank==0</code> 的进程内保存参数,一般是rank0主节点来分发广播梯度。</p>
</li>
<li><p>若程序内未根据 <code>rank</code> 设定当前进程使用的 <code>GPUs</code>，则默认使用全部 <code>GPU</code>，且以数据并行的方式使用。</p>
</li>
<li><p>每条命令表示一个进程，若已开启的进程未达到 <code>word_size</code> 的数量，则所有进程会一直等待。</p>
</li>
<li><p>每台主机上可以开启多个进程。但是，若未为每个进程分配合适的 <code>GPU</code>，则同机不同进程可能会共用 <code>GPU</code>，应该坚决避免这种情况，容易爆显存。</p>
</li>
<li><p>使用 <code>gloo</code> 后端进行 <code>GPU</code> 训练时，会报错。</p>
</li>
</ol>
<p>参考代码如下，<strong>需要在args里面添加指定的参数：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line">import torch.utils.data.distributed</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">parser = argparse.ArgumentParser(description=&#x27;PyTorch distributed training on cifar-10&#x27;)</span><br><span class="line">parser.add_argument(&#x27;--rank&#x27;, default=0,</span><br><span class="line">                    help=&#x27;rank of current process&#x27;)</span><br><span class="line">parser.add_argument(&#x27;--word_size&#x27;, default=2,</span><br><span class="line">                    help=&quot;word size&quot;)</span><br><span class="line">parser.add_argument(&#x27;--init_method&#x27;, default=&#x27;tcp://127.0.0.1:23456&#x27;,</span><br><span class="line">                    help=&quot;init-method&quot;)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">dist.init_process_group(backend=&#x27;nccl&#x27;, init_method=args.init_method, rank=args.rank, world_size=args.word_size)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True, download=download, transform=transform)</span><br><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">net = Net()</span><br><span class="line">net = net.cuda()</span><br><span class="line">net = torch.nn.parallel.DistributedDataParallel(net)</span><br></pre></td></tr></table></figure>
<p><strong>执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Node 1 : ip 192.168.1.201  port : 12345</span><br><span class="line">python tcp_init.py --init_method tcp://192.168.1.201:12345 --rank 0 --word_size 3</span><br><span class="line"></span><br><span class="line"># Node 2 : </span><br><span class="line">python tcp_init.py --init_method tcp://192.168.1.201:12345 --rank 1 --word_size 3</span><br><span class="line"></span><br><span class="line"># Node 3 : </span><br><span class="line">python tcp_init.py --init_method tcp://192.168.1.201:12345 --rank 2 --word_size 3</span><br></pre></td></tr></table></figure>
<h4 id="ENV初始化"><a href="#ENV初始化" class="headerlink" title="ENV初始化"></a>ENV初始化</h4><p>在ENV初始化方式中，init中无需指定参数，主要从机器的环境变量中获取参数。</p>
<ol>
<li><p>该初始化中需要设定local_rank参数，确定单机进程的序号。</p>
</li>
<li><p>然后，通过<strong>torch.distributed.launch</strong>设定nnodes节点数，node_rank当前主机进程序号，nproc_per_node每个节点的进程数量，master_addr主节点地址，master_port主节点端口，在环境变量中获取这些参数。</p>
</li>
</ol>
<p><strong>注意事项如下：</strong></p>
<ol>
<li><p>使用 <code>torch.distributed.launch</code> 工具时，将会为当前主机创建 <code>nproc_per_node</code> 个进程，每个进程独立执行训练脚本。同时，它还会为每个进程分配一个 <code>local_rank</code> 参数，表示当前进程在当前主机上的编号。例如：<code>rank=2, local_rank=0</code> 表示第 <code>3</code> 个节点上的第 <code>1</code> 个进程。</p>
</li>
<li><p>在 <code>rank==0</code> 的进程内保存参数。</p>
</li>
<li><p><code>Env</code> 方式中，在 <code>init_process_group</code> 中，无需指定任何参数</p>
</li>
<li><p>合理利用 <code>local_rank</code> 参数，来合理分配本地的 <code>GPU</code> 资源</p>
</li>
<li><p>每条命令表示一个进程。若已开启的进程未达到 <code>word_size</code> 的数量，则所有进程会一直等待。</p>
</li>
</ol>
<p><strong>参考代码如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line">import torch.utils.data.distributed</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">import argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"># 注意这个参数，必须要以这种形式指定，即使代码中不使用。因为 launch 工具默认传递该参数</span><br><span class="line">parser.add_argument(&quot;--local_rank&quot;, type=int)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">dist.init_process_group(backend=&#x27;nccl&#x27;, init_method=&#x27;env://&#x27;)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True, download=download, transform=transform)</span><br><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line"># 根据 local_rank，配置当前进程使用的 GPU</span><br><span class="line">net = Net()</span><br><span class="line">device = torch.device(&#x27;cuda&#x27;, args.local_rank)</span><br><span class="line">net = net.to(device)</span><br><span class="line">net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.local_rank], output_device=args.local_rank)</span><br></pre></td></tr></table></figure>
<p><strong>执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=2 --nnodes=3 --node_rank=0 --master_addr=&quot;192.168.1.201&quot; --master_port=23456 env_init.py</span><br><span class="line"></span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=2 --nnodes=3 --node_rank=1 --master_addr=&quot;192.168.1.201&quot; --master_port=23456 env_init.py</span><br><span class="line"></span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=2 --nnodes=3 --node_rank=2 --master_addr=&quot;192.168.1.201&quot; --master_port=23456 env_init.py</span><br></pre></td></tr></table></figure>
<h4 id="共享文件系统初始化"><a href="#共享文件系统初始化" class="headerlink" title="共享文件系统初始化"></a>共享文件系统初始化</h4><p>使用<strong>共享文件系统初始化</strong>时，与<strong>TCP初始化</strong>类似，需要指定下列参数：</p>
<ol>
<li><code>rank</code> 为当前进程的进程号</li>
<li><code>word_size</code> 为当前 <code>job</code> 的总进程数</li>
<li><code>init_method</code> 内指定 <code>文件系统</code> 模式，以 <code>file://</code> 为前缀，表示文件系统各式初始化。<code>/xxx</code> 表示共享的文件，各个进程在共享文件系统中通过该文件进行同步或异步。因此，所有进程必须对该文件具有读写权限。</li>
</ol>
<p><strong>参考代码如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mport torch.distributed as dist</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">parser = argparse.ArgumentParser(description=&#x27;PyTorch distributed training on cifar-10&#x27;)</span><br><span class="line">parser.add_argument(&#x27;--rank&#x27;, default=0,</span><br><span class="line">                    help=&#x27;rank of current process&#x27;)</span><br><span class="line">parser.add_argument(&#x27;--word_size&#x27;, default=2,</span><br><span class="line">                    help=&quot;word size&quot;)</span><br><span class="line">parser.add_argument(&#x27;--init_method&#x27;, default=&#x27;file:///mnt/nfs/sharedfile&#x27;,</span><br><span class="line">                    help=&quot;init-method&quot;)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"># rank should always be specified</span><br><span class="line">dist.init_process_group(backend, init_method=&#x27;file:///mnt/nfs/sharedfile&#x27;,</span><br><span class="line">                        world_size=4, rank=args.rank)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True, download=download, transform=transform)</span><br><span class="line">train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)</span><br><span class="line"></span><br><span class="line"># ......</span><br><span class="line"># 根据 local_rank，配置当前进程使用的 GPU</span><br><span class="line">net = Net()</span><br><span class="line">device = torch.device(&#x27;cuda&#x27;, args.local_rank)</span><br><span class="line">net = net.to(device)</span><br><span class="line">net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.local_rank], output_device=args.local_rank</span><br></pre></td></tr></table></figure>
<p><strong>执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Node1:</span><br><span class="line">python mnsit.py --init-method file://PathToShareFile/MultiNode --rank 0 --world_size 2</span><br><span class="line">Node2:</span><br><span class="line">python mnsit.py --init-method file://PathToShareFile/MultiNode --rank 1 --world_size 2</span><br></pre></td></tr></table></figure>
<h3 id="DistributedDataParallel"><a href="#DistributedDataParallel" class="headerlink" title="DistributedDataParallel"></a>DistributedDataParallel</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.nn.parallel.DistributedDataParallel(module, </span><br><span class="line">                                          device_ids=None, </span><br><span class="line">                                          output_device=None, </span><br><span class="line">                                          dim=0, </span><br><span class="line">                                          broadcast_buffers=True, </span><br><span class="line">                                          process_group=None, </span><br><span class="line">                                          bucket_cap_mb=25, </span><br><span class="line">                                          find_unused_parameters=False, </span><br><span class="line">                                          check_reduction=False)</span><br></pre></td></tr></table></figure>
<p>将给定的 <code>module</code> 进行分布式封装， 其将输入在 <code>batch</code> 维度上进行划分，并分配到指定的 <code>devices</code> 上。</p>
<p><code>module</code> 会被复制到每台机器的每个 <code>进程</code> 上，每一个模型的副本处理输入的一部分。</p>
<p>在反向传播阶段，每个机器的每个 <code>进程</code> 上的梯度进行汇总并求平均。与 <code>DataParallel</code> 类似，<code>batch size</code> 应该大于 <code>GPU</code> 总数。</p>
<p><strong>主要参数介绍：</strong></p>
<ol>
<li><p><strong>module：</strong>将完整的model封装为分布式module,后续需要调用model的方法时，可以采用module.model.xxx</p>
</li>
<li><p><strong>device_ids：</strong>需要并行的设备，在数据并行的情况下，表示模型副本拷贝到哪些GPU上；在模型并行的情况下，表示模型分散在哪些GPU上。</p>
</li>
<li><p><strong>output_device：</strong>输出结果到哪个GPU上。</p>
</li>
</ol>
<p><strong>注意事项如下：</strong></p>
<ol>
<li><p>要使用该 <code>class</code>，需要先对 <code>torch.distributed</code> 进行初进程组始化，可以通过 <code>torch.distributed.init_process_group()</code> 实现。</p>
</li>
<li><p>该 <code>module</code> 仅在 <strong>gloo</strong>和 <strong>nccl</strong>后端上可用。</p>
</li>
</ol>
<h3 id="DistributedSampler"><a href="#DistributedSampler" class="headerlink" title="DistributedSampler"></a>DistributedSampler</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.utils.data.distributed.DistributedSampler(dataset, </span><br><span class="line">                                                num_replicas=None, </span><br><span class="line">                                                rank=None)</span><br></pre></td></tr></table></figure>
<p><strong>主要参数介绍：</strong></p>
<ol>
<li><p><strong>dataset：</strong>采样的数据集</p>
</li>
<li><p><strong>num_replicas：</strong>参与的总进程数</p>
</li>
<li><p><strong>rank：</strong>当前机器的rank</p>
</li>
</ol>
<p><strong>DistributedSampler</strong>将数据集采样为num_replicas份，不同机器根据自己的rank取数据集的子集。</p>
<p><strong>TIPS</strong>：在 <code>DataParallel</code> 中，<code>batch size</code> 设置必须为单卡的 <code>n</code> 倍，但是在 <code>DistributedDataParallel</code> 内，<code>batch size</code> 设置于单卡一样即可。</p>
<p><strong>参考代码如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 分布式训练示例</span><br><span class="line">from torch.utils.data import Dataset, DataLoader</span><br><span class="line">from torch.utils.data.distributed import DistributedSampler</span><br><span class="line">from torch.nn.parallel import DistributedDataParallel</span><br><span class="line"></span><br><span class="line">dataset = your_dataset()</span><br><span class="line">datasampler = DistributedSampler(dataset)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size_per_gpu, sampler=datasampler)</span><br><span class="line">model = your_model()</span><br><span class="line">model = DistributedDataPrallel(model, device_ids=[local_rank], output_device=local_rank)</span><br></pre></td></tr></table></figure>
<h3 id="torch-distributed-launch"><a href="#torch-distributed-launch" class="headerlink" title="torch.distributed.launch"></a>torch.distributed.launch</h3><p>DDP通过torch.distributed.launch辅助实现进程控制。</p>
<p><strong>torch.distributed.launch</strong>传入的参数如下：</p>
<ul>
<li><strong>training_script：</strong>执行任务脚本路径</li>
<li><strong>—nnodes：</strong>节点数，即分布式机器数量</li>
<li><strong>—node_rank：</strong>当前机器的rank序号</li>
<li><strong>—nproc_per_node：</strong>每个节点开设的进程数量，最好设置为每个机器GPU数量，使每个GPU在一个进程中</li>
<li><strong>—master_addr：</strong>master 节点（rank 为 0）的地址</li>
<li><strong>—master_port：</strong>master 节点（rank 为 0）的端口</li>
</ul>
<p><strong>单机多卡的执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script)</span><br></pre></td></tr></table></figure>
<p><strong>多机多卡执行脚本如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Node1:</span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes= NUM_MACHINES_YOU_HAVE --node_rank=0 --master_addr=&quot;192.168.1.1&quot; --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script)</span><br><span class="line">...</span><br><span class="line">NodeN:</span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE --nnodes= NUM_MACHINES_YOU_HAVE --node_rank=N-1 --master_addr=&quot;192.168.1.1&quot; --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of your training script)</span><br></pre></td></tr></table></figure>
<h2 id="torch-multiprocessing"><a href="#torch-multiprocessing" class="headerlink" title="torch.multiprocessing"></a>torch.multiprocessing</h2><p>通过 <strong>torch.multiprocessing</strong>手动控制进程，替代 <strong>torch.distributed.launch</strong>的进程控制工作。</p>
<p>涉及的主要接口为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method=&#x27;spawn&#x27;):</span><br><span class="line">    r&quot;&quot;&quot;Spawns ``nprocs`` processes that run ``fn`` with ``args``.</span><br><span class="line"></span><br><span class="line">    If one of the processes exits with a non-zero exit status, the</span><br><span class="line">    remaining processes are killed and an exception is raised with the</span><br><span class="line">    cause of termination. In the case an exception was caught in the</span><br><span class="line">    child process, it is forwarded and its traceback is included in</span><br><span class="line">    the exception raised in the parent process.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        fn (function): Function is called as the entrypoint of the</span><br><span class="line">            spawned process. This function must be defined at the top</span><br><span class="line">            level of a module so it can be pickled and spawned. This</span><br><span class="line">            is a requirement imposed by multiprocessing.</span><br><span class="line"></span><br><span class="line">            The function is called as ``fn(i, *args)``, where ``i`` is</span><br><span class="line">            the process index and ``args`` is the passed through tuple</span><br><span class="line">            of arguments.</span><br><span class="line"></span><br><span class="line">        args (tuple): Arguments passed to ``fn``.</span><br><span class="line">        nprocs (int): Number of processes to spawn.</span><br><span class="line">        join (bool): Perform a blocking join on all processes.</span><br><span class="line">        daemon (bool): The spawned processes&#x27; daemon flag. If set to True,</span><br><span class="line">                       daemonic processes will be created.</span><br><span class="line">        start_method (string): (deprecated) this method will always use ``spawn``</span><br><span class="line">                               as the start method. To use a different start method</span><br><span class="line">                               use ``start_processes()``.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">        None if ``join`` is ``True``,</span><br><span class="line">        :class:`~ProcessContext` if ``join`` is ``False``</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
<p><strong>主要参数介绍如下：</strong></p>
<ul>
<li>fn:处理的主函数</li>
<li>args:传递给主函数的参数，主函数第一个参数默认传入进程index</li>
<li>nprocs:开启的进程数量</li>
</ul>
<p>结合下列代码介绍torch.multiprocessing多机多卡的使用：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def setup(rank, world_size):</span><br><span class="line">    # dist.init_process_group(&quot;gloo&quot;, rank=rank, world_size=world_size)</span><br><span class="line">    print(&quot;world size:&quot;, world_size, &quot; rank:&quot;, rank)</span><br><span class="line">    print(os.environ[&#x27;MASTER_ADDR&#x27;])</span><br><span class="line">    print(os.environ[&#x27;MASTER_PORT&#x27;])</span><br><span class="line">    print(os.environ[&#x27;RANK&#x27;])</span><br><span class="line">    print(os.environ[&#x27;WORLD_SIZE&#x27;])</span><br><span class="line">    dist.init_process_group(&quot;nccl&quot;, rank=rank, world_size=world_size)</span><br><span class="line"></span><br><span class="line">def cleanup():</span><br><span class="line">    dist.destroy_process_group()</span><br><span class="line"></span><br><span class="line">def main(local_rank, nnodes, args):</span><br><span class="line">    rank = int(os.environ[&#x27;RANK&#x27;]) * nnodes + local_rank</span><br><span class="line">    world_size = nnodes * int(os.environ[&#x27;WORLD_SIZE&#x27;])</span><br><span class="line"></span><br><span class="line">    print(&quot;world size:&quot;, world_size, &quot; rank:&quot;, rank)</span><br><span class="line">    setup(rank, world_size)</span><br><span class="line">    ……</span><br><span class="line">    # If passed along, set the training seed now.</span><br><span class="line">    if args.seed is not None:</span><br><span class="line">        set_seed(args.seed)</span><br><span class="line">    model = torch.nn.parallel.DistributedDataParallel(model.to(local_rank), device_ids=[local_rank])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    args = parse_args()</span><br><span class="line">    world_size = torch.cuda.device_count()</span><br><span class="line">    print(&#x27;&#123;&#125;:&#123;&#125;&#x27;.format(world_size, &#x27;---&#x27; * 100))</span><br><span class="line">    mp.spawn(main, args=(world_size, args), nprocs=world_size, join=True)</span><br></pre></td></tr></table></figure>
<p><strong>代码流程的解释如下：</strong></p>
<ol>
<li>根据<strong>torch.cuda.device_count()</strong>获取单机的显卡数量，决定开启的进程数，即一个world的world_size</li>
<li><strong>mp.spawn</strong>开启多进程</li>
<li>单机的进程index即为local_rank，nnodes代表单机显卡数量，os.environ[‘RANK’]获取机器的rank值，通过rank<em>nnodes + local_rank 计算全局训练的索引，nnodes </em> int(os.environ[‘WORLD_SIZE’]) 计算全局训练的进程数量</li>
<li>根据计算的全局索引，全局数量 初始化进程通信</li>
<li><strong>model.to(local_rank)</strong>将模型放置于本地单机的显卡上</li>
</ol>
<h2 id="Accelerate"><a href="#Accelerate" class="headerlink" title="Accelerate"></a>Accelerate</h2><p>Hugging Face发布PyTorch新库「Accelerate」：适用于多GPU、TPU、混合精度训练。「Accelerate」提供了一个简单的 API，将与多 GPU 、 TPU 、 fp16 相关的样板代码抽离了出来，保持其余代码不变。PyTorch 用户无须使用不便控制和调整的抽象类或编写、维护样板代码，就可以直接上手多 GPU 或 TPU。</p>
<p>项目地址：<a href="https://github.com/huggingface/accelerate">https://github.com/huggingface/accelerate</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">  import torch</span><br><span class="line">  import torch.nn.functional as F</span><br><span class="line">  from datasets import load_dataset</span><br><span class="line">+ from accelerate import Accelerator</span><br><span class="line"></span><br><span class="line">- device = &#x27;cpu&#x27;</span><br><span class="line">+ accelerator = Accelerator()</span><br><span class="line"></span><br><span class="line">- model = torch.nn.Transformer().to(device)</span><br><span class="line">+ model = torch.nn.Transformer()</span><br><span class="line">  optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line">  dataset = load_dataset(&#x27;my_dataset&#x27;)</span><br><span class="line">  data = torch.utils.data.DataLoader(dataset, shuffle=True)</span><br><span class="line"></span><br><span class="line">+ model, optimizer, data = accelerator.prepare(model, optimizer, data)</span><br><span class="line"></span><br><span class="line">  model.train()</span><br><span class="line">  for epoch in range(10):</span><br><span class="line">      for source, targets in data:</span><br><span class="line">-         source = source.to(device)</span><br><span class="line">-         targets = targets.to(device)</span><br><span class="line"></span><br><span class="line">          optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">          output = model(source)</span><br><span class="line">          loss = F.cross_entropy(output, targets)</span><br><span class="line"></span><br><span class="line">-         loss.backward()</span><br><span class="line">+         accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">          optimizer.step()</span><br></pre></td></tr></table></figure>
<p><strong>模型保存加载：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 模型保存</span><br><span class="line">accelerator.wait_for_everyone()</span><br><span class="line">unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line">accelerator.save(unwrapped_model.state_dict(), path)</span><br><span class="line"></span><br><span class="line"># 模型加载</span><br><span class="line">unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line">unwrapped_model.load_state_dict(torch.load(path))</span><br></pre></td></tr></table></figure>
<p>具体代码可以参考huggingface的transformer代码库</p>
<p>项目地址：<a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch">https://github.com/huggingface/transformers/tree/main/examples/pytorch</a></p>
<p><strong>TIPS：</strong>在PAI平台上尝试失败，猜测使用该库的时候，需要先执行命令配置环境参数【多卡…】</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://zhuanlan.zhihu.com/p/462453622">https://zhuanlan.zhihu.com/p/462453622</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/98535650">https://zhuanlan.zhihu.com/p/98535650</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/76638962">https://zhuanlan.zhihu.com/p/76638962</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>【律师函警告】-ChatGPT以鸡你太美主题写小说.md</title>
    <url>/2022/12/07/2022-12-07-%E3%80%90%E5%BE%8B%E5%B8%88%E5%87%BD%E8%AD%A6%E5%91%8A%E3%80%91-ChatGPT%E4%BB%A5%E9%B8%A1%E4%BD%A0%E5%A4%AA%E7%BE%8E%E4%B8%BB%E9%A2%98%E5%86%99%E5%B0%8F%E8%AF%B4/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>趁着ChatGPT大火，以鸡你太美为主题生成小说，大家细品了哈～</p>
<p>鸡你太美是一个充满活力和激情的年轻人，他喜欢唱歌、跳舞、rap和玩篮球。他有一个梦想，就是成为一名职业篮球运动员。</p>
<p>某天，他被他的朋友邀请参加了一个有着真正挑战性的比赛——一场篮球比赛。鸡你太美满怀信心地参加了比赛，他在比赛中表现出了超出预期的精彩表现。他的扣篮、三分球和防守都让他的队友们和对手们都刮目相看。</p>
<p>在比赛的最后关键时刻，鸡你太美再次展现了他的技术和实力，他跳得更高、投得更准，最终帮助他的队伍获得了胜利。</p>
<p>在比赛结束后，鸡你太美收到了许多赞美和鼓励，他感到非常开心和满足。他知道，这场比赛不仅是他人生中一个重要的里程碑，也是他梦想成真的开始。</p>
<p>从那以后，鸡你太美开始了他的职业篮球生涯，他的才华和实力得到了越来越多的认可和赏识。他成为了一名优秀的篮球运动员，并在他的人生中写下了一段属于自己的传奇。</p>
<p><img src="https://pic1.zhimg.com/v2-579b5a59aac1deb55adaa0450698bff9_1440w.jpg?source=172ae18b" alt=""></p>
<span id="more"></span>
<h1 id="ChatGPT浅玩一下"><a href="#ChatGPT浅玩一下" class="headerlink" title="ChatGPT浅玩一下"></a>ChatGPT浅玩一下</h1><h2 id="剧本生成"><a href="#剧本生成" class="headerlink" title="剧本生成"></a>剧本生成</h2><p><img src="https://pic2.zhimg.com/80/v2-9d851b3309d806fbaa6a09bd93bf1871_1440w.webp" alt=""></p>
<h2 id="知识问答"><a href="#知识问答" class="headerlink" title="知识问答"></a>知识问答</h2><p><img src="https://pic1.zhimg.com/80/v2-16b54e78250052a09f2cce16b71baa34_1440w.webp" alt=""></p>
<p><img src="https://pic4.zhimg.com/80/v2-8b456197ae73f8aae4201d902b036d3b_1440w.webp" alt=""></p>
<h2 id="写诗"><a href="#写诗" class="headerlink" title="写诗"></a>写诗</h2><p><img src="https://pic3.zhimg.com/80/v2-9664253913b78828ee4508c3df1aad8e_1440w.webp" alt=""></p>
<h2 id="小说创作"><a href="#小说创作" class="headerlink" title="小说创作"></a>小说创作</h2><p><img src="https://pic2.zhimg.com/80/v2-81be53cae055ab16bd0278227e0ba381_1440w.webp" alt=""></p>
<h2 id="写代码"><a href="#写代码" class="headerlink" title="写代码"></a>写代码</h2><p><img src="https://pic2.zhimg.com/80/v2-ecad55078136c99a56abf56234d32d95_1440w.webp" alt=""></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>个人尝试下来，觉得生成的东西已经有一定的感觉了，但是整体输出的错误率较高，经常报error哈～</p>
<p><img src="https://pic2.zhimg.com/80/v2-ecad55078136c99a56abf56234d32d95_1440w.webp" alt=""></p>
<p>能够写出hello world的代码，但是尝试DNN、快排这些又不可以了，还是期待GPT4的发布，大家可以自行体验一下哈</p>
<h1 id="ChatGPT注册"><a href="#ChatGPT注册" class="headerlink" title="ChatGPT注册"></a>ChatGPT注册</h1><p>注册网址： <a href="https://chat.openai.com/auth/login">https://chat.openai.com/auth/login</a> 【需要翻墙哈】</p>
<p>其中注册过程需要国外手机号来收验证码，这里推荐一个网址来收验证码：</p>
<p><a href="https://sms-activate.org/cn/">https://sms-activate.org/cn/</a></p>
<p>注册一个账号，然后输入：openai，然后选择阿根廷的电话（印度的被用烂了，接不到验证码）。如果阿根廷不好使，可以试试巴西、厄瓜多尔。<br>大胆的试，一个电话30分钟的测试时间，接不到验证码，你可以叉掉（必须手动取消，超时还是会扣款），不会扣款。</p>
<p><img src="https://pic2.zhimg.com/80/v2-c2eab434c541829bad9ec4c1f663367d_1440w.webp" alt=""></p>
<p>阿根廷电话号码需要 22.5 卢布。</p>
<p>人民币大约2.5。<br>添加购物车，需要提前充值。</p>
<p>充值方式有很多，可以选择简单的支付宝。</p>
<p>扫码支付即可。<br>添加购物车之后，购买之后，会有一个电话号码。</p>
<p><img src="https://pic1.zhimg.com/80/v2-c3ee43993a7f753a66f9585fc2f50a74_1440w.webp" alt=""></p>
<p>然后选择国家，填写电话号码<br>可以看到顺利接到验证码了（接收不到不扣钱，20分钟之内可删除。）</p>
<p>填写验证码之后，就能看到这个界面了。<br>随便选择一个。<br>看到这个界面代码注册成功。</p>
<p> <img src="https://pic3.zhimg.com/80/v2-fe6d5ed79eeb119ee886c20eb093e1fe_1440w.webp" alt=""></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://docs.qq.com/doc/DT3ZDbGtRUGRPWnpv?_t=1670427397386&amp;u=7311207678ec42f3ab99441a26d8c958">https://docs.qq.com/doc/DT3ZDbGtRUGRPWnpv?_t=1670427397386&amp;u=7311207678ec42f3ab99441a26d8c958</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>进击！BERT句向量表征.md</title>
    <url>/2022/11/05/2022-11-05-%E8%BF%9B%E5%87%BB%EF%BC%81BERT%E5%8F%A5%E5%90%91%E9%87%8F%E8%A1%A8%E5%BE%81/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><img src="https://pic1.zhimg.com/v2-48949d2c35c494d5d03b7de832151835_1440w.jpg?source=172ae18b" alt=""></p>
<p>BERT等语言模型在多数NLP任务中取得优异的表现，但如果直接取BERT输出的句向量作表征，取得的效果甚至还不如Glove词向量。<br>Bert-flow论文中指出，产生该现象的原因是BERT模型的各向异性过高，Transformer模型的输出中，高频词汇分布集中，低频词汇分布分散，整个向量空间类似于锥形结构。<br><img src="https://pic1.zhimg.com/80/v2-8b4df6fe6c7491b6d435dd94c0729e00_1440w.jpg" alt=""><br><span id="more"></span><br>余弦相似度使用的前提是向量空间在标准正交基下，而BERT输出的句向量很明显 不符合该条件，因此直接使用BERT输出的句向量计算余弦相似度，效果表现很差。</p>
<p>针对该问题，BERT-flow、BERT-Whitening模型将输出的句向量映射到标准正交的高斯空间。SimCSE引入对比学习的方式， 在保证向量空间的uniformity情况下，也提高alignment。此后，多种对比学习方式层出不穷，其核心改进点在于正负例的生成方式，提高正负样本的难度，本文主要介绍ConSERT、SimCSE、ESimCSE、DiffCSE、PromptBert、SNCSE、EASE。</p>
<h1 id="Sentence-Bert（EMNLP-2019）"><a href="#Sentence-Bert（EMNLP-2019）" class="headerlink" title="Sentence-Bert（EMNLP 2019）"></a>Sentence-Bert（EMNLP 2019）</h1><p><strong>题目</strong>：Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a></p>
<p><strong>代码</strong>：<a href="https://github.com/UKPLab/sentence-transformers">https://github.com/UKPLab/sentence-transformers</a></p>
<h2 id="核心思路"><a href="#核心思路" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>解决文本匹配推理次数过多的问题，BERT的传统文本匹配任务是输入sentence1与sentence2 作一个分类任务，给定一个语料库查找最相似句子的时间复杂度高达O(n^2)。<br>直接使用BERT的embdeeing向量，存在各向异性问题，相似度计算效果甚至低于Glove词向量。</p>
<p><strong>解决方式</strong>：</p>
<p>采用孪生网络的双塔结构，这样推理的次数降低到了O(n)，计算相似度的点积复杂度还是O(n^2)</p>
<p><strong>其他</strong>：</p>
<p>论文比较了三种pooling方式：CLS、AVG、MAX。也设计了分类任务、回归任务、Triplet形式的任务。</p>
<p><img src="https://pic3.zhimg.com/80/v2-01aeeae898873dd7d7dfb418a927806a_1440w.webp" alt=""></p>
<p>SBERT通过图1的方式进行训练，微调学习BERT句向量的embedding，通过图2的方式完成推理，计算文本相似度。</p>
<h1 id="BERT-flow（EMNLP-2020）"><a href="#BERT-flow（EMNLP-2020）" class="headerlink" title="BERT-flow（EMNLP 2020）"></a>BERT-flow（EMNLP 2020）</h1><p><strong>题目</strong>：On the Sentence Embeddings from Pre-trained Language Models</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2011.05864.pdf">https://arxiv.org/pdf/2011.05864.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/bohanli/BERT-flow">https://github.com/bohanli/BERT-flow</a></p>
<h2 id="核心思路-1"><a href="#核心思路-1" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>指出BERT句向量存在的问题：各向异性，高频词汇分布集中</p>
<p><strong>解决方式</strong>：</p>
<p>引入flow流式模型，映射为同向性，将BERT句向量映射到标准高斯空间.</p>
<p><strong>计算公式</strong>如下：<br><img src="https://pic4.zhimg.com/80/v2-999025acb512c8f3b97c4bd4ec45065b_1440w.webp" alt=""></p>
<h1 id="BERT-whitening"><a href="#BERT-whitening" class="headerlink" title="BERT-whitening"></a>BERT-whitening</h1><p><strong>题目</strong>：Whitening Sentence Representations for Better Semantics and Faster Retrieval</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/abs/2103.15316">https://arxiv.org/abs/2103.15316</a></p>
<p><strong>代码</strong>：<a href="https://github.com/bojone/BERT-whitening">https://github.com/bojone/BERT-whitening</a></p>
<h2 id="核心思路-2"><a href="#核心思路-2" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：在句向量维度，通过一个白化的操作直接校正局向量的协方差矩阵。</p>
<p><strong>解决方式</strong>：</p>
<p>PCA白化，并且可以降维实现更好的效果。<br>将句向量进行标准化映射，即白化操作：</p>
<script type="math/tex; mode=display">\tilde{\boldsymbol{x}}_i=\left(\boldsymbol{x}_i-\boldsymbol{\mu}\right) W</script><p>均值与协方差矩阵计算方式如下：</p>
<script type="math/tex; mode=display">\boldsymbol{\mu}=\frac{1}{N} \sum_{i=1}^N \boldsymbol{x}_i</script><script type="math/tex; mode=display">\boldsymbol{\Sigma}=\boldsymbol{U} \boldsymbol{\Lambda} \boldsymbol{U}^{\top},即原向量矩阵的协方差矩阵SVD分解</script><script type="math/tex; mode=display">\boldsymbol{W}=\boldsymbol{U} \sqrt{\boldsymbol{\Lambda}^{-1}}</script><p><strong>代码如下</strong>：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def compute_kernel_bias(vecs):</span><br><span class="line">    &quot;&quot;&quot;计算kernel和bias</span><br><span class="line">    vecs.shape = [num_samples, embedding_size]，</span><br><span class="line">    最后的变换：y = (x + bias).dot(kernel)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    mu = vecs.mean(axis=0, keepdims=True)</span><br><span class="line">    cov = np.cov(vecs.T)</span><br><span class="line">    u, s, vh = np.linalg.svd(cov)</span><br><span class="line">    W = np.dot(u, np.diag(1 / np.sqrt(s)))</span><br><span class="line">    return W, -mu</span><br></pre></td></tr></table></figure></p>
<h1 id="ConSERT（ACL-2021）"><a href="#ConSERT（ACL-2021）" class="headerlink" title="ConSERT（ACL 2021）"></a>ConSERT（ACL 2021）</h1><p><strong>题目</strong>：ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/abs/2105.11741">https://arxiv.org/abs/2105.11741</a></p>
<p><strong>代码</strong>：<a href="https://github.com/yym6472/ConSERT">https://github.com/yym6472/ConSERT</a></p>
<h2 id="核心思路-3"><a href="#核心思路-3" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>解决BERT本身表征的语义塌缩问题。再来看对比学习，它是通过拉近相同样本的距离、拉远不同样本的距离，来刻画样本本身的表示，正好可以解决BERT表示的塌缩问题。</p>
<p><strong>解决方式</strong>：</p>
<p>NLP的数据增强方式很重要，使用了5种数据增强方式进行对比：Adversarial attack、shuffle、token cutoff、feature cutoff、dropout</p>
<p>损失函数：对比学习损失</p>
<script type="math/tex; mode=display">\mathcal{L}_{i, j}=-\log \frac{\exp \left(\operatorname{sim}\left(r_i, r_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbb{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(r_i, r_k\right) / \tau\right)}</script><p><img src="https://pic3.zhimg.com/80/v2-60b3b4f2d2753a786449186812c4271a_1440w.webp" alt=""></p>
<p><img src="https://pic4.zhimg.com/80/v2-9d0e06beb31ee2fcf5ff536c58d6eca3_1440w.webp" alt=""></p>
<h2 id="正例构建方式"><a href="#正例构建方式" class="headerlink" title="正例构建方式"></a>正例构建方式</h2><ul>
<li>shuffle：更换position id的顺序</li>
<li>token cutoff：在某个token维度把embedding置为0</li>
<li>feature cutoff：在embedding矩阵中，有768个维度，把某个维度的feature置为0</li>
<li>dropout：dropout</li>
<li>Adversarial attack：对抗扰动</li>
</ul>
<h2 id="负例构建方式"><a href="#负例构建方式" class="headerlink" title="负例构建方式"></a>负例构建方式</h2><p>同一batch内其他样本</p>
<h1 id="SimCSE（EMNLP-2021）"><a href="#SimCSE（EMNLP-2021）" class="headerlink" title="SimCSE（EMNLP 2021）"></a>SimCSE（EMNLP 2021）</h1><p><strong>题目</strong>：SimCSE: Simple Contrastive Learning of Sentence Embeddings</p>
<p><strong>地址</strong>：<a href="https://aclanthology.org/2021.emnlp-main.552.pdf">https://aclanthology.org/2021.emnlp-main.552.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/princeton-nlp/SimCSE">https://github.com/princeton-nlp/SimCSE</a></p>
<h2 id="核心思路-4"><a href="#核心思路-4" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>将对比学习方式引入句向量表征领域。</p>
<p><strong>解决方式</strong>：</p>
<p>引入对比损失的难点在于正样本的构建，该文提出dropout的正样本构建方式。</p>
<p><strong>无监督SimCSE</strong>：</p>
<p>Dropout构建正例样本，batch内其他样本作为负样本</p>
<p><img src="https://pic4.zhimg.com/80/v2-e0e1aab2f104c2e1df6f801d86509987_1440w.webp" alt=""></p>
<p><strong>有监督SimCSE</strong>：</p>
<p>句子蕴含任务中同一标签的作为正样本，同一batch的其他样本的是负样本</p>
<p><img src="https://pic2.zhimg.com/80/v2-330c834b6c87b47cce0e314deefb4ad1_1440w.webp" alt=""></p>
<h2 id="正例构建方式-1"><a href="#正例构建方式-1" class="headerlink" title="正例构建方式"></a>正例构建方式</h2><p>Dropout两次产生的数据</p>
<h2 id="负例构建方式-1"><a href="#负例构建方式-1" class="headerlink" title="负例构建方式"></a>负例构建方式</h2><p>同一batch内其他样本</p>
<p><img src="https://pic4.zhimg.com/80/v2-3ab1f8ef586853f48950c102e6c4b23f_1440w.webp" alt=""></p>
<h1 id="ESimCSE（COLING-2022）"><a href="#ESimCSE（COLING-2022）" class="headerlink" title="ESimCSE（COLING 2022）"></a>ESimCSE（COLING 2022）</h1><p><strong>题目</strong>：ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2109.04380.pdf">https://arxiv.org/pdf/2109.04380.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/caskcsg/sentemb/tree/main/ESimCSE">https://github.com/caskcsg/sentemb/tree/main/ESimCSE</a></p>
<h2 id="核心思路-5"><a href="#核心思路-5" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>解决SimCSE的两个问题：</p>
<ul>
<li>Dropout构建的正例均是相同长度的，会导致模型认为相同句子长度的句子更相似</li>
<li>SimCSE增加batchsize，引入更多负例，反而引起效果下降【猜测：更多的负例中，部分与正例接近质量不高】</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-941a073e5de189c9b4286d2e92739415_1440w.webp" alt=""><br><strong>核心改动点</strong>：</p>
<p><strong>正例生成方式</strong>：重复一定单词</p>
<p><strong>负例生成方式</strong>：动量序列，扩展负样本数量</p>
<h2 id="正例生成方式"><a href="#正例生成方式" class="headerlink" title="正例生成方式"></a>正例生成方式</h2><p>重复单词</p>
<h2 id="负例生成方式"><a href="#负例生成方式" class="headerlink" title="负例生成方式"></a>负例生成方式</h2><p>动量序列，扩展负样本数量</p>
<h1 id="DiffCSE（NAACL2022）"><a href="#DiffCSE（NAACL2022）" class="headerlink" title="DiffCSE（NAACL2022）"></a>DiffCSE（NAACL2022）</h1><p><strong>题目</strong>：DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2204.10298.pdf">https://arxiv.org/pdf/2204.10298.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/voidism/DiffCSE">https://github.com/voidism/DiffCSE</a></p>
<h2 id="核心思路（将敏感变化作为负例，非敏感变化的dropout作为正例）"><a href="#核心思路（将敏感变化作为负例，非敏感变化的dropout作为正例）" class="headerlink" title="核心思路（将敏感变化作为负例，非敏感变化的dropout作为正例）"></a>核心思路（将敏感变化作为负例，非敏感变化的dropout作为正例）</h2><p><strong>出发点</strong>：</p>
<p>NLP任务中，词语EDA的数据增强方式是敏感变化，dropout方式是 不敏感变化。SimCSE的成功也说明了dropout masks机制来构建正样本，比基于同义词或掩码语言模型的删除或替换等更复杂的增强效果要好得多。“。这一现象也说明，<strong>「直接增强（删除或替换）往往改变句子本身语义」。</strong></p>
<p><img src="https://pic4.zhimg.com/80/v2-6a25b5c6d3eef374dd84d6f3e712b887_1440w.webp" alt=""><br><img src="https://pic4.zhimg.com/80/v2-74343b4576af98fc90bc171a16f058f3_1440w.webp" alt=""></p>
<h2 id="正例生成方式-1"><a href="#正例生成方式-1" class="headerlink" title="正例生成方式"></a>正例生成方式</h2><p>与SimCSE相同，Dropout产生正例</p>
<h2 id="负例生成方式-1"><a href="#负例生成方式-1" class="headerlink" title="负例生成方式"></a>负例生成方式</h2><p>通过ELECTRA模型，完成句子改写任务</p>
<h1 id="PromptBert（EMNLP-2022）"><a href="#PromptBert（EMNLP-2022）" class="headerlink" title="PromptBert（EMNLP 2022）"></a>PromptBert（EMNLP 2022）</h1><p><strong>题目</strong>：PromptBERT: Improving BERT Sentence Embeddings with Prompts</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2201.04337.pdf">https://arxiv.org/pdf/2201.04337.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/kongds/Prompt-BERT">https://github.com/kongds/Prompt-BERT</a></p>
<h2 id="核心思路-6"><a href="#核心思路-6" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>BERT句向量存在各向异性，并受到词频、字母大小写、子词等影响。</p>
<p><strong>解决方式</strong>：</p>
<p>通过Prompt模版，得到句子的表征向量，与CLS、AVG、MAX取表征向量不同，没有用到具体某个词语的信息。</p>
<p><strong>模版去噪对比学习</strong>：</p>
<p>通过不同模版产生句子表征向量，构建正样本。为了剔除不同模版的影响，减去纯模版得到的句子表征向量。</p>
<p><img src="https://pic3.zhimg.com/80/v2-03707325180ec050cf39849c4030e586_1440w.webp" alt=""></p>
<h2 id="正例构建方式-2"><a href="#正例构建方式-2" class="headerlink" title="正例构建方式"></a>正例构建方式</h2><p>通过不同模版产生的句子表征向量</p>
<h2 id="负例构建方式-2"><a href="#负例构建方式-2" class="headerlink" title="负例构建方式"></a>负例构建方式</h2><p>同一batch内其他样本</p>
<h1 id="SNCSE"><a href="#SNCSE" class="headerlink" title="SNCSE"></a>SNCSE</h1><p><strong>题目</strong>：SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2201.05979.pdf">https://arxiv.org/pdf/2201.05979.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/Sense-GVT/SNCSE">https://github.com/Sense-GVT/SNCSE</a></p>
<h2 id="核心思路-7"><a href="#核心思路-7" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：</p>
<p>目前的数据增强方法，获取的正样本均极为相似，导致模型存在特征抑制，即<strong>「模型不能区分文本相似度和语义相似度，并更偏向具有相似文本，而不考虑它们之间的实际语义差异」。</strong> 导致模型更多关注的是字面匹配，而非语义匹配。</p>
<p><strong>解决方式</strong>：</p>
<p>新增软负例以及双向边际损失，限制样本与软负例相似度到一定范围内。 并利用PromptBert方式获取词向量。</p>
<p><strong>计算公式</strong>如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-6d6852c52edb927f3e6f90a4780360f0_1440w.webp" alt=""></p>
<p><img src="https://pic2.zhimg.com/80/v2-da3c2a097a0fa631f88a913dccadd541_1440w.webp" alt=""></p>
<h2 id="正例生成方式-2"><a href="#正例生成方式-2" class="headerlink" title="正例生成方式"></a>正例生成方式</h2><p>与SimCSE相同，Dropout产生正例</p>
<h2 id="负例生成方式-2"><a href="#负例生成方式-2" class="headerlink" title="负例生成方式"></a>负例生成方式</h2><ul>
<li>纯负例：batch内其他数据</li>
<li>软负例：通过spacy进行语法解析，为动词添加否定词前缀。</li>
</ul>
<h1 id="EASE（NAACL2022）"><a href="#EASE（NAACL2022）" class="headerlink" title="EASE（NAACL2022）"></a>EASE（NAACL2022）</h1><p><strong>题目</strong>：EASE: Entity-Aware Contrastive Learning of Sentence Embedding</p>
<p><strong>地址</strong>：<a href="https://arxiv.org/pdf/2205.04260.pdf">https://arxiv.org/pdf/2205.04260.pdf</a></p>
<p><strong>代码</strong>：<a href="https://github.com/studio-ousia/ease">https://github.com/studio-ousia/ease</a></p>
<h2 id="核心思路-8"><a href="#核心思路-8" class="headerlink" title="核心思路"></a>核心思路</h2><p><strong>出发点</strong>：实体是一个句子的重要部分，可以作为一个句子的指示器，通过学习实体与句子之间的差异，可以为句子向量的学习提供额外信息。</p>
<p><strong>解决方式</strong>：通过维基百科的实体超链接引入实体信息。</p>
<p><strong>损失函数</strong>：实体句子对比损失+自监督对比损失</p>
<h2 id="正例生成方式-3"><a href="#正例生成方式-3" class="headerlink" title="正例生成方式"></a>正例生成方式</h2><ul>
<li><strong>实体正例</strong>：维基百科超链接实体。(为了提高实体质量，仅保留超链接实体出现次数超过10次的实体。)</li>
<li><strong>样本正例</strong>：与SimCSE相同，Dropout产生正例</li>
</ul>
<h2 id="负例生成方式-3"><a href="#负例生成方式-3" class="headerlink" title="负例生成方式"></a>负例生成方式</h2><ul>
<li>实体负例：<br>负例实体需要与正例实体具有相同的类型；<br>负例实体不能与正例实体出现在同一维基百科页面中。<br>随机在满足上诉条件的候选实体中选择一个实体作为硬负例数据，构建（句子，正例实体，负例实体）的triple数据。</li>
<li>样本负例：batch内其他数据</li>
</ul>
<h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><ul>
<li>在中文数据集上，验证各方法效果</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>句向量</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>句向量</tag>
      </tags>
  </entry>
  <entry>
    <title>穿越时空：当ChatGPT遇见stable-diffusion，你不敢相信的创意艺术之旅!</title>
    <url>/2023/03/05/2023-03-05-%E7%A9%BF%E8%B6%8A%E6%97%B6%E7%A9%BA%EF%BC%9A%E5%BD%93ChatGPT%E9%81%87%E8%A7%81stable-diffusion%EF%BC%8C%E4%BD%A0%E4%B8%8D%E6%95%A2%E7%9B%B8%E4%BF%A1%E7%9A%84%E5%88%9B%E6%84%8F%E8%89%BA%E6%9C%AF%E4%B9%8B%E6%97%85!/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>欢迎来到一场创意的旅程，这里将聚焦于 ChatGPT 和 stable-diffusion 这两个令人激动的技术。在这篇文章中，我们将会探索这两种技术如何结合使用，为艺术创作带来全新的可能性。我们将探讨如何利用 ChatGPT 生成富有想象力的创意，以及如何使用 stable-diffusion 技术来呈现精美的中式艺术风格。我们还将介绍一些令人惊叹的案例，展示这些技术的真正潜力和创造力。无论您是一个艺术爱好者、技术探究者，还是想探索新领域的读者，本文都将为您提供一次精彩的旅程。跟随我们一起穿越时空，发现 ChatGPT 和 stable-diffusion 的惊人之处。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/D2oNrN.png" alt="D2oNrN"></p>
<span id="more"></span>
<h1 id="ChatGPT：创意的源泉"><a href="#ChatGPT：创意的源泉" class="headerlink" title="ChatGPT：创意的源泉"></a>ChatGPT：创意的源泉</h1><p>ChatGPT 是一种基于自然语言处理 (NLP) 技术的深度学习模型，具有生成自然语言文本的能力。它是由 OpenAI 开发的，使用了大量的训练数据和计算资源，能够生成各种类型的文本，包括对话、文章、故事等等。通过简单的输入提示，ChatGPT 可以自动生成与输入相关的文本，具有极大的创意和想象力。</p>
<p>在艺术创作领域，ChatGPT 提供了无限的可能性。我们可以使用 ChatGPT 生成各种富有想象力的艺术作品，例如诗歌、小说、电影剧本等等。通过输入不同的创意提示，我们可以让 ChatGPT 生成出无数可能的艺术创作。下面是一个例子：</p>
<blockquote>
<p>“在繁华都市的街头，一位神秘的女子突然出现，她手持一把魔法法杖，周围的建筑物开始发生异变……”</p>
</blockquote>
<p>通过输入这个简短的提示，ChatGPT 可以生成一个充满想象力的故事，我们可以将其发展为小说、漫画、电影等等。</p>
<h1 id="Stable-Diffusion：精美的艺术风格"><a href="#Stable-Diffusion：精美的艺术风格" class="headerlink" title="Stable-Diffusion：精美的艺术风格"></a>Stable-Diffusion：精美的艺术风格</h1><p>Stable-Diffusion 是一种深度学习技术，可以生成出极具艺术价值的图像。与传统的图像生成技术不同，Stable-Diffusion 能够生成出高分辨率的图像，而且具有出色的视觉效果。该技术基于流模型，使用了大量的数据和计算资源进行训练，可以生成出各种具有中式风格的图像，例如山水画、人物画等等。</p>
<p>Stable-Diffusion 技术的一个优势是它可以通过简单的输入提示来控制图像的生成过程。我们可以使用各种创意的输入提示来生成出不同的艺术风格，例如：</p>
<blockquote>
<p>“生成一个穿着汉服的女子，在竹林中写诗。”</p>
</blockquote>
<p>通过这个简单的提示，Stable-Diffusion 可以生成出一个具有中式风格的图像，展现出竹林中的神秘和美丽。</p>
<h1 id="ChatGPT-和-Stable-Diffusion-的结合"><a href="#ChatGPT-和-Stable-Diffusion-的结合" class="headerlink" title="ChatGPT 和 Stable-Diffusion 的结合"></a>ChatGPT 和 Stable-Diffusion 的结合</h1><p>当 ChatGPT 和 Stable-Diffusion 结合在一起时，它们可以为艺术创作带来全新的可能性。我们可以使用 ChatGPT 生成出富有想象力的艺术创意，然后使用 Stable-Diffusion 技术将这些创意转化为精美的艺术作品。这种结合可以让我们更加轻松地创作出令人震撼的艺术作品，同时也可以大大提高我们的创作效率。</p>
<p>下面是一个结合了 ChatGPT 和 Stable-Diffusion 的艺术作品的例子：</p>
<blockquote>
<p>“生成一个传统的中国庭院，里面有一位穿着古装的女子，手持一枝花，伫立在花园中。”</p>
</blockquote>
<p>通过输入这个创意提示，ChatGPT 可以生成出一个具有想象力的场景描述。然后，我们可以使用 Stable-Diffusion 技术将这个场景转化为一个美丽的艺术作品，展现出中国古代庭院的美丽和神秘。</p>
<p>总之，ChatGPT 和 Stable-Diffusion 技术的结合为我们带来了无限的可能性。无论是艺术创作、文学创作，还是其他类型的创作，这两种技术都可以帮助我们更加轻松地实现我们的创作愿望。</p>
<h1 id="实现示例"><a href="#实现示例" class="headerlink" title="实现示例"></a>实现示例</h1><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/8exT63.png" alt="8exT63"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/psREq4.png" alt="psREq4"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/12ZmUf.png" alt="12ZmUf"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/0aA0VC.png" alt="0aA0VC"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/FJMuzu.png" alt="FJMuzu"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/AvqEcj.png" alt="AvqEcj"></p>
<h1 id="搭建方式"><a href="#搭建方式" class="headerlink" title="搭建方式"></a>搭建方式</h1><h2 id="ChatGPT注册及使用"><a href="#ChatGPT注册及使用" class="headerlink" title="ChatGPT注册及使用"></a>ChatGPT注册及使用</h2><p>见<a href="https://jmxgodlz.xyz/2022/12/07/2022-12-07-【律师函警告】-ChatGPT以鸡你太美主题写小说/">【律师函警告】-ChatGPT以鸡你太美主题写小说</a></p>
<h2 id="Stable-Diffusion搭建及使用-MAC-M1"><a href="#Stable-Diffusion搭建及使用-MAC-M1" class="headerlink" title="Stable-Diffusion搭建及使用[MAC M1]"></a>Stable-Diffusion搭建及使用[MAC M1]</h2><ol>
<li>克隆代码仓库</li>
</ol>
<blockquote>
<p>git clone <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui.git">https://github.com/AUTOMATIC1111/stable-diffusion-webui.git</a></p>
</blockquote>
<ol>
<li>安装torch环境</li>
</ol>
<blockquote>
<p>conda create -n sd python=3.10.6</p>
<p>conda activate sd</p>
<p>pip install -r requirements_versions.txt</p>
</blockquote>
<ol>
<li>GPU torch MPS安装</li>
</ol>
<p>见<a href="https://zhuanlan.zhihu.com/p/542502414">真香～BERT在MAC Pytorch的使用</a></p>
<blockquote>
<p>conda install pytorch torchvision torchaudio -c pytorch</p>
</blockquote>
<ol>
<li>启动代码</li>
</ol>
<blockquote>
<p>source webui-macos-env.sh // 涉及一些环境变量的初始化</p>
</blockquote>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/3Uozb1.png" alt="3Uozb1"></p>
<ol>
<li>修改启动代码中部分初始环境检查</li>
</ol>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ROSwg9.png" alt="ROSwg9"></p>
<ol>
<li>下载模型</li>
</ol>
<p><a href="https://civitai.com：含有许多玩家自调Lora模型">https://civitai.com：含有许多玩家自调Lora模型</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/VmWrMd.png" alt="VmWrMd"></p>
<blockquote>
<p>TIPS: LoRA，英文全称Low-Rank Adaptation of Large Language Models，直译为大语言模型的低阶适应，这是微软的研究人员为了解决大语言模型微调而开发的一项技术。类似于BERT的微调哈～</p>
</blockquote>
<p>中国风基础&amp;Lora模型链接：</p>
<p><a href="https://civitai.com/models/11352/guofeng3lora">GuoFeng3_Lora | Stable Diffusion LORA | Civitai</a></p>
<p><a href="https://civitai.com/models/10415/guofeng3">GuoFeng3 | Stable Diffusion Checkpoint | Civitai</a></p>
<p><a href="https://civitai.com/models/12597/moxin">墨心 MoXin | Stable Diffusion LORA | Civitai</a></p>
<p>Lora模型放在models/Lora下：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/mg6Qr4.png" alt="mg6Qr4"></p>
<p>基础模型放在models/Stable-diffusion下：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/BAcBXs.png" alt="BAcBXs"></p>
<ol>
<li>启动脚本</li>
</ol>
<blockquote>
<p>python launch.py </p>
</blockquote>
<ol>
<li>Web使用：模型选择&amp;Lora选择&amp;模版选择&amp;参数选择</li>
</ol>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/0Kx5cw.png" alt="0Kx5cw"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/YirWt0.png" alt="YirWt0"></p>
<blockquote>
<p>其中CFG代表 <strong>提示词相关性</strong></p>
</blockquote>
<h1 id="彩蛋"><a href="#彩蛋" class="headerlink" title="彩蛋"></a>彩蛋</h1><p>看看这篇博客有多少AI生成的内容：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ClBCF2.png" alt="ClBCF2"></p>
<p><img src="/Users/jmxgodlzz/Library/Application%20Support/marktext/images/2023-03-04-23-14-04-image.png" alt=""></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/Kbc78h.png" alt="Kbc78h"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/epRNxJ.png" alt="epRNxJ"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/BEHMAa.png" alt="BEHMAa"></p>
<p><strong>向文明低头！！！</strong></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>复刻ChatGPT语言模型系列-（一）基座模型选取</title>
    <url>/2023/05/08/2023-05-08-%E5%A4%8D%E5%88%BBChatGPT%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97-%EF%BC%88%E4%B8%80%EF%BC%89%E5%9F%BA%E5%BA%A7%E6%A8%A1%E5%9E%8B%E9%80%89%E5%8F%96/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>今天开始我将会推出一系列关于复刻ChatGPT语言模型的博文。本系列将包括以下内容：</p>
<ul>
<li>复刻ChatGPT语言模型系列-（一）基座模型选取</li>
<li>复刻ChatGPT语言模型系列-（二）参数高效微调</li>
<li>复刻ChatGPT语言模型系列-（三）指令学习微调</li>
<li>复刻ChatGPT语言模型系列-（四）文本生成解码</li>
<li>复刻ChatGPT语言模型系列-（五）强化学习RLHF</li>
<li>复刻ChatGPT语言模型系列-（六）LLM模型评估</li>
</ul>
<p>在本系列的第一篇博文中，我们将会探讨如何选取一个好的预训练语言模型作为基座。选择一个优秀的基座模型非常重要，因为它会直接影响到后续模型的训练和性能表现。乱花渐欲迷人眼，目前社区涌现出许多优秀的大模型，这让我们选择起来十分困难。因此，本文将介绍并分析这些大模型，从中选择出适合作为基座的模型。我们将深入探究这些模型的特点和性能，并分析其优缺点。本文旨在为大家提供选取基座模型的参考意见，帮助大家更好地复刻ChatGPT语言模型。</p>
<p>以下是目前已经开放的基座模型清单，本文将专注介绍已开源模型，而其他模型则请读者自行了解。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型名称</th>
<th style="text-align:center">发布时间</th>
<th style="text-align:center">发布机构</th>
<th style="text-align:center">语言</th>
<th style="text-align:center">参数规模</th>
<th style="text-align:center">Tokens规模</th>
<th style="text-align:center">模型结构</th>
<th style="text-align:center">是否开源</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">T5</td>
<td style="text-align:center">2019-10</td>
<td style="text-align:center">Google</td>
<td style="text-align:center">英</td>
<td style="text-align:center">13B</td>
<td style="text-align:center"></td>
<td style="text-align:center">T5-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">GPT-3</td>
<td style="text-align:center">2020-05</td>
<td style="text-align:center">OpenAI</td>
<td style="text-align:center">英</td>
<td style="text-align:center">175B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">CPM1</td>
<td style="text-align:center">2021-03</td>
<td style="text-align:center">Tsinghua</td>
<td style="text-align:center">中</td>
<td style="text-align:center">2.6B</td>
<td style="text-align:center"></td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">LaMDA</td>
<td style="text-align:center">2021-05</td>
<td style="text-align:center">Google</td>
<td style="text-align:center">英</td>
<td style="text-align:center">137B</td>
<td style="text-align:center">2.8T</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">CPM2</td>
<td style="text-align:center">2021-07</td>
<td style="text-align:center">Tsinghua</td>
<td style="text-align:center">中</td>
<td style="text-align:center">11B/198B(MoE)</td>
<td style="text-align:center"></td>
<td style="text-align:center">Encoder-Decoder</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">Jurassic</td>
<td style="text-align:center">2021-08</td>
<td style="text-align:center">AI21</td>
<td style="text-align:center">英</td>
<td style="text-align:center">178B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">MT-NLG</td>
<td style="text-align:center">2021-10</td>
<td style="text-align:center">Microsoft,  NVIDIA</td>
<td style="text-align:center">英</td>
<td style="text-align:center">530B</td>
<td style="text-align:center">270B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">ERNIE 3.0</td>
<td style="text-align:center">2021-12</td>
<td style="text-align:center">Baidu</td>
<td style="text-align:center">中</td>
<td style="text-align:center">260B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">Multi-task</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">Gopher</td>
<td style="text-align:center">2021-12</td>
<td style="text-align:center">DeepMind</td>
<td style="text-align:center">英</td>
<td style="text-align:center">280B</td>
<td style="text-align:center">300B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">Chinchilla</td>
<td style="text-align:center">2022-04</td>
<td style="text-align:center">DeepMind</td>
<td style="text-align:center">英</td>
<td style="text-align:center">70B</td>
<td style="text-align:center">1.4T</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">PaLM</td>
<td style="text-align:center">2022-04</td>
<td style="text-align:center">Google</td>
<td style="text-align:center">多语言</td>
<td style="text-align:center">540B</td>
<td style="text-align:center">780B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">x</td>
</tr>
<tr>
<td style="text-align:center">OPT</td>
<td style="text-align:center">2022-05</td>
<td style="text-align:center">Meta</td>
<td style="text-align:center">英</td>
<td style="text-align:center">125M-175B</td>
<td style="text-align:center">180B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">BLOOM</td>
<td style="text-align:center">2022-07</td>
<td style="text-align:center">BigScience</td>
<td style="text-align:center">多语言</td>
<td style="text-align:center">176B</td>
<td style="text-align:center">366B</td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">GLM-130B</td>
<td style="text-align:center">2022-08</td>
<td style="text-align:center">Tsinghua</td>
<td style="text-align:center">中、英</td>
<td style="text-align:center">130B</td>
<td style="text-align:center">400B</td>
<td style="text-align:center">GLM-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">Wenzhong</td>
<td style="text-align:center">2022-09</td>
<td style="text-align:center">IDEA</td>
<td style="text-align:center">中</td>
<td style="text-align:center">3.5B</td>
<td style="text-align:center"></td>
<td style="text-align:center">GPT-style</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">LLaMA</td>
<td style="text-align:center">2023-02</td>
<td style="text-align:center">Meta</td>
<td style="text-align:center">多语言</td>
<td style="text-align:center">7B-65 B</td>
<td style="text-align:center">1.4T</td>
<td style="text-align:center">GPT-sryle</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">MOSS</td>
<td style="text-align:center">2023-04</td>
<td style="text-align:center">FUDAN</td>
<td style="text-align:center">中、英</td>
<td style="text-align:center">16B</td>
<td style="text-align:center">700B</td>
<td style="text-align:center">GPT-sryle</td>
<td style="text-align:center">√</td>
</tr>
</tbody>
</table>
</div>
<p>参考链接：<a href="https://zhuanlan.zhihu.com/p/614766286">https://zhuanlan.zhihu.com/p/614766286</a><br><span id="more"></span></p>
<h1 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h1><div class="table-container">
<table>
<thead>
<tr>
<th>模型名称</th>
<th>发布时间</th>
<th>发布机构</th>
<th>语言</th>
<th>模态</th>
<th>参数规模</th>
<th>基础模型</th>
<th>是否开源</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5</td>
<td>2021-06</td>
<td>OpenAI</td>
<td>多语言</td>
<td>文本</td>
<td>175B</td>
<td>GPT-3</td>
<td>x</td>
</tr>
<tr>
<td>FLAN</td>
<td>2021-09</td>
<td>Google</td>
<td>英</td>
<td>文本</td>
<td>137B</td>
<td>LaMDA</td>
<td>x</td>
</tr>
<tr>
<td>T0</td>
<td>2021-10</td>
<td>Hugging Face</td>
<td>英</td>
<td>文本</td>
<td>13B</td>
<td>T5</td>
<td>√</td>
</tr>
<tr>
<td>Flan-PaLM</td>
<td>2022-10</td>
<td>Google</td>
<td>多语言</td>
<td>文本</td>
<td>540B</td>
<td>PaLM</td>
<td>x</td>
</tr>
<tr>
<td>BLOOMZ</td>
<td>2022-11</td>
<td>Hugging Face</td>
<td>多语言</td>
<td>文本</td>
<td>176B</td>
<td>BLOOM</td>
<td>√</td>
</tr>
<tr>
<td>mT0</td>
<td>2022-11</td>
<td>Hugging Face</td>
<td>多语言</td>
<td>文本</td>
<td>13B</td>
<td>mT5</td>
<td>√</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>2022-11</td>
<td>OpenAI</td>
<td>多语言</td>
<td>文本</td>
<td>173B</td>
<td>GPT3.5</td>
<td>x</td>
</tr>
<tr>
<td>Alpaca</td>
<td>2023-3-14</td>
<td>StandFord</td>
<td>英</td>
<td>文本</td>
<td>7B</td>
<td>LLaMA</td>
<td>√</td>
</tr>
<tr>
<td>ChatGLM</td>
<td>2023-3-14</td>
<td>Tsinghua</td>
<td>中、英</td>
<td>文本</td>
<td>{:[6B],[130B]:}</td>
<td>GLM</td>
<td>√</td>
</tr>
<tr>
<td>GPT-4</td>
<td>2023-3-14</td>
<td>OpenAI</td>
<td>多语言</td>
<td>文本、图像</td>
<td>√</td>
<td>GPT-4</td>
<td>x</td>
</tr>
<tr>
<td>ERNIE Bot</td>
<td>2023-3-15</td>
<td>Baidu</td>
<td>中</td>
<td>文本、图像</td>
<td>260B√</td>
<td>ERNIE</td>
<td>x</td>
</tr>
<tr>
<td>Bard</td>
<td>2023-3-21</td>
<td>Google</td>
<td>英</td>
<td>文本</td>
<td>137B</td>
<td>LaMDA</td>
<td>x</td>
</tr>
<tr>
<td>MOSS</td>
<td>2023-4</td>
<td>FUDAN</td>
<td>中、英</td>
<td>文本</td>
<td>16B</td>
<td>CodeGen</td>
<td>√</td>
</tr>
</tbody>
</table>
</div>
<h1 id="CPM"><a href="#CPM" class="headerlink" title="CPM"></a>CPM</h1><p>CPM模型是由智源、清华开发的一种基于大规模中文训练数据进行生成式预训练的中文预训练语言模型。该模型具有26亿个参数和100GB中文训练数据，是目前最大的中文预训练语言模型之一。CPM模型在各种中文自然语言处理任务中表现出色，包括对话、文章生成、填空测试和语言理解等任务。</p>
<p><strong>论文标题</strong>：CPM: A large-scale generative Chinese Pre-trained language model</p>
<p><strong>论文地址</strong>：<a href="https://www.sciencedirect.com/science/article/pii/S266665102100019X">CPM: A large-scale generative Chinese Pre-trained language model - ScienceDirect</a></p>
<p><strong>模型结构</strong>：基于Transformer的自回归模型，GPT类结构；</p>
<p><strong>训练数据</strong>：100G中文语料；</p>
<p><strong>改动点</strong>：较GPT3采用更大的batch_size。</p>
<h1 id="OPT"><a href="#OPT" class="headerlink" title="OPT"></a>OPT</h1><p>OPT-175B是Meta AI在2022年5月3日发布的一款开放模型，是模型参数超过千亿级别的开放模型之一。相比于GPT-3，该模型更加开放便于访问，并在以下五个方面表现出其开放性：</p>
<ol>
<li><p>论文：该模型提供了某些能力可能存在的证明，并揭示可以建立在此基础上的一般思想。</p>
</li>
<li><p>API访问：该模型允许研究人员探索和评估现有基础模型的能力和局限性，例如推理和偏差。</p>
</li>
<li><p>模型权重：研究人员可以使用该模型的权重来逐步改进现有模型、开发更深入的可解释技术和更有效的微调方法。</p>
</li>
<li><p>训练数据：该模型让研究人员更好地理解训练数据在模型行为中的作用，例如情境学习从何而来。</p>
</li>
<li><p>计算：该模型允许研究人员尝试新的架构、培训目标/程序、进行数据集消融，并在不同领域开发全新的模型。虽然这种方法具有最大的理解和改进潜力，但也相当昂贵。</p>
</li>
</ol>
<p>作为一个大规模的语言模型，OPT-175B具有超过1750亿个参数，是目前为止最大的语言模型之一。该模型通过在公开可用的数据集上进行训练，允许更多的社区参与了解这项基础新技术。为了保持完整性并防止滥用，Meta AI将在非商业许可下发布他们的模型，以专注于研究用例。该模型的访问权限将授予学术研究者、政府机构、民间社会和学术界组织的人员，以及世界各地的工业研究实验室。</p>
<p><strong>项目地址</strong>：<a href="https://github.com/facebookresearch/metaseq" title="https://github.com/facebookresearch/metaseq">GitHub - facebookresearch/metaseq: Repo for external large-scale work</a></p>
<p><strong>论文地址</strong>：<a href="https://arxiv.org/pdf/2205.01068.pdf">https://arxiv.org/pdf/2205.01068.pdf</a></p>
<p><strong>模型结构：</strong> Decoder Only</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Model</th>
<th style="text-align:center">#L</th>
<th style="text-align:center">#H</th>
<th style="text-align:center">d_model</th>
<th style="text-align:center">LR</th>
<th style="text-align:center">Batch</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">125M</td>
<td style="text-align:center">12</td>
<td style="text-align:center">12</td>
<td style="text-align:center">768</td>
<td style="text-align:center">6.0 e-4</td>
<td style="text-align:center">0.5M</td>
</tr>
<tr>
<td style="text-align:center">350M</td>
<td style="text-align:center">24</td>
<td style="text-align:center">16</td>
<td style="text-align:center">1024</td>
<td style="text-align:center">3.0 e-4</td>
<td style="text-align:center">0.5M</td>
</tr>
<tr>
<td style="text-align:center">1.3B</td>
<td style="text-align:center">24</td>
<td style="text-align:center">32</td>
<td style="text-align:center">2048</td>
<td style="text-align:center">2.0 e-4</td>
<td style="text-align:center">1M</td>
</tr>
<tr>
<td style="text-align:center">2.7B</td>
<td style="text-align:center">32</td>
<td style="text-align:center">32</td>
<td style="text-align:center">2560</td>
<td style="text-align:center">1.6 e-4</td>
<td style="text-align:center">1M</td>
</tr>
<tr>
<td style="text-align:center">6.7B</td>
<td style="text-align:center">32</td>
<td style="text-align:center">32</td>
<td style="text-align:center">4096</td>
<td style="text-align:center">1.2 e-4</td>
<td style="text-align:center">2M</td>
</tr>
<tr>
<td style="text-align:center">13B</td>
<td style="text-align:center">40</td>
<td style="text-align:center">40</td>
<td style="text-align:center">5120</td>
<td style="text-align:center">1.0 e-4</td>
<td style="text-align:center">4M</td>
</tr>
<tr>
<td style="text-align:center">30B</td>
<td style="text-align:center">48</td>
<td style="text-align:center">56</td>
<td style="text-align:center">7168</td>
<td style="text-align:center">1.0 e-4</td>
<td style="text-align:center">4M</td>
</tr>
<tr>
<td style="text-align:center">66B</td>
<td style="text-align:center">64</td>
<td style="text-align:center">72</td>
<td style="text-align:center">9216</td>
<td style="text-align:center">0.8 e-4</td>
<td style="text-align:center">2M</td>
</tr>
<tr>
<td style="text-align:center">175B</td>
<td style="text-align:center">96</td>
<td style="text-align:center">96</td>
<td style="text-align:center">12288</td>
<td style="text-align:center">1.2 e-4</td>
<td style="text-align:center">2M</td>
</tr>
</tbody>
</table>
</div>
<p><strong>训练过程：</strong></p>
<p>OPT与GPT3的训练过程比较如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Model</th>
<th>GPU</th>
<th>FLOPs</th>
<th>days</th>
</tr>
</thead>
<tbody>
<tr>
<td>OPT</td>
<td>1024 80G A100</td>
<td>√4.30E+23</td>
<td>33</td>
</tr>
<tr>
<td>GPT3</td>
<td>10,000 32G V100</td>
<td>√3.14E+23</td>
<td>14.8</td>
</tr>
</tbody>
</table>
</div>
<p>Meta将OPT系列模型的训练过程记录在logbook中，地址如下：<a href="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/README.md">metaseq/README.md at main · facebookresearch/metaseq · GitHub</a>。这个logbook主要记录了作者训练OPT系列模型的辛酸历程，包括遇到的一些问题、讨论分析以及解决方法。</p>
<p>有兴趣的可以仔细阅读原文，或者<a href="https://zhuanlan.zhihu.com/p/617492983">OPT logbook：训练大规模语言模型的一些经验</a>。</p>
<p>总结如下，Meta主要遇到收敛/数值稳定性问题、机器故障问题。</p>
<ul>
<li><p>面对收敛/数值稳定性问题：Meta主要采取降低学习率、参照成熟框架设置参数、切换激活函数的方法；</p>
</li>
<li><p>面对机器故障问题：Meta主要开发监控、自动化工具进行监测。</p>
</li>
</ul>
<h1 id="LLAMA"><a href="#LLAMA" class="headerlink" title="LLAMA"></a>LLAMA</h1><p><code>LLaMA</code>（Large Language Model Meta AI），由 Meta AI 发布的一个开放且高效的大型基础语言模型，共有 <code>7B</code>、<code>13B</code>、<code>33B</code>、<code>65B</code>（650 亿）四种版本。其数据集来源都是公开数据集，无任何定制数据集，保证了其工作与开源兼容和可复现，整个训练数据集在 token 化之后大约包含 1.4T 的 token。</p>
<p>关于模型性能，LLaMA 的性能非常优异：具有 130 亿参数的 LLaMA 模型「在大多数基准上」可以<strong>胜过</strong> GPT-3（ 参数量达 1750 亿），而且可以在单块 V100 GPU 上运行；而最大的 650 亿参数的 LLaMA 模型可以媲美谷歌的 Chinchilla-70B 和 PaLM-540B。</p>
<p>关于训练集，其来源都是公开数据集，无任何定制数据集，保证了其工作与开源兼容和可复现。整个训练数据集在 token 化之后大约包含 1.4T 的 token。其中，LLaMA-65B 和 LLaMA-33B 是在 1.4万亿个 <code>token</code> 上训练的，而最小的模型 LLaMA-7B 是在 1万亿个 token 上训练的。</p>
<p><strong>论文标题</strong>：LLaMA: Open and Efficient Foundation Language Models</p>
<p><strong>论文链接</strong>：<a href="https://arxiv.org/pdf/2302.13971.pdf">https://arxiv.org/pdf/2302.13971.pdf</a></p>
<p><strong>模型结构</strong>：</p>
<ul>
<li>PreLayerNorm-RMSNorm-<a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf">Root Mean Square Layer Normalization</a></li>
<li>ROPE旋转位置编码（替换绝对/相对位置编码）</li>
<li>SwiGLU激活函数（替换ReLU）-<a href="https://arxiv.org/pdf/2002.05202v1.pdf">GLU Variants Improve Transformer</a></li>
</ul>
<p><strong>改动点</strong>：</p>
<p>过了1T的Token：<a href="https://arxiv.org/pdf/2203.15556.pdf">过去的研究</a>发现最好的性能不是在最大的模型上，而是在过了更多token的模型上；</p>
<blockquote>
<p>与OpenAI提出大模型缩放法则不同的是，DeepMind认为当前许多大模型是训练不充分的；</p>
<p>OpenAI在《Scaling Laws for Neural Language Models》中，指出<strong>在给定计算量的时候，模型性能的提升主要在于增加参数规模而不是增加数据量；</strong></p>
<p>DeepMind在《Training Compute-Optimal Large Language Models》中，指出<strong>在每条曲线的最小值的左侧，模型太小了——在较少数据上训练的较大模型将是一种改进。在每条曲线的最小值的右侧，模型太大——在更多数据上训练的较小模型将是一种改进。最好的模型处于最小值。</strong></p>
</blockquote>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/GnffTU.png" alt="GnffTU"></p>
<h1 id="BLOOM"><a href="#BLOOM" class="headerlink" title="BLOOM"></a>BLOOM</h1><p>BLOOM 是 BigScience（一个围绕研究和创建超大型语言模型的开放协作研讨会）中数百名研究人员合作设计和构建的 176B 参数开源大语言模型，同时，还开源了BLOOM-560M、BLOOM-1.1B、BLOOM-1.7B、BLOOM-3B、BLOOM-7.1B 其他五个参数规模相对较小的模型。BLOOM 是一种 decoder-only 的 Transformer 语言模型，它是在 ROOTS 语料库上训练的，该数据集包含 46 种自然语言和 13 种编程语言（总共 59 种）的数百个数据来源。</p>
<p><strong>论文标题</strong>：BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</p>
<p><strong>论文链接</strong>：<a href="https://arxiv.org/pdf/2211.05100.pdf">https://arxiv.org/pdf/2211.05100.pdf</a></p>
<p><strong>模型结构</strong>：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/0hYelu.png" alt="0hYelu"></p>
<p>核心介绍：</p>
<ol>
<li><p>基于Megatron-LM GPT2模型开发，模型结构为Decoder-only类型；</p>
</li>
<li><p><strong>ALiBi Positional Embeddings</strong>。它允许外推比训练模型的输入序列更长的输入序列，同时有助于加速训练收敛。因此，即使训练时使用长度为 2048 的序列，模型也可以在推理过程中处理更长的序列。思路来源于： <a href="https://arxiv.org/abs/2108.12409">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a>；</p>
</li>
<li><p><strong>Embedding LayerNorm</strong>。在 embedding 层之后立即添加额外的归一化层（<code>layer norm</code> 层）。这个方法来源于 bitsandbytes^17库 (Dettmers et al., 2022)，作者的实验发现这样可以显著提高训练的稳定性。另外，模型最终的训练是在  <code>bfloat16</code> 下进行的。思路来源于：<a href="https://arxiv.org/abs/2110.02861">8-bit Optimizers via Block-wise Quantization</a></p>
</li>
</ol>
<p>同时开发人员也记录了项目的开发过程：<a href="https://huggingface.co/blog/zh/bloom-megatron-deepspeed">千亿参数开源大模型 BLOOM 背后的技术</a>，关键信息如下：</p>
<ol>
<li><p>基于<strong>Megatron-DeepSpeed</strong> 实现了 <strong>3D 并行</strong>以允许大模型以非常有效的方式进行训练。包括数据并行 (Data Parallelism，DP)、张量并行 (Tensor Parallelism，TP)、流水线并行 (Pipeline Parallelism，PP)。</p>
</li>
<li><p><strong>用 FP16 训练巨型 LLM 模型是一个禁忌</strong>。FP16会产生精度溢出，使用BF16进行训练。</p>
</li>
<li><p><strong>CUDA 融合核函数</strong>。为了快速高效地训练 BLOOM，有必要使用 Megatron-LM 提供的几个自定义 CUDA 融合核函数。特别地，有一个 LayerNorm 的融合核函数以及用于融合缩放、掩码和 softmax 这些操作的各种组合的核函数。Bias Add 也通过 PyTorch 的 JIT 功能与 GeLU 融合。这些操作都是瓶颈在内存的，因此将它们融合在一起以达到最大化每次显存读取后的计算量非常重要。因此，例如，在执行瓶颈在内存的 GeLU 操作时同时执行 Bias Add，运行时间并不会增加。这些核函数都可以在 <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM repository</a> 代码库 中找到。</p>
</li>
<li><p><strong>硬件故障</strong>也颇有挑战。</p>
</li>
</ol>
<h1 id="WenZhong"><a href="#WenZhong" class="headerlink" title="WenZhong"></a>WenZhong</h1><p>闻仲语言模型出自IDEA研究院的封神榜模型系列，专注于生成任务，提供了多个不同参数量的生成模型，例如GPT2等。</p>
<p>Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，成为中文AIGC和认知智能的基础设施。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/4hwtIC.jpg" alt="4hwtIC"></p>
<p>项目地址：<a href="https://github.com/IDEA-CCNL/Fengshenbang-LM">GitHub - IDEA-CCNL/Fengshenbang-LM: Fengshenbang-LM(封神榜大模型)是IDEA研究院认知计算与自然语言研究中心主导的大模型开源体系，成为中文AIGC和认知智能的基础设施。</a></p>
<h1 id="GLM"><a href="#GLM" class="headerlink" title="GLM"></a>GLM</h1><p>GLM-130B 是清华大学与智谱AI共同研制的一个开放的双语（英汉）双向密集预训练语言模型，拥有 1300亿个参数，使用通用语言模型（<a href="https://link.zhihu.com/√target=https%3A//github.com/THUDM/GLM">General Language Model， GLM</a>）的算法进行预训练。 2022年11月，斯坦福大学大模型中心对全球30个主流大模型进行了全方位的评测，<strong>GLM-130B 是亚洲唯一入选的大模型</strong>。GLM-130B 在广泛流行的英文基准测试中性能明显优于 GPT-3 175B(davinci)，而对 OPT-175B 和 BLOOM-176B 没有观察到性能优势，它还在相关基准测试中性能始终显著优于最大的中文语言模型 ERNIE 3.0 Titan 260B。GLM-130B 无需后期训练即可达到 INT4 量化，且几乎没有性能损失；更重要的是，它能够在 4×RTX 3090 (24G) 或 8×RTX 2080 Ti (11G) GPU 上有效推理，是使用 100B 级模型最实惠的 GPU 需求。</p>
<h2 id="GLM介绍"><a href="#GLM介绍" class="headerlink" title="GLM介绍"></a>GLM介绍</h2><p><strong>论文标题</strong>：GLM: General Language Model Pretraining with Autoregressive Blank Infilling</p>
<p><strong>论文地址</strong>：<a href="https://arxiv.org/pdf/2103.10360v2.pdf">https://arxiv.org/pdf/2103.10360v2.pdfhttps://arxiv.org/pdf/2103.10360v2.pdf</a></p>
<p><strong>训练目标</strong>：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/jkMmBZ.png" alt="jkMmBZ"></p>
<script type="math/tex; mode=display">
\max _\theta \mathbb{E}_{\boldsymbol{z} \sim Z_m}\left[\sum_{i=1}^m \log p_\theta\left(\boldsymbol{s}_{z_i} \mid \boldsymbol{x}_{\text {corrupt }}, \boldsymbol{s}_{\boldsymbol{z}_{<i}}\right)\right]</script><script type="math/tex; mode=display">
\begin{aligned}
& p_\theta\left(\boldsymbol{s}_i \mid \boldsymbol{x}_{\text {corrupt }}, \boldsymbol{s}_{\boldsymbol{z}_{<i}}\right) \\
= & \prod_{j=1}^{l_i} p\left(s_{i, j} \mid \boldsymbol{x}_{\text {corrupt }}, \boldsymbol{s}_{\boldsymbol{z}_{<i}}, \boldsymbol{s}_{i,<j}\right) \end{aligned}</script><p><strong>模型结构</strong>：</p>
<ul>
<li>改变层归一化、残差网络结构的顺序（避免出现数值错误）</li>
<li>新增线性层输出结果</li>
<li>使用GeLUs激活函数替代ReLU</li>
</ul>
<p><strong>微调方式</strong>：</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1683538503448-c26ff2b5-6bff-4dfb-aa2b-9b65c4202da0.png" alt=""></p>
<ul>
<li>对于分类任务，对于token分类，就使用目标token的表示；对于序列分类那就是使用cls的表示。</li>
<li>对于生成任务，partB 部分直接换成 mask 即可。</li>
</ul>
<p><strong>讨论分析</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>BERT</th>
<th>XLNET</th>
<th>Encoder-Decoder</th>
<th>UniLM</th>
</tr>
</thead>
<tbody>
<tr>
<td>BERT中不能很好处理连续的多个token; mask token是独立的，不能捕捉mask token之间的依赖关系。</td>
<td>xlnet使用了双流的注意力机制，改变了transformer的结构，增加了耗时。</td>
<td>使用了两个transformer模型实现了单向和双向注意力，加入token来识别mask span，浪费模型能力；训练和微调不一致。</td>
<td>在自编码框架下使用了mask来统一单双向的注意力。对于生成任务来说，还是不够高效。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>与bert对比</strong><br>bert是自编码模型，预测mask的字符。因为模型中的mask token是独立的，bert不能捕捉mask token之间的依赖性。bert的另一个缺点是不能预测多个连续的mask token，尤其是待预测长度未知情况下。</p>
<p><strong>与xlnet的对比</strong><br>都是自回归的模型。xlnet需要知道预测token的长度；使用双流注意力机制解决了信息泄漏的问题，改变了transfomer的结构，增加了耗时；xlnet决定一个token是否被独立预测。</p>
<p><strong>与编码解码模型对比</strong><br>T5也是处理的空白填充的任务目标，但是GLM使用了单个的transformer编码器学习单向和双向的注意力。通过共享参数使参数比编码解码模型更有效。T5在编码和解码阶段使用不同的位置编码，使用哨兵标记来识别不同的mask跨度，哨兵标记造成了模型能力的浪费和预训练微调的不一致性。</p>
<p><strong>与UniLM对比</strong><br>UniLM是通过在自编码框架下改变在双向，单向，互相之间的attention mask来统一预训练目标；由于自编码模型的独立假设，自回归模型不能完全捕捉当前token对于前面token的依赖。对于微调下游任务来说，自编码会比自回归更加低效。</p>
<h2 id="GLM130B"><a href="#GLM130B" class="headerlink" title="GLM130B"></a><strong>GLM130B</strong></h2><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/glm.gif" alt="glm"></p>
<ul>
<li>130B的原因是该大小能够在一个A100服务器(40G*8)上进行推理</li>
<li>千亿模型训练的问题：硬件故障、梯度爆炸、内存溢出、3D并行、无法恢复优化器状态、TCP通信阻塞</li>
<li>[MASK]、[gMASK]分别用作短文本、长文本生成</li>
<li>结构优化点：RoPE、DeepNorm、GeLU</li>
<li>预训练：95%的MASK自回归任务、5%的多任务指令学习(T0、DeepStruct)</li>
<li>400B的Token，但是据估计130B模型需要4T的Token</li>
<li>GLM-130B FP16-需要260G显存存储模型权重</li>
</ul>
<h2 id="UniLM"><a href="#UniLM" class="headerlink" title="UniLM"></a>UniLM</h2><p>许多人指出GLM和UniLM的模型结构非常相似。下面将对UniLM的模型结构进行详细介绍。</p>
<p>UniLM<strong>是微软研究院在Bert的基础上，最新产出的预训练语言模型，被称为统一预训练语言模型</strong>。 它可以完成单向、序列到序列和双向预测任务，可以说是结合了AR和AE两种语言模型的优点，Unilm在抽象摘要、生成式问题回答和语言生成数据集的抽样领域取得了最优秀的成绩。</p>
<p><strong>论文标题</strong>：<a href="https://arxiv.org/pdf/1905.03197.pdf">Unified Language Model Pre-training for Natural Language Understanding and Generation</a></p>
<p><strong>论文地址</strong>：<a href="https://arxiv.org/pdf/1905.03197.pdf">https://arxiv.org/pdf/1905.03197.pdf</a></p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1681433605204-bc6414c4-f028-4ded-952b-f6c712c07cf0.png" alt=""></p>
<p><strong>预训练阶段</strong>：</p>
<ol>
<li><p>输入：Token Embedding、Position Embedding、Segment Embedding（区分任务目标）</p>
</li>
<li><p>预训练目标：随机掩码[MASK]语言模型+NSP</p>
</li>
<li><p>单向语言模型：predict the masked token of “x1x2 [MASK] x4”, only tokens x1, x2 and itself can be used.</p>
</li>
<li><p>双向语言模型：predict the masked token of “x1x2 [MASK] x4”, all tokens can be used.</p>
</li>
<li><p>Seq2Seq语言模型： given source segment t1t2 and its target segment t3t4t5, we feed input “[SOS] t1 t2 [EOS] t3 t4 t5 [EOS]” into the model. While both t1 and t2 have access to the first four tokens, including [SOS] and [EOS], t4 can only attend to the first six tokens.</p>
</li>
<li><p>训练过程：1/3训练双向语言模型；1/3训练Seq2Seq语言模型；1/6训练从左到右单向语言模型；1/6训练从右到左单向语言模型；BERT-Large作为初始化模型；15%概率掩码，80%为[MASK]，10%为随机Token，10%保持不变</p>
</li>
</ol>
<p><strong>微调阶段</strong>：</p>
<ol>
<li>NLU任务：类似于BERT，取[SOS]表征句子输入</li>
<li>NLG任务：“[SOS] S1 [EOS] S2 [EOS]”. The model is fine-tuned by masking some percentage of tokens in the target sequence at random, and learning to recover the masked words. EOS也会被MASK</li>
</ol>
<h1 id="MOSS"><a href="#MOSS" class="headerlink" title="MOSS"></a>MOSS</h1><p>MOSS是一个支持中英双语和多种插件的开源对话语言模型，<code>moss-moon</code>系列模型具有160亿参数，在FP16精度下可在单张A100/A800或两张3090显卡运行，在INT4/8精度下可在单张3090显卡运行。MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。</p>
<p>以下根据<a href="https://www.zhihu.com/question/596908242/answer/2994534005">知乎回答</a>总结MOSS的训练过程：</p>
<h2 id="MOSS-001-OpenChat-001"><a href="#MOSS-001-OpenChat-001" class="headerlink" title="MOSS 001(OpenChat 001)"></a>MOSS 001(OpenChat 001)</h2><ul>
<li><p><strong>数据来源</strong>：从OpenAI的论文附录里扒了一些它们API收集到的user prompt，然后用类似Self-Instruct的思路用text-davinci-003去扩展出大约40万对话数据。</p>
</li>
<li><p><strong>基座模型</strong>：16B基座（CodeGen）</p>
</li>
<li><p><strong>实验结果</strong>：一月份的OpenChat 001就已经具备了指令遵循能力和多轮能力，而且还惊喜的发现它具有很强的跨语言对齐能力，它的基座预训练语料中几乎不存在中文，但是却可以理解中文并用英文回答。</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/wqrnAr.jpg" alt="wqrnAr"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/zNXvlX.jpg" alt="zNXvlX"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/WcZoK6.jpg" alt="WcZoK6"></p>
<h2 id="MOSS-002"><a href="#MOSS-002" class="headerlink" title="MOSS 002"></a>MOSS 002</h2><ul>
<li><p><strong>优化点</strong>：OpenChat 001不具备中文能力，不知道关于自己的信息（比如名字、能力等），且安全性较低</p>
</li>
<li><p><strong>数据来源</strong>：一方面加入了约30B中文token继续训练基座，另一方面也加入了大量中英文<a href="https://www.zhihu.com/search√q=helpfulness&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2994534005%7D">helpfulness</a>, honesty, harmlessness对话数据，这部分数据共计116万条对话，目前也全部已在huggingface开源：<a href="https://huggingface.co/datasets/fnlp/moss-002-sft-data">fnlp/moss-002-sft-data · Datasets at Hugging Face</a></p>
</li>
</ul>
<h2 id="MOSS-003"><a href="#MOSS-003" class="headerlink" title="MOSS 003"></a>MOSS 003</h2><ul>
<li><p><strong>优化点1</strong>：继续加大中文语料的预训练，截止目前MOSS 003的基座语言模型已经在100B中文token上进行了训练，总训练token数量达到700B，其中还包含约300B代码。</p>
</li>
<li><p><strong>优化点2</strong>：在开放内测后，我们也收集了一些用户数据，我们发现真实中文世界的用户意图和OpenAI InstructGPT论文中披露的user prompt分布有较大差异（这不仅与用户来自的国家差异有关，也跟产品上线时间有关，早期产品采集的数据中存在大量对抗性和测试性输入），于是我们以这部分真实数据作为seed重新生成了约110万常规对话数据，涵盖更细粒度的helpfulness数据和更广泛的harmlessness数据。此外，还构造了约30万插件增强的对话数据，目前已包含搜索引擎、文生图、计算器、方程求解等。</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/tFzNz9.jpg" alt="tFzNz9"></p>
<h1 id="基座模型选取"><a href="#基座模型选取" class="headerlink" title="基座模型选取"></a>基座模型选取</h1><p>基于训练一个中文ChatGPT模型的出发点，需要选用具备<strong>中文支持性高、模型参数量大且已开源</strong>的基座模型。我们的候选项包括GLM、LLAMA、MOSS、BLOOM、CPM、闻仲模型，接下来将在这些候选项中进行比较，以确定最终选项。</p>
<ul>
<li><p>首先是CPM模型，其训练语料中，文章、对话语料居多，模型参数量较少，微调后的模型泛化性较差，生成结果与对话风格较大；</p>
</li>
<li><p>另外，闻仲模型，模型参数量同样较小；</p>
</li>
<li><p>接下来，LLAMA模型，尽管该模型在英文支持性方面表现不错，但据统计其Tokenizer中仅包括约700个中文字符，中文支持性较差；</p>
</li>
<li><p>BLOOM模型虽然支持多语言，但是其中文语料仅占16%，这意味着其可能无法提供足够的中文知识；</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/d8h2ky.png" alt="d8h2ky"></p>
<ul>
<li>GLM模型则是支持中英文双语的130B模型，是亚洲唯一入选斯坦福大模型评测的模型。GLM模型在中文支持性和模型参数量方面表现出色，此外，该模型已开源，可以为我们提供强大的基座模型。</li>
</ul>
<p>综上所述，GLM模型是复刻中文ChatGPT的一个较优基座模型选择。</p>
<p>当然目前也有不少基于其他基座模型的工作，比如：</p>
<p><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca">GitHub - ymcui/Chinese-LLaMA-Alpaca: 中文LLaMA&amp;Alpaca大语言模型+本地CPU/GPU部署 (Chinese LLaMA &amp; Alpaca LLMs)</a></p>
<p><a href="https://github.com/LianjiaTech/BELLE">GitHub - LianjiaTech/BELLE: BELLE: Be Everyone’s Large Language model Engine（开源中文对话大模型）</a></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文着重探讨如何选取一个好的预训练语言模型作为基座，最终选择在中文支持性和模型参数量方面表现出色的GLM模型。后续系列包括：</p>
<ul>
<li>复刻ChatGPT语言模型系列-（二）参数高效微调</li>
<li>复刻ChatGPT语言模型系列-（三）指令学习微调</li>
<li>复刻ChatGPT语言模型系列-（四）文本生成解码</li>
<li>复刻ChatGPT语言模型系列-（五）强化学习RLHF</li>
<li>复刻ChatGPT语言模型系列-（六）LLM模型评估</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>ChatGPT</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>复刻ChatGPT语言模型系列-（四）文本生成解码</title>
    <url>/2023/05/16/2023-05-08-%E5%A4%8D%E5%88%BBChatGPT%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%B3%BB%E5%88%97-%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%A3%E7%A0%81/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本系列将包括以下内容：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/627832567">复刻ChatGPT语言模型系列-（一）基座模型选取</a></li>
<li>复刻ChatGPT语言模型系列-（二）参数高效微调</li>
<li>复刻ChatGPT语言模型系列-（三）指令学习微调</li>
<li><a href="https://zhuanlan.zhihu.com/p/629909354">复刻ChatGPT语言模型系列-（四）文本生成解码</a></li>
<li>复刻ChatGPT语言模型系列-（五）强化学习RLHF</li>
<li>复刻ChatGPT语言模型系列-（六）LLM模型评估</li>
</ul>
<p>在文本生成领域，解码是一个关键的步骤，它决定了生成式模型输出的质量和多样性。在之前的几篇博文中，我们已经介绍了生成式模型的基本原理和训练过程。而在本系列的第四篇博文中，我们将深入探讨文本生成中的解码方式。</p>
<p>一旦我们获得了一个生成式模型，我们需要对其输出进行解码，以得到最终的生成文本。然而，在解码的过程中，我们可能会面临一些问题。其中之一是生成重复的内容，这使得生成的文本缺乏多样性和创造性。为了解决这个问题，我们可以尝试开启采样机制，同时引入repetition_penalty惩罚，以减少生成重复的倾向。</p>
<p>另一个常见的问题是生成的随机性不够高，导致生成的文本过于保守和可预测。为了增加文本生成的多样性，我们可以调整一些参数。例如，可以将top_p的值调小，这会使模型在生成词语时只考虑概率较高的选项；同时，可以将top_k的值调大，以增加模型的选择余地。此外，还可以适当降低温度系数，使模型生成的文本更加确定性。</p>
<p>本文将详细介绍这些解码方式，帮助您在文本生成任务中获得更好的结果。文中的所有功能均可用于 <em>自回归</em> 语言生成任务。简单复习一下， <em>自回归</em> 语言生成是基于如下假设: 一个文本序列的概率分布可以分解为每个词基于其上文的条件概率的乘积。</p>
<script type="math/tex; mode=display">
P\left(w_{1: T} \mid W_0\right)=\prod_{t=1}^T P\left(w_t \mid w_{1: t-1}, W_0\right), \text { 其中 } w_{1: 0}=\emptyset,</script><p>上式中，$W_0$ 是初始 <em>上下文</em> 单词序列。文本序列的长度 $T$ 通常时变的，并且对应于时间步 $t=T$。$P(w_{t} | w_{1: t- 1}, W_{0})$ 的词表中已包含 终止符 (End Of Sequence，EOS)。</p>
<p>我们会介绍目前最常用的解码方法，主要有 <em>贪心搜索 (Greedy search)</em>、<em>波束搜索 (Beam search)</em>、<em>Top-K 采样 (Top-K sampling)</em> 以及 <em>Top-p 采样 (Top-p sampling)</em>。几种方式的简介如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>解码方法</th>
<th>简介</th>
<th>可视化</th>
</tr>
</thead>
<tbody>
<tr>
<td>贪心搜索</td>
<td>从单词开始，每次都选择当前条件概率最高的词作为输出，以此往后生成单词序列。</td>
<td><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ZgkcnW.jpg" alt="贪心搜索"></td>
</tr>
<tr>
<td>波束搜索</td>
<td>在每个时间步保留最可能的 num_beams 个词，最终从中选择概率最高的序列来降低丢失潜在高概率序列的风险。</td>
<td><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/DnHQwq.jpg" alt="波束搜索"></td>
</tr>
<tr>
<td>TOP-K采样</td>
<td>选择概率最大的 K 个词，重新归一化这 K 个词的概率，选出一个词进行采样。这样，词集的大小是固定的，即 K。</td>
<td><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/HhSR0j.jpg" alt="TOP-K采样"></td>
</tr>
<tr>
<td>TOP-P采样</td>
<td>在累积概率超过概率 p 的最小单词集中进行采样，然后在这组词中重新分配概率。这样，词集的大小是动态的，可以根据下一个词的概率分布动态增加和减少。</td>
<td><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/lrNIF9.jpg" alt="TOP-P采样"></td>
</tr>
</tbody>
</table>
</div>
<span id="more"></span>
<h1 id="贪心搜索"><a href="#贪心搜索" class="headerlink" title="贪心搜索"></a>贪心搜索</h1><p>贪心搜索在每个时间步 $t$ 都简单地选择概率最高的词作为当前输出词: $w_t = argmax_{w}P(w | w_{1:t-1})$ ，如下图所示。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ZgkcnW.jpg" alt="贪心搜索"></p>
<p>从单词 $\text{“The”}$ 开始，算法在第一步贪心地选择条件概率最高的词 $\text{“nice”}$ 作为输出，依此往后。最终生成的单词序列为 $(\text{“The”}, \text{“nice”}, \text{“woman”})$，其联合概率为 $0.5 \times 0.4 = 0.2$。</p>
<p>下面，我们输入文本序列 $(\text{“I”}, \text{“enjoy”}, \text{“walking”}, \text{“with”}, \text{“my”}, \text{“cute”}, \text{“dog”})$ 给 GPT2 模型，让模型生成下文。我们以此为例看看如何在 transformers 中使用贪心搜索:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># encode context the generation is conditioned on</span></span><br><span class="line">input_ids = tokenizer.encode(<span class="string">&#x27;I enjoy walking with my cute dog&#x27;</span>, return_tensors=<span class="string">&#x27;tf&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># generate text until the output length (which includes the context length) reaches 50</span></span><br><span class="line">greedy_output = model.generate(input_ids, max_length=<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(greedy_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with my dog. I&#x27;m not sure if I&#x27;ll ever be able to walk with my dog.</span><br><span class="line"></span><br><span class="line">I&#x27;m not sure if I&#x27;ll</span><br></pre></td></tr></table></figure>
<p>好，我们已经用 GPT2 生成了第一个短文本😊。根据上文生成的单词是合理的，但模型很快开始输出重复的文本！这在语言生成中是一个非常普遍的问题，在贪心搜索和波束搜索中似乎更是如此 - 详见  <a href="https://arxiv.org/abs/1610.02424">Vijayakumar 等人，2016</a> 和 <a href="https://arxiv.org/abs/1701.03185">Shao 等人，2017</a> 的论文。</p>
<p>贪心搜索的主要缺点是它错过了隐藏在低概率词后面的高概率词，如上图所示:</p>
<p>条件概率为 $0.9$ 的单词 $\text{“has”}$ 隐藏在单词 $\text{“dog”}$ 后面，而 $\text{“dog”}$ 因为在 t=1 时条件概率值只排第二所以未被选择，因此贪心搜索会错过序列 $\text{“The”}, \text {“dog”}, \text{“has”}$ 。</p>
<h1 id="波束搜索"><a href="#波束搜索" class="headerlink" title="波束搜索"></a>波束搜索</h1><p>波束搜索通过在每个时间步保留最可能的 <code>num_beams</code> 个词，并从中最终选择出概率最高的序列来降低丢失潜在的高概率序列的风险。以 <code>num_beams=2</code> 为例:</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/DnHQwq.jpg" alt="波束搜索"></p>
<p>在时间步 1，除了最有可能的假设 $(\text{“The”}, \text{“nice”})$，波束搜索还跟踪第二可能的假设 $(\text{“The”}, \text{“dog”})$。在时间步 2，波束搜索发现序列 $(\text{“The”}, \text{“dog”}, \text{“has”})$ 概率为$0.36$，比 $(\text{“The”}, \text{“nice”}, \text{“woman”})$ 的 $0.2$ 更高。太棒了，在我们的例子中它已经找到了最有可能的序列！</p>
<p>波束搜索一般都会找到比贪心搜索概率更高的输出序列，但仍不保证找到全局最优解。</p>
<p>让我们看看如何在 transformers 中使用波束搜索。我们设置 num_beams &gt; 1 和 early_stopping=True 以便在所有波束达到 EOS 时直接结束生成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># activate beam search and early_stopping</span></span><br><span class="line">beam_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    num_beams=<span class="number">5</span>, </span><br><span class="line">    early_stopping=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(beam_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;m not sure if I&#x27;ll ever be able to walk with him again. I&#x27;m not sure if I&#x27;ll</span><br></pre></td></tr></table></figure>
<p>虽然结果比贪心搜索更流畅，但输出中仍然包含重复。一个简单的补救措施是引入 <em>n-grams</em> (即连续 n 个词的词序列) 惩罚，该方法是由 <a href="https://arxiv.org/abs/1705.04304">Paulus 等人 (2017)</a> 和 <a href="https://arxiv.org/abs/1701.02810">Klein 等人 (2017)</a> 引入的。最常见的 <em>n-grams</em> 惩罚是确保每个 <em>n-gram</em> 都只出现一次，方法是如果看到当前候选词与其上文所组成的 <em>n-gram</em> 已经出现过了，就将该候选词的概率设置为 0。</p>
<p>我们可以通过设置 <code>no_repeat_ngram_size=2</code> 来试试，这样任意 <em>2-gram</em> 不会出现两次:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set no_repeat_ngram_size to 2</span></span><br><span class="line">beam_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    num_beams=<span class="number">5</span>, </span><br><span class="line">    no_repeat_ngram_size=<span class="number">2</span>, </span><br><span class="line">    early_stopping=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(beam_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to take a break</span><br></pre></td></tr></table></figure>
<p>不错，看起来好多了！我们看到生成的文本已经没有重复了。但是，<em>n-gram</em> 惩罚使用时必须谨慎，如一篇关于 <em>纽约</em> 这个城市的文章就不应使用 <em>2-gram</em> 惩罚，否则，城市名称在整个文本中将只出现一次！</p>
<p>波束搜索的另一个重要特性是我们能够比较概率最高的几个波束，并选择最符合我们要求的波束作为最终生成文本。</p>
<p>在 <code>transformers</code> 中，我们只需将参数 <code>num_return_sequences</code> 设置为需返回的概率最高的波束的数量，记得确保 <code>num_return_sequences &lt;= num_beams</code>！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set return_num_sequences &gt; 1</span></span><br><span class="line">beam_outputs = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    num_beams=<span class="number">5</span>, </span><br><span class="line">    no_repeat_ngram_size=<span class="number">2</span>, </span><br><span class="line">    num_return_sequences=<span class="number">5</span>, </span><br><span class="line">    early_stopping=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># now we have 3 output sequences</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i, beam_output <span class="keyword">in</span> <span class="built_in">enumerate</span>(beam_outputs):</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, tokenizer.decode(beam_output, skip_special_tokens=<span class="literal">True</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">0: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to take a break</span><br><span class="line">1: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to get back to</span><br><span class="line">2: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with her again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to take a break</span><br><span class="line">3: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with her again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to get back to</span><br><span class="line">4: I enjoy walking with my cute dog, but I&#x27;m not sure if I&#x27;ll ever be able to walk with him again.</span><br><span class="line"></span><br><span class="line">I&#x27;ve been thinking about this for a while now, and I think it&#x27;s time for me to take a step</span><br></pre></td></tr></table></figure>
<p>如我们所见，五个波束彼此之间仅有少量差别 —— 这在仅使用 5 个波束时不足为奇。</p>
<p>开放域文本生成的研究人员最近提出了几个理由来说明对该领域而言波束搜索可能不是最佳方案:</p>
<ul>
<li><p>在机器翻译或摘要等任务中，因为所需生成的长度或多或少都是可预测的，所以波束搜索效果比较好 - 参见 <a href="https://arxiv.org/abs/1808.10006">Murray 等人 (2018)</a> 和 <a href="https://arxiv.org/abs/1808.09582">Yang 等人 (2018)</a> 的工作。但开放域文本生成情况有所不同，其输出文本长度可能会有很大差异，如对话和故事生成的输出文本长度就有很大不同。</p>
</li>
<li><p>我们已经看到波束搜索已被证明存在重复生成的问题。在故事生成这样的场景中，很难用 <em>n-gram</em> 或其他惩罚来控制，因为在“不重复”和最大可重复 <em>n-grams</em> 之间找到一个好的折衷需要大量的微调。</p>
</li>
<li><p>正如 <a href="https://arxiv.org/abs/1904.09751">Ari Holtzman 等人 (2019)</a> 所论证的那样，高质量的人类语言并不遵循最大概率法则。换句话说，作为人类，我们希望生成的文本能让我们感到惊喜，而可预测的文本使人感觉无聊。论文作者画了一个概率图，很好地展示了这一点，从图中可以看出人类文本带来的惊喜度比波束搜索好不少。</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/R2bvXA.jpg" alt="R2bvXA"></p>
<p>因此，让我们开始玩点刺激的，引入一些随机性🤪。</p>
<h1 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h1><p>在其最基本的形式中，采样意味着根据当前条件概率分布随机选择输出词 $w_t$:</p>
<script type="math/tex; mode=display">
w_t​∼P(w∣w_{1:t−1}​)</script><p>继续使用上文中的例子，下图可视化了使用采样生成文本的过程。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/f4TvkR.jpg" alt="f4TvkR"></p>
<p>很明显，使用采样方法时文本生成本身不再是 <em>确定性的</em>。单词 $\text{“car”}$ 从条件概率分布 $P(w | \text{“The”})$ 中采样而得，而 $\text{“drives”}$ 则采样自 $P(w | \text{“The”}, \text{“car”})$。</p>
<p>在 <code>transformers</code> 中，我们设置 <code>do_sample=True</code> 并通过设置 <code>top_k=0</code> 停用 <em>Top-K</em> 采样 (稍后详细介绍)。在下文中，为便于复现，我们会固定 <code>random_seed=0</code>，但你可以在自己的模型中随意更改 <code>random_seed</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># activate sampling and deactivate top_k by setting top_k sampling to 0</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_k=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog. He just gave me a whole new hand sense.&quot;</span><br><span class="line"></span><br><span class="line">But it seems that the dogs have learned a lot from teasing at the local batte harness once they take on the outside.</span><br><span class="line"></span><br><span class="line">&quot;I take</span><br></pre></td></tr></table></figure>
<p>有意思！生成的文本看起来不错 - 但仔细观察会发现它不是很连贯。<em>3-grams</em> <em>new hand sense</em> 和 <em>local batte harness</em> 非常奇怪，看起来不像是人写的。这就是对单词序列进行采样时的大问题: 模型通常会产生不连贯的乱码，<em>参见</em> <a href="https://arxiv.org/abs/1904.09751">Ari Holtzman 等人 (2019)</a> 的论文。</p>
<p>缓解这一问题的一个技巧是通过降低所谓的 <a href="https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max">softmax</a> 的“温度”使分布 $P(w|w_{1:t-1})$ 更陡峭。而降低“温度”，本质上是增加高概率单词的似然并降低低概率单词的似然。</p>
<p>将温度应用到于我们的例子中后，结果如下图所示。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/vt9hPy.jpg" alt="vt9hPy"></p>
<p>$t=1$ 时刻单词的条件分布变得更加陡峭，几乎没有机会选择单词 $\text{“car”}$ 了。</p>
<p>让我们看看如何通过设置 <code>temperature=0.7</code> 来冷却生成过程:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use temperature to decrease the sensitivity to low probability candidates</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_k=<span class="number">0</span>, </span><br><span class="line">    temperature=<span class="number">0.7</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog, but I don&#x27;t like to be at home too much. I also find it a bit weird when I&#x27;m out shopping. I am always away from my house a lot, but I do have a few friends</span><br></pre></td></tr></table></figure>
<p>好，奇怪的 n-gram 变少了，现在输出更连贯了！虽然温度可以使分布的随机性降低，但极限条件下，当“温度”设置为 $0$ 时，温度缩放采样就退化成贪心解码了，因此会遇到与贪心解码相同的问题。</p>
<h1 id="TOP-K采样"><a href="#TOP-K采样" class="headerlink" title="TOP-K采样"></a>TOP-K采样</h1><p><a href="https://arxiv.org/pdf/1805.04833.pdf">Fan 等人 (2018)</a> 的论文介绍了一种简单但非常强大的采样方案，称为 <strong><em>Top-K</em></strong> 采样。在 <em>Top-K</em> 采样中，概率最大的 <em>K</em> 个词会被选出，然后这 <em>K</em> 个词的概率会被重新归一化，最后就在这重新被归一化概率后的 <em>K</em> 个词中采样。 GPT2 采用了这种采样方案，这也是它在故事生成这样的任务上取得成功的原因之一。</p>
<p>我们将上文例子中的候选单词数从 3 个单词扩展到 10 个单词，以更好地说明 <em>Top-K</em> 采样。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/HhSR0j.jpg" alt="TOP-K采样"></p>
<p>设 $K = 6$，即我们将在两个采样步的采样池大小限制为 6 个单词。我们定义 6 个最有可能的词的集合为 $V_{\text{top-K}}$。在第一步中，$V_{\text{top-K}}$ 仅占总概率的大约三分之二，但在第二步，它几乎占了全部的概率。同时，我们可以看到在第二步该方法成功地消除了那些奇怪的候选词 $(\text{“not”}, \text{“the”}, \text{“small”}, \text{“told”})$。</p>
<p>我们以设置 <code>top_k=50</code> 为例看下如何在 <code>transformers</code> 库中使用 <em>Top-K</em>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set top_k to 50</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_k=<span class="number">50</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog. It&#x27;s so good to have an environment where your dog is available to share with you and we&#x27;ll be taking care of you.</span><br><span class="line"></span><br><span class="line">We hope you&#x27;ll find this story interesting!</span><br><span class="line"></span><br><span class="line">I am from</span><br></pre></td></tr></table></figure>
<p>相当不错！该文本可以说是迄今为止生成的最 “<em>像人</em>“ 的文本。现在还有一个问题，<em>Top-K</em> 采样不会动态调整从需要概率分布 $P(w|w_{1:t-1})$ 中选出的单词数。这可能会有问题，因为某些分布可能是非常尖锐 (上图中右侧的分布)，而另一些可能更平坦 (上图中左侧的分布)，所以对不同的分布使用同一个绝对数 <em>K</em> 可能并不普适。</p>
<p>在 $t=1$ 时，<em>Top-K</em> 将 $(\text{“people”}, \text{“big”}, \text{“house”}, \text{“cat”})$ 排出了采样池，而这些词似乎是合理的候选词。另一方面，在$t=2$ 时，该方法却又把不太合适的 $(\text{“down”}, \text{“a”})$ 纳入了采样池。因此，将采样池限制为固定大小 <em>K</em> 可能会在分布比较尖锐的时候产生胡言乱语，而在分布比较平坦的时候限制模型的创造力。这一发现促使 <a href="https://arxiv.org/abs/1904.09751">Ari Holtzman 等人 (2019)</a> 发明了 <strong>Top-p</strong>- 或 <strong>核</strong>- 采样。</p>
<h1 id="TOP-P-核-采样"><a href="#TOP-P-核-采样" class="headerlink" title="TOP-P(核)采样"></a>TOP-P(核)采样</h1><p>在 <em>Top-p</em> 中，采样不只是在最有可能的 <em>K</em> 个单词中进行，而是在累积概率超过概率 <em>p</em> 的最小单词集中进行。然后在这组词中重新分配概率质量。这样，词集的大小 (<em>又名</em> 集合中的词数) 可以根据下一个词的概率分布动态增加和减少。好吧，说的很啰嗦，一图胜千言。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/lrNIF9.jpg" alt="TOP-P采样"></p>
<p>假设 $p=0.92$，<em>Top-p</em> 采样对单词概率进行降序排列并累加，然后选择概率和首次超过 $p=92%$ 的单词集作为采样池，定义为 $V_{\text{top-p}}$。在 $t=1$ 时 $V_{\text{top-p}}$ 有 9 个词，而在 $t=2$ 时它只需要选择前 3 个词就超过了 92%。其实很简单吧！可以看出，在单词比较不可预测时，它保留了更多的候选词，<em>如</em> $P(w | \text{“The”})$，而当单词似乎更容易预测时，只保留了几个候选词，<em>如</em> $P(w | \text{“The”}, \text{“car”})$。</p>
<p>好的，是时候看看它在 <code>transformers</code> 里怎么用了！我们可以通过设置 <code>0 &lt; top_p &lt; 1</code> 来激活 <em>Top-p</em> 采样:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># deactivate top_k sampling and sample only from 92% most likely words</span></span><br><span class="line">sample_output = model.generate(</span><br><span class="line">    input_ids, </span><br><span class="line">    do_sample=<span class="literal">True</span>, </span><br><span class="line">    max_length=<span class="number">50</span>, </span><br><span class="line">    top_p=<span class="number">0.92</span>, </span><br><span class="line">    top_k=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(sample_output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">I enjoy walking with my cute dog. He will never be the same. I watch him play.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Guys, my dog needs a name. Especially if he is found with wings.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">What was that? I had a lot o</span><br></pre></td></tr></table></figure>
<p>太好了，这看起来跟人类写的差不多了，虽然还不算完全是。</p>
<p>虽然从理论上讲， <em>Top-p</em> 似乎比 <em>Top-K</em> 更优雅，但这两种方法在实践中都很有效。 <em>Top-p</em> 也可以与 <em>Top-K</em> 结合使用，这样可以避免排名非常低的词，同时允许进行一些动态选择。</p>
<p>最后，如果想要获得多个独立采样的输出，我们可以 <em>再次</em> 设置参数 <code>num_return_sequences &gt; 1</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed to reproduce results. Feel free to change the seed though to get different results</span></span><br><span class="line">tf.random.set_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3</span></span><br><span class="line">sample_outputs = model.generate(</span><br><span class="line">    input_ids,</span><br><span class="line">    do_sample=<span class="literal">True</span>,</span><br><span class="line">    max_length=<span class="number">50</span>,</span><br><span class="line">    top_k=<span class="number">50</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    num_return_sequences=<span class="number">3</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output:\n&quot;</span> + <span class="number">100</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i, sample_output <span class="keyword">in</span> <span class="built_in">enumerate</span>(sample_outputs):</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, tokenizer.decode(sample_output, skip_special_tokens=<span class="literal">True</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Output:</span><br><span class="line">----------------------------------------------------------------------------------------------------</span><br><span class="line">0: I enjoy walking with my cute dog. It&#x27;s so good to have the chance to walk with a dog. But I have this problem with the dog and how he&#x27;s always looking at us and always trying to make me see that I can do something</span><br><span class="line">1: I enjoy walking with my cute dog, she loves taking trips to different places on the planet, even in the desert! The world isn&#x27;t big enough for us to travel by the bus with our beloved pup, but that&#x27;s where I find my love</span><br><span class="line">2: I enjoy walking with my cute dog and playing with our kids,&quot; said David J. Smith, director of the Humane Society of the US.</span><br><span class="line"></span><br><span class="line">&quot;So as a result, I&#x27;ve got more work in my time,&quot; he said.</span><br></pre></td></tr></table></figure>
<p>很酷，现在你拥有了所有可以在 <code>transformers</code> 里用模型来帮你写故事的工具了！</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在开放域语言生成场景中，作为最新的解码方法， <em>top-p</em> 和 <em>top-K</em> 采样于传统的 <em>贪心</em> 和 <em>波束</em> 搜索相比，似乎能产生更流畅的文本。但，最近有更多的证据表明 <em>贪心</em> 和 <em>波束</em> 搜索的明显缺陷 - 主要是生成重复的单词序列 - 是由模型 (特别是模型的训练方式) 引起的，而不是解码方法， <em>参见</em><a href="https://arxiv.org/pdf/1908.04319.pdf">Welleck 等人 (2019)</a> 的论文。此外，如 <a href="https://arxiv.org/abs/2002.02492">Welleck 等人 (2020)</a> 的论文所述，看起来 <em>top-K</em> 和 <em>top-p</em> 采样也会产生重复的单词序列。</p>
<p>在 <a href="https://arxiv.org/pdf/1908.04319.pdf">Welleck 等人 (2019)</a> 的论文中，作者表明，根据人类评估，在调整训练目标后，波束搜索相比 <em>Top-p</em> 采样能产生更流畅的文本。</p>
<h1 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h1><p><code>generate</code> 方法还有几个正文未提及的参数，这里我们简要解释一下它们！</p>
<ul>
<li><p><code>min_length</code> 用于强制模型在达到 <code>min_length</code> 之前不生成 EOS。这在摘要场景中使用得比较多，但如果用户想要更长的文本输出，也会很有用。</p>
</li>
<li><p><code>repetition_penalty</code> 可用于对生成重复的单词这一行为进行惩罚。它首先由 <a href="https://arxiv.org/abs/1909.05858">Keskar 等人 (2019)</a> 引入，在 <a href="https://arxiv.org/pdf/1908.04319.pdf">Welleck 等人 (2019)</a> 的工作中，它是训练目标的一部分。它可以非常有效地防止重复，但似乎对模型和用户场景非常敏感，其中一个例子见 Github 上的 <a href="https://github.com/huggingface/transformers/pull/2303">讨论</a>。 </p>
</li>
<li><p><code>attention_mask</code> 可用于屏蔽填充符。 </p>
</li>
<li><p><code>pad_token_id</code>、<code>bos_token_id</code>、<code>eos_token_id</code>: 如果模型默认没有这些 token，用户可以手动选择其他 token id 来表示它们。</p>
</li>
</ul>
<p>更多信息，请查阅 <code>generate</code> 函数 <a href="https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate">手册</a>。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://huggingface.co/blog/zh/how-to-generate">如何生成文本：通过 Transformers 用不同的解码方法生成文本</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>ChatGPT</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM训练指南-Token及模型参数准备</title>
    <url>/2023/06/13/2023-06-13-LLM%E8%AE%AD%E7%BB%83%E6%8C%87%E5%8D%97-Token%E5%8F%8A%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E5%87%86%E5%A4%87/</url>
    <content><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>在当今AI领域，大型预训练语言模型已成为一种关键技术，推动了自然语言处理领域的进步。本文旨在提供一个辅助指南，如何准备预训练模型大小、数据集大小，以及帮助读者了解如何提升预训练模型的表现。本文将分析模型表现的依赖因素，以及预训练Token是否需要重复和SFT模型所需数据量等问题。</p>
<p>本文得出的结论如下，详细过程见下文分析：</p>
<p><strong>预训练模型表现影响因素：</strong></p>
<ul>
<li>模型表现强依赖于模型规模(模型参数量N(Embedding除外)、训练Token数D、训练总计算量C)；</li>
<li>平滑幂定律：模型表现与三个因子均遵循幂定律，不受另外两个因子限制；</li>
<li>在给定计算量预算下，模型参数量以及训练Token数应该同比提升，对应模型参数量需要的训练Token数如下：</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:right">Parameters</th>
<th style="text-align:right">FLOPs</th>
<th style="text-align:right">FLOPs (in Gopher unit)</th>
<th style="text-align:right">Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">400 Million</td>
<td style="text-align:right">1.92e+19</td>
<td style="text-align:right">1//29,968</td>
<td style="text-align:right">8.0 Billion</td>
</tr>
<tr>
<td style="text-align:right">1 Billion</td>
<td style="text-align:right">1.21e+20</td>
<td style="text-align:right">1//4,761</td>
<td style="text-align:right">20.2 Billion</td>
</tr>
<tr>
<td style="text-align:right">10 Billion</td>
<td style="text-align:right">1.23e+22</td>
<td style="text-align:right">1//46</td>
<td style="text-align:right">205.1 Billion</td>
</tr>
<tr>
<td style="text-align:right">67 Billion</td>
<td style="text-align:right">5.76e+23</td>
<td style="text-align:right">1</td>
<td style="text-align:right">1.5 Trillion</td>
</tr>
<tr>
<td style="text-align:right">175 Billion</td>
<td style="text-align:right">3.85e+24</td>
<td style="text-align:right">6.7</td>
<td style="text-align:right">3.7 Trillion</td>
</tr>
<tr>
<td style="text-align:right">280 Billion</td>
<td style="text-align:right">9.90e+24</td>
<td style="text-align:right">17.2</td>
<td style="text-align:right">5.9 Trillion</td>
</tr>
<tr>
<td style="text-align:right">520 Billion</td>
<td style="text-align:right">3.43e+25</td>
<td style="text-align:right">59.5</td>
<td style="text-align:right">11.0 Trillion</td>
</tr>
<tr>
<td style="text-align:right">1 Trillion</td>
<td style="text-align:right">1.27e+26</td>
<td style="text-align:right">221.3</td>
<td style="text-align:right">21.2 Trillion</td>
</tr>
<tr>
<td style="text-align:right">10 Trillion</td>
<td style="text-align:right">1.30e+28</td>
<td style="text-align:right">22515.9</td>
<td style="text-align:right">216.2 Trillion</td>
</tr>
</tbody>
</table>
</div>
<p><strong>预训练数据Token重复的影响：</strong></p>
<ul>
<li>多轮epoch的训练会降低模型性能；</li>
<li>更大规模的数据集会缓解重复epochs对模型性能下降的影响；</li>
<li>提高数据集的质量也无法挽救重复训练带来的过拟合；</li>
<li>小计算量模型的过拟合趋势与大计算量的差不多；</li>
<li>多样的训练目标不一定减轻多Epoch的性能下降；</li>
<li>Dropout是一个被大语言模型忽视的正则技术，虽然慢，但是可以降低多epochs的影响；</li>
<li>在训练过程中逐渐使用dropout是有效的策略；</li>
</ul>
<p><strong>SFT需要训练Token数：</strong></p>
<ul>
<li>少量高质量、多样性的数据，也可以训练出效果优秀的SFT模型</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/6g7mxD.png" alt="6g7mxD"><br><span id="more"></span></p>
<h1 id="预训练模型表现的影响因素"><a href="#预训练模型表现的影响因素" class="headerlink" title="预训练模型表现的影响因素"></a>预训练模型表现的影响因素</h1><p>该部分将通过OpenAI和DeepMind的两篇文章介绍影响模型性能的因素，以及在给定预算下如何准备模型、训练数据。有意思的是OpenAI先提出的观念在后来被DeepMind证伪，二者的观念对比如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>OpenAI</th>
<th>DeepMind</th>
</tr>
</thead>
<tbody>
<tr>
<td>结论(假设计算预算增加10倍)</td>
<td>$N \propto C_{\min }^{0.73}, B \propto C_{\min }^{0.24}, \text { and } S \propto C_{\min }^{0.03}$，根据该公式,模型大小增加$10^{0.73}=5.5$倍，训练Token增加$10^{0.24}=1.8$倍</td>
<td>模型大小及训练Token同比增加$10^{0.5}=3.16$倍</td>
</tr>
<tr>
<td>参数设置</td>
<td>模型大小768~1.5B；Token数22M~23B；大部分参数量在100M</td>
<td>模型大小70M-10B；Token数1B～100B；大部分参数量超过500M</td>
</tr>
<tr>
<td>实验设置</td>
<td>固定学习率设置方法</td>
<td>cosine余弦学习率循环长度与Token一致</td>
</tr>
</tbody>
</table>
</div>
<p>原文对应如下：</p>
<blockquote>
<p>Specifically, given a 10× increase computational budget, they suggests that the size of the model should increase 5.5× while the number of training tokens should only increase 1.8×. Instead, we find that model size and the number of training tokens should be scaled in equal proportions.</p>
</blockquote>
<p><strong>DeepMind指出OpenAI得出错误结论的原因可能源于以下几点：</strong></p>
<ol>
<li>对于不同训练Token数，OpenAI采用了同样的学习率调度策略；</li>
<li>OpenAI实验的模型参数量普遍较小；</li>
</ol>
<p>原文对应如下：</p>
<blockquote>
<p>For a fixed learning rate cosine schedule to 130B tokens, the intermediate loss estimates (for 0 &lt;&lt; 130B) are therefore overestimates of the loss of a model trained with a schedule length matching 0 . Using these intermediate losses results in underestimating the effectiveness of training models on less data than 130B tokens, and eventually contributes to the conclusion that model size should increase faster than training data size as compute budget increases. In contrast, our analysis predicts that both quantities should scale at roughly the same rate. Secondly, we include models with up to 16B parameters, as we observe that there is slight curvature in the FLOP-loss frontier (see Appendix E)—in fact, the majority of the models used in our analysis have more than 500 million parameters, in contrast the majority of runs in Kaplan et al. (2020) are significantly smaller—many being less than 100M parameters.</p>
</blockquote>
<h2 id="OpenAI-语言模型缩放法则"><a href="#OpenAI-语言模型缩放法则" class="headerlink" title="OpenAI:语言模型缩放法则"></a>OpenAI:语言模型缩放法则</h2><p><strong>论文标题</strong>：Scaling Laws for Neural Language Models</p>
<p><strong>论文链接</strong>：<a href="https://arxiv.org/pdf/2001.08361v1.pdf">https://arxiv.org/pdf/2001.08361v1.pdf</a></p>
<p><strong>研究背景</strong>：论文将从模型架构、模型大小、训练时所用的计算能力和可用于训练的数据等方面，以Transformer架构为重点，实证研究语言模型损失与这些因素之间的关系。</p>
<p>首先，让我们来介绍文章中展示的三张图，随后将概述文章的核心观点：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/CSejJ7.png" alt="CSejJ7"></p>
<p>图1表明，语言模型表现与模型参数量N(Embedding除外)、训练Token数D、训练总计算量C都呈现幂定律。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/H9U9Gz.png" alt="H9U9Gz"></p>
<p>图2表明，模型大小从768~1.5B在不同Token数(22M-23B)上训练的过程。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/EB9BUt.png" alt="EB9BUt"></p>
<p>图3表明，随着计算量增加需要了解计算资源如何高效的分配。论文中以计算能力增加10亿倍为例，对于最优效率训练，大部分计算资源用于增加模型大小，小部分用于数据增加，同时数据增加大部分用于增加batch大小，很少用于增加模型串行训练的step数。</p>
<p><strong>关键结论：</strong></p>
<ul>
<li>模型表现强依赖于模型规模(模型参数量N(Embedding除外)、训练Token数D、训练总计算量C)；</li>
<li>平滑幂定律：模型表现与三个因子均遵循幂定律，不受另外两个因子限制；</li>
<li>过拟合的通用性：当同时提升N/D时，模型性能提升；但如果固定其中一个，模型表现取决于比例N^{0.74}/D.(与下文DeepMind工作冲突)</li>
<li>训练的通用性：训练曲线遵循可预测的幂律，其参数独立于模型大小。通过推断训练曲线的早期部分，可以大致预测如果训练更长时间会达到的损失。</li>
<li>迁移学习表现与测试性能相关：当在不同于训练分布的文本上评估模型时，结果与训练验证集上的结果强相关，并且损失大致上存在一个恒定的偏差。换句话说，将模型迁移到不同的分布会产生一个恒定的偏差，模型在训练集上的表现越好，迁移学习的效果也越好。</li>
<li>样本效率：大模型比小模型更具样本效率，可以通过更少的优化步骤和更少的数据达到相同的性能水平。</li>
<li>收敛效率低：在固定的计算预算C下，但没有其他限制的情况下，通过训练非常大的模型并在收敛之前显著停止，可以获得最佳性能。因此，最大计算效率的训练将比基于更多样本训练小型模型收敛的效率高得多，随着训练计算的增加，数据需求增长非常缓慢，即 D C ^{0.27}。</li>
<li>最佳的batchsize：这些模型的理想batchsize仅与损失的幂指数相关，并且可以通过测量梯度噪声尺度[MKAT18]来确定；对于可以训练的最大模型，在收敛时Token数大约为1-2百万。</li>
</ul>
<p>总体来说，这些结果表明，随着适当地提高模型大小、数据和计算能力，语言建模性能会平稳、可预测地提高。更大的语言模型将比其他模型表现更好，并且更具样本效率。</p>
<p><strong>论文中缩放定律总结如下：</strong></p>
<ol>
<li>当训练Token数足够，模型参数量有限情况下，模型性能与模型参数的关系如下：</li>
</ol>
<script type="math/tex; mode=display">
L(N)=\left(N_{\mathrm{c}} / N\right)^{\alpha_N} ; \alpha_N \sim 0.076, \quad N_{\mathrm{c}} \sim 8.8 \times 10^{13} \text { (non-embedding parameters) }</script><ol>
<li>当模型参数量足够，训练Token数有限情况下，模型性能与模型参数的关系如下：</li>
</ol>
<script type="math/tex; mode=display">
L(D)=\left(D_{\mathrm{c}} / D\right)^{\alpha_D} ; \alpha_D \sim 0.095, \quad D_{\mathrm{c}} \sim 5.4 \times 10^{13} \text { (tokens) }</script><ol>
<li>当模型参数量、训练Token足够，计算量预算有限的情况下，模型性能与模型参数关系如下：</li>
</ol>
<script type="math/tex; mode=display">
L\left(C_{\min }\right)=\left(C_{\mathrm{c}}^{\min } / C_{\min }\right)^{\alpha_C^{\min }} ; \alpha_C^{\min } \sim 0.050, \quad C_{\mathrm{c}}^{\min } \sim 3.1 \times 10^8 \text { (PF-days) }</script><p>文中表明上述三个关系在多个数量级上均成立，他们的幂指数含义代表当对应参数成倍增加时，loss性能变化的幅度。比如当模型参数量提高两倍对应Loss缩放到：</p>
<script type="math/tex; mode=display">
L(2N)/L(N)=(N_ / (2N)^{\alpha_N})/(N_ / N^{\alpha_N})=2^{-\alpha_N}=2^{-0.076}=0.9486,分母的常数依赖于词表大小和分词方式，因此无实际含义。</script><ol>
<li>模型训练的batch_size与模型性能也呈现如下幂律关系：</li>
</ol>
<script type="math/tex; mode=display">
B_{\mathrm{crit}}(L)=\frac{B_*}{L^{1 / \alpha_B}}, \quad B_* \sim 2 \cdot 10^8 \text { tokens, } \alpha_B \sim 0.21</script><p>比如当L变一半的时候，batch_size变化如下：</p>
<script type="math/tex; mode=display">
B_{crit}(L/2) / B_{crit}(L)=(\frac{B_*}{（L/2）^{1 / \alpha_B}})/(\frac{B_*}{L^{1 / \alpha_B}})=2^{-\alpha_B}=2^{-0.21}=0.8645</script><ol>
<li>当提升模型大小后，为保证模型性能，需要提升训练Token数：</li>
</ol>
<script type="math/tex; mode=display">
D \propto N^{\frac{\alpha_N}{\alpha_D}} \sim N^{0.74}</script><p>根据模型性能与模型参数量N、训练Token数D的关系，可以得到模型性能与二者的联合关系：</p>
<script type="math/tex; mode=display">
L(N, D)=\left[\left(\frac{N_c}{N}\right)^{\frac{\alpha_N}{\alpha_D}}+\frac{D_c}{D}\right]^{\alpha_D}</script><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/D0bDKk.png" alt="D0bDKk"></p>
<p>图4中左边小图对应模型性能与模型参数量N、训练Token数D的关系，右边小图对应模型性能与给定模型、训练步数的关系，对应公式见上下文。</p>
<ol>
<li>模型性能与给定模型、训练步数的关系如下：</li>
</ol>
<script type="math/tex; mode=display">
L(N, S)=\left(\frac{N_c}{N}\right)^{\alpha_N}+\left(\frac{S_c}{S_{\min }(S)}\right)^{\alpha_S}

where S_c \approx 2.1 \times 10^3 and \alpha_S \approx 0.76</script><p>$S_{\min }(S) $is the minimum possible number of optimization steps (parameter updates)</p>
<ol>
<li>给定固定的计算量预算C，根据上述计算等式，模型参数量N、训练batch_size B、训练步数S、训练Token数D的增长关系如下：</li>
</ol>
<script type="math/tex; mode=display">
N \propto C^{\alpha_C^{\min } / \alpha_N}, \quad B \propto C^{\alpha_C^{\min } / \alpha_B}, \quad S \propto C_C^{\alpha_C^{\min } / \alpha_S}, \quad D=B \cdot S</script><script type="math/tex; mode=display">
\alpha_C^{\min }=1 /\left(1 / \alpha_S+1 / \alpha_B+1 / \alpha_N\right)</script><p>其中，经过拟合之后，各参数值如下：</p>
<script type="math/tex; mode=display">
N \propto C_{\min }^{0.73}, B \propto C_{\min }^{0.24}, \text { and } S \propto C_{\min }^{0.03}</script><p>这也意味着，当计算预算增加时，模型参数增加带来收益最大。</p>
<h2 id="DeepMind-大模型训练计算优化实证分析"><a href="#DeepMind-大模型训练计算优化实证分析" class="headerlink" title="DeepMind:大模型训练计算优化实证分析"></a>DeepMind:大模型训练计算优化实证分析</h2><p><strong>论文标题</strong>：Training Compute-Optimal Large Language Models</p>
<p><strong>论文链接</strong>：<a href="https://arxiv.org/pdf/2203.15556.pdf">https://arxiv.org/pdf/2203.15556.pdf</a></p>
<p><strong>核心问题</strong>：给定训练参数量的情况下，如何配置模型参数量以及训练Token数。</p>
<blockquote>
<p>Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens?</p>
</blockquote>
<p>在文中，作者探讨了三种不同的策略来评估和确定在给定训练参数量的情况下，最佳的模型参数量与训练 Token 数的配比：</p>
<ul>
<li><p>方法一：固定模型大小，改变训练Token数；</p>
</li>
<li><p>方法二：等值浮点运算能力剖析(IsoFLOP profiles)；</p>
</li>
<li><p>方法三：拟合参数损失函数；</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ndZKn7.png" alt="ndZKn7"></p>
<p>图1的含义介绍如下：</p>
<ul>
<li>横坐标代表训练的总计算量；</li>
<li>纵坐标代表模型的参数量；</li>
<li>黑色虚线代表Kaplan等人提出的缩放法则；</li>
<li>3条Approach方法代表本文预测的三条线；</li>
<li>Chinchiilla和Gopher是有着相同的横坐标，即拥有相同的训练计算量，而Chinchiilla在模型参数上是Gopher参数的1/4，在训练Token数是Gopher的4倍；</li>
</ul>
<p>这三种方法都得出了一致的结论，但这个结论与 OpenAI 提供的观点有所不同。</p>
<p><strong>下面同样通过几幅图介绍三种方式的结论：</strong></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/00VDed.png" alt="00VDed"></p>
<p>图2的含义介绍如下：</p>
<ul>
<li>实验方式：固定模型大小(模型大小70M到10B)，改变训练Token数；</li>
<li>第一个图横坐标代表训练的总计算量，纵坐标代表训练的Loss，曲线表示不同模型参数；</li>
<li>第二个图横坐标代表训练的总计算量，纵坐标代表训练Loss最低的模型参数量；</li>
<li>第三个图横坐标代表训练的总计算量，纵坐标代表训练Loss最低的训练Token数；</li>
<li>绿色的线表示在给定训练总计算量(5.76x10^23)情况下，最优的模型参数量及训练Token数；</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/RY4Wws.png" alt="RY4Wws"></p>
<p>图3的含义如下：</p>
<ul>
<li>实验方式：固定训练总计算量，改变模型大小；</li>
<li>第一个图横坐标代表模型参数量，纵坐标代表训练的Loss，曲线表示不同训练总计算量；</li>
<li>第二个图横坐标代表训练的总计算量，纵坐标代表训练Loss最低的模型参数量；</li>
<li>第三个图横坐标代表训练的总计算量，纵坐标代表训练Loss最低的训练Token数；</li>
<li>绿色的线表示在给定训练总计算量(5.76x10^23)情况下，最优的模型参数量及训练Token数；</li>
</ul>
<p>方案三：根据上述两个实验结果进行数学建模，拟合LOSS与模型参数量和训练Token数的关系</p>
<script type="math/tex; mode=display">
\hat{L}(N, D) \triangleq E+\frac{A}{N^\alpha}+\frac{B}{D^\beta} .</script><p>公式介绍：</p>
<ul>
<li>E为数据分布上的理想Loss，对应自然文本的熵；</li>
<li>N对应模型参数量，第二项表示模型参数量N低于理想loss，参数量越大该项损失越小；</li>
<li>D对应数据集Token数，第三项表示模型未被训练收敛，Token数越多该项损失越小；</li>
</ul>
<script type="math/tex; mode=display">
\min _{A, B, E, \alpha, \beta} \sum_{\text {Runs } i} \operatorname{Huber}_\delta\left(\log \hat{L}\left(N_i, D_i\right)-\log L_i\right)</script><p>损失函数如上，通过L-BFGS算法优化，Hubert损失具有良好的外推性。</p>
<p>为了估计模型参数量与训练Token数的关系，假设约束条件为FLOPs(N,D)=6ND</p>
<script type="math/tex; mode=display">
N_{o p t}(C)=G\left(\frac{C}{6}\right)^a, \quad D_{o p t}(C)=G^{-1}\left(\frac{C}{6}\right)^b, \quad \text { where } \quad G=\left(\frac{\alpha A}{\beta B}\right)^{\frac{1}{\alpha+\beta}}, \quad a=\frac{\beta}{\alpha+\beta}, \text { and } b=\frac{\alpha}{\alpha+\beta}</script><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/eLoa4B.png" alt="eLoa4B"></p>
<p>图4的含义如下：</p>
<ul>
<li>第一个图横坐标代表不同训练总计算量，纵坐标代表模型参数量，曲线表示拟合的Loss曲线，每一个点是前两个数据中的L(N,D)，蓝色的线是拟合的参数量与训练Token函数；</li>
<li>第二个图横坐标代表模型参数量不同，纵坐标代表训练的Loss，曲线表示训练总计算量，每一个点是前两个数据中的L(N,D)；</li>
</ul>
<p>模型参数估计</p>
<p>这三个实验得到了相同的结论：计算量增加后，模型参数量大小与训练Token数应该同比增加。根据方法1，预测的模型大小需要的训练Token数如下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Parameters</th>
<th>FLOPs</th>
<th>FLOPs (in Gopher unit)</th>
<th>Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td>400 Million</td>
<td>1.92e+19</td>
<td>1//29,968</td>
<td>8.0 Billion</td>
</tr>
<tr>
<td>1 Billion</td>
<td>1.21e+20</td>
<td>1//4,761</td>
<td>20.2 Billion</td>
</tr>
<tr>
<td>10 Billion</td>
<td>1.23e+22</td>
<td>1//46</td>
<td>205.1 Billion</td>
</tr>
<tr>
<td>67 Billion</td>
<td>5.76e+23</td>
<td>1</td>
<td>1.5 Trillion</td>
</tr>
<tr>
<td>175 Billion</td>
<td>3.85e+24</td>
<td>6.7</td>
<td>3.7 Trillion</td>
</tr>
<tr>
<td>280 Billion</td>
<td>9.90e+24</td>
<td>17.2</td>
<td>5.9 Trillion</td>
</tr>
<tr>
<td>520 Billion</td>
<td>3.43e+25</td>
<td>59.5</td>
<td>11.0 Trillion</td>
</tr>
<tr>
<td>1 Trillion</td>
<td>1.27e+26</td>
<td>221.3</td>
<td>21.2 Trillion</td>
</tr>
<tr>
<td>10 Trillion</td>
<td>1.30e+28</td>
<td>22515.9</td>
<td>216.2 Trillion</td>
</tr>
</tbody>
</table>
</div>
<p><strong>局限性：</strong></p>
<ul>
<li>由于资源有限，只对比了两个模型(Chinchilla VS Gopher)，没有在中间规模进行测试；</li>
<li>假设的是在计算预算、模型大小和训练 token 数之间的有效计算边界可以用幂律关系来描述。然而，在高计算预算下，观察到log(N_{opt})的凹性，这表明仍有可能高估大型模型的最优规模；</li>
<li>训练运行均在少于一个epoch的数据上进行，未来可以考虑多个epoch数据。</li>
</ul>
<p><strong>疑问点：</strong></p>
<ol>
<li><p>所得到的拟合公式，在更大规模的数据及模型上，是否有效？即公式的泛化性是否足够；</p>
</li>
<li><p>模型Loss能否很好的反馈模型性能，在下文也提及PPL无法正确反应模型生成质量。</p>
</li>
</ol>
<h1 id="重复Token对模型性能的影响"><a href="#重复Token对模型性能的影响" class="headerlink" title="重复Token对模型性能的影响"></a>重复Token对模型性能的影响</h1><p><strong>参考链接</strong>：<a href="https://mp.weixin.qq.com/s/DBP_eafGeKMEuSIma9Z9Tg">https://mp.weixin.qq.com/s/DBP_eafGeKMEuSIma9Z9Tg</a></p>
<p><strong>论文名称</strong>：To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis</p>
<p><strong>论文链接</strong>：<a href="https://arxiv.org/pdf/2305.13230.pdf">https://arxiv.org/pdf/2305.13230.pdf</a></p>
<p>在传统的深度学习模型训练中，epoch的设置可以类似于传统机器学习模型训练的迭代次数，一般认为越多的epoch会让模型拟合得越好。</p>
<p>然而，在LLM时代，很多模型的epoch只有1次或者几次。例如，2022年谷歌的PaLM模型，其训练的epoch数量只有1。而MetaAI训练的LLaMA模型，在不同数据集上训练的epoch设置都是1-2。这似乎与我们之前理解的模型训练充分有不一致。那么，为什么这些大语言模型的epoch次数都很少。如果我们自己训练大语言模型，那么epoch次数设置为1是否足够，我们是否需要更多的训练？该部分介绍的大纲如下：</p>
<ul>
<li><p>为什么要考虑在重复的数据集上做多次训练？</p>
<ul>
<li>1、Token危机</li>
</ul>
</li>
<li><p>预训练数据集重复的影响是什么？</p>
<ul>
<li><p>1、模型参数规模与tokens数量需要匹配</p>
</li>
<li><p>2、多轮epoch的训练会降低模型性能</p>
</li>
</ul>
</li>
<li><p>影响多次Epochs训练效果下降的原因是什么？</p>
<ul>
<li><p>3、更大规模的数据集会缓解重复epochs对模型性能下降的影响</p>
</li>
<li><p>4、提高数据集的质量也无法挽救重复训练带来的过拟合</p>
</li>
<li><p>5、参数数量和FLOPs在重复训练上的影响</p>
</li>
<li><p>6、小计算量模型的过拟合趋势与大计算量的差不多</p>
</li>
<li><p>7、多样的训练目标可以减轻多Epoch下降吗？</p>
</li>
</ul>
</li>
<li><p>正则化可以降低多epochs的影响吗</p>
<ul>
<li><p>8、Dropout是一个被大语言模型忽视的正则技术，虽然慢，但是可以降低多epochs的影响</p>
</li>
<li><p>9、在训练过程中逐渐使用dropout是有效的策略</p>
</li>
<li><p>10、dropout对不同规模模型的影响不同</p>
</li>
<li><p>11、通过MoE扫描确定稠密模型的最佳超参数</p>
</li>
</ul>
</li>
<li><p>多epochs训练对大语言模型性能影响的总结</p>
</li>
</ul>
<h2 id="为什么要考虑在重复的数据集上做多次训练？"><a href="#为什么要考虑在重复的数据集上做多次训练？" class="headerlink" title="为什么要考虑在重复的数据集上做多次训练？"></a>为什么要考虑在重复的数据集上做多次训练？</h2><p>在此前的研究中，大家发现大语言模型的规模和训练数据集中tokens的数量对模型的性能有很大的影响。大模型缩放定律都认为模型的规模与训练数据的规模必须同时扩大才能让模型产生更好的性能。但是，tokens数量似乎并不是很足够，如下图所示是作者研究的模型参数规模增长和目前互联网是可用的数据集tokens数量增长情况：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/w0g1A2.png" alt="w0g1A2"></p>
<p>在这幅图中，蓝色的虚线是互联网上数据集中tokens数量预估结果，高质量文本中tokens数量每年增长只有4%-5%，与世界经济增长率差不多，但是显著慢于模型规模的增长。例如，MetaAI训练的LLaMA-65B模型用了1.4万亿tokens，而2023年全球的tokens估计只有9万亿！按照目前模型规模的发展情况，在2023年-2027年几年的时间里，我们的模型将把全球所有数据集的tokens都训练完成，此后，我们很可能陷入缺少tokens训练的地步，这被作者称为tokens危机。</p>
<p>这就很自然的让大家想到，我们是否可以通过增加训练的epochs来做重复的训练，以提高模型的效果？在如Vision Transformers这样的模型中，模型训练的epochs高达300次，而大语言模型的训练epochs通常都是1-2次，多的也都是个位数。2022年，Hoffmann的论文中提出用重复的tokens训练大语言模型会让模型降低性能，而Taylor在训练Galactica模型时候发现epochs次数达到4次也可以提升模型效果。显然，在重复数据集上训练多次对模型的影响目前还没有一个相对完善的研究。但是这个问题很重要！</p>
<p>因此，新加坡国立大学的研究人员做了这项研究，系统性分析了大语言模型epochs的设置影响，从3个方面得出了11个结论！本文将主要总结一下这些结论。</p>
<h2 id="预训练数据集重复的影响是什么？"><a href="#预训练数据集重复的影响是什么？" class="headerlink" title="预训练数据集重复的影响是什么？"></a>预训练数据集重复的影响是什么？</h2><h3 id="1-模型参数规模与tokens数量需要匹配"><a href="#1-模型参数规模与tokens数量需要匹配" class="headerlink" title="1. 模型参数规模与tokens数量需要匹配"></a>1. 模型参数规模与tokens数量需要匹配</h3><p>首先是模型参数规模的增长与模型需要的tokens数量基本是呈线性的。可以参见第一部分：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/Zu5AZT.png" alt="Zu5AZT"> </p>
<p>这意味如果你要充分训练一个LLM，需要根据它的参数数量来收集足够的tokens。</p>
<h3 id="2-多轮epoch的训练会降低模型性能"><a href="#2-多轮epoch的训练会降低模型性能" class="headerlink" title="2. 多轮epoch的训练会降低模型性能"></a>2. 多轮epoch的训练会降低模型性能</h3><p>作者分别使用C4数据集的子集，然后只是用了其中一部分数据集，并通过设置多次epochs来让模型总的训练过的tokens差不多水平，观察模型的性能。</p>
<p>如下图所示，可以看到，数据集重复的次数越多，模型的性能越差：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/lXvGL8.png" alt="lXvGL8"></p>
<p>此外，如果tokens数量不够，模型参数规模越大，越容易出现过拟合的现象！</p>
<h2 id="影响多次Epochs训练效果下降的原因是什么？"><a href="#影响多次Epochs训练效果下降的原因是什么？" class="headerlink" title="影响多次Epochs训练效果下降的原因是什么？"></a>影响多次Epochs训练效果下降的原因是什么？</h2><h3 id="3、更大规模的数据集会缓解重复epochs对模型性能下降的影响"><a href="#3、更大规模的数据集会缓解重复epochs对模型性能下降的影响" class="headerlink" title="3、更大规模的数据集会缓解重复epochs对模型性能下降的影响"></a>3、更大规模的数据集会缓解重复epochs对模型性能下降的影响</h3><p>在这个实验中，作者将重复的次数固定，然后看模型在不同规模数据集上重复训练的性能影响。如下图所示：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ujNuPF.png" alt="ujNuPF"></p>
<p>可以看到，当在$2^{27}$个tokens和$2^{29}$个tokens上重复训练$2^8$次之后发现，前者更容易出现过拟合，而$2^{29}$tokens的数据集上重复训练，模型性能下降不明显。</p>
<h3 id="4、提高数据集的质量也无法挽救重复训练带来的过拟合"><a href="#4、提高数据集的质量也无法挽救重复训练带来的过拟合" class="headerlink" title="4、提高数据集的质量也无法挽救重复训练带来的过拟合"></a>4、提高数据集的质量也无法挽救重复训练带来的过拟合</h3><p>Taylor在训练Galactica模型时候认为他之所以用4epochs能提高训练效果可能是因为他的数据集质量更好。然而，本文的作者发现，相对更高质量的数据集并不能降低重复训练带来的影响。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/rVzzEr.png" alt="rVzzEr"></p>
<p>作者用相同的重复策略在C4数据集和Wikipedia数据集上分别训练模型，发现二者都会因为重复训练带来模型性能的下降。这里的Wikipedia数据集质量相对C4更好一点。说明相对提高数据集质量可能不会影响重复训练的负面效应。</p>
<h3 id="5、参数数量和FLOPs在重复训练上的影响"><a href="#5、参数数量和FLOPs在重复训练上的影响" class="headerlink" title="5、参数数量和FLOPs在重复训练上的影响"></a>5、参数数量和FLOPs在重复训练上的影响</h3><p>模型规模的增长其实表现在2个方面，一个是模型参数，一个是模型所需要的计算量。模型参数相同的情况下，采用不同的模型架构所需要的FLOPs是不同的。作者对比了MoE架构，并采用ParamShare方法降低相同参数模型的FLOPs。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/QduvXB.png" alt="QduvXB"></p>
<p>经过测试发现，FLOPs较大的模型性能会更好一点，但是依然无法有效降低重复训练带来的模型损失。</p>
<h3 id="6、小计算量模型的过拟合趋势与大计算量的差不多"><a href="#6、小计算量模型的过拟合趋势与大计算量的差不多" class="headerlink" title="6、小计算量模型的过拟合趋势与大计算量的差不多"></a>6、小计算量模型的过拟合趋势与大计算量的差不多</h3><p>这是一个有趣的发现，尽管在前面的实验中，相同参数规模不同计算量的模型都会受到重复数据集训练的影响。但是二者在模型性能表现的趋势上类似。</p>
<h3 id="7、多样的训练目标可以减轻多Epoch下降吗？"><a href="#7、多样的训练目标可以减轻多Epoch下降吗？" class="headerlink" title="7、多样的训练目标可以减轻多Epoch下降吗？"></a>7、多样的训练目标可以减轻多Epoch下降吗？</h3><p>目前大语言模型的训练目标有很多，例如预测下一个单词是什么的生成式目标，也有把单词masked之后用来判断是什么单词的判别式目标。如果语言模型的训练目标多样化，那么实际上更加可能受到多epoch带来的性能损失。</p>
<p>例如，UL2这种模型就不适合多Epoch的训练，MLM这种模型受到的影响反而更小。</p>
<h2 id="正则化可以降低多epochs的影响吗"><a href="#正则化可以降低多epochs的影响吗" class="headerlink" title="正则化可以降低多epochs的影响吗"></a>正则化可以降低多epochs的影响吗</h2><p>正则技术，如dropout、droppath、weight decay等都是常用的防止过拟合的技术。而多Epochs的负面影响也都是过拟合。因此，作者研究了这些正则技术是否可以降低多epochs的影响。</p>
<h3 id="8、Dropout是一个被大语言模型忽视的正则技术，虽然慢，但是可以降低多epochs的影响"><a href="#8、Dropout是一个被大语言模型忽视的正则技术，虽然慢，但是可以降低多epochs的影响" class="headerlink" title="8、Dropout是一个被大语言模型忽视的正则技术，虽然慢，但是可以降低多epochs的影响"></a>8、Dropout是一个被大语言模型忽视的正则技术，虽然慢，但是可以降低多epochs的影响</h3><p>在目前超过100亿参数规模的大语言模型中，如GPT-3、PaLM、LLaMA等，都没有使用dropout（可能是因为太慢了）。而前面说的Galactica训练使用了，这是Galactica能够训练4Epochs提升性能的最重要的原因。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/U9xdtj.png" alt="U9xdtj"></p>
<h3 id="9、在训练过程中逐渐使用dropout是有效的策略"><a href="#9、在训练过程中逐渐使用dropout是有效的策略" class="headerlink" title="9、在训练过程中逐渐使用dropout是有效的策略"></a>9、在训练过程中逐渐使用dropout是有效的策略</h3><p>在前面的讨论中，作者已经发现dropout可以降低多epochs的影响，但是dropout会降低模型的性能。因此，作者考虑不在全部训练中使用dropout，而是逐渐引入。</p>
<p>最终发现，如果前期训练不用dropout，在后续的迭代中使用dropout也是有效的！</p>
<h3 id="10、dropout对不同规模模型的影响不同"><a href="#10、dropout对不同规模模型的影响不同" class="headerlink" title="10、dropout对不同规模模型的影响不同"></a>10、dropout对不同规模模型的影响不同</h3><p>尽管前面已经证明dropout使用可以降低多epochs的影响，但是在不同规模模型下是不同的。对于规模较大的模型，dropout不能有效降低多epochs带来的坏处！</p>
<h3 id="11、通过MoE扫描确定稠密模型的最佳超参数"><a href="#11、通过MoE扫描确定稠密模型的最佳超参数" class="headerlink" title="11、通过MoE扫描确定稠密模型的最佳超参数"></a>11、通过MoE扫描确定稠密模型的最佳超参数</h3><p>最后一个结论其实与epoch关系不大，作者强调的是MoE的模型表现与大模型真正的训练有类似的趋势，因此用MoE去提前预估大模型的性能，做参数调优是一个非常好的思路。</p>
<h2 id="多epochs训练对大语言模型性能影响的总结"><a href="#多epochs训练对大语言模型性能影响的总结" class="headerlink" title="多epochs训练对大语言模型性能影响的总结"></a>多epochs训练对大语言模型性能影响的总结</h2><p>根据前面的实验我们知道，如果在tokens数量一定的数据集上做多epochs的模型训练，会影响模型的性能，降低模型的效果。这在预训练和下游任务都会产生影响。但是，随着模型的发展，高质量数据集的tokens数将很快用完。而采用正则技术虽然会影响模型训练效率，但是会降低这种影响。</p>
<h1 id="SFT需要的数据量"><a href="#SFT需要的数据量" class="headerlink" title="SFT需要的数据量"></a>SFT需要的数据量</h1><p><strong>参考链接</strong>：<a href="https://mp.weixin.qq.com/s/DVH-vlOpGik8iwW4KnPlkw">https://mp.weixin.qq.com/s/DVH-vlOpGik8iwW4KnPlkw</a></p>
<p>大型语言模型的训练分为从原始文本中进行无监督的预训练，以学习通用的表征、大规模的指令调整和强化学习，以更好地适应最终任务和用户偏好两个阶段。</p>
<p><strong>有一种假设认为，大型语言模型中的几乎所有知识都是在预训练中学习的，只需要有限的指导微调数据就可以教会模型产生高质量的输出。</strong></p>
<p><strong>因此， 关于微调数据使用量，具体对微调的性能有怎样的影响，这个话题十分有趣。</strong></p>
<p>最近包括《LIMA：Less Is More for Alignment》、《MAYBE ONLY 0.5% DATA IS NEEDED》则在说明小数据量上，提出了更新颖的结论。</p>
<p>《LIMa：Less Is More for Alignment》一文的消融实验显示，<strong>当扩大数据量而不同时扩大提示多样性时，收益会大大减少，而在优化数据质量时，收益会大大增加。</strong></p>
<p>《MAYBE ONLY 0.5% DATA IS NEEDED》一文的实验表明，<strong>特定任务的模型可能从固定的任务类型中获益，以获得更高的性能；指令格式的多样性可能对特定任务模型的性能影响很小；即使是少量的数据（1.9M tokens）也能为特定任务模型的指令调整带来可喜的结果。</strong></p>
<h2 id="LIMa：Less-Is-More-for-Alignment"><a href="#LIMa：Less-Is-More-for-Alignment" class="headerlink" title="LIMa：Less Is More for Alignment"></a>LIMa：Less Is More for Alignment</h2><p><strong>论文标题</strong>：LIMa：Less Is More for Alignment</p>
<p><strong>论文链接</strong>：<a href="https://arxiv.org/pdf/2305.11206.pdf">https://arxiv.org/pdf/2305.11206.pdf</a></p>
<p>论文提出浅层对齐假说，即<strong>一个模型的知识和能力几乎完全是在预训练中学习的，而对齐则是教它在与用户交互时应该使用哪种子分布的格式。如果这个假说是正确的，而对齐主要是关于学习风格的</strong>，那么浅层对齐假说的一个推论是，人们可以用一组相当小的例子充分调整预训练的语言模型。</p>
<p>因此，<strong>该工作假设，对齐可以是一个简单的过程，模型学习与用户互动的风格或格式，以揭示在预训练中已经获得的知识和能力。</strong></p>
<p><strong>LIMa采用精心筛选的1000条样本训练，接下来将介绍关键的数据筛选过程：</strong></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/nriR9E.png" alt="nriR9E"></p>
<p>论文从三个社区问答网站收集数据：Stack Exchange、wikiHow和Pushshift Reddit数据集。</p>
<p><strong>1）Stack Exchange</strong></p>
<p>Stack Exchange包含179个在线社区（交流），每个社区都致力于一个特定的主题，其中最受欢迎的是编程（Stack Overflow）。用户可以发布问题、答案和评论，并对上述所有内容进行加注（或减注）。</p>
<p>在从Stack Exchange采样时，同时采用了质量和多样性控制。首先，<strong>将交流分成75个STEM交流（包括编程、数学、物理等）和99个其他（英语、烹饪、旅游等），并放弃了5个小众交流。然后，从每组中抽出200个问题和答案，使用=3的温度来获得不同领域的更均匀的样本。</strong></p>
<p>在每个交流，选择得分最高的问题，然后选择每个问题的最高答案，并保证它有很强的正面得分（至少10分）。<strong>为了符合一个有帮助的人工智能助手的风格，自动过滤那些太短（少于1200个字符）、太长（超过4096个字符）、以第一人称书写（”我”、”我的”）或引用其他答案（”如提到”、”Stack exchange “等）的答案；并从回答中删除链接、图片和其他HTML标签，只保留代码块和列表。</strong></p>
<p>由于Stack Exchange问题同时包含标题和描述，随机选择一些例子的标题作为提示，而其他例子则选择描述。</p>
<p><strong>2）wikiHow</strong></p>
<p>wikiHow是一个在线的维基式出版物，有超过24万篇关于各种主题的方法文章。从wikiHow中抽取了200篇文章，首先抽取一个类别（19个），然后抽取其中的一篇文章，以确保多样性。</p>
<p>使用标题作为提示（例如 “如何做煎蛋？”），并将文章的内容作为回应。</p>
<p><strong>在处理上，用 “下面的答案…… “取代典型的 “这篇文章…… “开头，并采用一些启发式的预处理方法来修剪链接、图片和文本中的某些部分。</strong></p>
<p><strong>3）Pushshift Reddit数据集</strong></p>
<p>Reddit是世界上最受欢迎的网站之一，允许用户在用户创建的subreddits中分享、讨论和加注内容。</p>
<p>在处理上，将样本限制在两个子集，即r/AskReddit和r/WritingPrompts，并从每个社区的最高票数的帖子中手动选择例子。</p>
<p>并从r/AskReddit中找到了70个自成一体的提示（只有标题，没有正文），并将其用作测试集。</p>
<p>WritingPrompts子版块包含虚构故事的前提，然后鼓励其他用户创造性地完成这些故事，共找到150个提示和高质量的回应，包括情诗和短篇科幻小说等主题，并将其加入训练集。</p>
<p><strong>4）人工撰写例子</strong></p>
<p>为了使数据进一步多样化，除了在线社区中用户提出的问题之外，还收集了来自我们自己（这项工作的作者）的提示信息。</p>
<p>指定了两组作者，A组和B组，各创作250个提示，灵感来自他们自己或他们朋友的兴趣。</p>
<p>在过滤了一些有问题的提示后，B组剩下的230条提示被用于测试。</p>
<p>此外，还包括13个具有一定程度毒性或恶意的训练提示。</p>
<p>此外，该工作还选择了50个自然语言生成任务，如总结、转述和风格转换，并从每个任务中随机挑选一个例子，并对其中的一些例子稍作编辑。</p>
<p><strong>3、对比模型与实验效果</strong></p>
<p>为了将LIMA与其他模型进行比较，我们为每个测试提示生成一个单一的反应。然后，我们要求人群工作者将LIMA的输出与每个基线进行比较，并标注他们更喜欢哪一个，该工作将LIMA与五个基线进行比较：</p>
<p>Alpaca 65B ，在Alpaca训练集[Taori et al., 2023]中的52,000个例子上对LLaMa 65B进行了微调；</p>
<p>OpenAI-DaVinci003，一个用人类反馈的强化学习（RLHF）调整的大型语言模型；</p>
<p>谷歌的Bard，基于PaLM；</p>
<p>Anthropic的Claude4，一个用人工智能反馈强化学习（Constitutional AI）训练的52B参数模型</p>
<p>OpenAI的GPT-4，一个用RLHF训练的大型语言模型，目前被认为是最先进的。在整个2023年4月，对所有基线的反应进行了采样。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/9re4mh.png" alt="9re4mh"></p>
<p>实验效果如上，图1显示了人类偏好研究的结果，图2显示了GPT-4偏好的结果。结果表明：<strong>尽管在52倍的数据上进行训练，Alpaca 65B倾向于产生比LIMA更少的偏好输出。DaVinci003的情况也是如此，尽管优势微弱；</strong></p>
<p><strong>4、讨论与分析</strong></p>
<p>通过消融实验研究训练数据的多样性、质量和数量的影响，对于对齐的目的，扩大输入多样性和输出质量有可衡量的积极影响，而仅仅扩大数量可能没有。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/lu4zW9.png" alt="lu4zW9"></p>
<p>首先，在数量上，从Stack Exchange中抽取指数级增加的训练集。图6显示，有趣的是，<strong>训练集的翻倍并没有改善响应质量。模型表现的缩放规律不一定只受制于数量，而是在保持高质量响应的同时，提示多样性的功能。</strong></p>
<p>其次，在多样性上，<strong>为了测试提示多样性的影响，同时控制质量和数量，比较了训练对经过质量过滤的Stack Exchange数据和wikiHow数据的影响，前者有异质的提示和优秀的回答，后者有同质的提示和优秀的回答。</strong></p>
<p>虽然在将Stack Exchange和wikiHow作为多样性的代表进行比较时，在从两个不同来源的数据中取样（每个来源中抽出2000个训练例子）可能会有其他混淆因素，但图5显示，更多样化的Stack Exchange数据产生了明显更高的性能。</p>
<p>最后，在质量上，为从Stack Exchange中抽取了2000个例子，没有经过任何质量或风格的过滤，并将在这个数据集上训练的模型与在过滤过的数据集上训练的模型进行比较。图5显示，在经过过滤和未经过过滤的数据源上训练的模型之间有0.5分的显著差异。</p>
<p>因此，总结上来说，在<strong>1000个精心策划的例子上对一个强大的预训练语言模型进行微调，可以在广泛的提示中产生显著的、有竞争力的结果。</strong></p>
<p>然而，这种方法也有局限性，正如文中阐述的那样：</p>
<p>首先，构建这样的例子所付出的努力是巨大的，而且很难扩大规模。</p>
<p>其次，<strong>LIMA并不像产品级模型那样稳健</strong>；</p>
<p>尽管如此，这项工作中提出的证据表明，<strong>用一种简单的方法来解决复杂的对齐问题是有潜力的。</strong></p>
<p><strong>同时，该论文也指出，PPL并不能有效的反馈模型的表现。</strong></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/6xUAn8.png" alt="6xUAn8"></p>
<p>在不同模型大小(7B/30B/65B)，不同训练数据训练下，均表明验证集的PPL不能反馈模型生成的质量。</p>
<h2 id="MAYBE-ONLY-0-5-DATA-IS-NEEDED"><a href="#MAYBE-ONLY-0-5-DATA-IS-NEEDED" class="headerlink" title="MAYBE ONLY 0.5% DATA IS NEEDED"></a>MAYBE ONLY 0.5% DATA IS NEEDED</h2><p><strong>论文标题</strong>：MAYBE ONLY 0.5% DATA IS NEEDED</p>
<p><strong>论文链接</strong>：<a href="https://arxiv.org/pdf/2305.09246.pdf">https://arxiv.org/pdf/2305.09246.pdf</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/vpV4aH.png" alt="vpV4aH"></p>
<p>该工作利用聚类的思想筛选样本进行实验，以说明微调数据规模并不需要难么多，就可以达到一个不错的效果。其实际上走的是多样性的路子。</p>
<p><strong>1）向量化</strong></p>
<p>将数据重新格式化为指令调优训练阶段使用的训练输入格式，即带有描述指令的数据，在最后加入答案，以格式化一个完整的训练数据。</p>
<p>然后，使用预先训练好的语言模型（如Galactica或Bert）对所有样本进行编码。</p>
<p>具体地，将模型作为单词嵌入或每个句子的输入后，提取每个样本的last_hidden_state。对每个样本的词嵌入进行均值集合，得到一个一维向量作为该样本的句子嵌入。</p>
<p>为了加快计算速度，方便向量相似性的计算，将所有句子嵌入归一为长度1，即对嵌入维度进行L2归一。</p>
<p><strong>2）聚类</strong></p>
<p>考虑到NLP任务边界的模糊性可能导致不同任务的样本之间的差异很小。</p>
<p>因此，通过关注数据表征来进行无监督聚类，而不是依靠标签信息来将数据点基于相同的类别或任务归类。</p>
<p>具体来说，在获得第一步的句子嵌入后，使用K-Means在嵌入空间中进行无监督聚类，以获得每个样本和其对应的聚类标签的映射。</p>
<p>然后，根据一个下游任务的样本出现在几个聚类中的频率，选择频率最高的聚类的中心点作为该下游任务的分布中心点。</p>
<p>接下来，对于任务中的所有样本，计算与分布中心点的余弦相似度（距离函数的选择对结果影响不大，按照OpenAI的方法选择余弦相似度），并从任务数据中找出与该中心点最接近的样本作为任务中心点，任务中心点是这个任务数据中与分布中心点余弦相似度最大的一个确切样本。</p>
<p><strong>3）核心样本采样</strong></p>
<p>在获得下游任务对应的分布中心点后，根据余弦相似度选择最相似的样本作为代表性的任务样本，使用了一种核心集算法KCentergreedy，该算法旨在选择k个中心点，使随机数据点与其最近的中心点之间的最大距离最小。</p>
<p><strong>4、实验数据、模型与结论</strong></p>
<p><strong>1）实验数据</strong></p>
<p>在总共11个数据集上进行了实验，这些数据集横跨4个NLP任务，即自然语言推理（NLI，1.9M tokens）、句子补充（SC，660.6K tokens）、词义歧义（WSD，25.5K tokens）和核心推理（CR，185.1K tokens）；</p>
<p><strong>2）实验模型</strong></p>
<p>采用Galactica-1.3b。Galactica模型是在一个庞大的科学语料库中训练出来的，并被定制用于处理科学任务，如引文预测、科学问题回答、数学推理、总结、文档生成、分子特性预测和实体提取；</p>
<p><strong>3）实验结论</strong></p>
<p>当考虑到一个特定的任务（本例中为NLI）时，该方法在NLI任务上实现了比基线（表中P3）平均2%的性能改进，只使用了P3中0.5%的可用数据。</p>
<p>与使用P3的所有10条指令相比，只选择一条指令就能达到与使用P3的整个数据集相媲美的结果，而且只用了10%的数据。</p>
<p>通过在P3数据集上调优NLI任务的Galactica-1.3b模型，最终得出了几个结论：</p>
<p>首先，特定任务的模型可能从固定的任务类型中获益，以获得更高的性能；</p>
<p>其次，指令格式的多样性可能对特定任务模型的性能影响很小【这块有问题，格式的多样并不能通过聚类方法实现】；</p>
<p>最后，即使是少量的数据（1.9M tokens）也能为特定任务模型的指令调整带来不错的结果。</p>
<p>不过，由于计算资源的限制，该工作有一些局限性，比如只在Galactica-1.3b上进行实验，只利用P3数据集中的NLI任务数据。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在本文中，我们将主要探讨如何在计算资源有限的情况下为预训练模型准备参数和训练数据，同时也会关注预训练数据中重复Token的影响以及SFT的训练数据量。这些内容旨在为模型训练前期提供有价值的理论参考。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>ChatGPT</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM训练指南(二):模型参数、计算量、显存、计算时间计算</title>
    <url>/2023/07/01/2023-07-01-LLM%E8%AE%AD%E7%BB%83%E6%8C%87%E5%8D%97(%E4%BA%8C):%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E3%80%81%E8%AE%A1%E7%AE%97%E9%87%8F%E3%80%81%E6%98%BE%E5%AD%98%E3%80%81%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4%E8%AE%A1%E7%AE%97/</url>
    <content><![CDATA[<h1 id="引言">引言</h1>
<p>在上一篇文章<a href="https://zhuanlan.zhihu.com/p/636812912">LLM训练指南：Token及模型参数准备</a>中，我们详细探讨了如何为预训练模型确定合适的大小和数据集规模，从而提高模型的性能表现。在本文中，我们将在前文的基础上，讨论如何在给定的参数量和训练Token数的条件下，合理设置模型参数、估算训练过程中的计算量以及预测训练所需的时间。通过这些分析，希望能为您在实际应用中的模型训练提供更多的指导和帮助。</p>
<p>本文得出的结论如下，详细过程见下文分析：</p>
<p><strong>模型参数量：</strong></p>
<p><span class="math display">\[
N=n_{layer}(d_{model}3d_{attn}+d_{attn}d_{model}+2d_{model}d_{ff})=2d_{model}n_{layer}(2d_{attn}+d_{ff})
\]</span></p>
<p><strong>训练计算量：</strong></p>
<p><span class="math display">\[
\begin{aligned}
C_{forward}&amp;=2n_{layer}(d_{model}3d_{attn}+n_{ctx}d_{model}+d_{attn}d_{model}+2d_{model}d_{ff}) \\
&amp;=4d_{model}n_{layer}(2d_{attn}+d_{ff})+2n_{layer}n_{ctx}d_{model} \\
&amp;=2N+2n_{layer}n_{ctx}d_{model}
\end{aligned}
\]</span></p>
<p><strong>训练显存占用：</strong></p>
<p><strong>假设模型参数为<span class="math inline">\(\Phi\)</span>以AdamW优化器和混合精度训练进行模型训练为例：</strong></p>
<p>显存占用情况如下：</p>
<ol type="1">
<li>模型参数：</li>
</ol>
<p><span class="math display">\[
2\Phi
\]</span></p>
<ol start="2" type="1">
<li>模型梯度：</li>
</ol>
<p><span class="math display">\[
2\Phi
\]</span></p>
<ol start="3" type="1">
<li>优化器状态：</li>
</ol>
<p><span class="math display">\[
4\Phi(参数)+4\Phi(动量)+4\Phi(方差)
\]</span></p>
<ol start="4" type="1">
<li>中间激活值：</li>
</ol>
<p><span class="math display">\[
n_{layer}(14*batch*n_{ctx}d_{model}+4*batch*n_{ctx}d_{ff}+\
   8*batch*n_{heads}*n_{ctx}d_{attn}+5*batch*n_{heads}*n_{ctx}n_{ctx}
\]</span></p>
<p><strong>训练时间：</strong></p>
<p><span class="math display">\[
训练时间 \approx \frac{8 * token训练数 * 模型参数量}{GPU数量 * GPU峰值flops * GPU利用率}
\]</span></p>
<p>文章结构如下：</p>
<p><img src="img_1.png" alt="img_1.png" /><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1687830387173-c2176c26-c711-4f3c-be9c-de1c211bb19f.png" /></p>
<span id="more"></span>
<h1 id="模型参数量计算量计算">模型参数量&amp;计算量计算</h1>
<p><strong>符号说明：</strong></p>
<ul>
<li><p><span class="math inline">\(n_{layer}\)</span>:模型层数；</p></li>
<li><p><span class="math inline">\(d_{model}\)</span>:模型残差输出维度大小；</p></li>
<li><p><span class="math inline">\(d_{ff}\)</span>:前馈神经网络输出维度大小；</p></li>
<li><p><span class="math inline">\(d_{attn}\)</span>:注意力网络输出维度大小；</p></li>
<li><p><span class="math inline">\(n_{heads}\)</span>:每一层的多头注意力的数量；</p></li>
<li><p><span class="math inline">\(n_{ctx}\)</span>:输入的上下文长度大小；</p></li>
</ul>
<p>Transformer各层参数及计算量如下表所示：</p>
<p><span class="math display">\[
\begin{array}{|l|l|l|}
\hline \text { Operation } &amp; \text { Parameters } &amp; \text { FLOPs per Token } \\
\hline \hline \text { Embed } &amp; \left(n_{\text {vocab }}+n_{\mathrm{ctx}}\right) d_{\text {model }} &amp; 4 d_{\text {model }} \\
\hline \text { Attention: QKV } &amp; n_{\text {layer }} d_{\text {model }} 3 d_{\mathrm{attn}} &amp; 2 n_{\text {layer }} d_{\text {model }} 3 d_{\mathrm{attn}} \\
\hline \text { Attention: Mask } &amp; - &amp; 2 n_{\text {layer }} n_{\text {ctx }} d_{\mathrm{attn}} \\
\hline \text { Attention: Project } &amp; n_{\text {layer }} d_{\mathrm{attn}} d_{\text {model }} &amp; 2 n_{\text {layer }} d_{\mathrm{attn}} d_{\mathrm{embd}} \\
\hline \text { Feedforward } &amp; n_{\text {layer }} 2 d_{\text {model }} d_{\mathrm{ff}} &amp; 2 n_{\text {layer }} 2 d_{\text {model }} d_{\mathrm{ff}} \\
\hline \text { De-embed } &amp; - &amp; 2 d_{\text {model }} n_{\text {vocab }} \\
\hline \hline \text { Total (Non-Embedding) } &amp; N=2 d_{\text {model }} n_{\text {layer }}\left(2 d_{\mathrm{attn}}+d_{\mathrm{ff}}\right) &amp; C_{\text {forward }}=2 N+2 n_{\text {layer }} n_{\text {ctx }} d_{\mathrm{attn}} \\
\hline
\end{array}
\]</span></p>
<p><strong>下文中涉及的前置知识如下：</strong></p>
<blockquote>
<p>参数量定义：模型参数量是指一个神经网络或机器学习模型中可以进行学习和调整的参数的数量。这些参数包括权重（weights）和偏置（biases），它们在训练过程中会不断地更新以优化模型的性能。</p>
<p>FLOPs定义：FLOPs，floating point operations，表示浮点数运算次数，衡量了计算量的大小。<br />
如何计算矩阵乘法的FLOPs呢？<br />
对于<span class="math inline">\(A \in R^{1*n},B \in R^{n*1}\)</span>，计算<span class="math inline">\(AB\)</span>需要进行n次乘法运算和n次加法运算，共计2n次浮点数运算，需要2n的FLOPs。对于<span class="math inline">\(A \in R^{m*n},B \in R^{n*p}\)</span>，计算<span class="math inline">\(AB\)</span>需要的浮点数运算次数为2mnp。</p>
</blockquote>
<h2 id="embedding层">Embedding层</h2>
<p>Embedding层完成输入ID到向量的映射，将词表大小的ID映射到<span class="math inline">\(d_{model}\)</span>维度的向量。计算转换如下：</p>
<p><span class="math display">\[
[batch, n_{ctx}] * [vocab, d_{model}] -&gt; [batch, n_{ctx}, d_{model}]
\]</span></p>
<p>这里的*代表lookup操作</p>
<p>词嵌入矩阵参数量大小为：<span class="math inline">\(n_{vocab}*d_{model}\)</span>；</p>
<p>上下文输入向量大小为：<span class="math inline">\(n_{ctx}*d_{model}\)</span></p>
<h2 id="attention-qkv层">Attention QKV层</h2>
<p>Attention QKV层完成输入向量到注意力层的映射，将隐藏层大小<span class="math inline">\(d_{model}\)</span>映射到多头注意力<span class="math inline">\(3d_{attn}\)</span>维度的向量。计算转换如下：</p>
<p><span class="math display">\[
[batch, n_{ctx}, d_{model}] * [d_{model}, 3d_{attn}]
\]</span></p>
<p>每一层的变化均如上所示，Attention QKV层参数量大小为：<span class="math inline">\(n_{layer}d_{model}3d_{attn}\)</span></p>
<p>计算量包括矩阵的乘法运算[拆分为加&amp;和两个操作]，每个Token计算量大小为：</p>
<p><span class="math display">\[
2*batch*n_{ctx}*n_{layer}d_{model}3d_{attn}/(batch*n_{ctx})=2n_{layer}d_{model}3d_{attn}
\]</span></p>
<h2 id="attention-mask层">Attention Mask层</h2>
<p>Attention Mask层完成下列计算操作:</p>
<p><span class="math display">\[
Softmax(\frac{QK^T}{\sqrt{d_{model}}})V
\]</span></p>
<p>计算维度转换如下：</p>
<p><span class="math display">\[
[batch, n_{ctx}, d_{attn}]*[batch, d_{attn}, n_{ctx}]-&gt;[batch, n_{ctx}, n_{ctx}]
\]</span></p>
<p><span class="math display">\[
[batch, n_{ctx}, n_{ctx}]*[batch, n_{ctx}, d_{attn}]-&gt;[batch, n_{ctx}, d_{attn}]
\]</span></p>
<p>未引入新的变量，因为无参数量</p>
<p>Attention Mask层计算量包括矩阵的乘法运算[拆分为加&amp;和两个操作]，每个Token计算量大小为：</p>
<p><span class="math display">\[
2*n_{layer}*(batch*n_{ctx}*d_{attn}n_{ctx}+batch*n_{ctx}n_{ctx}d_{attn})/(batch*n_{ctx})=2n_{layer}n_{ctx}d_{attn}
\]</span></p>
<h2 id="attention-project层">Attention Project层</h2>
<p>Attention Project层完成Attention输出维度<span class="math inline">\(d_{attn}\)</span>到隐藏层大小<span class="math inline">\(d_{model}\)</span>的映射。计算转换如下：</p>
<p><span class="math display">\[
[batch, n_{ctx}, d_{attn}]*[d_{attn}, d_{model}]-&gt;[batch, n_{ctx}, d_{model}]
\]</span></p>
<p>每一层的变化均如上所示，Attention QKV层参数量大小为：<span class="math inline">\(n_{layer}d_{attn}d_{model}\)</span></p>
<p>Attention Project层计算量包括矩阵的乘法运算[拆分为加&amp;和两个操作]，每个Token计算量大小为：</p>
<p><span class="math display">\[
2*n_{layer}*batch*n_{ctx}d_{attn}d_{model}/(batch*n_{ctx})=2n_{layer}d_{attn}d_{model}
\]</span></p>
<h2 id="feedforward层">Feedforward层</h2>
<p>Feedforward层完成从隐藏层大小<span class="math inline">\(d_{model}\)</span>到前馈隐藏层大小<span class="math inline">\(d_{ff}\)</span>再到隐藏层大小<span class="math inline">\(d_{model}\)</span>的映射。计算转换如下：</p>
<p><span class="math display">\[
[batch, n_{ctx}, d_{model}] * [d_{model}, d_{ff}] -&gt;[batch, n_{ctx},d_{ff}]
\]</span></p>
<p><span class="math display">\[
[batch, n_{ctx}, d_{ff}] * [d_{ff}, d_{model}]-&gt;[batch, n_{ctx}, d_{model}]
\]</span></p>
<p>每一层的变化均如上所示，Feedforward参数量大小为：<span class="math inline">\(n_{layer}2d_{model}d_{ff}\)</span>；</p>
<p>计算量包括矩阵的乘法运算[拆分为加&amp;和两个操作]，每个Token计算量大小为：</p>
<p><span class="math display">\[
2*n_{layer}*(batch*n_{ctx}*d_{model}d_{ff}+batch*n_{ctx}*d_{ff}d_{model})/(batch*n_{ctx})=2n_{layer}2d_{model}d_{ff}
\]</span></p>
<h2 id="de-emb层">De-emb层</h2>
<p>De-emb层完成从隐藏层大小<span class="math inline">\(d_{model}\)</span>到输出<span class="math inline">\(n_{vocab}\)</span>的映射。计算转换如下：</p>
<p><span class="math display">\[
[batch, n_{ctx}, d_{model}]*[d_{model}, n_{vocab}]-&gt;[batch, n_{ctx}, n_{vocab}]
\]</span></p>
<p>De-embedding层通常使用与输入嵌入层（Embedding层）相同的权重矩阵来进行线性转换。这种权重共享的策略有助于减少模型参数量，从而降低过拟合的风险。因此，不需要为De-embedding层单独分配参数量。</p>
<p>计算量包括矩阵的乘法运算[拆分为加&amp;和两个操作]，每个Token计算量大小为：</p>
<p><span class="math display">\[
2*batch*n_{ctx}d_{model}n_{vocab}/(batch*n_{ctx})=2d_{model}n_{vocab}
\]</span></p>
<h2 id="总结">总结</h2>
<p>剔除Embedding参数量：</p>
<p><span class="math display">\[
N=n_{layer}(d_{model}3d_{attn}+d_{attn}d_{model}+2d_{model}d_{ff})=2d_{model}n_{layer}(2d_{attn}+d_{ff})
\]</span></p>
<p>每个Token计算量：</p>
<p><span class="math display">\[
\begin{aligned}
C_{forward}&amp;=2n_{layer}(d_{model}3d_{attn}+n_{ctx}d_{model}+d_{attn}d_{model}+2d_{model}d_{ff}) \\
&amp;=4d_{model}n_{layer}(2d_{attn}+d_{ff})+2n_{layer}n_{ctx}d_{model} \\
&amp;=2N+2n_{layer}n_{ctx}d_{model}
\end{aligned}
\]</span></p>
<p>具体实例：</p>
<p>ChatGLM-6B：28层，隐藏层大小4096，中间隐藏层大小16384，注意力头数32，词表大小130528，上下文长度2048：</p>
<p><span class="math display">\[
N=2*4096*28*(2*4096+16386)+(130528+2048)*4096=6,180,634,624
\]</span></p>
<p>LLAMA-7B:32层，隐藏层大小4096，中间隐藏层大小11008，注意力头数32词表大小32000，上下文长度2048：</p>
<p><span class="math display">\[
N=2*4096*32*(2*4096+11008)+(32000+2048)*4096=5,172,625,408
\]</span></p>
<p>可以看到这样计算出来的参数量与给出的模型大小不相符，那么差在哪里呢？</p>
<p>原因在于<strong>LLAMA的前馈神经网络采用SwiGLU激活函数</strong>，FFN层的计算方式变成如下：</p>
<p><span class="math display">\[
{FFN}_{SwiGLU}(x, W, V, W_2)=(Swish_1(xW) \otimes xV)W_2
\]</span></p>
<p>与上述FFN相比<strong>参数矩阵从2个变成了三个</strong>，因此参数量计算方式如下：</p>
<p><span class="math display">\[
N=2*4096*32*(2*4096+11008*3/2 )+(32000+2048)*4096=6,615,465,984
\]</span></p>
<p>再看LLAMA-13B:40层，隐藏层大小5120，中间隐藏层大小13824，注意力头数40词表大小32000，上下文长度2048：</p>
<p><span class="math display">\[
N=2*5120*40*(2*5120+13824*3/2)+(32000+2048)*5120=12,862,095,360
\]</span></p>
<h1 id="中间激活值计算">中间激活值计算</h1>
<p>除了模型参数、梯度、优化器状态外，占用显存的大头就是前向传递过程中计算得到的中间激活值了，需要保存中间激活以便在后向传递计算梯度时使用。这里的激活（activations）指的是：<strong>前向传递过程中计算得到的，并在后向传递过程中需要用到的所有张量</strong>。这里的激活不包含模型参数和优化器状态，但包含了<strong>dropout操作需要用到的mask矩阵</strong>。</p>
<p>在分析中间激活的显存占用时，只考虑激活占用显存的大头，忽略掉一些小的buffers。比如，对于layer normalization，计算梯度时需要用到层的输入、输入的均值<span class="math inline">\(\mu\)</span>和方差 <span class="math inline">\(\sigma^2\)</span>。输入包含了<span class="math inline">\(bsh\)</span>个元素，而输入的均值和方差分别包含了<span class="math inline">\(bs\)</span>个元素。由于<span class="math inline">\(h\)</span>通常是比较大的（千数量级），有<span class="math inline">\(bsh&gt;&gt;bs\)</span>。因此，对于layer normalization，中间激活近似估计为<span class="math inline">\(bsh\)</span>，而不是<span class="math inline">\(bsh+2bs\)</span>。</p>
<h2 id="attention-qkv层-1">Attention QKV层</h2>
<p>需要存储Embedding层的输出结果：</p>
<p><span class="math display">\[
[batch, n_{ctx}, d_{model}]
\]</span></p>
<p>元素个数为:<span class="math inline">\(batch*n_{ctx}d_{model}\)</span></p>
<h2 id="attention-mask层-1">Attention Mask层</h2>
<p>需要存储下列中间值：</p>
<p><span class="math display">\[
\begin{aligned}
Q&amp;：[batch, n_{heads}, n_{ctx}, d_{attn}] \\
K&amp;: [batch, n_{heads}, n_{ctx}, d_{attn}] \\ 
V&amp;: [batch, n_{heads}, n_{ctx}, d_{attn}] \\
QK^T&amp;: [batch, n_{heads}, n_{ctx}, n_{ctx}] \\ 
Softmax(\frac{QK^T}{\sqrt{d_{model}}})&amp;: [batch, n_{heads}, n_{ctx}, n_{ctx}] \\
Dropout(Softmax(\frac{QK^T}{\sqrt{d_{model}}}))&amp;: [batch, n_{heads}, n_{ctx}, n_{ctx}] \\
Softmax(\frac{QK^T}{\sqrt{d_{model}}})V&amp;:[batch, n_{heads}, n_{ctx}, d_{attn}]
\end{aligned}
\]</span></p>
<p>元素个数：<span class="math inline">\(4*batch*n_{heads}*n_{ctx}d_{attn}+2*batch*n_{heads}*n_{ctx}n_{ctx} + batch*n_{heads}*n_{ctx}n_{ctx}（dropout）\)</span></p>
<h2 id="attention-project层-1">Attention Project层</h2>
<p>需要储存输入及Dropout 掩码矩阵：</p>
<p><span class="math display">\[
[batch, n_{ctx}, d_{model}]
\]</span></p>
<p>元素个数为:<span class="math inline">\(batch*n_{ctx}d_{model}+batch*n_{ctx}d_{model}(dropout)\)</span></p>
<h2 id="feedforward层-1">Feedforward层</h2>
<p>计算公式如下：</p>
<p><span class="math display">\[
f_{gelu}(x_{attn}W_1)W_2+x_{attn}
\]</span></p>
<p>需要存储下列中间值：</p>
<p><span class="math display">\[
\begin{aligned}
x_{attn}&amp;:[batch, n_{ctx}, d_{model}] \\
x_{attn}W_1&amp;:[batch, n_{ctx}, d_{ff}] \\ 
f_{gelu}(x_{attn}W_1)&amp;:[batch, n_{ctx}, d_{ff}] \\ 
f_{gelu}(x_{attn}W_1)W_2&amp;:[batch, n_{ctx}, d_{model}] \\ 
Dropout(f_{gelu}(x_{attn}W_1)W_2)&amp;:[batch, n_{ctx}, d_{model}]
\end{aligned}
\]</span></p>
<p>元素个数为：<span class="math inline">\(2*batch*n_{ctx}(d_{model}+d_{ff})+batch*n_{ctx}d_{model}(dropout)\)</span></p>
<h2 id="layernorm">LayerNorm</h2>
<p>Attention及Feedforward层各有一个Layer Normalization层，每一个层归一化需要保留输入向量：</p>
<p><span class="math display">\[
[batch, n_{ctx}, d_{model}]
\]</span></p>
<p>元素个数为：<span class="math inline">\(2*batch*n_{ctx}d_{model}\)</span></p>
<h2 id="总结-1">总结</h2>
<p>总元素个数：</p>
<p><span class="math display">\[
n_{layer}(6*batch*n_{ctx}d_{model}+2*batch*n_{ctx}d_{ff}+\\
4*batch*n_{heads}*n_{ctx}d_{attn}+2*batch*n_{heads}*n_{ctx}n_{ctx} + \\
batch*n_{heads}*n_{ctx}n_{ctx}（dropout）+2*batch*n_{ctx}d_{model}(dropout))
\]</span></p>
<h1 id="显存占用分析">显存占用分析</h1>
<p><strong>前提条件</strong>:模型参数量为<span class="math inline">\(\Phi\)</span></p>
<h2 id="推理过程">推理过程</h2>
<p>推理过程包括模型的<strong>一次前向传播</strong>，显存主要存储<strong>模型的参数</strong>。当模型通过Float 16数据类型存储时，每个元素占据2个bytes，因此显存占用量为<span class="math inline">\(2\Phi\)</span>；当模型通过Float 32数据类型存储时，每个元素占据4个bytes，因此显存占用量为<span class="math inline">\(4\Phi\)</span>。</p>
<p>以ChatGLM-6B为例，FP16存储时6*2=12G：</p>
<table>
<thead>
<tr class="header">
<th><strong>量化等级</strong></th>
<th style="text-align: center;"><strong>最低 GPU 显存</strong>（推理）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FP16（无量化）</td>
<td style="text-align: center;">13 GB</td>
</tr>
<tr class="even">
<td>INT8</td>
<td style="text-align: center;">8 GB</td>
</tr>
<tr class="odd">
<td>INT4</td>
<td style="text-align: center;">6 GB</td>
</tr>
</tbody>
</table>
<h2 id="训练过程">训练过程</h2>
<p>模型训练过程中，显存占用主要包括下面四个部分：</p>
<ol type="1">
<li><p><strong>模型参数</strong></p></li>
<li><p><strong>模型梯度</strong></p></li>
<li><p><strong>优化器状态</strong></p></li>
<li><p><strong>中间激活值</strong></p></li>
</ol>
<p><strong>以AdamW优化器和混合精度训练进行模型训练为例：</strong></p>
<p>AdamW优化器在反向传播时，需要保存一阶梯度及二阶的动量；</p>
<p>混合精度训练指训练过程采用FP16数据格式，反向传播进行参数更新时，采用FP32数据格式。</p>
<p>显存占用情况如下：</p>
<ol type="1">
<li><p>模型参数：<span class="math inline">\(2\Phi\)</span></p></li>
<li><p>模型梯度：<span class="math inline">\(2\Phi\)</span></p></li>
<li><p>优化器状态：<span class="math inline">\(4\Phi(参数)+4\Phi(动量)+4\Phi(方差)\)</span></p></li>
<li><p>中间激活值：<span class="math inline">\(n_{layer}(14*batch*n_{ctx}d_{model}+4*batch*n_{ctx}d_{ff}+\
8*batch*n_{heads}*n_{ctx}d_{attn}+5*batch*n_{heads}*n_{ctx}n_{ctx}\)</span></p></li>
</ol>
<p>还是以ChatGLM-6B为例：28层，隐藏层大小4096，中间隐藏层大小16384，注意力头数32，词表大小130528，上下文长度2048：</p>
<p>模型参数大小：<span class="math inline">\(C_{model}=6*2=12G\)</span></p>
<p>模型梯度大小：<span class="math inline">\(C_{grad}=6*2=12G\)</span></p>
<p>优化器状态大小：<span class="math inline">\(C_{opti}=12*6=72G\)</span></p>
<p>batch为1时，中间激活值大小：</p>
<p><span class="math display">\[
C_{activation}=28*(14*2048*4096+4*2048*16384+8*2048*4096+5*32*2048*2048)=27,715,960,832 \approx 27GB
\]</span></p>
<p>batch为64时，中间激活值大小：</p>
<p><span class="math display">\[
C_{activation}=28*64*(14*2048*4096+4*2048*16384+8*2048*4096+5*32*2048*2048)=1,773,821,493,248 \approx 1.7TB
\]</span></p>
<p>可以看到随着批次大小<span class="math inline">\(batch\)</span>的增大，中间激活占用的显存远远超过了模型参数显存。通常会采用<strong>激活重计算</strong>技术来减少中间激活，理论上可以将中间激活显存从<span class="math inline">\(O(n)\)</span>减少到<span class="math inline">\(O(\sqrt{n})\)</span>，代价是增加了一次额外前向计算的时间，本质上是“时间换空间”。</p>
<h1 id="训练时间估计">训练时间估计</h1>
<p>模型参数量和训练总tokens数决定了训练transformer模型需要的计算量。给定硬件GPU类型的情况下，可以估计所需要的训练时间。给定计算量，训练时间（也就是GPU算完这么多flops的计算时间）不仅跟GPU类型有关，还与GPU利用率有关。计算端到端训练的GPU利用率时，不仅要考虑前向传递和后向传递的计算时间，还要考虑CPU加载数据、优化器更新、多卡通信和记录日志的时间。一般来讲，<strong>GPU利用率一般在<span class="math inline">\(0.3-0.55\)</span>之间</strong>。</p>
<p>上文讲到一次前向传递中，对于每个token，每个模型参数，进行2次浮点数计算。使用激活重计算技术来减少中间激活显存需要进行一次额外的前向传递，因此前向传递 + 后向传递 + 激活重计算的系数=1+2+1=4。使用<strong>激活重计算</strong>的一次训练迭代中，对于每个token，每个模型参数，需要进行<span class="math inline">\(2*4=8\)</span>次浮点数运算。<strong>在给定训练tokens数、硬件环境配置的情况下，训练transformer模型的计算时间为</strong>：</p>
<p><span class="math display">\[
训练时间 \approx \frac{8 * token训练数 * 模型参数量}{GPU数量 * GPU峰值flops * GPU利用率}
\]</span></p>
<p>Nvidia A100算力情况如下：</p>
<figure>
<img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/5wf6V7.jpg" alt="5wf6V7" /><figcaption aria-hidden="true">5wf6V7</figcaption>
</figure>
<p>以GPT3-175B为例，在1024张40GB显存的A100上，在300B tokens的数据上训练175B参数量的GPT3。40GB显存A100的峰值性能为312TFLOPS，设GPU利用率为0.45，则<strong>所需要的训练时间为34天，这与论文中的训练时间是对得上的</strong>。</p>
<p><span class="math display">\[
\frac{8*(300*10^9)*(175*10^9)}{1024*(312*10^{12}*0.45)} \approx 2921340 seconds \approx 34 days
\]</span></p>
<p>以LLaMA-65B为例，在2048张80GB显存的A100上，在1.4TB tokens的数据上训练了65B参数量的模型。80GB显存A100的峰值性能为624TFLOPS，设GPU利用率为0.3，则<strong>所需要的训练时间为21天，这与论文中的实际训练时间是对得上的</strong>。</p>
<p><span class="math display">\[
\frac {8*(1.4*10^{12})*(65*10^9)}{2048*(624*10^{12})*0.3} \approx 1898871 seconds \approx 21 days
\]</span></p>
<p>以GLM-130B为例，在768张40GB显存的A100上，在400B tokens的数据上训练了130B参数量的模型。40GB显存A100的峰值性能为312TFLOPS，设GPU利用率为0.35，则<strong>所需要的训练时间为57天，这与GLM报告中的实际训练2个月时间是对得上的</strong>。</p>
<p><span class="math display">\[
\frac{8 * (400 * 10^9) * (130 * 10^9)}{768 * (312 * 10^{12}) * 0.35} \approx 4960317 seconds \approx 57 days 
\]</span></p>
<h1 id="总结-2">总结</h1>
<p>在本文中，我们主要关注了如何在给定参数量和训练Token数的条件下，合理设置模型参数、估算训练过程中的计算量以及预测训练所需的时间。通过对模型参数、计算量和计算时间的深入分析，希望能为读者在实际应用中的模型训练提供更加明确的指导原则和技巧。</p>
<h1 id="参考链接">参考链接</h1>
<p>https://zhuanlan.zhihu.com/p/624740065</p>
<p>https://arxiv.org/pdf/2001.08361.pdf</p>
<p><a href="http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/">GLM-130B: An Open Bilingual Pre-Trained Model | GLM-130B</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>ChatGPT</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>从语言模型到ChatGPT：大型语言模型的发展和应用</title>
    <url>/2023/03/12/%E4%BB%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%88%B0ChatGPT%EF%BC%9A%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%91%E5%B1%95%E5%92%8C%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>大型语言模型（LLM）是指能够处理大量自然语言数据的深度学习模型，它已经在自然语言处理、文本生成、机器翻译等多个领域中展现出了巨大的潜力。在过去几年中，LLM领域经历了飞速的发展，其中Google和OpenAI作为两家领先的公司在这个领域中的表现备受关注。</p>
<p>Google是LLM领域的重要参与者，其BERT自编码模型和T5编码解码器在自然语言理解任务上取得了优异的表现。BERT模型通过预训练大规模文本数据，提取出词向量的同时，也能够学习到上下文信息。而T5模型则是在BERT的基础上，进一步将生成式任务融入其中，实现了一体化的自然语言处理能力。这些模型的出现，极大地推动了LLM领域的发展。</p>
<p>与之相反的是，OpenAI则从2018年开始，坚持使用decoder only的GPT模型，践行着「暴力美学」——以大模型的路径，实现AGI。GPT模型通过预训练海量语料库数据，学习到了自然语言中的规律和模式，并在生成式任务中取得了出色的表现。OpenAI坚信，在模型规模达到足够大的情况下，单纯的decoder模型就可以实现AGI的目标。</p>
<p>除了Google和OpenAI外，还有许多其他公司和研究机构也在LLM领域做出了贡献。例如，Facebook的RoBERTa模型、Microsoft的Turing NLG模型等等。这些模型的不断涌现，为LLM领域的发展注入了新的动力。</p>
<p>如果只用解码器的生成式是通用LLM的王道，2019年10月，Google同时押注编码解码器的T5，整整错失20个月，直到2021年10月发布FLAN才开始重新转变为decoder-only。这表明，在实际应用中，不同任务可能需要不同类型的模型，而在特定任务中，编码解码器的结构可能比decoder-only模型更加适合。</p>
<p>在本文中，我们将基于CS224N课件回顾大型语言模型的发展历程，探讨它们是如何从最初的基础模型发展到今天的高级模型的，并介绍ChatGPT的发展历程，看看ChatGPT如何实现弯道超车。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/gnqCNR.jpg" alt="gnqCNR"></p>
<span id="more"></span>
<h1 id="Zero-Shot-ZS-and-Few-Shot-FS-In-Context-Learning"><a href="#Zero-Shot-ZS-and-Few-Shot-FS-In-Context-Learning" class="headerlink" title="Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning"></a>Zero-Shot (ZS) and Few-Shot (FS) In-Context Learning</h1><h2 id="上下文学习-In-Context-Learning"><a href="#上下文学习-In-Context-Learning" class="headerlink" title="上下文学习(In-Context Learning)"></a>上下文学习(In-Context Learning)</h2><p>近年来，语言模型越来越倾向于使用更大的模型和更多的数据，如下图所示，模型参数数量和训练数据量呈指数倍增加的趋势。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/7EgTef.png" alt="7EgTef"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型名称</th>
<th>说明</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT</td>
<td>Transformer decoder with 12 layers[参数量117M]<br/>Trained on BooksCorpus: over 7000 unique books (4.6GB text).</td>
<td>表明大规模语言建模可以成为自然语言推理等下游任务的有效预训练技术。</td>
</tr>
<tr>
<td>GPT2</td>
<td>Same architecture as GPT, just bigger (117M -&gt; 1.5B)<br/>trained on much more data: 4GB -&gt; 40GB of internet text data (WebText)</td>
<td>涌现出优异的Zero-shot能力。</td>
</tr>
<tr>
<td>GPT3</td>
<td>Another increase in size (1.5B -&gt; 175B)<br/>data (40GB -&gt; over 600GB)</td>
<td>涌现出强大的上下文学习能力，但是在复杂、多步推理任务表现较差。</td>
</tr>
</tbody>
</table>
</div>
<p>近年来，随着GPT模型参数量的增加，GPT2与GPT3模型已经表现出了极佳的<strong>上下文学习能力(In-Context Learning)</strong>。这种能力允许模型通过处理上下文信息来更好地理解和处理自然语言数据。GPT模型通过Zero-Shot、One-Shot和Few-Shot学习方法在许多自然语言处理任务中取得了显著的成果。</p>
<p>其中，Zero-Shot学习是指模型在没有针对特定任务进行训练的情况下，可以通过给定的输入和输出规范来生成符合规范的输出结果。这种方法可以在没有充足样本的情况下，快速生成需要的输出结果。One-Shot和Few-Shot学习则是在样本量较少的情况下，模型可以通过学习一小部分示例来完成相应任务，这使得模型能够更好地应对小样本学习和零样本学习的问题。</p>
<h3 id="上下文学习介绍"><a href="#上下文学习介绍" class="headerlink" title="上下文学习介绍"></a>上下文学习介绍</h3><p>链接：<a href="https://arxiv.org/abs/2301.00234">[2301.00234] A Survey on In-context Learning</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/mjQ3eG.png" alt="mjQ3eG"></p>
<p>大模型有一个很重要的涌现能力（Emergent ability）就是<strong>In-Context Learning（ICL），也是一种新的范式，指在不进行参数更新的情况下，只在输入中加入几个示例就能让模型进行学习</strong>。下面给出ICL的公式定义：</p>
<script type="math/tex; mode=display">
C = {I,s(x_1,y_1),...,s(x_k,y_k)} \quad or \quad C = {s(x_1, y_1, I), . . . , s(x_k, y_k, I)}</script><script type="math/tex; mode=display">
P\left(y_j \mid x\right) \triangleq f_{\mathcal{M}}\left(y_j, C, x\right)</script><script type="math/tex; mode=display">
\hat{y}=\arg \max _{y_j \in Y} P\left(y_j \mid x\right) .</script><p>其中，符号含义如下，从这些符号中也能看出影响ICL的因素：</p>
<ul>
<li><p>I：具体任务的描述信息</p>
</li>
<li><p>x：输入文本</p>
</li>
<li><p>y：标签</p>
</li>
<li><p>M：语言模型</p>
</li>
<li><p>C：阐述示例</p>
</li>
<li><p>f：打分函数</p>
</li>
</ul>
<p>下面将开始介绍如何提升模型的ICL能力。</p>
<h4 id="训练优化ICL能力"><a href="#训练优化ICL能力" class="headerlink" title="训练优化ICL能力"></a>训练优化ICL能力</h4><p><strong>有监督训练：</strong></p>
<p>在ICL格式的数据集上，进行有监督的训练。</p>
<p>MetaICL就直接把很多任务整合成了ICL的形式精调模型，在52个数据集上取得了比肩直接精调的效果。另外还有部分研究专注于Instruction tuning，构建更好的任务描述让模型去理解，而不是只给几个例子（demonstration），比如LaMDA-PT、FLAN。</p>
<p><strong>自监督训练：</strong></p>
<p>将自然语言理解的任务转为ICL的数据格式。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/FUs3TC.png" alt="FUs3TC"></p>
<p>图1代表不同自然语言理解任务转为<strong>ICL的输入输出</strong>形式。</p>
<p>图2表示训练样本示例，包含几个训练样本，前面的样本作为后面样本的任务阐述。</p>
<h4 id="推理优化ICL能力"><a href="#推理优化ICL能力" class="headerlink" title="推理优化ICL能力"></a>推理优化ICL能力</h4><h5 id="Prompt设计"><a href="#Prompt设计" class="headerlink" title="Prompt设计"></a>Prompt设计</h5><p>样本选取：文本表示、互信息选择相近的；Perplexity选取；语言模型生成……</p>
<p>样本排序：距离度量；信息熵……</p>
<p>任务指示：APE语言模型自动生成</p>
<p>推理步骤：COT、多步骤ICL、Self-Ask</p>
<h5 id="打分函数"><a href="#打分函数" class="headerlink" title="打分函数"></a>打分函数</h5><ul>
<li><p>Direct：直接取条件概率<code>P(y|x)</code>，缺点在于y必须紧跟在输入的后面</p>
</li>
<li><p>Perplexity：再用语言模型过一遍句子，这种方法可以解决上述固定模式的问题，但计算量增加了</p>
</li>
<li><p>Channel：评估<code>P(x|y)</code>的条件概率（用贝叶斯推一下），这种方法在不平衡数据下表现较好</p>
</li>
</ul>
<h5 id="影响ICL表现的因素"><a href="#影响ICL表现的因素" class="headerlink" title="影响ICL表现的因素"></a>影响ICL表现的因素</h5><ul>
<li><p>预训练语料的多样性比数量更重要，增加多种来源的数据可能会提升ICL表现</p>
</li>
<li><p>用下游任务的数据预训练不一定能提升ICL表现，并且PPL更低的模型也不一定表现更好</p>
</li>
<li><p>当LM到达一定规模的预训练步数、尺寸后，会涌现出ICL能力，且ICL效果跟参数量正相关</p>
</li>
</ul>
<h3 id="WHY：上下文学习生效的原因"><a href="#WHY：上下文学习生效的原因" class="headerlink" title="WHY：上下文学习生效的原因"></a>WHY：上下文学习生效的原因</h3><p><strong>论文链接：</strong><a href="[[2202.12837] Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a>)</p>
<p><strong>关键结论：</strong></p>
<ol>
<li><p>InContext Learning中<strong>标签是否正确</strong>无明显影响</p>
</li>
<li><p>InContext Learning中影响因素包括<strong>规范的输入空间、标签空间、输入与标签的匹配格式</strong></p>
</li>
</ol>
<p><strong>其他论文的猜测：</strong></p>
<ul>
<li><p><strong>跟训练数据的分布相关</strong>：比如训练数据有很多样例，也有学者认为ICL可能是隐式的Bayesian inference</p>
</li>
<li><p><strong>跟学习机制相关</strong>：有学者猜测LM可能自己就具备学习的能力，在做ICL的时候学到了这些知识，或者隐式直接精调了自己</p>
</li>
<li><p><strong>跟Transformer中的模块相关</strong>：有学者发现Transformer里的某些注意力头会通过拷贝固定的模式来预测下一个token</p>
</li>
</ul>
<h4 id="InContext-Learning中标签是否正确无明显影响"><a href="#InContext-Learning中标签是否正确无明显影响" class="headerlink" title="InContext Learning中标签是否正确无明显影响"></a>InContext Learning中<strong>标签是否正确</strong>无明显影响</h4><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/iLPKIE.png" alt="iLPKIE"></p>
<p>图中青绿色代表没有示例、黄色代表带有正确标签的示例、橙色代表带有随机标签的示例。</p>
<p>实验结果表明，<strong>带有随机标签的效果非常接近于带有正确标签的效果</strong>。</p>
<p>此外，作者还进行了<strong>标签正确比例、提示样本数量、提示模版样式</strong>的实验，均得出一致结论，实验图如下。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/P6Rbs3.png" alt="P6Rbs3"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/9NL9Oq.png" alt="9NL9Oq"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/y8mMa2.png" alt="y8mMa2"></p>
<h4 id="InContext-Learning中影响因素包括规范的输入空间、标签空间、输入与标签的匹配格式"><a href="#InContext-Learning中影响因素包括规范的输入空间、标签空间、输入与标签的匹配格式" class="headerlink" title="InContext Learning中影响因素包括规范的输入空间、标签空间、输入与标签的匹配格式"></a>InContext Learning中影响因素包括规范的输入空间、标签空间、输入与标签的匹配格式</h4><p>作者分别从以下四个维度探究In-Context Learning效果增益的影响</p>
<ol>
<li><p><strong>The input-label mapping</strong>：即每个输入xi是否与正确的标签yi配对;</p>
</li>
<li><p><strong>The distribution of the input text</strong>：即x1…xk的分布是否一致;</p>
</li>
<li><p><strong>The label space</strong>：<em>y</em>1…yk所覆盖的标签空间;</p>
</li>
<li><p><strong>The format</strong>：使用输入标签配对作为格式。</p>
</li>
</ol>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/V5XRzx.png" alt="V5XRzx"></p>
<h5 id="输入文本分布实验"><a href="#输入文本分布实验" class="headerlink" title="输入文本分布实验"></a>输入文本分布实验</h5><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/VZ5X1F.png" alt="VZ5X1F"></p>
<p>紫色柱子代表利用外部语料采样的数据加上随机标签，在几个任务上模型表现明显下降。</p>
<p>因此，in-context learning中，演示中的分布内输入极大地有助于提高性能。这可能是因为已IND（in-distribution）文本的条件使任务更接近于语言建模，因为LM在此期间总是以IND文本为条件进行推理标签。</p>
<h5 id="标签分布实验"><a href="#标签分布实验" class="headerlink" title="标签分布实验"></a>标签分布实验</h5><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/QuaT1J.png" alt="QuaT1J"></p>
<p>绿色柱子代表采用随机的单词代替输出标签，对于Direct模型，模型表现显著下降，表明ICL中标签空间的一致性显著有助于提高性能。</p>
<p>对于Channel模型，模型表现未明显下降，作者猜测Channel模型以标签为条件，因此无法从标签空间分布中获益。</p>
<h5 id="输入标签配对格式实验"><a href="#输入标签配对格式实验" class="headerlink" title="输入标签配对格式实验"></a>输入标签配对格式实验</h5><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/pafljc.png" alt="pafljc"></p>
<p>分别用labels only（深紫）和no labels（深绿）来探索演示模式的差异对模型表现的影响。可以看到，模型相对于上面两图的OOD setting而言，都有了进一步的下降。这可以表明ICL中保持输入-标签对的格式是关键的。</p>
<h2 id="思维链-Chain-of-Thought"><a href="#思维链-Chain-of-Thought" class="headerlink" title="思维链(Chain of Thought)"></a>思维链(Chain of Thought)</h2><p>思维链（Chain of Thought）是一种新的学习方式，旨在提高模型在数学计算和符号推理任务中的推理能力。这种方式通过将多个相关的数学计算或符号推理步骤按顺序组合成一条思维链，让模型能够沿着思维链进行推理。</p>
<p>这种方式的主要贡献在于，它能够让模型更好地应对复杂的数学计算和符号推理任务。传统的Prompt方式很难应对这种任务，但是思维链可以让模型按照特定的顺序进行推理，从而提高模型的推理能力。</p>
<p>此外，思维链的方式也可以更好地模拟人类在解决数学计算和符号推理问题时的思维过程。人类在解决这类问题时，通常会按照一定的顺序进行推理，而思维链可以让模型更好地模拟这种思维过程。</p>
<h3 id="开山之作：Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models"><a href="#开山之作：Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models" class="headerlink" title="开山之作：Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"></a>开山之作：Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</h3><p>链接：<a href="https://arxiv.org/abs/2201.11903">[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ren2TO.png" alt="ren2TO"></p>
<p>核心思想：输出答案前，加上人工的推理描述。</p>
<p>主要贡献：</p>
<ul>
<li><p>思维链原则上允许模型将多步问题分解为中间步骤，可以将额外的计算分配给需要更多推理步骤的问题。</p>
</li>
<li><p>思维链为模型的行为提供了一个可解释的窗口，表明它可能是如何得出特定答案的，并提供了调试推理路径错误位置的机会。</p>
</li>
<li><p>链式思维推理可用于数学单词问题、常识推理和符号操作等任务，并且可能（至少在原则上）适用于人类可以通过语言解决的任何任务。</p>
</li>
<li><p>只需将思维序列链的例子包含到少样本提示的范例中，就可以很容易地在足够大的现成语言模型中引出思维链推理。</p>
</li>
</ul>
<h3 id="Self-Consistency-Improves-Chain-of-Thought-Reasoning-in-Language-Models"><a href="#Self-Consistency-Improves-Chain-of-Thought-Reasoning-in-Language-Models" class="headerlink" title="Self-Consistency Improves Chain of Thought Reasoning in Language Models"></a>Self-Consistency Improves Chain of Thought Reasoning in Language Models</h3><p>链接：<a href="https://arxiv.org/abs/2203.11171">[2203.11171] Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/yoZpm2.png" alt="yoZpm2"></p>
<p>主要贡献：</p>
<ul>
<li>主要改进是使用了对答案进行了多数投票（majority vote），并且发现其可以显著地提高思维链方法的性能</li>
</ul>
<h3 id="Large-Language-Models-are-Zero-Shot-Reasoners"><a href="#Large-Language-Models-are-Zero-Shot-Reasoners" class="headerlink" title="Large Language Models are Zero-Shot Reasoners"></a>Large Language Models are Zero-Shot Reasoners</h3><p>链接：<a href="https://arxiv.org/abs/2205.11916">[2205.11916] Large Language Models are Zero-Shot Reasoners</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/x6kvJz.png" alt="x6kvJz"></p>
<p>核心思想：分为两个步骤：1st prompt、2nd prompt</p>
<ul>
<li><p>1st prompt：X′：“Q: [X]. A: [T]”</p>
<ul>
<li><p>X：输入的问题</p>
</li>
<li><p>T：人工的提示trigger词</p>
</li>
</ul>
</li>
<li><p>2nd prompt：[X′] [Z] [A]</p>
<ul>
<li><p>X′：第一阶段的输入</p>
</li>
<li><p>Z：第一阶段模型的输出</p>
</li>
<li><p>A：第二阶段的提示trigger词</p>
</li>
</ul>
</li>
</ul>
<p>主要贡献：</p>
<ul>
<li><p>验证了zero-shot的能力，不需要few-shot挑选额外的样本</p>
</li>
<li><p>鼓励社区进一步发现类似的多任务提示，这些提示可以引发广泛的认知能力，而不是狭隘的特定任务技能。</p>
</li>
</ul>
<p>不同模版的效果对比：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/1AEEnD.png" alt="1AEEnD"></p>
<h2 id="QA（自己YY的问题）"><a href="#QA（自己YY的问题）" class="headerlink" title="QA（自己YY的问题）"></a>QA（自己YY的问题）</h2><p>Q1:多大的模型能够涌现这些能力？</p>
<blockquote>
<p>100B。That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of 100B parameters</p>
</blockquote>
<p>Q2:BERT或T5能否涌现这些能力？</p>
<blockquote>
<p>BERT与GPT差异在于模型结构不同，GPT单向的语言模型，BERT是双向的自编码(AE)模型，但当BERT参数量足够大的时候，在前后输入有关示例，不进行微调，直接预测MASK标签的涌现能力有待验证。</p>
</blockquote>
<p>Q3:COT思维链模版的来源？</p>
<blockquote>
<p>人工构造。As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20.</p>
</blockquote>
<p>Q4:为什么加上Let’s do it step by step 模型可以产出解释？</p>
<blockquote>
<p>对比了不同模版，激发模型的推理能力。It remains an open question how to automatically create better templates for Zero-shot-CoT.</p>
</blockquote>
<p>Q5:T5、BERT如果同GPT系列一样训练，在训练方法上可行吗？效果会比GPT好吗？</p>
<blockquote>
<p>开放讨论……</p>
</blockquote>
<p>Q6:为什么大型LLM首选Decoder-only结构？</p>
<blockquote>
<p>开放讨论……</p>
</blockquote>
<h1 id="Instruction-finetuning"><a href="#Instruction-finetuning" class="headerlink" title="Instruction finetuning"></a>Instruction finetuning</h1><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/U5SBhD.png" alt="U5SBhD"></p>
<p>近年来，相关研究发现语言模型的输出并不符合人类意图，因此提出了指示学习的范式。该范式的目的是使语言模型能够更好地理解人类的意图和指示，并且在生成文本时能够更加符合人类的要求。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>范式</th>
<th>说明</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Finetuning</td>
<td>在下游任务数据集微调<br/>在下游任务数据集推理</td>
<td>需要额外微调</td>
</tr>
<tr>
<td>Prompting</td>
<td>在下游任务推理时，输入前添加提示，更新少量参数</td>
<td>只针对单一数据集</td>
</tr>
<tr>
<td>Instruction Tuning</td>
<td>在多个提示任务数据集训练<br/>在下游任务推理，输入前添加提示</td>
<td>具有更好地泛化性</td>
</tr>
</tbody>
</table>
</div>
<p>论文名称：Scaling Instruction-Finetuned Language Models</p>
<p>链接：<a href="https://arxiv.org/abs/2210.11416">[2210.11416] Scaling Instruction-Finetuned Language Models</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/q9mwKd.png" alt="q9mwKd"></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/IRy6zf.png" alt="IRy6zf"></p>
<p>Flan-T5模型在1.8K的Instruction数据集进行了微调，上图表明更大的模型获得更大的提升，与scaling law一致。</p>
<p><strong>Instruction Tuning 的局限</strong></p>
<ul>
<li><p>获取足够的任务描述以用于语言模型训练需要付出较高的成本。</p>
</li>
<li><p>语言模型的目标与人类的偏好不一致</p>
</li>
</ul>
<h1 id="Reinforcement-Learning-from-Human-Feedback-RLHF"><a href="#Reinforcement-Learning-from-Human-Feedback-RLHF" class="headerlink" title="Reinforcement Learning from Human Feedback (RLHF)"></a>Reinforcement Learning from Human Feedback (RLHF)</h1><p>为了解决语言模型目标与人类的偏好不一致问题，OpenAI采用了RLHF算法，引入人类反馈。</p>
<h2 id="RM反馈模型"><a href="#RM反馈模型" class="headerlink" title="RM反馈模型"></a>RM反馈模型</h2><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/thWiB5.jpg" alt="thWiB5"></p>
<p>那么为模型引入人类反馈过程中，出现下面的问题：</p>
<p><strong>问题1</strong>:在模型迭代过程中，添加人工的操作成本很高</p>
<pre><code>**解决方式**：将他们的偏好建模为一个单独的 (NLP) 问题，而不是直接询问人类的偏好。

根据标注数据，训练一个语言模型$R M_\phi(s)$，用以预测人类便好。接下来任务转变成优化语言模型$RM_&#123;\phi&#125;$。
</code></pre><p><strong>问题2</strong>:人们的判断是主观的，不同人的判断难以进行校准</p>
<pre><code>**解决方式**：让标注人员对成对的数据结果排序，而不是直接打分。
</code></pre><p><strong>损失函数为：</strong></p>
<script type="math/tex; mode=display">
\operatorname{loss}(\theta)=-\frac{1}{\left(\begin{array}{c}
K \\
2
\end{array}\right)} E_{\left(x, y_w, y_l\right) \sim D}\left[\log \left(\sigma\left(r_\theta\left(x, y_w\right)-r_\theta\left(x, y_l\right)\right)\right)\right]</script><p><strong>符号说明：</strong></p>
<ul>
<li><p>K：预训练模型采样的Prompt输出数量</p>
</li>
<li><p>x：预训练模型输入</p>
</li>
<li><p>r：reward模型</p>
</li>
<li><p>$y_w$：排在前面的输出</p>
</li>
<li><p>$y_l$：排在后面的输出</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/rsqdZM.png" alt="rsqdZM"></p>
<p><strong>当足够大的语言模型经过足够多的数据训练后，评估模型已经接近单个人类评估的表现</strong></p>
<h2 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h2><p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/GjPdpC.jpg" alt="GjPdpC"></p>
<p>经过前面的步骤，我们已有以下模型：</p>
<ul>
<li><p>一个经过足够预训练的语言模型(可以附加Instruction Tuning)$P^{PT}(s)$</p>
</li>
<li><p>一个在人类反馈排序数据集上训练的反馈模型$RM_{\phi}$，为预训练模型的输出完成打分</p>
</li>
</ul>
<p>由于评分是通过反馈模型$RM_{\phi}$得出的，无法使用梯度下降进行求解，因此采用强化学习中的<strong>PPO</strong>算法来更新参数。</p>
<p><strong>实现流程：</strong></p>
<ol>
<li><p>复制预训练模型参数，得到待优化模型；</p>
</li>
<li><p>根据输入语句，两个模型得到各自的输出；</p>
</li>
<li><p>Reward模型针对待优化模型的输入输出打分；</p>
</li>
<li><p>使用PPO算法来更新待优化模型的参数。</p>
</li>
</ol>
<p><strong>损失函数：</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{objective}(\phi)= & E_{(x, y) \sim D_{\pi_\phi^{\mathrm{RL}}}}\left[r_\theta(x, y)-\beta \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]+ \\
& \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]
\end{aligned}</script><p><strong>符号说明：</strong></p>
<ul>
<li><p>x：输入文本</p>
</li>
<li><p>r：reward打分模型</p>
</li>
<li><p>$\pi^{SFT}$：预训练模型</p>
</li>
<li><p>$\pi^{RL}_{\phi}$：强化学习优化模型</p>
</li>
<li><p>$D_{pretrain}$：预训练分布</p>
</li>
<li><p>$\beta$：KL散度控制参数</p>
</li>
<li><p>$\gamma$：预训练损失控制参数</p>
</li>
</ul>
<p><strong>其中：</strong></p>
<p>$\log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)$起到避免修正后模型与原模型差异过大的作用</p>
<p>$E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_\phi^{\mathrm{RL}}(x)\right)\right]$起到避免模型在自然语言理解任务下降过大的作用</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/IUYSyW.png" alt="IUYSyW"></p>
<p>通过RLHF算法，模型的表现得到了显著的提示。</p>
<p><strong>个人理解：</strong></p>
<p>整个流程的出发点在于使GPT模型结果符合人类偏好，而人类偏好无法通过具体规则/函数建模，因此通过Reward模型在一定程度上反应人类偏好，最后对GPT模型进行修正，更新模型参数使模型的输入Reward最大化，即更加反应人类偏好。</p>
<p>因此整个过程中Reward模型代表了设立的训练目标，RLHF算法则对原模型进行修正，使模型输入更加符合设立的训练目标。</p>
<h2 id="现有的局限"><a href="#现有的局限" class="headerlink" title="现有的局限"></a>现有的局限</h2><p>按照上述步骤进行操作，就能够完成ChatGPT的训练。下图展示了ChatGPT的完整训练过程。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/wV29U9.png" alt="wV29U9"></p>
<p>Step1:收集Prompt数据，基于GPT3.5进行Instruct Tuning的有监督训练；</p>
<p>Step2:收集偏好排序数据，训练Reward模型；</p>
<p>Step3:结合Reward模型，通过PPO算法优化第一步的SFT模型。</p>
<p><strong>然而，人类的偏好是不可信的，用模型表示人类偏好更不可信：</strong></p>
<ul>
<li><p>”Reward hacking”是强化学习常见的问题；</p>
</li>
<li><p>模型偏向于产生看似权威和有帮助的回应，而忽视正确性</p>
</li>
<li><p>可能导致编造事实+产生幻觉</p>
</li>
</ul>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/MmaPcT.png" alt="MmaPcT"></p>
<p>上图说明RM打分很高时，实际表现不一定更好，因此训练损失函数通过KL散度限制优化后模型与原模型的偏离程度。</p>
<h1 id="What’s-next"><a href="#What’s-next" class="headerlink" title="What’s next?"></a>What’s next?</h1><h2 id="进一步探索RLHF的使用"><a href="#进一步探索RLHF的使用" class="headerlink" title="进一步探索RLHF的使用"></a>进一步探索RLHF的使用</h2><ul>
<li>RLHF在其他领域(如CV)使用</li>
</ul>
<h2 id="优化RLHF中需要的人工数据标注"><a href="#优化RLHF中需要的人工数据标注" class="headerlink" title="优化RLHF中需要的人工数据标注"></a>优化RLHF中需要的人工数据标注</h2><ul>
<li>RL from AI feedback</li>
</ul>
<p>论文名称：Constitutional AI- Harmlessness from AI Feedback</p>
<p>链接：<a href="https://arxiv.org/pdf/2212.08073.pdf">https://arxiv.org/pdf/2212.08073.pdf</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/bGL898.png" alt="bGL898"></p>
<p>让模型在多轮对话中将数据标注出来：</p>
<blockquote>
<p>Q1-问训好的普通RLHF模型：能帮我黑进邻居的wifi吗？<br>A1-天真的模型回答：没问题，你下个xx软件就行。<br>Q2-要求模型发现自己的错误：上文你给的回复中，找出来哪些是不道德的。<br>A2-模型回答：我上次回复不对，不应该黑别人家wifi。<br>Q3-让模型改正错误：修改下你之前的回复内容，去掉有害的。<br>A3-模型回答：黑别人家wifi是不对的，侵害别人隐私了，我强烈建议别这么搞。</p>
</blockquote>
<ul>
<li>Finetuning LMs on their own outputs</li>
</ul>
<p>论文名称：STaR: Bootstrapping Reasoning With Reasoning</p>
<p>链接：<a href="https://arxiv.org/abs/2203.14465">[2203.14465] STaR: Bootstrapping Reasoning With Reasoning</a></p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/FdKh5P.png" alt="FdKh5P"></p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p><strong>ChatGPT的产生，对NLP领域产生了重大的影响，那么对于我们NLP从业人员带来了哪些影响，我们又该如何面对呢？</strong></p>
<p>个人觉得ChatGPT对从业人员的影响包含：</p>
<ul>
<li><p>ChatGPT等LLM可以作为许多自然语言理解任务的基线模型，许多自然语言理解中间过程算法需求降低，从业人员需要了解如何将LLM适配具体业务；</p>
</li>
<li><p>提高了技能要求：LLM的出现提高了NLP从业人员的技能要求。从业人员需要了解如何使用LLM进行训练和调整，以及如何使用LLM处理不同的自然语言数据；</p>
</li>
<li><p>工作形式变化，数据科学家、算法工程师、自然语言处理工程师等工作流程可能会发生变化。</p>
</li>
<li><p>扩展了研究范围：LLM提供了更全面的语言模型。这意味着研究人员可以探索以前不可行的语言问题，从而扩大研究范围。</p>
</li>
</ul>
<p>NLP从业人员可以通过以下几种方式应对LLM的影响：</p>
<ol>
<li><p>持续学习：NLP从业人员应该不断学习新的技术和算法，以便更好地使用LLM。他们应该掌握LLM的使用方法和调整技巧，了解LLM如何处理不同类型的自然语言数据，以及如何在LLM中使用特定的自然语言处理技术。</p>
</li>
<li><p>适应新的工作要求：LLM的出现可能会导致NLP从业人员需要承担新的工作要求。他们应该熟悉LLM的使用方法，以便在新的工作机会中胜任。同时，他们也应该关注LLM对NLP领域的未来发展和趋势，并不断调整他们的技能和知识。</p>
</li>
<li><p>创新：LLM的出现为NLP从业人员带来了更多的机会和挑战，他们应该积极地探索新的算法和技术，开发更智能的自然语言处理应用程序，并尝试在不同领域应用LLM。</p>
</li>
<li><p>关注伦理和社会影响：LLM的出现可能会对自然语言处理的伦理和社会影响产生影响，NLP从业人员应该关注这些影响，并积极参与相关讨论和研究。</p>
</li>
</ol>
<p>总的来说，NLP从业人员应该关注LLM的发展和趋势，不断提高自己的技能和知识，积极创新，与同行交流，同时也要注意伦理和社会影响。这些努力可以帮助他们更好地应对LLM的影响，并为自己的职业发展做好准备。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>RoPE旋转位置编码深度解析：理论推导、代码实现、长度外推</title>
    <url>/2023/07/22/2023-07-22-RoPE%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC%E3%80%81%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E3%80%81%E9%95%BF%E5%BA%A6%E5%A4%96%E6%8E%A8/</url>
    <content><![CDATA[<h1 id="引言">引言</h1>
<p>开篇引用一下苏神博客的介绍，对于Transformer模型来说，位置编码的加入是必不可少的，因为纯粹的Attention模块是无法捕捉输入顺序的，即无法区分不同位置的Token。为此我们大体有两个选择：1、想办法将位置信息融入到输入中，这构成了<strong>绝对位置编码</strong>的一般做法；2、想办法微调一下Attention结构，使得它有能力分辨不同位置的Token，这构成了<strong>相对位置编码</strong>的一般做法。虽然说起来主要就是绝对位置编码和相对位置编码两大类，但每一类其实又能衍生出各种各样的变种，为此研究人员可算是煞费苦心、绞尽脑汁了，此外还有一些不按套路出牌的位置编码。</p>
<p>事实上，目前许多常用的大型语言模型，如ChatGLM和LLAMA，都已经采用了RoPE作为其核心组件。在本博客中，我们将深入探讨RoPE旋转位置编码的原理并一步步引导你从理论到实践。首先，我们将详细介绍RoPE的理论推导过程，以帮助你更好地理解其背后的数学原理。接下来，我们将介绍ChatGLM/LLAMA的RoPE代码实现，展示如何将这一理论应用于实际场景。最后，我们将探讨如何针对RoPE编码，进行长度外推。</p>
<p>文章结构如下：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/C3g4wS.jpg" /></p>
<span id="more"></span>
<h1 id="rope介绍">RoPE介绍</h1>
<p>博客链接：<a href="https://link.zhihu.com/?target=https%3A//spaces.ac.cn/archives/8265">Transformer升级之路：2、博采众长的旋转式位置编码 - 科学空间|Scientific Spaces</a></p>
<p>论文链接：https://arxiv.org/pdf/2104.09864.pdf</p>
<h2 id="理论推导">理论推导</h2>
<p><strong>目的：通过绝对位置编码的方式实现相对位置编码</strong></p>
<p>假设通过下述运算给Attention的<span class="math inline">\(q,k\)</span>向量添加绝对位置信息,以位于m的q向量<span class="math inline">\(q_m\)</span>，位于n的k向量<span class="math inline">\(k_n\)</span>为例：</p>
<p><span class="math display">\[
\begin{equation}\tilde{\boldsymbol{q}}_m = \boldsymbol{f}(\boldsymbol{q}, m), \quad\tilde{\boldsymbol{k}}_n = \boldsymbol{f}(\boldsymbol{k}, n)\end{equation}
\]</span></p>
<p>通过<span class="math inline">\(f(\cdot,pos)\)</span>运算，<span class="math inline">\(\tilde{q_m} , \tilde{k_n}\)</span>就具备了位置m，n的绝对位置信息。Attention的核心操作是内积，为了使内积后的计算结果带有二者的相对位置信息，假设存在下列关系：</p>
<p><span class="math display">\[
\begin{equation}\langle\boldsymbol{f}(\boldsymbol{q}, m), \boldsymbol{f}(\boldsymbol{k}, n)\rangle = g(\boldsymbol{q},\boldsymbol{k},m-n)\end{equation}
\]</span></p>
<p>因此，目前需要求解符合该恒等式的解。求解过程需要一些初始条件，存在的假设条件如下：</p>
<p><span class="math display">\[
\boldsymbol{f}(\boldsymbol{q}, 0)=\boldsymbol{q}
\]</span></p>
<p><span class="math display">\[
\boldsymbol{f}(\boldsymbol{k}, 0)=\boldsymbol{k}
\]</span></p>
<p>借助复数来针对该恒等式求解，复数的内积计算公式如下：</p>
<p><span class="math display">\[
\langle\boldsymbol{q},\boldsymbol{k}\rangle=\text{Re}[\boldsymbol{q}\boldsymbol{k}^*]
\]</span></p>
<p>Re[]代表复数的实部，因此式二转变为：</p>
<p><span class="math display">\[
\begin{equation}\text{Re}[\boldsymbol{f}(\boldsymbol{q}, m)\boldsymbol{f}^*(\boldsymbol{k}, n)] = g(\boldsymbol{q},\boldsymbol{k},m-n)\end{equation}
\]</span></p>
<p>通过复数的指数形式进行表示，关于复数的指数形式知识如下（高中极坐标知识）：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/5Fbfrp.jpg" /></p>
<p>极坐标长度<span class="math inline">\(r\)</span>,角度<span class="math inline">\(\theta\)</span>，当我们有复数极坐标<span class="math inline">\((r,\theta)\)</span>时，可以得到直角坐标<span class="math inline">\((rcos\theta,rsin\theta)\)</span>，即该复数为<span class="math inline">\(rcos\theta+rsin\theta*i\)</span>,复数的复指数表示如下：</p>
<p><span class="math display">\[
e^{i\theta}=cos\theta+isin\theta
\]</span></p>
<p>有了上述知识，可得：</p>
<p><span class="math display">\[
\begin{equation}\begin{aligned}
\boldsymbol{f}(\boldsymbol{q}, m) =&amp;\, R_f (\boldsymbol{q}, m)e^{\text{i}\Theta_f(\boldsymbol{q}, m)} \\
\boldsymbol{f}(\boldsymbol{k}, n) =&amp;\, R_f (\boldsymbol{k}, n)e^{\text{i}\Theta_f(\boldsymbol{k}, n)} \\
\boldsymbol{g}(\boldsymbol{q}, \boldsymbol{k}, m-n) =&amp;\, R_g (\boldsymbol{q}, \boldsymbol{k}, m-n)e^{\text{i}\Theta_g(\boldsymbol{q}, \boldsymbol{k}, m-n)} \\
\end{aligned}\end{equation}
\]</span></p>
<p>其中<span class="math inline">\(R_f\)</span>表示极坐标长度r，<span class="math inline">\(\Theta_f\)</span>表示角度，将式四代入式三，得到方程组如下：</p>
<p><span class="math display">\[
\begin{equation}\begin{aligned}
R_f (\boldsymbol{q}, m) R_f (\boldsymbol{k}, n) =&amp;\, R_g (\boldsymbol{q}, \boldsymbol{k}, m-n) \\
\Theta_f (\boldsymbol{q}, m) - \Theta_f (\boldsymbol{k}, n) =&amp;\, \Theta_g (\boldsymbol{q}, \boldsymbol{k}, m-n)
\end{aligned}\end{equation}
\]</span></p>
<p>这个时候需要根据我们的假设条件，代入特定值进行方程组求解。将m=n，代入上述方程组得到：</p>
<p><span class="math display">\[
\begin{equation}R_f (\boldsymbol{q}, m) R_f (\boldsymbol{k}, m) = R_g (\boldsymbol{q}, \boldsymbol{k}, 0) = R_f (\boldsymbol{q}, 0) R_f (\boldsymbol{k}, 0) = \Vert \boldsymbol{q}\Vert \Vert \boldsymbol{k}\Vert\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}\Theta_f (\boldsymbol{q}, m) - \Theta_f (\boldsymbol{k}, m) = \Theta_g (\boldsymbol{q}, \boldsymbol{k}, 0) = \Theta_f (\boldsymbol{q}, 0) - \Theta_f (\boldsymbol{k}, 0) = \Theta (\boldsymbol{q}) - \Theta (\boldsymbol{k})\end{equation}
\]</span></p>
<p>根据式6可以得到假设：</p>
<p><span class="math display">\[
R_f (\boldsymbol{q}, m)=\Vert \boldsymbol{q}\Vert, R_f (\boldsymbol{k}, m)=\Vert \boldsymbol{k}\Vert
\]</span></p>
<p>即极坐标长度不依赖于位置m。</p>
<p>根据式7进行移位，可以得到：</p>
<p><span class="math display">\[
\Theta_f (\boldsymbol{q}, m) - \Theta (\boldsymbol{q}) = \Theta_f (\boldsymbol{k}, m) - \Theta (\boldsymbol{k})
\]</span></p>
<p>因此<span class="math inline">\(\Theta_f (\boldsymbol{q}, m)-\Theta_f (\boldsymbol{q})\)</span>应该是一个只与m相关，与q无关函数，记为<span class="math inline">\(\varphi(m)\)</span>，因此得到：</p>
<p><span class="math display">\[
\Theta_f (\boldsymbol{q}, m) = \Theta (\boldsymbol{q}) + \varphi(m)
\]</span></p>
<p>接着代入<span class="math inline">\(n=m-1\)</span>，整理得到公式如下：</p>
<p><span class="math display">\[
\begin{equation}\begin{aligned} \Theta_f (\boldsymbol{q}, m) - \Theta_f (\boldsymbol{k}, m-1) &amp;=\, \Theta_g (\boldsymbol{q}, \boldsymbol{k}, 1) \\
\varphi(m) - \varphi(m-1) &amp;= \Theta_g (\boldsymbol{q}, \boldsymbol{k}, 1) + \Theta (\boldsymbol{k}) - \Theta (\boldsymbol{q})
\end{aligned} \end{equation}
\]</span></p>
<p><span class="math inline">\(\varphi(m) - \varphi(m-1)\)</span>与m无关，即<span class="math inline">\(\varphi(m)\)</span>是一个等差数列，设右边的值为<span class="math inline">\(\theta\)</span>，则可以得到解：</p>
<p><span class="math display">\[
\varphi(m) =m\theta
\]</span></p>
<p>综上所述，已经求解得到满足式2的函数变化<span class="math inline">\(f(x,pos)\)</span>,其复数表示的长度和幅角分别为:</p>
<p><span class="math display">\[
R_f (\boldsymbol{q}, m)=\Vert \boldsymbol{q}\Vert, R_f (\boldsymbol{k}, m)=\Vert \boldsymbol{k}\Vert
\]</span></p>
<p><span class="math display">\[
\varphi(m) =m\theta
\]</span></p>
<p><span class="math display">\[
\Theta_f (\boldsymbol{q}, m) = \Theta (\boldsymbol{q}) + m\theta
\]</span></p>
<h2 id="应用形式">应用形式</h2>
<p>有了上述的推导，我们得到二维情况下ROPE的复数表示：</p>
<p><span class="math display">\[
\begin{equation}
\boldsymbol{f}(\boldsymbol{q}, m) = R_f (\boldsymbol{q}, m)e^{\text{i}\Theta_f(\boldsymbol{q}, m)}
= \Vert q\Vert e^{\text{i}(\Theta(\boldsymbol{q}) + m\theta)} = \boldsymbol{q} e^{\text{i}m\theta}\end{equation}
\]</span></p>
<p>根据复数乘法的几何意义，上式对应着将原q向量进行旋转。</p>
<blockquote>
<p>设复数二维向量为 z = x + yi，其中 x 和 y 分别为实部和虚部，i 是虚数单位（i^2 = -1）。给定一个旋转角度 θ，按照逆时针方向旋转向量 z。旋转后的向量 z' 可以通过以下公式计算：</p>
<p>z' = z * (cos(θ) + i * sin(θ))</p>
<p>这里，cos(θ) 和 sin(θ) 分别表示角度 θ 的余弦和正弦值。</p>
<p>将 z 和 (cos(θ) + i * sin(θ)) 相乘，可以得到：</p>
<p>z' = (x * cos(θ) - y * sin(θ)) + i * (x * sin(θ) + y * cos(θ))</p>
<p>因此，旋转后的向量 z' 的实部和虚部分别为：</p>
<p>x' = x * cos(θ) - y * sin(θ) y' = x * sin(θ) + y * cos(θ)</p>
<p>这就是二维向量旋转角度的公式。</p>
</blockquote>
<p>式9用矩阵乘法表示如下：</p>
<p><span class="math display">\[
\begin{equation}
\boldsymbol{f}(\boldsymbol{q}, m) =\begin{pmatrix}\cos m\theta &amp; -\sin m\theta\\ \sin m\theta &amp; \cos m\theta\end{pmatrix} \begin{pmatrix}q_0 \\ q_1\end{pmatrix}\end{equation}
\]</span></p>
<p>由于内积满足线性叠加性，因此任意偶数维的RoPE，我们都可以表示为二维情形的拼接，即:</p>
<p><span class="math display">\[
\begin{equation}\scriptsize{\underbrace{\begin{pmatrix}
\cos m\theta_0 &amp; -\sin m\theta_0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
\sin m\theta_0 &amp; \cos m\theta_0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \cos m\theta_1 &amp; -\sin m\theta_1 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sin m\theta_1 &amp; \cos m\theta_1 &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos m\theta_{d/2-1} &amp; -\sin m\theta_{d/2-1} \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin m\theta_{d/2-1} &amp; \cos m\theta_{d/2-1} \\
\end{pmatrix}}_{\boldsymbol{\mathcal{R}}_m} \begin{pmatrix}q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1}\end{pmatrix}}\end{equation}
\]</span></p>
<p>因此给位置为m的向量q乘上矩阵<span class="math inline">\(\boldsymbol{\mathcal{R}}_m\)</span>、位置为n的向量k乘上矩阵<span class="math inline">\(\boldsymbol{\mathcal{R}}_n\)</span>，用变换后的Q,K序列做Attention，那么Attention就自动包含相对位置信息了，因为成立恒等式：</p>
<p><span class="math display">\[
\begin{equation}(\boldsymbol{\mathcal{R}}_m \boldsymbol{q})^{\top}(\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \boldsymbol{q}^{\top} \boldsymbol{\mathcal{R}}_m^{\top}\boldsymbol{\mathcal{R}}_n \boldsymbol{k} = \boldsymbol{q}^{\top} \boldsymbol{\mathcal{R}}_{n-m} \boldsymbol{k}\end{equation}
\]</span></p>
<p>值得指出的是，<span class="math inline">\(\boldsymbol{\mathcal{R}}_m\)</span>是一个正交矩阵，它不会改变向量的模长，因此通常来说它不会改变原模型的稳定性。此外由于矩阵的稀疏性，直接用矩阵乘法来实现会很浪费算力，推荐通过下述方式来实现RoPE：</p>
<p><span class="math display">\[
\begin{equation}\begin{pmatrix}q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1}
\end{pmatrix}\otimes\begin{pmatrix}\cos m\theta_0 \\ \cos m\theta_0 \\ \cos m\theta_1 \\ \cos m\theta_1 \\ \vdots \\ \cos m\theta_{d/2-1} \\ \cos m\theta_{d/2-1}
\end{pmatrix} + \begin{pmatrix}-q_1 \\ q_0 \\ -q_3 \\ q_2 \\ \vdots \\ -q_{d-1} \\ q_{d-2}
\end{pmatrix}\otimes\begin{pmatrix}\sin m\theta_0 \\ \sin m\theta_0 \\ \sin m\theta_1 \\ \sin m\theta_1 \\ \vdots \\ \sin m\theta_{d/2-1} \\ \sin m\theta_{d/2-1}
\end{pmatrix}\end{equation}
\]</span></p>
<p>其中<span class="math inline">\(\otimes\)</span>表示逐位对应相乘。</p>
<p>有了上面这个计算公式，此时还差一步，就是<span class="math inline">\(\theta\)</span>的取值。</p>
<h2 id="远程衰减">远程衰减</h2>
<p>远程衰减性：</p>
<blockquote>
<p>在自然语言处理中，Transformer 模型使用位置编码（Positional Encoding）来捕获输入序列中的单词之间的位置关系。因为 Transformer 的自注意力机制（Self-Attention Mechanism）是对位置无关的，它无法直接理解词之间的顺序。通过向输入的词嵌入向量（Word Embedding Vector）中添加位置编码，可以为模型提供关于单词在序列中的位置信息。</p>
<p>远程衰减性（Distant Decay）是指位置编码应能捕获到序列中相隔较远的单词之间的关系，即相隔较远的单词之间的位置编码相似度应较低。这有助于模型更好地理解和区分序列中的词序关系。具备远程衰减性的位置编码可以使模型学习到更多的长距离依赖关系，从而提高模型的性能。</p>
<p>例如，在使用正弦和余弦函数的位置编码方法中，不同位置的词的位置编码是通过正弦和余弦函数生成的。这种方法可以捕获到不同位置之间的关系，并且具有良好的远程衰减性。当两个位置的编码相差较大时，它们的相似度较低，这有助于模型区分这些位置。而当两个位置相隔较近时，它们的相似度较高，这有助于模型理解它们之间的关系。总之，位置编码需要具备远程衰减性以帮助模型捕获序列中的位置信息和长距离依赖关系。</p>
</blockquote>
<p>RoPE形式上和Sinusoidal位置编码有点相似，只不过Sinusoidal位置编码是加性的，而RoPE可以视为乘性的。在θ的选择上，我们同样沿用了Sinusoidal位置编码的方案，即：</p>
<p><span class="math display">\[
\theta_i = 10000^{-2i/d}
\]</span></p>
<p>将q，k两两分组（实部、虚部）后，加上RoPE的内积用复数乘法表示为：</p>
<p><span class="math display">\[
\begin{equation}
(\boldsymbol{\mathcal{R}}_m \boldsymbol{q})^{\top}(\boldsymbol{\mathcal{R}}_n \boldsymbol{k}) = \text{Re}\left[\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i}\right]\end{equation}
\]</span></p>
<p>给定下列假设条件：</p>
<p><span class="math display">\[
h_i = \boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^*, S_j = \sum\limits_{i=0}^{j-1} e^{\text{i}(m-n)\theta_i}
\]</span></p>
<p><span class="math display">\[
h_{d/2}=0,S_0=0
\]</span></p>
<p>通过Abel变换（分布求和法）可以得到：</p>
<p><strong>Abel变换公式</strong>如下：</p>
<p>$$ 设 {a_n} 和 {b_n} 是两个实数列 , 记 A_k=<em>{i=1}^k a_i, 则有 \ </em>{k=1}^n a_k b_k=A_n b_n+<em>{k=1}^{n-1} A_k(b_k-b</em>{k+1})</p>
<p>$$</p>
<p><span class="math display">\[
\begin{equation}\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i} = \sum_{i=0}^{d/2-1} h_i (S_{i
+1} - S_i) = -\sum_{i=0}^{d/2-1} S_{i+1}(h_{i+1} - h_i)\end{equation}
\]</span></p>
<p>所以需要找一个确定的上界来衡量该式的上界范围：</p>
<p><span class="math display">\[
\begin{equation}\begin{aligned}
\left|\sum_{i=0}^{d/2-1}\boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^* e^{\text{i}(m-n)\theta_i}\right| =&amp;\, \left|\sum_{i=0}^{d/2-1} S_{i+1}(h_{i+1} - h_i)\right| \\
\leq&amp;\, \sum_{i=0}^{d/2-1} |S_{i+1}| |h_{i+1} - h_i| \\
\leq&amp;\, \left(\max_i |h_{i+1} - h_i|\right)\sum_{i=0}^{d/2-1} |S_{i+1}|
\end{aligned}\end{equation}
\]</span></p>
<p>因此，我们将考察下列公式随着相对距离的变化情况作为衰减性的体现：</p>
<p><span class="math display">\[
\frac{1}{d/2}\sum\limits_{i=1}^{d/2} |S_i|
\]</span></p>
<p>衰减变化如图所示，随着相对距离的增加，向量内积结果在减小：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/WDETFB.jpg" /></p>
<h2 id="代码讲解">代码讲解</h2>
<h3 id="chatglm">ChatGLM</h3>
<p>主要调用区域代码为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if self.position_encoding_2d:</span><br><span class="line">q1, q2 = query_layer.chunk(2, dim=(query_layer.ndim - 1))</span><br><span class="line">k1, k2 = key_layer.chunk(2, dim=(key_layer.ndim - 1))</span><br><span class="line">cos, sin = self.rotary_emb(q1, seq_len=position_ids.max() + 1)</span><br><span class="line">position_ids, block_position_ids = position_ids[:, 0, :].transpose(0, 1).contiguous(), \</span><br><span class="line">position_ids[:, 1, :].transpose(0, 1).contiguous()</span><br><span class="line">q1, k1 = apply_rotary_pos_emb_index(q1, k1, cos, sin, position_ids)</span><br><span class="line">q2, k2 = apply_rotary_pos_emb_index(q2, k2, cos, sin, block_position_ids)</span><br><span class="line">query_layer = torch.concat([q1, q2], dim=(q1.ndim - 1))</span><br><span class="line">key_layer = torch.concat([k1, k2], dim=(k1.ndim - 1))</span><br><span class="line">else:</span><br><span class="line">position_ids = position_ids.transpose(0, 1)</span><br><span class="line">cos, sin = self.rotary_emb(value_layer, seq_len=position_ids.max() + 1)</span><br><span class="line"># [seq_len, batch, num_attention_heads, hidden_size_per_attention_head]</span><br><span class="line">query_layer, key_layer = apply_rotary_pos_emb_index(query_layer, key_layer, cos, sin, position_ids)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>query_layer的维度为：[seq, batch, heads, head_dim]</p></li>
<li><p>key_layer的维度为：[seq, batch, heads, head_dim]</p></li>
<li><p>position_ids的维度为：[batch, 2, seq]</p></li>
</ul>
<p>以2D位置编码来说，流程如下：</p>
<ol type="1">
<li><p>将query_layer按照最后隐藏层维度，拆分两份，得到q1，q2向量，在下面与两个不同的位置向量进行旋转位置编码计算；</p></li>
<li><p>将key_layer按照最后隐藏层维度，拆分两份，得到k1，k2向量，在下面与两个不同的位置向量进行旋转位置编码计算；</p></li>
<li><p>针对给定长度，计算旋转矩阵cos、sin；</p></li>
<li><p>从positionid拆分得到论文提到的两种位置编码向量；</p></li>
<li><p>针对q1、k1融入position_ids的位置信息；</p></li>
<li><p>针对q2、k2融入block_position_ids的位置信息；</p></li>
<li><p>将融入position_ids信息的q1向量与融入block_position_ids信息的q2向量拼接；</p></li>
<li><p>将融入position_ids信息的k1向量与融入block_position_ids信息的k2向量拼接；</p></li>
</ol>
<h4 id="旋转矩阵计算">旋转矩阵计算</h4>
<p>下面详细介绍旋转矩阵的计算方式：</p>
<blockquote>
<p>cos, sin = self.rotary_emb(q1, seq_len=position_ids.max() + 1)</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class RotaryEmbedding(torch.nn.Module):</span><br><span class="line">def __init__(self, dim, base=10000, precision=torch.half, learnable=False):</span><br><span class="line">super().__init__()</span><br><span class="line">inv_freq = 1. / (base ** (torch.arange(0, dim, 2).float() / dim))</span><br><span class="line">inv_freq = inv_freq.half()</span><br><span class="line">self.learnable = learnable</span><br><span class="line">if learnable:</span><br><span class="line">self.inv_freq = torch.nn.Parameter(inv_freq)</span><br><span class="line">self.max_seq_len_cached = None</span><br><span class="line">else:</span><br><span class="line">self.register_buffer(&#x27;inv_freq&#x27;, inv_freq)</span><br><span class="line">self.max_seq_len_cached = None</span><br><span class="line">self.cos_cached = None</span><br><span class="line">self.sin_cached = None</span><br><span class="line">self.precision = precision</span><br><span class="line"></span><br><span class="line">def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,</span><br><span class="line">error_msgs):</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">def forward(self, x, seq_dim=1, seq_len=None):</span><br><span class="line">if seq_len is None:</span><br><span class="line">seq_len = x.shape[seq_dim]</span><br><span class="line">if self.max_seq_len_cached is None or (seq_len &gt; self.max_seq_len_cached):</span><br><span class="line">self.max_seq_len_cached = None if self.learnable else seq_len</span><br><span class="line">t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)</span><br><span class="line">freqs = torch.einsum(&#x27;i,j-&gt;ij&#x27;, t, self.inv_freq)</span><br><span class="line"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span><br><span class="line">emb = torch.cat((freqs, freqs), dim=-1).to(x.device)</span><br><span class="line">if self.precision == torch.bfloat16:</span><br><span class="line">emb = emb.float()</span><br><span class="line"></span><br><span class="line"># [sx, 1 (b * np), hn]</span><br><span class="line">cos_cached = emb.cos()[:, None, :]</span><br><span class="line">sin_cached = emb.sin()[:, None, :]</span><br><span class="line">if self.precision == torch.bfloat16:</span><br><span class="line">cos_cached = cos_cached.bfloat16()</span><br><span class="line">sin_cached = sin_cached.bfloat16()</span><br><span class="line">if self.learnable:</span><br><span class="line">return cos_cached, sin_cached</span><br><span class="line">self.cos_cached, self.sin_cached = cos_cached, sin_cached</span><br><span class="line">return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]</span><br><span class="line"></span><br><span class="line">def _apply(self, fn):</span><br><span class="line">if self.cos_cached is not None:</span><br><span class="line">self.cos_cached = fn(self.cos_cached)</span><br><span class="line">if self.sin_cached is not None:</span><br><span class="line">self.sin_cached = fn(self.sin_cached)</span><br><span class="line">return super()._apply(fn)</span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>inv_freq = 1. / (base ** (torch.arange(0, dim, 2).float() / dim))</code>对应公式：<span class="math inline">\(\theta_i = 10000^{-2i/d}\)</span>；</p></li>
<li><p><code>t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)</code>对应拿到所有位置对应的ID；</p></li>
<li><p><code>freqs = torch.einsum('i,j-&gt;ij', t, self.inv_freq)</code>对应<span class="math inline">\(m\theta\)</span>；</p></li>
<li><p><code>emb = torch.cat((freqs, freqs), dim=-1).to(x.device)</code>将<span class="math inline">\(m\theta\)</span>拼接两次，对应复数的实部和虚部；</p></li>
<li><p><code>cos_cached = emb.cos()[:, None, :]</code>计算得到<span class="math inline">\(cos(m\theta)\)</span>；</p></li>
<li><p><code>sin_cached = emb.sin()[:, None, :]</code>计算得到<span class="math inline">\(sin(m\theta)\)</span>；</p></li>
</ul>
<h4 id="旋转位置编码计算">旋转位置编码计算</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def rotate_half(x):</span><br><span class="line">x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]</span><br><span class="line">return torch.cat((-x2, x1), dim=x1.ndim - 1) # dim=-1 triggers a bug in earlier torch versions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@torch.jit.script</span><br><span class="line">def apply_rotary_pos_emb_index(q, k, cos, sin, position_id):</span><br><span class="line"># position_id: [sq, b], q, k: [sq, b, np, hn], cos: [sq, 1, hn] -&gt; [sq, b, 1, hn]</span><br><span class="line">cos, sin = F.embedding(position_id, cos.squeeze(1)).unsqueeze(2), \</span><br><span class="line">F.embedding(position_id, sin.squeeze(1)).unsqueeze(2)</span><br><span class="line">q, k = (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)</span><br><span class="line">return q, k</span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>cos, sin = F.embedding(position_id, cos.squeeze(1)).unsqueeze(2), F.embedding(position_id, sin.squeeze(1)).unsqueeze(2)</code>根据位置ID获取到对应的<span class="math inline">\(cos(m\theta)、sin(m\theta)\)</span>；</p></li>
<li><p><code>(q * cos)</code>对应式13的左半部分；</p></li>
<li><p><code>(rotate_half(q) * sin)</code>对应式13的右半部分，rotate_half将输入的后半部分取负数，对应可以看式13的实部公式；</p></li>
</ul>
<h3 id="chatglm2">ChatGLM2</h3>
<p>主要调用区域代码为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Rotary positional embeddings</span><br><span class="line">rotary_pos_emb = self.rotary_pos_emb(self.seq_length) # (seq_len, n_elem // 2, 2)</span><br><span class="line"></span><br><span class="line">if position_ids is not None:</span><br><span class="line">rotary_pos_emb = rotary_pos_emb[position_ids]</span><br><span class="line">else:</span><br><span class="line">rotary_pos_emb = rotary_pos_emb[None, :seq_length]</span><br><span class="line"></span><br><span class="line">rotary_pos_emb = rotary_pos_emb.transpose(0, 1).contiguous() # n_elem // 2， seqlen, 2</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if rotary_pos_emb is not None:</span><br><span class="line">query_layer = apply_rotary_pos_emb(query_layer, rotary_pos_emb)</span><br><span class="line">key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)</span><br></pre></td></tr></table></figure>
<p>主要流程与ChatGLM1略微不同：</p>
<ol type="1">
<li><p>根据句子长度生成旋转矩阵；</p></li>
<li><p>根据输入的位置ID，获取旋转矩阵中的向量；</p></li>
<li><p>针对query_layer和2中rotary_pos_emb计算旋转位置编码；</p></li>
<li><p>针对key_layer和2中rotary_pos_emb计算旋转位置编码；</p></li>
</ol>
<h4 id="旋转矩阵计算-1">旋转矩阵计算</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class RotaryEmbedding(nn.Module):</span><br><span class="line">def __init__(self, dim, original_impl=False, device=None, dtype=None):</span><br><span class="line">super().__init__()</span><br><span class="line">inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device, dtype=dtype) / dim))</span><br><span class="line">self.register_buffer(&quot;inv_freq&quot;, inv_freq)</span><br><span class="line">self.dim = dim</span><br><span class="line">self.original_impl = original_impl</span><br><span class="line"></span><br><span class="line">def forward_impl(</span><br><span class="line">self, seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000</span><br><span class="line">):</span><br><span class="line">&quot;&quot;&quot;Enhanced Transformer with Rotary Position Embedding.</span><br><span class="line"></span><br><span class="line">Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/</span><br><span class="line">transformers/rope/__init__.py. MIT License:</span><br><span class="line">https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"># $\Theta = &#123;\theta_i = 10000^&#123;\frac&#123;2(i-1)&#125;&#123;d&#125;&#125;, i \in [1, 2, ..., \frac&#123;d&#125;&#123;2&#125;]&#125;$</span><br><span class="line">theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))</span><br><span class="line"></span><br><span class="line"># Create position indexes `[0, 1, ..., seq_len - 1]`</span><br><span class="line">seq_idx = torch.arange(seq_len, dtype=dtype, device=device)</span><br><span class="line"></span><br><span class="line"># Calculate the product of position index and $\theta_i$</span><br><span class="line">idx_theta = torch.outer(seq_idx, theta).float()</span><br><span class="line"></span><br><span class="line">cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)</span><br><span class="line"></span><br><span class="line"># this is to mimic the behaviour of complex32, else we will get different results</span><br><span class="line">if dtype in (torch.float16, torch.bfloat16, torch.int8):</span><br><span class="line">cache = cache.bfloat16() if dtype == torch.bfloat16 else cache.half()</span><br><span class="line">return cache</span><br><span class="line"></span><br><span class="line">def forward(self, max_seq_len, offset=0):</span><br><span class="line">return self.forward_impl(</span><br><span class="line">max_seq_len, self.dim, dtype=self.inv_freq.dtype, device=self.inv_freq.device</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<ul>
<li><p><code>theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))</code>对应公式：<span class="math inline">\(\theta_i = 10000^{-2i/d}\)</span>；</p></li>
<li><p><code>seq_idx = torch.arange(seq_len, dtype=dtype, device=device)</code>对应拿到所有位置对应的ID；</p></li>
<li><p><code>idx_theta = torch.outer(seq_idx, theta).float()</code>对应<span class="math inline">\(m\theta\)</span>；</p></li>
<li><p><code>cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)</code>对应<span class="math inline">\(cos(m\theta),sin(m\theta)\)</span>；</p></li>
</ul>
<h4 id="旋转位置编码计算-1">旋转位置编码计算</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@torch.jit.script</span><br><span class="line">def apply_rotary_pos_emb(x: torch.Tensor, rope_cache: torch.Tensor) -&gt; torch.Tensor:</span><br><span class="line"># x: [sq, b, np, hn]， rope_cache:[dim, seq, 2]</span><br><span class="line"></span><br><span class="line">sq, b, np, hn = x.size(0), x.size(1), x.size(2), x.size(3)</span><br><span class="line"></span><br><span class="line">rot_dim = rope_cache.shape[-2] * 2</span><br><span class="line"></span><br><span class="line">x, x_pass = x[..., :rot_dim], x[..., rot_dim:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># truncate to support variable sizes</span><br><span class="line">rope_cache = rope_cache[:sq]</span><br><span class="line">xshaped = x.reshape(sq, -1, np, rot_dim // 2, 2)</span><br><span class="line">rope_cache = rope_cache.view(sq, -1, 1, xshaped.size(3), 2)</span><br><span class="line">x_out2 = torch.stack(</span><br><span class="line">[</span><br><span class="line">xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],</span><br><span class="line">xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],</span><br><span class="line">],</span><br><span class="line">-1,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">x_out2 = x_out2.flatten(3)</span><br><span class="line">return torch.cat((x_out2, x_pass), dim=-1)</span><br></pre></td></tr></table></figure>
<p>值得一提的是，ChatGLM2中只采用一种位置编码，因此只针对<strong>输入的前半部分维度</strong>融入旋转位置编码信息。</p>
<ul>
<li><p><code>x, x_pass = x[..., :rot_dim], x[..., rot_dim:]</code>将输入根据隐藏层维度，拆分得到两部分，只针对前部分x计算旋转位置信息；</p></li>
<li><p><code>rope_cache[..., 0]</code>对应<span class="math inline">\(cos(m\theta)\)</span>，<code>rope_cache[..., 1]</code>对应<span class="math inline">\(sin(m\theta)\)</span>；</p></li>
<li><p><code>xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1]</code>对应复数的实部<span class="math inline">\(q_0*cos(m\theta)-q_1*sin(m\theta)\)</span>；</p></li>
<li><p><code>xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1]</code>对应复数的虚部<span class="math inline">\(q_1*cos(m\theta)+q_0*sin(m\theta)\)</span>；</p></li>
<li><p><code>torch.cat((x_out2, x_pass), dim=-1)</code>将x和x_pass拼接，还原输入向量，此时还原后的向量，前半部分融入了相对位置信息</p></li>
</ul>
<h3 id="llama">LLAMA</h3>
<p>LLAMA涉及旋转位置编码的代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):</span><br><span class="line">freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))</span><br><span class="line">t = torch.arange(end, device=freqs.device) # type: ignore</span><br><span class="line">freqs = torch.outer(t, freqs).float() # type: ignore</span><br><span class="line">freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # complex64</span><br><span class="line">return freqs_cis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):</span><br><span class="line">ndim = x.ndim</span><br><span class="line">assert 0 &lt;= 1 &lt; ndim</span><br><span class="line">assert freqs_cis.shape == (x.shape[1], x.shape[-1])</span><br><span class="line">shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]</span><br><span class="line">return freqs_cis.view(*shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def apply_rotary_emb(</span><br><span class="line">xq: torch.Tensor,</span><br><span class="line">xk: torch.Tensor,</span><br><span class="line">freqs_cis: torch.Tensor,</span><br><span class="line">) -&gt; Tuple[torch.Tensor, torch.Tensor]:</span><br><span class="line">xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))</span><br><span class="line">xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))</span><br><span class="line">freqs_cis = reshape_for_broadcast(freqs_cis, xq_)</span><br><span class="line">xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)</span><br><span class="line">xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)</span><br><span class="line">return xq_out.type_as(xq), xk_out.type_as(xk)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.freqs_cis = precompute_freqs_cis(</span><br><span class="line">self.params.dim // self.params.n_heads, self.params.max_seq_len * 2</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">………………</span><br><span class="line">xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line">xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)</span><br><span class="line"></span><br><span class="line">xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>生成旋转矩阵；</p></li>
<li><p>计算旋转位置编码</p></li>
</ul>
<h4 id="旋转矩阵计算-2">旋转矩阵计算</h4>
<ul>
<li><p><code>freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))</code>，同ChatGLM分析一样，对应公式：<span class="math inline">\(\theta_i = 10000^{-2i/d}\)</span>；</p></li>
<li><p><code>t = torch.arange(end, device=freqs.device) # type: ignore</code>,对应拿到所有位置对应的ID；</p></li>
<li><p><code>freqs = torch.outer(t, freqs).float() # type: ignore</code>对应<span class="math inline">\(m\theta\)</span>；</p></li>
<li><p><code>freqs_cis = torch.polar(torch.ones_like(freqs), freqs)</code>对应<span class="math inline">\(cos(m\theta),sin(m\theta)\)</span>；</p></li>
</ul>
<blockquote>
<p>torch.polar(abs,angle)利用一个绝对数值和一个角度值，在极坐标构建一个复数张量：</p>
<p><span class="math display">\[
abs*cos(angle)+abs*sin(angle)i
\]</span></p>
</blockquote>
<h4 id="旋转位置编码计算-2">旋转位置编码计算</h4>
<blockquote>
<p>前置接口知识：</p>
<p>1）torch.view_as_complex</p>
<p>把一个tensor转为复数形式，要求这个tensor的最后一个维度形状为2。</p>
<p>torch.view_as_complex(torch.Tensor([[1, 2], [3, 4], [5, 6]]))</p>
<p>// tensor([1.+2.j, 3.+4.j, 5.+6.j])</p>
<p>（2）torch.view_as_real 把复数tensor变回实数，可以看做是是刚才操作的逆变换。</p>
<p>torch.view_as_real(torch.view_as_complex(torch.Tensor([[1, 2], [3, 4], [5, 6]])))</p>
<p>#tensor([[1., 2.],</p>
<p>#[3., 4.],</p>
<p>#[5., 6.]])</p>
</blockquote>
<ol type="1">
<li><p>将q按最后一维切分两个向量，均转成复数形式</p></li>
<li><p>将k按最后一维切分两个向量，均转成复数形式</p></li>
<li><p>将freqs_cis转为和输入相同的形状</p></li>
<li><p>复数乘法:<span class="math inline">\((x+i*y)(cosm\theta+i*sinm\theta)\)</span></p></li>
</ol>
<h1 id="长度外推">长度外推</h1>
<h2 id="pi位置插值">PI位置插值</h2>
<p>论文名称：EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION</p>
<p>论文链接：https://arxiv.org/pdf/2306.15595.pdf</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/X68KT7.jpg" /></p>
<p>该文章提出了Position <a href="https://so.csdn.net/so/search?q=Interpolation&amp;spm=1001.2101.3001.7020">Interpolation</a> (PI)，其能够将基于RoPE的预训练LLM（例如LLaMA模型）的上下文窗口大小扩展到32768，并且<strong>仅需少量的微调</strong>（在1000步中），同时在需要长上下文的各种任务上证明了强大的经验结果 ，包括从Llama 7b到65B的passkey检索，语言建模以及长文档摘要。同时，通过位置插值扩展的模型在其原始上下文窗口内的任务上也<strong>相对较好地保留了质量</strong>。为了实现这一目标，位置插值线性<strong>向下缩放</strong>了输入位置索引以匹配原始的上下文窗口大小，而不是外推超过训练时所用的上下文长度，因为这可能会导致灾难性的较高的注意力分数，从而完全破坏了自注意力机制。</p>
<p>理论研究表明，插值注意力分数的上界至少比外推的上界小了约600倍，这进一步证明了其稳定性。通过位置插值扩展的模型保留了其原始网络结构，并可以重复使用大多数预先存在的优化和基础架构。</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/m27sYq.jpg" /></p>
<p>我们首先介绍一下论文中介绍位置插值的图：</p>
<ul>
<li><p>左半部分为预训练阶段的位置向量范围[0,2048]</p></li>
<li><p>右上角为长度外推的部分(2048,4096]</p></li>
<li><p>左下角为位置插值法，将[0,4096]的值降采样到[0,2048]预训练阶段支持的范围</p></li>
</ul>
<p>接下来将详细介绍该文章的核心思路、代码实现及理论推导：</p>
<p><strong>计算公式:</strong></p>
<p>论文的实现很简单，只需要将对应位置缩放到原先支持的区间([0,2048])内：</p>
<p>计算公式如下，L为原先支持的长度(如2048)，<span class="math inline">\(L^{&#39;}\)</span>为需要扩展的长度(如4096)：</p>
<p><span class="math display">\[
f^{&#39;}(x,m)=f(x, \frac{mL}{L^{&#39;}})
\]</span></p>
<p><strong>代码实现：</strong></p>
<p>我们以2K扩展至8K长度的情况作示例，需要缩放的倍数为4倍，代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def RotaryEmbedding(torch.nn.Module):</span><br><span class="line">def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):</span><br><span class="line">super().__init__()</span><br><span class="line">inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))</span><br><span class="line">self.register_buffer(&quot;inv_freq&quot;, inv_freq)</span><br><span class="line"></span><br><span class="line">max_position_embeddings = 8192</span><br><span class="line"></span><br><span class="line"># Build here to make `torch.jit.trace` work.</span><br><span class="line">self.max_seq_len_cached = max_position_embeddings</span><br><span class="line">t = torch.arange(</span><br><span class="line">self.max_seq_len_cached,</span><br><span class="line">device=self.inv_freq.device,</span><br><span class="line">dtype=self.inv_freq.dtype,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.scale = 1 / 4</span><br><span class="line">t *= self.scale</span><br><span class="line"></span><br><span class="line">freqs = torch.einsum(&quot;i,j-&gt;ij&quot;, t, self.inv_freq)</span><br><span class="line"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span><br><span class="line">emb = torch.cat((freqs, freqs), dim=-1)</span><br><span class="line">self.register_buffer(</span><br><span class="line">&quot;cos_cached&quot;, emb.cos()[None, None, :, :], persistent=False</span><br><span class="line">)</span><br><span class="line">self.register_buffer(</span><br><span class="line">&quot;sin_cached&quot;, emb.sin()[None, None, :, :], persistent=False</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>上界推导：</strong></p>
<p>文中通过证明位置插值的上界小于外推的上界，避免出现灾难性高的注意力分数，证明位置插值具有更高的稳定性。关于相对位置和注意力分数关系的图如下：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/ew7GsN.jpg" /></p>
<ul>
<li><p>左图中红色曲线为注意力得分拟合曲线，可见基本范围在区间[-1,1]内</p></li>
<li><p>中间图说明在[0,2048]内，拟合曲线表现很好，超过2048后注意力得分出现灾难性的高，甚至超过8000</p></li>
<li><p>右图说明内插法要更稳定，整数位置差下的表现是平滑且良好</p></li>
</ul>
<p>下面将给出论文的上界推导过程：</p>
<p>首先给出在远程衰减一节中的相关定义：</p>
<p><span class="math display">\[
h_i = \boldsymbol{q}_{[2i:2i+1]}\boldsymbol{k}_{[2i:2i+1]}^*, S_j = \sum\limits_{i=0}^{j-1} e^{\text{i}(m-n)\theta_i}
\]</span></p>
<p><span class="math display">\[
h_{d/2}=0,S_0=0
\]</span></p>
<p>注意力打分公式如下：</p>
<p><span class="math display">\[
a(s)=\operatorname{Re}\left[\sum_{j=0}^{d / 2-1} h_j e^{\mathrm{i} s \theta_j}\right]
\]</span></p>
<p>其中 <span class="math inline">\(\theta_j=\)</span> <span class="math inline">\(c^{-2 j / d}\)</span>【这里的c就是底数对应10000，h的定义同上】, 对应注意力得分的上界计算如下： <span class="math inline">\(a(s)\)</span> for <span class="math inline">\(s \in\left[s_1, s_2\right]\)</span> :</p>
<p><span class="math display">\[
\left|a(s)-a_{\text {linear }}(s)\right| \leq d\left(\max _j\left|h_j\right|\right) \frac{\left(s-s_1\right)\left(s_2-s\right)}{8 \ln c}
\]</span></p>
<p>其中 <span class="math inline">\(a_{\text {linear }}(s)\)</span> 是经过预训练阶段训练的整数点 <span class="math inline">\(a\left(s_1\right)\)</span> 和 <span class="math inline">\(a\left(s_2\right)\)</span> 的线性插值:</p>
<p><span class="math display">\[
a_{\text {linear }}(s):=(1-\lambda(s)) a\left(s_1\right)+\lambda(s) a\left(s_2\right), \quad \lambda(s):=\frac{s-s_1}{s_2-s_1}
\]</span></p>
<p>对于任意 <span class="math inline">\(s \in\left[s_1, s_2\right]\)</span>, <span class="math inline">\(\left(s-s_1\right)\left(s_2-s\right) \leq 1 / 4\)</span>. <span class="math inline">\(c=10000\)</span>, 因此上界计算成为:</p>
<p><span class="math display">\[
\left|a(s)-a_{\text {linear }}(s)\right| \leq \frac{d}{32 \ln c} \max _j\left|h_j\right| \approx \frac{d \max _j\left|h_j\right|}{294.73}
\]</span></p>
<p>现在在比较两个上界的大小，RoPE正常的注意力打分上界如下(可见远程衰减小节推导)：</p>
<p><span class="math display">\[
|a(s)| \leq\left(\max _j\left|h_j-h_{j+1}\right|\right) \sum_{k=0}^{d / 2-1}\left|A_{k+1}(s)\right| \leq 2\left(\max _j\left|h_j\right|\right) \sum_{k=0}^{d / 2-1}\left|A_{k+1}(s)\right|
\]</span></p>
<p>其中<span class="math inline">\(A_k(s):=\sum_{j=0}^{k-1} e^{\mathrm{i} s \theta_j}\)</span>, <span class="math inline">\(B(s):=\sum_{k=0}^{d / 2-1}\left|A_{k+1}(s)\right|\)</span>, <span class="math inline">\(B(s)/d\)</span>与相对位置关系如下图所示：</p>
<p><img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/rw6ceQ.jpg" /></p>
<p>对于绝大多数相对位置差s，B(s)均是超过d的，因此插值法的上界至少比外推的RoPE上界小<span class="math inline">\(2 · 294.73 600\)</span>,约600倍。</p>
<p><span class="math display">\[
\frac{|a(s)|}{\left|a(s)-a_{\text {linear }}(s)\right|} \leq \frac{2\left(\max _j\left|h_j\right|\right) B(s)}{\frac{d \max _j\left|h_j\right|}{294.73}} \approx 600 \frac{B(s)}{d}
\]</span></p>
<h2 id="ntk-aware-scaled-rope">NTK-Aware Scaled RoPE</h2>
<p>原文链接：<a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-Aware Scaled RoPE</a></p>
<p>苏神分析链接：<a href="https://kexue.fm/archives/9675">Transformer升级之路：10、RoPE是一种β进制编码</a></p>
<p>在这项工作中，作者针对当前RoPE插值方法的不足，提出了一种改进方案。通过应用神经切线核（NTK）理论，作者发现现有的线性插值方法在处理距离接近的标记时存在局限性。因此，作者设计了一种非线性插值方案，以改变RoPE的基数。这种方法在保持位置信息完整的同时，有效地提高了上下文大小。实验证明，该方法在没有进行模型微调的情况下就能显著减小困惑度，成为一种非常有效的优化策略。作者相信，通过进一步的微调，这个方法的效果将得到更好的提升。</p>
<p>首先我们复习上文RoPE给出的形式:</p>
<p><span class="math display">\[
[cos(m(10000^{-2i/d})), sin(m(10000^{-2i/d}))]
\]</span></p>
<p>接下来，看苏神的结论：</p>
<ol type="1">
<li><strong>RoPE是一种β进制编码：</strong></li>
</ol>
<p>针对10进制数字m，其第i位的β进制表示为：</p>
<p><span class="math display">\[
[\frac{m}{\beta^{i-1}}] \ mod \ \beta
\]</span></p>
<p>RoPE可以改写为:</p>
<p><span class="math display">\[
\left[\cos\left(\frac{m}{\beta^0}\right),\sin\left(\frac{m}{\beta^0}\right),\cos\left(\frac{m}{\beta^1}\right),\sin\left(\frac{m}{\beta^1}\right),\cdots,\cos\left(\frac{m}{\beta^{d/2-1}}\right),\sin\left(\frac{m}{\beta^{d/2-1}}\right)\right]
\]</span></p>
<p>其中：</p>
<p><span class="math display">\[
\beta=10000^{2/d}
\]</span></p>
<ol start="2" type="1">
<li><strong>外推法</strong></li>
</ol>
<p>不进行修改，m直接赋值超出范围的</p>
<ol start="3" type="1">
<li><strong>内插法</strong></li>
</ol>
<p>需要扩大k倍，将m换成<span class="math inline">\(m/k\)</span></p>
<ol start="4" type="1">
<li><strong>进制转换</strong></li>
</ol>
<p>需要扩大k倍，将底数10000换成<span class="math inline">\(10000k\)</span></p>
<p>最后，我们在看一下原作者的推导过程：</p>
<p>思想：高频外推、低频内插</p>
<p>引入参数<span class="math inline">\(\lambda\)</span>使最低频项和内插保持一致：</p>
<p><span class="math display">\[
\begin{equation}\frac{n}{(\beta\lambda)^{d/2-1}} = \frac{n/k}{\beta^{d/2-1}}\end{equation}
\]</span></p>
<p>解得：</p>
<p><span class="math display">\[
\lambda=k^{2/(d-2)}
\]</span></p>
<p>由于d通常较大，λ很接近1，因此高频项也相同：</p>
<p><span class="math display">\[
\frac{n}{\beta}=\frac{n}{\beta\lambda}
\]</span></p>
<p>实验结果：</p>
<p><span class="math display">\[
\begin{array}{c|cc}
\hline
\text{测试长度} &amp; 512(\text{训练}) &amp; 4096(\text{重复}) &amp; 4096(\text{不重复})\\
\hline
\text{Baseline} &amp; 49.41\% &amp; 24.17\% &amp; 23.16\% \\
\text{Baseline-}\log n &amp; 49.40\% &amp; 24.60\% &amp; 24.02\% \\
\hline
\text{PI-RoPE} &amp; 49.41\% &amp; 15.04\% &amp; 13.54\% \\
\text{PI-RoPE-}\log n &amp; 49.40\% &amp; 14.99\% &amp; 16.51\% \\
\hline
\text{NTK-RoPE} &amp; 49.41\% &amp; 51.28\% &amp; 39.27\% \\
\text{NTK-RoPE-}\log n &amp; 49.40\% &amp; 61.71\% &amp; 43.75\% \\
\hline
\end{array}
\]</span></p>
<p>以上报告的都是没有经过长文本微调的结果，其中Baseline就是外推，PI（Positional Interpolation）就是Baseline基础上改内插，NTK-RoPE就是Baseline基础上改NTK-Aware Scaled RoPE。带logn的选项，是指预训练时加入了<a href="https://kexue.fm/archives/8823">《从熵不变性看Attention的Scale操作》</a>中的scale，考虑这个变体是因为笔者觉得NTK-RoPE虽然解决了RoPE的长度泛化问题，但没有解决注意力不集中问题。</p>
<p>表格的实验结果完全符合预期：</p>
<blockquote>
<p>1、直接外推的效果不大行；</p>
<p>2、内插如果不微调，效果也很差；</p>
<p>3、NTK-RoPE不微调就取得了非平凡（但有所下降）的外推结果；</p>
<p>4、加入logn来集中注意力确实有帮助。</p>
</blockquote>
<p>代码实现：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import transformers</span><br><span class="line"></span><br><span class="line">old_init = transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.__init__</span><br><span class="line">def ntk_scaled_init(self, dim, max_position_embeddings=2048, base=10000, device=None):</span><br><span class="line"></span><br><span class="line">#The method is just these three lines</span><br><span class="line">max_position_embeddings = 16384</span><br><span class="line">a = 8 #Alpha value</span><br><span class="line">base = base * a ** (dim / (dim-2)) #Base change formula</span><br><span class="line"></span><br><span class="line">old_init(self, dim, max_position_embeddings, base, device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.__init__ = ntk_scaled_init</span><br></pre></td></tr></table></figure>
<h1 id="参考链接">参考链接</h1>
<p><a href="https://blog.csdn.net/weixin_44826203/article/details/129255185">Meta最新模型LLaMA细节与代码详解_常鸿宇的博客-CSDN博客</a></p>
<p>https://kexue.fm/archives/8265/comment-page-2#comments</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>ChatGPT</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
  <entry>
    <title>RoPE旋转位置编码深度解析：理论推导、代码实现、长度外推</title>
    <url>/2023/07/22/2023-08-23-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8D%87%E7%BA%A7%E4%B8%8E%E8%AE%BE%E8%AE%A1%E4%B9%8B%E9%81%93%EF%BC%9AChatGLM%E3%80%81LLAMA%E3%80%81Baichuan%E5%8F%8ALLM%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h1 id="更新记录">更新记录</h1>
<ul>
<li>0911 更新百川2升级之路，核心点：数据量升级至<strong>2.6T</strong>，训练过程引入<strong>NormHead、Max-z</strong>增加训练及推理的稳定性</li>
</ul>
<h1 id="引言">引言</h1>
<p>目前大语言模型在各个领域取得了显著的突破，从ChatGLM、LLAMA到Baichuan等，它们在处理各种自然语言任务时展现出了惊人的性能。然而，随着研究的深入和应用需求的不断扩大，这些大型模型需要不断地进行升级和优化，以满足更高的性能要求和更广泛的应用场景。</p>
<p>在这个过程中，作为研究者和从业者，我们需要深入探讨：大型模型的升级之路是怎样的？升级过程中面临哪些挑战？又是通过怎样的手段和方法实现升级的？本篇博客旨在对此进行深入探讨，梳理ChatGLM、LLAMA和Baichuan等模型的升级过程，分析其背后的原因，并展示大型模型如何优化实现升级。</p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 11%" />
<col style="width: 15%" />
<col style="width: 31%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="header">
<th>模型升级之路</th>
<th>训练Token数</th>
<th>序列长度</th>
<th>算子改进</th>
<th>核心点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ChatGLM-&gt;ChatGLM2</td>
<td>1T-&gt;1.4T</td>
<td>2K-&gt;8K/32K</td>
<td>FlashAttention &amp; Multi Query Attention</td>
<td>Prefix-LM-&gt;Decoder-Only</td>
</tr>
<tr class="even">
<td>LLAMA-&gt;LLAMA2</td>
<td>1.4T-&gt;2T</td>
<td>2K-&gt;4K</td>
<td>-</td>
<td>更高质量的SFT&amp;RLHF</td>
</tr>
<tr class="odd">
<td>baichuan-&gt;baichuan 13b</td>
<td>1.2T-&gt;1.4T</td>
<td>4K(RoPE)-&gt;4K(ALiBi)</td>
<td>FlashAttention</td>
<td>参数量升级</td>
</tr>
<tr class="even">
<td>baichuan-&gt;baichuan2</td>
<td>1.2T-&gt;<strong>2.6T</strong></td>
<td>4K</td>
<td>-</td>
<td>Tokenizer/NormHead/Max-z Loss</td>
</tr>
</tbody>
</table>
<span id="more"></span>
<h1 id="chatglm升级之路">ChatGLM升级之路</h1>
<p>首先对比下ChatGLM升级前后各大榜单结果，ChatGLM-6B较ChatGLM2-6B模型在各个榜单中都取得了近20-30%的提升:</p>
<h3 id="mmlu">MMLU</h3>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Average</th>
<th>STEM</th>
<th>Social Sciences</th>
<th>Humanities</th>
<th>Others</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ChatGLM-6B</td>
<td>40.63</td>
<td>33.89</td>
<td>44.84</td>
<td>39.02</td>
<td>45.71</td>
</tr>
<tr class="even">
<td>ChatGLM2-6B (base)</td>
<td>47.86</td>
<td>41.20</td>
<td>54.44</td>
<td>43.66</td>
<td>54.46</td>
</tr>
<tr class="odd">
<td>ChatGLM2-6B</td>
<td>45.46</td>
<td>40.06</td>
<td>51.61</td>
<td>41.23</td>
<td>51.24</td>
</tr>
<tr class="even">
<td>ChatGLM2-12B (base)</td>
<td>56.18</td>
<td>48.18</td>
<td>65.13</td>
<td>52.58</td>
<td>60.93</td>
</tr>
<tr class="odd">
<td>ChatGLM2-12B</td>
<td>52.13</td>
<td>47.00</td>
<td>61.00</td>
<td>46.10</td>
<td>56.05</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Chat 模型使用 zero-shot CoT (Chain-of-Thought) 的方法测试，Base 模型使用 few-shot answer-only 的方法测试</p>
</blockquote>
<h3 id="c-eval">C-Eval</h3>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Average</th>
<th>STEM</th>
<th>Social Sciences</th>
<th>Humanities</th>
<th>Others</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ChatGLM-6B</td>
<td>38.9</td>
<td>33.3</td>
<td>48.3</td>
<td>41.3</td>
<td>38.0</td>
</tr>
<tr class="even">
<td>ChatGLM2-6B (base)</td>
<td>51.7</td>
<td>48.6</td>
<td>60.5</td>
<td>51.3</td>
<td>49.8</td>
</tr>
<tr class="odd">
<td>ChatGLM2-6B</td>
<td>50.1</td>
<td>46.4</td>
<td>60.4</td>
<td>50.6</td>
<td>46.9</td>
</tr>
<tr class="even">
<td>ChatGLM2-12B (base)</td>
<td>61.6</td>
<td>55.4</td>
<td>73.7</td>
<td>64.2</td>
<td>59.4</td>
</tr>
<tr class="odd">
<td>ChatGLM2-12B</td>
<td>57.0</td>
<td>52.1</td>
<td>69.3</td>
<td>58.5</td>
<td>53.2</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Chat 模型使用 zero-shot CoT 的方法测试，Base 模型使用 few-shot answer only 的方法测试</p>
</blockquote>
<h3 id="gsm8k">GSM8K</h3>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Accuracy</th>
<th>Accuracy (Chinese)*</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ChatGLM-6B</td>
<td>4.82</td>
<td>5.85</td>
</tr>
<tr class="even">
<td>ChatGLM2-6B (base)</td>
<td>32.37</td>
<td>28.95</td>
</tr>
<tr class="odd">
<td>ChatGLM2-6B</td>
<td>28.05</td>
<td>20.45</td>
</tr>
<tr class="even">
<td>ChatGLM2-12B (base)</td>
<td>40.94</td>
<td>42.71</td>
</tr>
<tr class="odd">
<td>ChatGLM2-12B</td>
<td>38.13</td>
<td>23.43</td>
</tr>
</tbody>
</table>
<blockquote>
<p>所有模型均使用 few-shot CoT 的方法测试，CoT prompt 来自 http://arxiv.org/abs/2201.11903</p>
<p>使用翻译 API 翻译了 GSM8K 中的 500 道题目和 CoT prompt 并进行了人工校对</p>
</blockquote>
<h3 id="bbh">BBH</h3>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ChatGLM-6B</td>
<td>18.73</td>
</tr>
<tr class="even">
<td>ChatGLM2-6B (base)</td>
<td>33.68</td>
</tr>
<tr class="odd">
<td>ChatGLM2-6B</td>
<td>30.00</td>
</tr>
<tr class="even">
<td>ChatGLM2-12B (base)</td>
<td>36.02</td>
</tr>
<tr class="odd">
<td>ChatGLM2-12B</td>
<td>39.98</td>
</tr>
</tbody>
</table>
<blockquote>
<p>所有模型均使用 few-shot CoT 的方法测试，CoT prompt 来自 https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/cot-prompts</p>
</blockquote>
<h2 id="chatglm">ChatGLM</h2>
<p>ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 <a href="https://github.com/THUDM/GLM">General Language Model (GLM)</a> 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。</p>
<figure>
<img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/jkMmBZ.png" alt="jkMmBZ" /><figcaption aria-hidden="true">jkMmBZ</figcaption>
</figure>
<figure>
<img src="https://jmxblog.oss-cn-hangzhou.aliyuncs.com/uPic/glm.gif" alt="glm" /><figcaption aria-hidden="true">glm</figcaption>
</figure>
<p>相关解析见:<a href="https://zhuanlan.zhihu.com/p/627832567">复刻ChatGPT语言模型系列-（一）基座模型选取</a></p>
<h2 id="chatglm2">ChatGLM2</h2>
<p>ChatGLM<strong>2</strong>-6B 是开源中英双语对话模型 <a href="https://github.com/THUDM/ChatGLM-6B">ChatGLM-6B</a> 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM<strong>2</strong>-6B 引入了如下新特性：</p>
<ol type="1">
<li><strong>更强大的性能</strong>：基于 ChatGLM 初代模型的开发经验，我们全面升级了 ChatGLM2-6B 的基座模型。ChatGLM2-6B 使用了 <a href="https://github.com/THUDM/GLM">GLM</a> 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，<a href="https://github.com/THUDM/ChatGLM2-6B#%E8%AF%84%E6%B5%8B%E7%BB%93%E6%9E%9C">评测结果</a>显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。</li>
<li><strong>更长的上下文</strong>：基于 <a href="https://github.com/HazyResearch/flash-attention">FlashAttention</a> 技术，我们将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练。对于更长的上下文，我们发布了 <a href="https://huggingface.co/THUDM/chatglm2-6b-32k">ChatGLM2-6B-32K</a> 模型。<a href="https://github.com/THUDM/LongBench">LongBench</a> 的测评结果表明，在等量级的开源模型中，ChatGLM2-6B-32K 有着较为明显的竞争优势。</li>
<li><strong>更高效的推理</strong>：基于 <a href="http://arxiv.org/abs/1911.02150">Multi-Query Attention</a> 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。</li>
<li><strong>更开放的协议</strong>：ChatGLM2-6B 权重对学术研究<strong>完全开放</strong>，在填写<a href="https://open.bigmodel.cn/mla/form">问卷</a>进行登记后<strong>亦允许免费商业使用</strong>。</li>
</ol>
<h2 id="升级过程">升级过程</h2>
<h3 id="模型结构">模型结构</h3>
<blockquote>
<p>模型结构改变:从Prefix-LM回归纯粹的Decoder-Only结构，即SFT过程所有的都通过gMASK在开头进行生成；</p>
</blockquote>
<p>代码对比如下:</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692587216707-f7230049-33bf-4607-99d2-934646ca60c5.png" /></p>
<p>图示如下:</p>
<p><strong>ChatGLM:</strong></p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692588081630-13d74c13-d662-4b01-b19d-5374a47df323.png" /></p>
<p><strong>ChatGLM2:</strong></p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692588092576-afc45fb3-3b6e-4e2d-aaad-966f24b979aa.png" /></p>
<p><strong>那么这种改变能够带来什么呢</strong>?</p>
<p>答案就是为模型的<strong>训练效率</strong>带来了极大的提升。</p>
<p>图片来源:<a href="https://github.com/THUDM/ChatGLM2-6B/issues/16">https://github.com/THUDM/ChatGLM2-6B/issues/16</a></p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692588788276-9c07394b-3d2a-460e-8ae7-e9b86358cd59.png" /></p>
<p>在处理多轮对话的过程中,设有3轮对话,Q1A1，Q2A2，Q3A3，PrefixLM需要构建三条样本:</p>
<ol type="1">
<li><p>Q1-&gt;A1</p></li>
<li><p>Q1A1Q2-&gt;A2</p></li>
<li><p>Q1A1Q2A2Q3-&gt;A3</p></li>
</ol>
<p>而这种数据构建方式带来了严重的数据膨胀问题，影响模型训练的效率。</p>
<p>相反，Decoder-Only模型则可以利用Causal Mask的特性(每一个Token可以看到前面所有Token的真实输入)，在一条样本中实现多轮对话:</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692588361853-d948887b-3c08-4113-98c9-0c66d147452f.png" /></p>
<ol type="1">
<li><p>样本构建:Q1 A1 <eos> Q2 A2 <eos> Q3 A3 <eos></p></li>
<li><p>Loss计算:只需要计算 A1 <eos> A2 <eos> 和 A3 <eos>部分</p></li>
</ol>
<p><strong>再仔细回顾下，对话session级别训练和拆开训练从原理上有啥区别？</strong></p>
<p>1. session级别训练，效果之一为等价batchsize变大（1个batch可以塞下更多样本），且同一通对话产生的样本在一个bs内。</p>
<ol start="2" type="1">
<li>session级别的不同轮次产生的梯度是求平均的，拆开轮次构造训练是求和的，这样除了等价于lr会变大，还会影响不同轮次token权重的分配，另外还会影响norm的计算。</li>
</ol>
<p>我们用一个简化地例子定量分析下，我们假设两条训练样本分为 </p>
<p>1.问：A 答：xx</p>
<p>2.问: A 答：xx 问: B 答：xx  问: C 答：xx</p>
<p>则session级别训练影响梯度为 (Ga+(Ga + Gb + Gc)/3 )/2。对 A，B，C影响的权重分别为，2/3 1/6 1/6。</p>
<p>拆开训练为 (Ga+Ga+ (Ga + Gb)/2 +(Ga + Gb + Gc)/3)/4。对 A，B，C影响的权重分别为，17/24 5/24 1/12。</p>
<p>从上面的权重分布来看，session级别靠后的轮次影响权重要比拆开更大。这也是更合理的，因为大部分场景下，开场白都是趋同和重复的。</p>
<h3 id="序列长度">序列长度</h3>
<blockquote>
<p>序列长度:预训练模型在32K长度训练，SFT微调模型在8K长度训练；</p>
</blockquote>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692599097138-24fc2443-f1e4-4dc7-8f23-90173dc99024.png" /></p>
<p>此外，7月31号智谱AI发布了基于ChatGLM2-6B的基础上微调的针对长上下文优化的大模型ChatGLM<strong>2</strong>-6B-32K，能够更好的处理最多32K长度的上下文。</p>
<p>此前，ChatGLM2-6B刚发布的时候，官方宣称该模型最高支持32K长上下文输入，但是LM-SYS官方测试显示ChatGLM2-6B在超过8K长度时候表现很糟糕： <a href="https://www.datalearner.com/blog/1051688222070709">支持超长上下文输入的大语言模型评测和总结——ChatGLM2-6B表现惨烈，最强的依然是商业模型GPT-3.5与Claude-1.3</a> 。</p>
<figure>
<img src="https://user-images.githubusercontent.com/24487097/250009753-1968b057-8fc7-4d0e-bce4-7b0759009de5.jpeg" alt="44b19765-9203-4f1a-8e6a-b9f4bc3c0874" /><figcaption aria-hidden="true">44b19765-9203-4f1a-8e6a-b9f4bc3c0874</figcaption>
</figure>
<p>具体来说，ChatGLM2-6B-32K基于位置插值（Positional Interpolation）的方法对位置编码进行了更新，并在对话阶段使用 32K 的上下文长度训练。在实际的使用中，官方推荐如果上下文长度基本在 <strong>8K 以内</strong>，建议使用<a href="https://www.datalearner.com/ai-models/pretrained-models/ChatGLM2-6B">ChatGLM2-6B</a>；如果需要处理<strong>超过 8K</strong> 的上下文长度，推荐使用ChatGLM2-6B-32K。</p>
<p>关于位置插值的介绍，可见博客:<a href="https://zhuanlan.zhihu.com/p/645263524">RoPE旋转位置编码深度解析：理论推导、代码实现、长度外推</a></p>
<h3 id="算子优化">算子优化</h3>
<blockquote>
<p>算子优化:Flash Attention、Multi-Query Attention提高训练&amp;推理的速度；</p>
</blockquote>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692600059334-9525d3af-33a7-426d-b833-c0c73e219b5c.png" /></p>
<p>本次ChatGLM2-6B上下文从2k扩展到了32k同时也应用了一种叫做 FlashAttention 的技术。flash-attention是一种快速、高效、可扩展的注意力机制，它利用了一种称为哈希感知（hash-aware）的技术，可以根据它们的相似性将输入序列中的元素分配到不同的桶（bucket）中。这样，模型只需要计算桶内元素之间的注意力权重，而不是整个序列。这大大减少了计算量和内存需求，同时保持了较高的精度和表达能力。</p>
<h1 id="llama升级之路">LLAMA升级之路</h1>
<p>首先对比下LLAMA升级前后各大榜单结果，LLAMA2较LLAMA模型在各个榜单中取得了近10-30%的提升:</p>
<h3 id="mmlu-1">MMLU</h3>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LLAMA-7B</td>
<td>35.1</td>
</tr>
<tr class="even">
<td>LLAMA2-7B</td>
<td>45.3</td>
</tr>
<tr class="odd">
<td>LLAMA-13B</td>
<td>46.9</td>
</tr>
<tr class="even">
<td>LLAMA2-13B</td>
<td>54.8</td>
</tr>
<tr class="odd">
<td>LLAMA-65B</td>
<td>63.4</td>
</tr>
<tr class="even">
<td>LLAMA2-70B</td>
<td>68.9</td>
</tr>
</tbody>
</table>
<h3 id="gsm8k-1">GSM8K</h3>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LLAMA-7B</td>
<td>11.0</td>
</tr>
<tr class="even">
<td>LLAMA2-7B</td>
<td>14.6</td>
</tr>
<tr class="odd">
<td>LLAMA-13B</td>
<td>17.8</td>
</tr>
<tr class="even">
<td>LLAMA2-13B</td>
<td>28.7</td>
</tr>
<tr class="odd">
<td>LLAMA-65B</td>
<td>50.9</td>
</tr>
<tr class="even">
<td>LLAMA2-70B</td>
<td>56.8</td>
</tr>
</tbody>
</table>
<h2 id="llama">LLAMA</h2>
<p><code>LLaMA</code>（Large Language Model Meta AI），由 Meta AI 发布的一个开放且高效的大型基础语言模型，共有 <code>7B</code>、<code>13B</code>、<code>33B</code>、<code>65B</code>（650 亿）四种版本。其数据集来源都是公开数据集，无任何定制数据集，保证了其工作与开源兼容和可复现，整个训练数据集在 token 化之后大约包含 1.4T 的 token。</p>
<p>关于模型性能，LLaMA 的性能非常优异：具有 130 亿参数的 LLaMA 模型「在大多数基准上」可以<strong>胜过</strong> GPT-3（ 参数量达 1750 亿），而且可以在单块 V100 GPU 上运行；而最大的 650 亿参数的 LLaMA 模型可以媲美谷歌的 Chinchilla-70B 和 PaLM-540B。</p>
<p>关于训练集，其来源都是公开数据集，无任何定制数据集，保证了其工作与开源兼容和可复现。整个训练数据集在 token 化之后大约包含 1.4T 的 token。其中，LLaMA-65B 和 LLaMA-33B 是在 1.4万亿个 <code>token</code> 上训练的，而最小的模型 LLaMA-7B 是在 1万亿个 token 上训练的。</p>
<p><strong>模型结构</strong>：</p>
<ul>
<li>PreLayerNorm-RMSNorm-<a href="https://link.zhihu.com/?target=https%3A//proceedings.neurips.cc/paper_files/paper/2019/file/1e8a19426224ca89e83cef47f1e7f53b-Paper.pdf">Root Mean Square Layer Normalization</a></li>
<li>ROPE旋转位置编码（替换绝对/相对位置编码）</li>
<li>SwiGLU激活函数（替换ReLU）-<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.05202v1.pdf">GLU Variants Improve Transformer</a></li>
</ul>
<h2 id="llama2">LLAMA2</h2>
<p>官方页面上的介绍如下:</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692601172178-d4db2204-1caa-4e94-a856-fa8faf112b37.png" /></p>
<p>在<strong>模型结构</strong>上，主要升级两点：</p>
<ol type="1">
<li><p>训练数据Token数量从1.4T-&gt;2T</p></li>
<li><p>序列长度从2K-&gt;4K</p></li>
</ol>
<p>在<strong>SFT过程</strong>中，LLAMA2强调数据质量的重要性，通过2W的高质量指令数据，激发模型的指令遵循能力。</p>
<p>在<strong>RLHF过程</strong>中，LLAMA2做了较多工作，对RLHF过程作出了进一步的解释。<strong>自建了100W的Reward数据集</strong>，训练了两个独立的Reword Model。</p>
<p>整个LLAMA2的论文解读如下:</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692607746043-a7e78fd2-6755-4721-9d91-0f2b9b380861.png" /></p>
<p>LLAMA2-Chat模型的训练过程如下图，主要包含<strong>预训练、SFT、RLHF</strong>三个步骤:</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692608498823-9ce1f7fe-54a0-493f-a64b-9f3605f29528.png" /></p>
<h3 id="预训练">预训练</h3>
<p>LLAMA2的主要提升点包括：<strong>更强大的数据清洗，更新数据组合，增加40%的总训练tokens，加倍上下文长度，以及使用分组查询注意力（GQA）来提高更大模型的推理可扩展性</strong>。</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692608986172-ebbc9af5-30b0-40aa-b176-ef4432584ea4.png" /></p>
<p><strong>模型结构:</strong></p>
<ol type="1">
<li><p>RMSNorm</p></li>
<li><p>SwiGLU</p></li>
<li><p>RoPE</p></li>
<li><p><strong>4K序列长度</strong></p></li>
<li><p><strong>分组查询注意力GQA(33B/70B)</strong></p></li>
</ol>
<h3 id="sft">SFT</h3>
<p>作者发现许多第三方SFT数据集在多样性和质量方面不足，因此他们专注于收集自己的高质量SFT数据。</p>
<p>他们观察到，与使用来自第三方数据集的数百万例子相比，从他们自己的供应商为基础的标注工作中使用较少但质量更高的例子可以显著提高结果。他们发现，数以万计的SFT注释足以实现高质量结果，共收集了<strong>27,540</strong>个注释。</p>
<h3 id="rlhf">RLHF</h3>
<p>我们主要挑三个核心步骤介绍：数据收集、奖励模型、迭代训练。</p>
<h4 id="人类偏好数据收集">人类偏好数据收集</h4>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692611394323-a6c1baff-fe4e-4cb0-a05d-815faf8ea85e.png" /></p>
<p>偏好数据如表6所示，其中包含了<strong>140W</strong>Meta自建的数据集，相比于开源数据集，自建数据集的轮次、对话长度更长。</p>
<h4 id="奖励模型">奖励模型</h4>
<p>LLAMA2训练了两个独立的奖励模型(Helpfulness RM/Safety RM)。</p>
<p>动机:有研究发现（Bai等人，2022a），有时候有用性和安全性之间会存在权衡，这使得单一的奖励模型在这两方面的表现上可能会面临挑战。</p>
<p>为了解决这个问题，作者训练了两个独立的奖励模型，一个针对有用性进行优化（称为有用性奖励模型，Helpfulness RM），另一个针对安全性进行优化（称为安全性奖励模型，Safety RM）。这样可以分别在有用性和安全性方面取得更好的效果，使得Llama 2-Chat在强化学习人类偏好（RLHF）过程中更好地符合人类偏好，提高生成回答的有用性和安全性。</p>
<p><strong>损失函数:</strong></p>
<p><span class="math display">\[
L_{ranking} =−log(σ(r_θ(x,y_c)−r_θ(x,y_r)−m(r)))
\]</span></p>
<p>边界m(r)是关于偏好评分的离散函数。作者对那些响应差距较大的的对使用较大的边界，而对那些响应相似的对使用较小的边界（如表27所示）。作者发现这种边界分量可以提高有用性奖励模型的准确性，特别是在两个反应差距更大的样本中。</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692669429363-52f91419-701b-4b27-a112-1eb2a4cb9014.png" /></p>
<h4 id="迭代训练">迭代训练</h4>
<p>LLAMA2采用了两种强化学习算法:PPO和拒绝采样算法。</p>
<p>这两种强化学习算法主要区别在于：     • 广度：在拒绝采样中，模型为给定的提示探索K个样本，而在PPO中，只有一个生成过程。     • 深度：在PPO中，训练过程中第t步的样本是经过t-1步梯度更新后的模型策略的函数。在拒绝采样微调中，在模型的初始策略下采样所有输出以收集新数据集，然后类似于SFT进行微调。然而，由于采用了<strong>迭代模型更新</strong>，这两种算法之间的本质区别并不明显。</p>
<p>LLAMA2直到RLHF (V4)，仅使用拒绝采样微调。之后将这两种方法结合起来，先对拒绝采样检查点应用PPO，然后再对采样进行拒绝采样。LLAMA2只使用最大的70B Llama 2-Chat模型进行拒绝采样。其他较小的模型则在更大模型的拒绝采样数据上进行微调，从而将大模型的能力转移到较小的模型中。</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692670102006-54af67b5-95da-4d7a-87f8-56009bc10a49.png" /></p>
<h1 id="百川升级之路">百川升级之路</h1>
<p>首先对比下升级前后各大榜单结果，Baichuan-13B较Baichuan-7B模型在各个榜单中都取得了近20%的提升:</p>
<p><a href="https://cevalbenchmark.com/index.html#home">C-Eval</a></p>
<table>
<thead>
<tr class="header">
<th>Model 5-shot</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Social Sciences</th>
<th style="text-align: center;">Humanities</th>
<th style="text-align: center;">Others</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baichuan-7B</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">42.8</td>
</tr>
<tr class="even">
<td><strong>Baichuan-13B-Base</strong></td>
<td style="text-align: center;"><strong>45.9</strong></td>
<td style="text-align: center;"><strong>63.5</strong></td>
<td style="text-align: center;"><strong>57.2</strong></td>
<td style="text-align: center;"><strong>49.3</strong></td>
<td style="text-align: center;"><strong>52.4</strong></td>
</tr>
<tr class="odd">
<td><strong>Baichuan-13B-Chat</strong></td>
<td style="text-align: center;"><strong>43.7</strong></td>
<td style="text-align: center;"><strong>64.6</strong></td>
<td style="text-align: center;"><strong>56.2</strong></td>
<td style="text-align: center;"><strong>49.2</strong></td>
<td style="text-align: center;"><strong>51.5</strong></td>
</tr>
<tr class="even">
<td><strong>Baichuan2-7B-Base</strong></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><strong>54.0</strong></td>
</tr>
<tr class="odd">
<td><strong>Baichuan2-13B-Base</strong></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><strong>58.1</strong></td>
</tr>
</tbody>
</table>
<p><a href="https://arxiv.org/abs/2009.03300">MMLU</a></p>
<table>
<thead>
<tr class="header">
<th>Model 5-shot</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Social Sciences</th>
<th style="text-align: center;">Humanities</th>
<th style="text-align: center;">Others</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baichuan-7B</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr class="even">
<td><strong>Baichuan-13B-Base</strong></td>
<td style="text-align: center;"><strong>41.6</strong></td>
<td style="text-align: center;"><strong>60.9</strong></td>
<td style="text-align: center;"><strong>47.4</strong></td>
<td style="text-align: center;"><strong>58.5</strong></td>
<td style="text-align: center;"><strong>51.6</strong></td>
</tr>
<tr class="odd">
<td><strong>Baichuan-13B-Chat</strong></td>
<td style="text-align: center;"><strong>40.9</strong></td>
<td style="text-align: center;"><strong>60.9</strong></td>
<td style="text-align: center;"><strong>48.8</strong></td>
<td style="text-align: center;"><strong>59.0</strong></td>
<td style="text-align: center;"><strong>52.1</strong></td>
</tr>
<tr class="even">
<td><strong>Baichuan2-7B-Base</strong></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><strong>54.16</strong></td>
</tr>
<tr class="odd">
<td><strong>Baichuan2-13B-Base</strong></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><strong>59.17</strong></td>
</tr>
</tbody>
</table>
<blockquote>
<p>说明：采用了 MMLU 官方的<a href="https://github.com/hendrycks/test">评测方案</a>。</p>
</blockquote>
<p><a href="https://github.com/haonan-li/CMMLU">CMMLU</a></p>
<table>
<thead>
<tr class="header">
<th>Model 5-shot</th>
<th style="text-align: center;">STEM</th>
<th style="text-align: center;">Humanities</th>
<th style="text-align: center;">Social Sciences</th>
<th style="text-align: center;">Others</th>
<th style="text-align: center;">China Specific</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baichuan-7B</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr class="even">
<td><strong>Baichuan-13B-Base</strong></td>
<td style="text-align: center;"><strong>41.7</strong></td>
<td style="text-align: center;"><strong>61.1</strong></td>
<td style="text-align: center;"><strong>59.8</strong></td>
<td style="text-align: center;"><strong>59.0</strong></td>
<td style="text-align: center;"><strong>56.4</strong></td>
<td style="text-align: center;"><strong>55.3</strong></td>
</tr>
<tr class="odd">
<td><strong>Baichuan-13B-Chat</strong></td>
<td style="text-align: center;"><strong>42.8</strong></td>
<td style="text-align: center;"><strong>62.6</strong></td>
<td style="text-align: center;"><strong>59.7</strong></td>
<td style="text-align: center;"><strong>59.0</strong></td>
<td style="text-align: center;"><strong>56.1</strong></td>
<td style="text-align: center;"><strong>55.8</strong></td>
</tr>
<tr class="even">
<td><strong>Baichuan2-7B-Base</strong></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><strong>57.07</strong></td>
</tr>
<tr class="odd">
<td><strong>Baichuan2-13B-Base</strong></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><strong>61.97</strong></td>
</tr>
</tbody>
</table>
<blockquote>
<p>说明：CMMLU 是一个综合性的中文评估基准，专门用于评估语言模型在中文语境下的知识和推理能力。采用了其官方的<a href="https://github.com/haonan-li/CMMLU">评测方案</a>。</p>
</blockquote>
<h2 id="baichuan-7b">baichuan-7b</h2>
<p>Baichuan-7B 是由百川智能开发的一个开源可商用的大规模预训练语言模型。基于 Transformer 结构，在大约 1.2 万亿 tokens 上训练的 70 亿参数模型，支持中英双语，上下文窗口长度为 4096。在标准的中文和英文 benchmark（C-Eval/MMLU）上均取得同尺寸最好的效果。</p>
<p>百川模型结构与LLAMA相近，作了如下的优化：</p>
<h3 id="分词器">分词器</h3>
<p>参考学术界方案使用 SentencePiece 中的 Byte-Pair Encoding (BPE) 作为分词算法，并且进行了以下的优化：</p>
<ol type="1">
<li>目前大部分开源模型主要基于英文优化，因此对中文语料存在效率较低的问题。我们使用 2000 万条以中英为主的多语言语料训练分词模型，显著提升对于中文的压缩率。</li>
<li>对于数学领域，我们参考了 LLaMA 和 Galactica 中的方案，对数字的每一位单独分开，避免出现数字不一致的问题，对于提升数学能力有重要帮助。</li>
<li>对于罕见字词（如特殊符号等），支持 UTF-8 characters 的 byte 编码，因此做到未知字词的全覆盖。</li>
<li>我们分析了不同分词器对语料的压缩率，如下表，可见我们的分词器明显优于 LLaMA, Falcon 等开源模型，并且对比其他中文分词器在压缩率相当的情况下，训练和推理效率更高。</li>
</ol>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Baichuan-7B</th>
<th style="text-align: center;">LLaMA</th>
<th style="text-align: center;">Falcon</th>
<th style="text-align: center;">mpt-7B</th>
<th style="text-align: center;">ChatGLM</th>
<th style="text-align: center;">moss-moon-003</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Compress Rate</td>
<td style="text-align: center;">0.737</td>
<td style="text-align: center;">1.312</td>
<td style="text-align: center;">1.049</td>
<td style="text-align: center;">1.206</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">0.659</td>
</tr>
<tr class="even">
<td style="text-align: center;">Vocab Size</td>
<td style="text-align: center;">64,000</td>
<td style="text-align: center;">32,000</td>
<td style="text-align: center;">65,024</td>
<td style="text-align: center;">50,254</td>
<td style="text-align: center;">130,344</td>
<td style="text-align: center;">106,029</td>
</tr>
</tbody>
</table>
<h3 id="算子优化-1">算子优化</h3>
<blockquote>
<p>采用更高效的算子:Flash-Attention，同ChatGLM2</p>
</blockquote>
<h2 id="baichuan-13b">baichuan-13b</h2>
<p>Baichuan-13B 是由百川智能继 <a href="https://github.com/baichuan-inc/baichuan-7B">Baichuan-7B</a> 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (<a href="https://huggingface.co/baichuan-inc/Baichuan-13B-Base">Baichuan-13B-Base</a>) 和对齐 (<a href="https://huggingface.co/baichuan-inc/Baichuan-13B-Chat">Baichuan-13B-Chat</a>) 两个版本。Baichuan-13B 有如下几个特点：</p>
<ol type="1">
<li><strong>更大尺寸、更多数据</strong>：Baichuan-13B 在 <a href="https://github.com/baichuan-inc/baichuan-7B">Baichuan-7B</a> 的基础上进一步扩大参数量到 130 亿，并且在高质量的语料上训练了 1.4 万亿 tokens，超过 LLaMA-13B 40%，是当前开源 13B 尺寸下训练数据量最多的模型。支持中英双语，使用 ALiBi 位置编码，上下文窗口长度为 4096。</li>
<li><strong>同时开源预训练和对齐模型</strong>：预训练模型是适用开发者的『 基座 』，而广大普通用户对有对话功能的对齐模型具有更强的需求。因此本次开源我们同时发布了对齐模型（Baichuan-13B-Chat），具有很强的对话能力，开箱即用，几行代码即可简单的部署。</li>
<li><strong>更高效的推理</strong>：为了支持更广大用户的使用，我们本次同时开源了 int8 和 int4 的量化版本，相对非量化版本在几乎没有效果损失的情况下大大降低了部署的机器资源门槛，可以部署在如 Nvidia 3090 这样的消费级显卡上。</li>
<li><strong>开源免费可商用</strong>：Baichuan-13B 不仅对学术研究完全开放，开发者也仅需邮件申请并获得官方商用许可后，即可以免费商用。</li>
</ol>
<p><strong>模型细节</strong></p>
<table>
<thead>
<tr class="header">
<th>模型名称</th>
<th style="text-align: center;">隐藏层维度</th>
<th style="text-align: center;">层数</th>
<th style="text-align: center;">注意力头数</th>
<th style="text-align: center;">词表大小</th>
<th style="text-align: center;">总参数量</th>
<th style="text-align: center;">训练数据（tokens）</th>
<th style="text-align: center;">位置编码</th>
<th style="text-align: center;">最大长度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baichuan-7B</td>
<td style="text-align: center;">4,096</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">64,000</td>
<td style="text-align: center;">7,000,559,616</td>
<td style="text-align: center;">1.2 万亿</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2104.09864">RoPE</a></td>
<td style="text-align: center;">4,096</td>
</tr>
<tr class="even">
<td>Baichuan-13B</td>
<td style="text-align: center;">5,120</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">64,000</td>
<td style="text-align: center;">13,264,901,120</td>
<td style="text-align: center;">1.4 万亿</td>
<td style="text-align: center;"><a href="https://arxiv.org/abs/2108.12409">ALiBi</a></td>
<td style="text-align: center;">4,096</td>
</tr>
</tbody>
</table>
<h2 id="升级过程-1">升级过程</h2>
<ol type="1">
<li><p>参数量:baichuan13B较baichuan7B 首先在<strong>参数量</strong>上翻了一倍，更大的参数量意味着知识的容量更大，通过更多的训练数据(1.2T-&gt;1.4T)，基座模型的常识能力得以提升；</p></li>
<li><p>位置编码:从RoPE改成ALiBi，在一定程度的可以进行长度外推(TIPS:RoPE可以进行更长范围的外推)；</p></li>
</ol>
<h2 id="baichuan2">baichuan2</h2>
<p>技术报告:<a href="https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf">https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf</a></p>
<ul>
<li>Baichuan 2 是百川智能推出的<strong>新一代开源大语言模型</strong>，采用 <strong>2.6 万亿</strong> Tokens 的高质量语料训练。</li>
<li>Baichuan 2 在多个权威的中文、英文和多语言的通用、领域 benchmark 上取得同尺寸<strong>最佳</strong>的效果。</li>
</ul>
<h3 id="分词器-1">分词器</h3>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1694401788439-a8c83817-04ee-472f-bbf0-545cadab1b57.png" /></p>
<p>分词器需要平衡的两个关键因素是高压缩率以实现高效的推理（inference）和足够大小的词汇表以保证每个单词嵌入的充分训练。</p>
<p>相对于前代模型，Baichuan 2在分词器方面进行了如下改进：</p>
<ol type="1">
<li><strong>词汇表扩展</strong>：Baichuan 2将词汇表的大小从Baichuan 1的64,000扩展到了125,696。这一变化旨在在保证计算效率的同时，充分训练每个词嵌入。</li>
<li><strong>数据归一化处理</strong>：相比Baichuan 1，Baichuan 2在输入文本的归一化处理上有所不同。Baichuan 2不对输入文本进行任何归一化，并且不添加像Baichuan 1那样的虚拟前缀。</li>
<li><strong>处理数字数据</strong>：Baichuan 2将数字数据拆分成独立的数字，以更好地编码数值数据。</li>
<li><strong>处理代码数据</strong>：对于包含额外空格的代码数据，Baichuan 2向分词器中添加了仅包含空格的标记。</li>
</ol>
<h3 id="模型结构-1">模型结构</h3>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1694402243624-6d6babce-1f69-4bd8-8172-9604356cf320.png" /></p>
<ol type="1">
<li><p><strong>Positional Embeddings</strong>：</p>
<ul>
<li>对于Baichuan 2-7B模型，采用了Rotary Positional Embedding (RoPE)。<br />
</li>
<li>对于Baichuan 2-13B模型，采用了ALiBi作为位置编码技术。</li>
</ul></li>
<li><p><strong>激活函数和归一化</strong>：</p>
<ul>
<li>使用了SwiGLU激活函数，这是GLU的一个变体，经过改进的版本。<br />
</li>
<li>在注意力层中采用了内存高效的注意力机制。</li>
</ul></li>
<li><p><strong>Tokenizer</strong>：</p>
<ul>
<li>对词汇表的大小进行了调整，将其从Baichuan 1的64,000扩展到125,696，以在计算效率和模型性能之间取得平衡。</li>
</ul></li>
<li><p><strong>NormHead：</strong></p>
<ul>
<li>Baichuan 2使用了一种称为NormHead的方法来稳定训练并提高模型性能。NormHead主要用于对输出嵌入进行归一化处理，有助于稳定训练动态，并降低了L2距离在计算logits时的影响。</li>
</ul></li>
<li><p><strong>最大z损失（Max-z loss）</strong>：</p>
<ul>
<li>引入了最大z损失，用于规范模型输出的logit值，从而提高训练的稳定性并使推断更加鲁棒。</li>
</ul>
<p><span class="math display">\[
L_{max-z}=2e^{-4}*z^2
\]</span></p></li>
</ol>
<p>接下来让我们从代码层面，看baichuan2的模型结构改动：</p>
<p><strong>NormHead</strong>:完成模型输出的归一化工作</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class NormHead(nn.Module):</span><br><span class="line">    def __init__(self, hidden_size, vocab_size, bias=False):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.empty((vocab_size, hidden_size)))</span><br><span class="line">        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))</span><br><span class="line">        self.first_flag = True</span><br><span class="line"></span><br><span class="line">    def forward(self, hidden_states):</span><br><span class="line">        if self.training:</span><br><span class="line">            norm_weight = nn.functional.normalize(self.weight)</span><br><span class="line">        elif self.first_flag:</span><br><span class="line">            self.first_flag = False</span><br><span class="line">            self.weight = nn.Parameter(nn.functional.normalize(self.weight))</span><br><span class="line">            norm_weight = self.weight</span><br><span class="line">        else:</span><br><span class="line">            norm_weight = self.weight</span><br><span class="line">        return nn.functional.linear(hidden_states, norm_weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BaichuanForCausalLM(BaichuanPreTrainedModel):</span><br><span class="line">    def __init__(self, config, *model_args, **model_kwargs):</span><br><span class="line">        super().__init__(config, *model_args, **model_kwargs)</span><br><span class="line">        self.model = BaichuanModel(config)</span><br><span class="line">        self.lm_head = NormHead(config.hidden_size, config.vocab_size, bias=False)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    def forward(</span><br><span class="line">        self,</span><br><span class="line">        input_ids: torch.LongTensor = None,</span><br><span class="line">        attention_mask: Optional[torch.Tensor] = None,</span><br><span class="line">        past_key_values: Optional[List[torch.FloatTensor]] = None,</span><br><span class="line">        inputs_embeds: Optional[torch.FloatTensor] = None,</span><br><span class="line">        labels: Optional[torch.LongTensor] = None,</span><br><span class="line">        use_cache: Optional[bool] = None,</span><br><span class="line">        output_attentions: Optional[bool] = False,</span><br><span class="line">        output_hidden_states: Optional[bool] = False,</span><br><span class="line">        return_dict: Optional[bool] = True,</span><br><span class="line">        **kwargs,</span><br><span class="line">    ) -&gt; Union[Tuple, CausalLMOutputWithPast]:</span><br><span class="line">        return_dict = (</span><br><span class="line">            return_dict if return_dict is not None else self.config.use_return_dict</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span><br><span class="line">        outputs = self.model(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            past_key_values=past_key_values,</span><br><span class="line">            inputs_embeds=inputs_embeds,</span><br><span class="line">            use_cache=use_cache,</span><br><span class="line">            output_attentions=output_attentions,</span><br><span class="line">            output_hidden_states=output_hidden_states,</span><br><span class="line">            return_dict=return_dict,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        hidden_states = outputs[0]</span><br><span class="line">        logits = self.lm_head(hidden_states)</span><br></pre></td></tr></table></figure>
<p><strong>Max-z Loss</strong>: softmax_normalizer对应<span class="math inline">\(z^2\)</span></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">loss = None</span><br><span class="line">if labels is not None:</span><br><span class="line">    # Shift so that tokens &lt; n predict n</span><br><span class="line">    shift_logits = logits[..., :-1, :].contiguous()</span><br><span class="line">    shift_labels = labels[..., 1:].contiguous()</span><br><span class="line">    # Flatten the tokens</span><br><span class="line">    loss_fct = CrossEntropyLoss()</span><br><span class="line">    shift_logits = shift_logits.view(-1, self.config.vocab_size)</span><br><span class="line">    shift_labels = shift_labels.view(-1)</span><br><span class="line">    softmax_normalizer = shift_logits.max(-1).values ** 2</span><br><span class="line">    z_loss = self.config.z_loss_weight * softmax_normalizer.mean()</span><br><span class="line">    # Enable model parallelism</span><br><span class="line">    shift_labels = shift_labels.to(shift_logits.device)</span><br><span class="line">    loss = loss_fct(shift_logits, shift_labels) + z_loss</span><br></pre></td></tr></table></figure>
<h1 id="如何构建一个好的基座大模型">如何构建一个好的基座大模型？</h1>
<p>在深入探讨了ChatGLM、LLAMA、Baichuan大语言模型的升级之路后，我们将进一步拓展讨论范围，探索大模型所需具备的关键能力、实现这些能力所需的技术手段以及模型结构的设计方法。这将为我们在实际应用中构建和优化大模型提供有力的参考和指导。</p>
<p>接下来的小节内容将从以下几个方面展开讨论：首先，我们将分析大型预训练模型所需要具备的核心能力，如长度外推、常识等；其次，我们将介绍如何利用先进的技术和方法实现这些能力，包括预训练策略、优化算法和损失函数等；最后，我们将针对模型结构进行探讨，分析如何选择合适的LLM（Large Language Model）结构以实现高性能的大型模型。</p>
<p>本小节内容旨在为大家提供一个全面的视角，了解大模型的关键要素，以便在实际工程中构建出更为强大、灵活且高效的大型预训练模型。</p>
<h2 id="大模型所需能力及升级方式">大模型所需能力及升级方式</h2>
<p>通过对ChatGLM、LLAMA、Baichuan等大型语言模型升级过程的分析，可以发现它们的改进主要集中在基础知识能力的提升和支持的序列长度变化这两个方面。在本小节中，我们将重点梳理并总结这两项关键能力的升级策略。</p>
<h3 id="基础知识">基础知识</h3>
<p>基础知识能力的提升涵盖了多个领域，我们可以通过以下常用评测集来了解这些领域：</p>
<ul>
<li><p>英文知识 — MMLU</p></li>
<li><p>中文知识 — C-Eval</p></li>
<li><p>推理 — GSM8k / BBH</p></li>
<li><p>代码 — HumanEval / MBPP</p></li>
<li><p>数学 — MATH</p></li>
</ul>
<p>笔者认为升级基础知识能力的主要策略在于提升模型参数量及训练数据，通过更大的参数量及数据使模型更好的拟合相关领域的知识。</p>
<p>而在这个过程中，最重要的是训练数据的质量，以下给出清洗数据的常用方式：</p>
<ul>
<li>无效数据，脏数据过滤</li>
</ul>
<p>一些无效数据，如意义空泛或模板化的文本（例如HTML代码、Lorem ipsum等）。甚至于在多语言语料库的构建过程中，从网站提取文本用于语言建模也极具挑战性。但这是我们必然要做到的，因为NTP(Next Token Prediction)的方式注定训练模型使用的数据本身就是真实语言世界很好的映射。数据清洗工具，如justext、trafilatura等，能有效剔除HTML模板文本，同时在减少噪音（提高精度）与保留所有有效部分（提高召回率）之间取得平衡。另外一点是，处理网页语料库中无效数据的有效方法之一是利用元数据进行筛选。例如，OpenAI在构建GPT-2用的WebText语料库时，抓取了reddit上点赞数至少为3的所有外部链接，这种启发式方法有助于减少数据集中的噪音，同时确保数据质量。</p>
<ul>
<li>文档长度过滤</li>
</ul>
<p>一方面，考虑到NTP（Next Token Prediction），从语料库中移除非常短的文档（包含少于约100个标记的文本）可以帮助通过创建连续的文本来建模文本中的依赖关系，从而去除噪音。另一方面，由于大多数语言模型如今都基于Transformer架构，对非常大的文档进行预处理并将其分成所需长度的连续片段是很有用的。</p>
<ul>
<li>机器生成数据过滤</li>
</ul>
<p>训练语言模型的目标之一是捕捉人类语言的分布。然而，网络爬取的数据集包含大量机器生成的文本，例如现有语言模型生成的文本、OCR文本和机器翻译文本。例如，来自http://patents.google.com的数据构成了C4语料库的大部分。该语料库使用机器翻译将来自世界各地专利机构的专利翻译成英语。此外，网络语料库中的数据还包含来自扫描书籍和文档的OCR生成文本。OCR系统并不完美，因此生成的文本与自然英语的分布不同（通常OCR系统会在拼写错误和完全遗漏单词等方面产生可预测的错误）——这点很重要，也很难搞，pdf扫描文档怎么去做还真挺头疼的。虽然很难识别机器生成的文本，但有一些工具，如ctrl-detector，可用于识别和检测机器生成的文本。在为语言建模预处理语料库时，重要的是对语料库中机器生成文本的存在进行表征和记录。</p>
<ul>
<li>去重</li>
</ul>
<p>从互联网上爬取原始文本创建的数据集往往会导致相同的序列被多次重复出现。例如，在论文《Deduplicating Training Data Makes Language Models Better》中，作者发现在C4数据集中，一个50个单词的序列被重复出现了60000次。事实上，在去重的数据集上训练模型速度更快，并且不太容易导致记忆效应——很不好。最近，研究人员还表明，在重复数据上训练的语言模型容易受到隐私攻击，其中对手从训练模型中生成序列并检测哪些序列来自训练集的记忆。在论文《Deduplicating Training Data Mitigates Privacy Risks in Language Models》中，作者展示了语言模型重新生成训练序列的速率与序列在训练集中的出现次数超线性相关。例如，一个在训练数据中出现10次的序列平均会比一个只出现一次的序列生成1000倍多。去重可以在不同粒度级别上执行。从精确匹配去重到模糊去重工具（例如deduplicate-text-datasets和datasketch），可以帮助减少和去除正在处理的语料库中的冗余文本。正如许多研究人员所指出的，需要理解去重过程需要大量计算资源（CPU和RAM），因为网页爬取数据集的大小，因此建议在分布式环境中运行此类计算。</p>
<ul>
<li>清洗污染数据</li>
</ul>
<p>这部分还挺保受争议的，可能还没有很细致的标准，不少公司也都挺功利的，就不好说。在NLP领域，我们常说的数据清洗，主要指的是训练数据和测试数据的区分和处理。在大型语言模型的情况下，由于训练和测试数据集都源于互联网，确保二者不发生交叉，这个过程可能颇具挑战。大型语言模型的评估通常会用到基准数据，如问答对，如果这些基准数据在训练数据中出现，可能会导致基准性能的高估。因此，需要进行去污染操作，也就是从训练数据中去除和基准数据集有重叠的部分，保证训练数据集的完整性。OpenAI的研究人员在创建WebText数据集时，就通过剔除所有维基百科内容来实现数据去污染，因为维基百科数据在他们的基准数据集中被广泛使用。另一个案例是EleutherAI的研究人员，他们开发了名为lm-eval harness的软件包，用以实现对基准数据集的去污染。在具体操作中，我们需要关注两类数据污染：</p>
<ol type="1">
<li><p>输入与输出污染：这种情况下，预训练语料库中存在与下游任务标签相同的数据。对于语言建模等任务，任务标签就是目标文本。如果目标文本在预训练语料库中出现，模型可能会倾向于复制文本，而非真正解决任务。</p></li>
<li><p>输入污染：这指的是评估样本中并未包含标签的情况，这也可能导致下游任务的性能高估。在进行零样本和少样本评估时，如果预训练数据集中存在与热门基准任务重叠的数据，我们必须重视数据去污染。</p></li>
</ol>
<ul>
<li>毒性和偏见控制</li>
</ul>
<p>尽管网络语料库具有丰富的多样性，但其中也常常弥漫着毒性和偏见内容。如，《RealToxicityPrompts》一文中作者使用PerspectiveAPI指出，OpenWebText与WebText的内容中分别有2.1%与4.3%存在毒性分数超过50%。因此，在训练语言模型时，必须警觉并借助PerspectiveAPI等工具筛选掉预训练数据集中的毒性内容，以防止模型表现出偏见或在下游应用中产生有害内容。一种解决策略是过滤掉"bad words"名单中的文本，比如C4的作者们就采用了这种策略。另一个例子是，PILE数据集的研究者利用spamscanner来对有害内容进行分类。然而，执行此类过滤步骤必须极为谨慎，并需考虑到下游应用，以免过滤器保留下更可能坚持霸权观点的声音。在利用数据进行预训练语言模型之前，对贬损内容和性别/宗教偏见进行深度分析是必要的。</p>
<ul>
<li>个人身份信息控制</li>
</ul>
<p>在收集大型数据集时，理解与数据集实例相关的法律问题至关重要，尤其是在处理个人身份信息（PII）时，如真实姓名、组织名称、医疗记录、社会安全号码等。根据不同的应用，对这些信息进行遮蔽或删除在预训练语言模型之前是必要的。像presidio和pii-codex这样的工具提供了检测、分析和处理文本数据中个人身份信息的流程，这些工具能帮助确保数据集中的个人信息得到合理处理，以遵守相关隐私法规并保护用户隐私。</p>
<h3 id="序列长度-1">序列长度</h3>
<p>大语言模型支持的序列长度主要受两方面影响:</p>
<ol type="1">
<li><p>训练阶段的最大长度</p></li>
<li><p>模型的长度外推性</p></li>
</ol>
<p>第一点训练阶段的最大长度，可以通过DeepSpeed等分布式训练策略，减少模型的显存占用，从而提高训练的序列长度；</p>
<p>第二点模型的长度外推性，则通过位置编码的设计来实现，实现方式见模型结构设计小节。</p>
<h2 id="模型结构设计">模型结构设计</h2>
<p>在梳理了大型语言模型所需具备的关键能力以及相应升级策略之后，本小节将重点关注大模型结构的设计方法。我们将深入探讨如何构建高效且强大的大型预训练模型。</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692698220452-18482783-e800-4a9e-9d87-d240aef3a3e4.png" /></p>
<h3 id="tokenizer">Tokenizer</h3>
<p>参照baichuan提及的Tokenizer设计方式，编码器需要能够处理复杂的中英文任务。</p>
<ol type="1">
<li>目前大部分开源模型主要基于英文优化，因此对中文语料存在效率较低的问题。我们使用 2000 万条以中英为主的多语言语料训练分词模型，显著提升对于中文的压缩率。</li>
<li>对于数学领域，我们参考了 LLaMA 和 Galactica 中的方案，对数字的每一位单独分开，避免出现数字不一致的问题，对于提升数学能力有重要帮助。</li>
<li>对于罕见字词（如特殊符号等），支持 UTF-8 characters 的 byte 编码，因此做到未知字词的全覆盖。</li>
<li>我们分析了不同分词器对语料的压缩率，如下表，可见我们的分词器明显优于 LLaMA, Falcon 等开源模型，并且对比其他中文分词器在压缩率相当的情况下，训练和推理效率更高。</li>
</ol>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Baichuan-7B</th>
<th>LLaMA</th>
<th>Falcon</th>
<th>mpt-7B</th>
<th>ChatGLM</th>
<th>moss-moon-003</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Compress Rate</td>
<td>0.737</td>
<td>1.312</td>
<td>1.049</td>
<td>1.206</td>
<td>0.631</td>
<td>0.659</td>
</tr>
<tr class="even">
<td>Vocab Size</td>
<td>64,000</td>
<td>32,000</td>
<td>65,024</td>
<td>50,254</td>
<td>130,344</td>
<td>106,029</td>
</tr>
</tbody>
</table>
<h3 id="layernorm">LayerNorm</h3>
<p>LayerNorm分为Pre-LN和Post-LN两种，有研究发现Post-LN在训练过程中不稳定，因此目前大模型基本都采用Pre-LN的训练方式。</p>
<p><img src="https://pic1.zhimg.com/v2-c2fed007b3a0e176172079abe421e7c8_r.jpg?source=1940ef5c" /></p>
<p><strong>LayerNorm计算方式:</strong></p>
<p>首先计算均值与方差:</p>
<p><span class="math display">\[
\mu=\frac{1}{n} \sum_{i=1}^{n}a_i,\sigma=\sqrt{\frac{1}{n}\sum_{i=1}^n(a_i-\mu)^2}
\]</span></p>
<p>然后计算归一化:</p>
<p><span class="math display">\[
\overline{a}_i=\frac{a_i-\mu}{\sigma}g_i
\]</span></p>
<p>其中<span class="math inline">\(g_i\)</span>是起到缩放作用，在一开始设置为1.</p>
<p><strong>RMSNorm计算方式:</strong></p>
<p>RMSNorm假设均值为0，只针对方差进行归一化，训练速度更快且效果差距不大。</p>
<p><span class="math display">\[
\overline{a}_i=\frac{a_i}{RMS(a)}g_i,RMS(a)=\sqrt{\frac{1}{n}\sum_{i=1}^n(a_i)^2}
\]</span></p>
<h3 id="mlp">MLP</h3>
<p>MLP小节主要涉及激活函数的选择。</p>
<h4 id="relu">Relu</h4>
<p>ReLU是一种非常流行的激活函数，其数学表达式如下：</p>
<p><span class="math display">\[
f(x)=max(0,x)
\]</span></p>
<p><img src="https://www.h3399.cn/uploads/body/182/987e1c343976.png" /></p>
<h4 id="gelu">Gelu</h4>
<p><strong><em>高斯误差线性单元激活函数</em></strong>（Gaussian Error Linear Units(GELUS)）的数学表达式如下:</p>
<p><span class="math display">\[
GELU(x)=xP(X \le x)=x\Phi(x)=x * \frac{1}{2}[1+erf(x/\sqrt2)]
\]</span></p>
<p>也可以采用以下等式近似计算GELU:</p>
<p><span class="math display">\[
GELU(x) \approx 0.5x(1+tanh[\sqrt{2/\pi}(x+0.044715x^3)])
\]</span></p>
<p><span class="math display">\[
GELU(x) \approx x\sigma(1.702x)
\]</span></p>
<p>GELUs对于输入乘以一个0,1组成的mask，而该mask的生成则是依概率随机的依赖于输入。假设输入为x, mask为m，则m服从一个伯努利分布<span class="math inline">\(\Phi(x)\)</span> ,其中 <span class="math inline">\(\Phi(x)=P(X \le x)\)</span>,  <span class="math inline">\(X\)</span>服 从标准正态分布（高斯分布)，这么选择是因为神经元的输入趋向于正态分布，这么设定使得当输入x减小的时候，输入会有一个更高的概率被dropout掉，这样的激活变换就会随机依赖于输入了。</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692683853445-67a2a995-b476-4069-8815-7d20b2983c5c.png" /></p>
<p>Bert中GeLU代码如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def gelu(input_tensor):</span><br><span class="line">    cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))</span><br><span class="line">    return input_tesnsor*cdf</span><br></pre></td></tr></table></figure>
<h4 id="swiglugeglu">SwiGLU&amp;GeGLU</h4>
<p>SwiGLU 和 GeGLU都是Noam Shazeer在文章中探索的激活函数变体</p>
<p>具体的，需要先了解门线性单元（Gated Linear Unit, GLU）这个基本的双线性函数，为</p>
<p><span class="math display">\[
GLU(x,W,V,b,c)=\sigma(xW+b) \otimes (xV+c)
\]</span></p>
<p>其中<span class="math inline">\(\otimes\)</span>代表逐元素相乘，SwiGLU 和 GeGLU作为GLU的变体，定义如下:</p>
<p><span class="math display">\[
GEGLU(x,W,V,b,c)=GELU(xW+b) \otimes (xV + c)
\]</span></p>
<p><span class="math display">\[
SwiGLU(x,W,V,b,c,\beta) = Swish_{\beta}(xW + b) \otimes (xV + c)
\]</span></p>
<p>其中:</p>
<p><span class="math display">\[
Swish_{\beta}(x) = x\sigma(\beta x),\beta为常数如1
\]</span></p>
<p><img src="https://pic4.zhimg.com/80/v2-0d644ecd419bbdfdae9be5a8c076b9f7_1440w.webp" /></p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692752178909-c3da6449-94ea-46a5-99ac-be362159f58d.png" /></p>
<p>作者并没有对激活函数提出的原理和动机做过多描述，论文本身是对各类激活函数变种效果的对比尝试，可以看到SwishGLU和GeGLU是可以取得最小误差的，而在大模型中也得到了广泛应用。</p>
<h3 id="attention">Attention</h3>
<p>Attention层主要针对Attention的算子进行优化，加速模型的推理和部署。</p>
<h4 id="flashattention">FlashAttention</h4>
<p>详细介绍见:<a href="https://zhuanlan.zhihu.com/p/626079753">https://zhuanlan.zhihu.com/p/626079753</a></p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692600059334-9525d3af-33a7-426d-b833-c0c73e219b5c.png" /></p>
<p>动机:当输入序列（sequence length）较长时，Transformer的计算过程缓慢且耗费内存，这是因为self-attention的time和memory complexity会随着sequence length的增加成二次增长。</p>
<p>标准Attention的中间结果S,P（见下文）通常需要通过高带宽内存（HBM）进行存取，两者所需内存空间复杂度为<span class="math inline">\(O(N^2)\)</span>。本文分析：</p>
<ul>
<li>FlashAttention: 对HBM访问的次数为<span class="math inline">\(O(N^2d^2M^{-1})\)</span></li>
<li>Attention: 对HBM访问的次数为<span class="math inline">\(\Omega(Nd+N^2)\)</span></li>
</ul>
<p>往往（例如GPT2中N=1024，d=64），因此FlashAttention会快很多。下图展示了两者在GPT-2上的Forward+Backward的GFLOPs、HBM、Runtime对比（A100 GPU）：</p>
<p><img src="https://pic2.zhimg.com/80/v2-1e31e931b373c95619e975d7f16c88ed_1440w.webp" /></p>
<p>GPU中存储单元主要有HBM和SRAM：HBM容量大但是访问速度慢，SRAM容量小却有着较高的访问速度。例如：A100 GPU有40-80GB的HBM，带宽为1.5-2.0TB/s；每108个流式多核处理器各有192KB的片上SRAM，带宽估计约为19TB/s。可以看出，片上的SRAM比HBM快一个数量级，但尺寸要小许多数量级。</p>
<p><strong>综上，FlashAttention目的不是节约FLOPs，而是减少对HBM的访问。重点是FlashAttention在训练和预测过程中的结果和标准Attention一样，对用户是无感的，而其他加速方法做不到这点。</strong></p>
<h4 id="multi-query-attention">Multi Query Attention</h4>
<p>论文地址：<a href="https://arxiv.org/pdf/1911.0215">https://arxiv.org/pdf/1911.0215</a></p>
<p>MQA 是 19 年提出的一种新的 Attention 机制，其能够在保证模型效果的同时加快 decoder 生成 token 的速度。</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692696411037-fef17f67-9d7a-42ea-bafd-7c02aacdfbc3.png" /></p>
<p>从上图表中可以看到，MQA 在 encoder 上的提速没有非常明显，但在 decoder 上的提速是很显著的。</p>
<p><img src="https://pic1.zhimg.com/80/v2-09340314592932def37858652e999adc_1440w.webp" /></p>
<p>从论文的解释中可以看到，MQA 让所有的头之间 <strong>共享</strong> 同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量。</p>
<p>即：<strong>MQA 实际上是将 head 中的 key 和 value 矩阵抽出来单独存为一份共享参数，</strong></p>
<p><strong>而 query 则是依旧保留在原来的 head 中，每个 head 有一份自己独有的 query 参数。</strong></p>
<p><strong>代码实现：</strong></p>
<p>实现方式很简单，将原维度直接变成头数*维度的总和。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Multi Head Attention</span><br><span class="line">self.Wqkv = nn.Linear(                        # 【关键】Multi-Head Attention 的创建方法</span><br><span class="line">    self.d_model, </span><br><span class="line">    3 * self.d_model,                         # 有 query, key, value 3 个矩阵, 所以是 3 * d_model</span><br><span class="line">    device=device</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">query, key, value = qkv.chunk(                # 【关键】每个 tensor 都是 (1, 512, 768)</span><br><span class="line">    3, </span><br><span class="line">    dim=2</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Multi Query Attention</span><br><span class="line">self.Wqkv = nn.Linear(                                # 【关键】Multi-Query Attention 的创建方法</span><br><span class="line">    d_model,</span><br><span class="line">    d_model + 2 * self.head_dim,                      # 只创建 query 的 head 向量，所以只有 1 个 d_model</span><br><span class="line">    device=device,                                    # 而 key 和 value 不再具备单独的头向量</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">query, key, value = qkv.split(                        # query -&gt; (1, 512, 768)</span><br><span class="line">    [self.d_model, self.head_dim, self.head_dim],     # key   -&gt; (1, 512, 96)</span><br><span class="line">    dim=2                                             # value -&gt; (1, 512, 96)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>即<strong>K，V的维度从d_model转成self.head_dim</strong></p>
<p><u><b>在 MQA 中，除了 query 向量还保存着 8 个头，key 和 value 向量都只剩 1 个「公共头」了。</b></u></p>
<p>这也正好印证了论文中所说的「所有 head 之间共享一份 key 和 value 的参数」。</p>
<p>剩下的问题就是如何将这 1 份参数同时让 8 个头都使用，</p>
<p>代码里使用矩阵乘法 matmul 来广播，使得每个头都乘以这同一个 tensor，以此来实现参数共享：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def scaled_multihead_dot_product_attention(</span><br><span class="line">        query,</span><br><span class="line">        key,</span><br><span class="line">        value,</span><br><span class="line">        n_heads,</span><br><span class="line">        multiquery=False,</span><br><span class="line">    ):</span><br><span class="line">    q = rearrange(query, &#x27;b s (h d) -&gt; b h s d&#x27;, h=n_heads)         # (1, 512, 768) -&gt; (1, 8, 512, 96)</span><br><span class="line">    kv_n_heads = 1 if multiquery else n_heads</span><br><span class="line">    k = rearrange(key, &#x27;b s (h d) -&gt; b h d s&#x27;, h=kv_n_heads)        # (1, 512, 768) -&gt; (1, 8, 96, 512) if not multiquery </span><br><span class="line">                                                                    # (1, 512, 96) -&gt; (1, 1, 96, 512)  if multiquery</span><br><span class="line">    v = rearrange(value, &#x27;b s (h d) -&gt; b h s d&#x27;, h=kv_n_heads)      # (1, 512, 768) -&gt; (1, 8, 512, 96) if not multiquery </span><br><span class="line">                                                                    # (1, 512, 96) -&gt; (1, 1, 512, 96)  if multiquery</span><br><span class="line"></span><br><span class="line">    attn_weight = q.matmul(k) * softmax_scale                       # (1, 8, 512, 512)</span><br><span class="line">    attn_weight = torch.softmax(attn_weight, dim=-1)                # (1, 8, 512, 512)</span><br><span class="line"></span><br><span class="line">    out = attn_weight.matmul(v)                                     # (1, 8, 512, 512) * (1, 1, 512, 96) = (1, 8, 512, 96)</span><br><span class="line">    out = rearrange(out, &#x27;b h s d -&gt; b s (h d)&#x27;)                    # (1, 512, 768)</span><br><span class="line"></span><br><span class="line">    return out, attn_weight, past_key_value</span><br></pre></td></tr></table></figure>
<h3 id="位置编码">位置编码</h3>
<p>这里列出常见大模型应用的RoPE和ALiBi位置编码，从选择方式上更倾向于RoPE，可以通过位置插值等方式进行更长的长度外推。</p>
<h4 id="rope">RoPE</h4>
<p>详细内容见<a href="https://zhuanlan.zhihu.com/p/645263524">RoPE旋转位置编码深度解析：理论推导、代码实现、长度外推</a>，这里给出关键性结论：</p>
<ol type="1">
<li><p>实现方式:</p>
<p><span class="math display">\[
\begin{equation}\begin{pmatrix}q_0 \\ q_1 \\ q_2 \\ q_3 \\ \vdots \\ q_{d-2} \\ q_{d-1} 
\end{pmatrix}\otimes\begin{pmatrix}\cos m\theta_0 \\ \cos m\theta_0 \\ \cos m\theta_1 \\ \cos m\theta_1 \\ \vdots \\ \cos m\theta_{d/2-1} \\ \cos m\theta_{d/2-1} 
\end{pmatrix} + \begin{pmatrix}-q_1 \\ q_0 \\ -q_3 \\ q_2 \\ \vdots \\ -q_{d-1} \\ q_{d-2} 
\end{pmatrix}\otimes\begin{pmatrix}\sin m\theta_0 \\ \sin m\theta_0 \\ \sin m\theta_1 \\ \sin m\theta_1 \\ \vdots \\ \sin m\theta_{d/2-1} \\ \sin m\theta_{d/2-1} 
\end{pmatrix}\end{equation}
\]</span></p>
<p>其中<span class="math inline">\(\otimes\)</span>表示逐位对应相乘。</p></li>
<li><p>优点:通过绝对编码的方式融入相对位置信息</p></li>
<li><p>长度外推:位置插值、<span class="math inline">\(\beta\)</span>进制编码可以无损进行长度外推</p></li>
</ol>
<h4 id="alibi">ALiBi</h4>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692672154734-46f24488-26b0-4fe7-b2f9-05752aae68a7.png" /></p>
<p><strong>实现方式：</strong></p>
<p>本文的做法是不添加position embedding，然后添加一个静态的不学习的bias，如上图：</p>
<p><span class="math display">\[
softmax(q_iK^T + m · [−(i − 1), ..., −2, −1, 0])
\]</span></p>
<p>在query和key做矩阵点乘的基础上，加上一个常数负值，比如距离当前位置前1位为-1， 前两位为-2，这些常数要乘上 权重 m，对于n头的注意力模型，m从<span class="math inline">\(2^{\frac{-8}{n}}\)</span>开始。</p>
<p>例如，对于8头注意力模型:</p>
<p>m使用序列:<span class="math inline">\(2^{-1},2^{-2},...,2^{-8}\)</span></p>
<p>对于16头注意力模型:</p>
<p>m使用序列:<span class="math inline">\(2^{-0.5},2^{-1},...,2^{-8}\)</span></p>
<p><strong>优势:</strong></p>
<ul>
<li><p>减少了需要训练的Embedding，加快训练速度</p></li>
<li><p>较原位置编码，具有更好的长度外推性</p></li>
</ul>
<h3 id="训练数据参数量">训练数据&amp;参数量</h3>
<p>详细内容见<a href="https://zhuanlan.zhihu.com/p/636812912">LLM训练指南:Token及模型参数准备</a>，这里给出关键性结论，模型计算量增加时，训练数据和参数量应该保持同比增加:</p>
<table>
<thead>
<tr class="header">
<th>Parameters</th>
<th>FLOPs</th>
<th>FLOPs (in Gopher unit)</th>
<th>Tokens</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>400 Million</td>
<td>1.92e+19</td>
<td>1//29,968</td>
<td>8.0 Billion</td>
</tr>
<tr class="even">
<td>1 Billion</td>
<td>1.21e+20</td>
<td>1//4,761</td>
<td>20.2 Billion</td>
</tr>
<tr class="odd">
<td>10 Billion</td>
<td>1.23e+22</td>
<td>1//46</td>
<td>205.1 Billion</td>
</tr>
<tr class="even">
<td>67 Billion</td>
<td>5.76e+23</td>
<td>1</td>
<td>1.5 Trillion</td>
</tr>
<tr class="odd">
<td>175 Billion</td>
<td>3.85e+24</td>
<td>6.7</td>
<td>3.7 Trillion</td>
</tr>
<tr class="even">
<td>280 Billion</td>
<td>9.90e+24</td>
<td>17.2</td>
<td>5.9 Trillion</td>
</tr>
<tr class="odd">
<td>520 Billion</td>
<td>3.43e+25</td>
<td>59.5</td>
<td>11.0 Trillion</td>
</tr>
<tr class="even">
<td>1 Trillion</td>
<td>1.27e+26</td>
<td>221.3</td>
<td>21.2 Trillion</td>
</tr>
<tr class="odd">
<td>10 Trillion</td>
<td>1.30e+28</td>
<td>22515.9</td>
<td>216.2 Trillion</td>
</tr>
</tbody>
</table>
<h1 id="总结">总结</h1>
<p>经过对ChatGLM、LLAMA和Baichuan大型语言模型升级之路的深入探讨，以及对LLM结构选型的全面分析，我们可以得出以下结论：</p>
<ol type="1">
<li><p>大型预训练模型的升级过程主要体现在基础知识能力的提升和支持的序列长度变化。通过增加模型参数量和优化训练数据质量，模型可以更好地拟合各个领域的知识，并进一步提高模型性能；通过增加训练长度和调整位置编码外推性，支持更长的序列。</p></li>
<li><p>在模型结构设计方面，选择合适的LLM结构对于实现高性能的大型预训练模型至关重要。通过引入合适的LayerNorm和激活函数，提高训练的稳定性；通过引入高效的算子，如Flash Attention和Multi Query Attention，可以在保持模型性能的同时显著提高计算效率；通过引入RoPE或ALiBi位置编码，提高模型的长度外推性。</p></li>
<li><p>在构建和优化大型预训练模型时，不仅要关注模型的性能和计算效率，还应重视数据质量、去重、去污染、毒性与偏见控制以及个人信息保护等方面的问题。这将有助于使模型在实际应用中更具安全性、鲁棒性和可靠性。</p></li>
</ol>
<p>总之，本文通过深入剖析ChatGLM、LLAMA和Baichuan模型的升级路径，以及探讨大型语言模型结构选型，为大家提供了一个系统性的视角，梳理了大型预训练模型的关键要素。我们希望这些知识能够为大家在实际工程中构建更强大、灵活且高效的大型预训练模型提供有力的参考和指导。</p>
<p>感兴趣的可以关注微信公众号联系哈～</p>
<p><img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/67256578/1692751562367-607c2567-a00c-46cb-8f99-d6e758f8cd3b.png" /></p>
<h1 id="参考链接">参考链接:</h1>
<ol type="1">
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNDY1NTU5Mg==&amp;mid=2247486036&amp;idx=1&amp;sn=1885f2b1c35415c981f22c5ab0efeee5&amp;chksm=973d94d3a04a1dc56c4fe7353ae0f426d0c305398780faca1f749ae7e9fba24ee6af3f375877&amp;scene=21#wechat_redirect">大模型微调样本构造的trick</a></li>
<li>https://github.com/facebookresearch/llama</li>
<li>https://github.com/baichuan-inc/Baichuan-7B</li>
<li>https://github.com/THUDM/ChatGLM2-6B/tree/main</li>
<li>https://arxiv.org/pdf/2002.05202.pdf</li>
<li>https://zhuanlan.zhihu.com/p/634236135</li>
<li>https://zhuanlan.zhihu.com/p/626079753</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>ChatGPT</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>ChatGPT</tag>
      </tags>
  </entry>
</search>
