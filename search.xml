<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>中文情感词典的构建</title>
    <url>/2019/02/28/2019-02-28-%E4%B8%AD%E6%96%87%E6%83%85%E6%84%9F%E8%AF%8D%E5%85%B8%E7%9A%84%E6%9E%84%E5%BB%BA/</url>
    <content><![CDATA[<p>本文介绍了情感词典的构建方式，文章主要分为：通用情感词典的构建、通用情感词典的扩展、领域情感词典的构建</p>
<span id="more"></span>
<blockquote>
<p>首先，国外英文的情感分析已经取得了很好的效果，得益于英文单词自身分析的便捷性与英文大量的数据集 WordNet。但由于中文的多变性，语义的多重性与数据集的缺乏，使得国内的情感分析暂落后于国外。本文将记录博主在项目中构建情感词典的经验，欢迎大家指正。<br>我们首先将情感词典分为通用情感词典与专用情感词典。</p>
</blockquote>
<h1 id="1-通用情感词典的构建"><a href="#1-通用情感词典的构建" class="headerlink" title="1.通用情感词典的构建"></a>1.通用情感词典的构建</h1><p>   通用情感词典的构建主要是通过将目前开源的情感词典整合起来，筛去重复和无用的单词。<br>   目前网上开源的情感词典包含有：知网（HowNet）情感词典、台湾大学（NTSUSD)简体中文情感极性词典、大连理工大学情感词汇本体。<br>   前两个都可以在网上找到，第三个需要到其学校官网申请，说明完用途即可获得。</p>
<h1 id="2-通用情感词典的扩展"><a href="#2-通用情感词典的扩展" class="headerlink" title="2.通用情感词典的扩展"></a>2.通用情感词典的扩展</h1><p>上述情感词典年代都已经比较久远，所以我们可以采取一定方法对其扩展。这里我们采用的方法是将词典的同义词添加到词典里。<br>我们通过使用哈工大整理的同义词词林来获取词典的同义词，需要一提的是第一版的同义词林年代较为久远，现在也有哈工大整理的同义词林扩展版。<br>使用的链接在这里：<a href="https://blog.csdn.net/sinat_33741547/article/details/80016713">哈工大同义词林扩展版</a><br>使用代码编写时也可以利用Python的Synonyms库来获取同义词。<br>其已经开源，链接为：<a href="https://github.com/huyingxi/Synonyms">synonyms</a><br>如：</p>
<pre><code>import synonyms
print(&quot;人脸: %s&quot; % (synonyms.nearby(&quot;人脸&quot;)))
print(&quot;识别: %s&quot; % (synonyms.nearby(&quot;识别&quot;)))
</code></pre><h1 id="3-领域情感词典的构建"><a href="#3-领域情感词典的构建" class="headerlink" title="3.领域情感词典的构建"></a>3.领域情感词典的构建</h1><p>构建特定领域的情感词典大体有两种方法：基于规则的情感词典构建方法、基于统计的情感词典构建方法。</p>
<p>基于规则的情感词典方法一般是用句型固定、句式不多变的情况。通过对语料进行句法分析，词性标注等操作，得到语料中常用的句型并将其抽象出来，根据抽象出来的句型模板来对新的语料进行模板匹配，以此来选择出新的语料中的情感词，将其加入到对应的情感词典中。</p>
<p>基于统计的情感词典构建方法需要利用PMI互信息计算与左右熵来发现所需要的新词。具体方法我们可以添加情感种子词，来计算分好词的语料中各个词语与情感种子词的互信息度与左右熵，再将互信息度与左右熵结合起来，选择出与情感词关联度最高的TopN个词语，将其添加到对应的情感词典。<br>这里可以参考链接<a href="https://www.jianshu.com/p/e9313fd692ef">link</a></p>
<h3 id="互信息度计算"><a href="#互信息度计算" class="headerlink" title="互信息度计算"></a>互信息度计算</h3><p><img src="https://img-blog.csdnimg.cn/20190228172006936.png" alt="互信息度计算"></p>
<ul>
<li>p(x,y)为两个词一起出现的概率</li>
<li>p(x)为词x出现的概率</li>
<li>p(y)为词y出现的概率</li>
</ul>
<hr>
<p>具体例子：4G， 上网卡，4G上网卡;如果4G的词频是2,上网卡的词频是10,4G上网卡的词频是1，那么记单单词的总数有N个，双单词的总数有M个，则有下面的公式<br><img src="https://img-blog.csdnimg.cn/20190228172528100.png" alt="具体例子"></p>
<h3 id="左右熵"><a href="#左右熵" class="headerlink" title="左右熵"></a>左右熵</h3><p>我们这里使用左右熵来衡量主要是想表示预选词的自由程度(4G上网卡为一个预选词），左右熵越大，表示这个词的左边右边的词换的越多，那么它就很有可能是一个单独的词。<br>我们这里的左右熵定义为(以左熵为例):<br><img src="https://img-blog.csdnimg.cn/20190228172807236.png" alt="左熵"><br>这里我们还是举一个具体的例子来理解它<br>假设4G上网卡左右有这么几种搭配<br>[买4G上网卡, 有4G上网卡，有4G上网卡， 丢4G上网卡]<br>那么4G上网卡的左熵为<br><img src="https://img-blog.csdnimg.cn/20190228172830315.png" alt="例子"><br>这里A = [买, 有, 丢]</p>
<blockquote>
<p>后面就是具体的实现了，这里的难点就在如何获得这些概率值，就博主看到的用法有：利用搜索引擎获取词汇共现率即p(x,y)、利用语料库获取各个词出现概率</p>
</blockquote>
<h2 id="最后我们只需要将这三步获得的情感词典进行整合就可以了"><a href="#最后我们只需要将这三步获得的情感词典进行整合就可以了" class="headerlink" title="最后我们只需要将这三步获得的情感词典进行整合就可以了"></a>最后我们只需要将这三步获得的情感词典进行整合就可以了</h2><p>参考文献：<br><a href="https://www.jianshu.com/p/e9313fd692ef">python3实现互信息和左右熵的新词发现</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>情感分析</category>
      </categories>
      <tags>
        <tag>情感分析</tag>
        <tag>情感词典</tag>
      </tags>
  </entry>
  <entry>
    <title>Coursera Machine Learning 学习笔记（二）Linear Regression</title>
    <url>/2019/03/02/2019-03-02-Coursera%20Machine%20Learning%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89Linear%20Regression/</url>
    <content><![CDATA[<p>文章为博主学习Coursera上的Machine Learning课程的笔记，Coursera Machine Learning 学习笔记（二）Linear Regression</p>
<span id="more"></span>
<blockquote>
<p>文章为博主学习Coursera上的Machine Learning课程的笔记，来记录自己的学习过程，欢迎大家一起学习交流</p>
</blockquote>
<h1 id="02：Linear-Regression"><a href="#02：Linear-Regression" class="headerlink" title="02：Linear Regression"></a>02：Linear Regression</h1><p>仍然以房价预测作为示例，具体示例仍需见课程内容。<br>符号含义：</p>
<ol>
<li>m 为数据集的大小</li>
<li>x’s为输入数据</li>
<li>y’s为对应的目标输出结果</li>
<li>(x,y)为所有训练数据</li>
<li>(x<sup>i</sup>, y<sup>i</sup>)为具体第i行数据，第i个训练数据</li>
</ol>
<p>假设函数h(x)，以一元线性回归为例：<br><img src="https://img-blog.csdnimg.cn/20190302112701595.png" alt="假设函数"></p>
<script type="math/tex; mode=display">\theta_0：截距  \theta_1：梯度</script><h5 id="Linear-regression-implementation（损失函数cost-function）"><a href="#Linear-regression-implementation（损失函数cost-function）" class="headerlink" title="Linear regression - implementation（损失函数cost function）"></a>Linear regression - implementation（损失函数cost function）</h5><p>计算由不同θ 取值带来的不同损失函数值，本质上是一个最小化问题：使下式取值最小</p>
<script type="math/tex; mode=display">Minimize ：(h_\theta(x)-y)^2</script><p>即可以看成下述式子：</p>
<script type="math/tex; mode=display">J(\theta_0,\theta_1) = \frac 1 {2m}\sum_1^m(h_\theta(x^{(i)})-y^{(i)})^2</script><script type="math/tex; mode=display">\frac1 m 是求平均\frac1 {2m}是为了数学计算方便</script><p>这个损失函数是均方误差，适用于多类回归问题，当然也可以有其他的损失函数。</p>
<h5 id="梯度下降算法（Gradient-descent-algorithm）"><a href="#梯度下降算法（Gradient-descent-algorithm）" class="headerlink" title="梯度下降算法（Gradient descent algorithm）"></a>梯度下降算法（Gradient descent algorithm）</h5><p><strong>目的：</strong> 使损失函数J最小</p>
<h6 id="工作方式"><a href="#工作方式" class="headerlink" title="工作方式"></a>工作方式</h6><ul>
<li>从随机初始化开始</li>
</ul>
<ol>
<li>对θ 随机赋值，可以为任何值</li>
<li>每次一点点改变θ，使J(θ)减小</li>
</ol>
<ul>
<li>每次沿着梯度下降最大的方向</li>
<li>重复上述操作直到达到局部最小值</li>
<li>从哪里开始的可能决定你到达哪个局部最优解<br><img src="https://img-blog.csdnimg.cn/2019030217133153.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwNjc2MDMz,size_16,color_FFFFFF,t_70" alt="梯度下降"><br><strong>一个更正规的定义：</strong><br>做下述操作直到收敛：<br><img src="https://img-blog.csdnimg.cn/20190302171951356.png" alt="参数更新"><br>符号解释：</li>
</ul>
<ol>
<li><strong>:=</strong><br>表示赋值<br>NB a = b 是一个正确的断言</li>
<li>α (alpha)<br>学习率，控制参数更新的步长<ol>
<li>如果学习率过大，可能无法得到最优解，误差会增大</li>
<li>如果学习率过小，那么达到最优解需要非常多步，耗费很长时间</li>
</ol>
</li>
</ol>
<p>注： 参数的更新必须<strong>同步</strong>即需要一个中间变量保存之前的值，原因是二者的式子中包含了对方，一方的更新会导致第二方式子内值的变化。<br><img src="https://img-blog.csdnimg.cn/20190302173355106.png" alt="在这里插入图片描述"><br><strong>当我们达到局部最优解时：</strong></p>
<ol>
<li>后面部分的梯度为0</li>
<li>各参数值就保持不变了 </li>
</ol>
<h5 id="使用梯度下降的线性回归算法"><a href="#使用梯度下降的线性回归算法" class="headerlink" title="使用梯度下降的线性回归算法"></a>使用梯度下降的线性回归算法</h5><ul>
<li>将梯度下降算法应用到最小化损失函数J(θ)上</li>
</ul>
<script type="math/tex; mode=display">\frac{\partial} {\partial\theta_j}=\frac 1 {2m}\sum_1^m(h_\theta(x^{(i)})-y^{(i)})^2=\frac{\partial} {\partial\theta_j}\frac 1 {2m}\sum_1^m(\theta_0+\theta_1x^{(i)}-y^{(i)})^2</script><p>按照求导公式可以推出：</p>
<script type="math/tex; mode=display">j=0:\frac{\partial} {\partial\theta_0}J(\theta_0,\theta_1)=\frac1 m\sum_1^m(h_\theta(x^{(i)})-y^{(i)})</script><script type="math/tex; mode=display">j=1:\frac{\partial} {\partial\theta_1}J(\theta_0,\theta_1)=\frac1 m\sum_1^m(h_\theta(x^{(i)})-y^{(i)})*x^{(i)}</script><p>注：因为线性回归是一个凸函数，是一个碗形的图，所以会趋于局部最优解</p>
<ul>
<li>现在的这种梯度下降算法又叫Batch Gradient Descent 原因是每一次都遍历了整个数据集，后面会提到取数据集中的部分进行的Gradient Descent</li>
<li>线性回归也有正规方程求解，但其中矩阵运算当数据集过大时不宜使用，这时就可以使用梯度下降</li>
</ul>
<h5 id="数值求解的正规方程法"><a href="#数值求解的正规方程法" class="headerlink" title="数值求解的正规方程法"></a>数值求解的正规方程法</h5><ul>
<li>直接通过数值求解来避免繁琐的迭代过程，从数学上求解出min(J(θ))<br><strong>正规方程的优缺点：</strong><br><strong>优点：</strong><br>1.不需要学习率这个参数<br>2.对某些问题可以很快的解决<br><strong>缺点：</strong><br>会很复杂</li>
</ul>
<h5 id="面对数据量很大的时候"><a href="#面对数据量很大的时候" class="headerlink" title="面对数据量很大的时候"></a>面对数据量很大的时候</h5><p>这个时候就需要<strong>将数据向量化</strong>利用线性代数中矩阵运算来完成计算</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Linear Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>Coursera Machine Learning 学习笔记（一）Introduction</title>
    <url>/2019/03/02/2019-03-02-Coursera%20Machine%20Learning%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89Introduction/</url>
    <content><![CDATA[<p>文章为博主学习Coursera上的Machine Learning课程的笔记，Coursera Machine Learning 学习笔记（一）Introduction</p>
<span id="more"></span>
<blockquote>
<p>文章为博主学习Coursera上的Machine Learning课程的笔记，来记录自己的学习过程，欢迎大家一起学习交流</p>
</blockquote>
<h1 id="01-Introduction"><a href="#01-Introduction" class="headerlink" title="01:Introduction"></a>01:Introduction</h1><h2 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><ul>
<li><p>Arthur Samuel(1959)<br>  <strong>Machine Learning:</strong>“Field of study that gives computers the ability to learn without being explicitly programmed”</p>
</li>
<li><p>Tom Michel(1999)<br>  <strong>Well posed learning problem:</strong>“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”<br>  在下棋的例子中：</p>
</li>
</ul>
<ol>
<li>E经验为1000场下棋游戏</li>
<li>T任务为下棋</li>
<li>P评价准则为是否获胜</li>
</ol>
<ul>
<li><p>学习算法的类型<br>  。 有监督学习（Supervised learning）</p>
<pre><code>      Teach the computer how to do something, then let it use it;s new found knowledge to do it

      注：一般是有标注的训练集
</code></pre><p>  。无监督学习（Unsupervised learning）<br>  Let the computer learn how to do something, and use this to determine structure and patterns in data</p>
<pre><code>      注：一般是无标注训练集，使机器从中提取特征，常见算法为聚类
</code></pre><p>  。强化学习（Reinforcement learning）</p>
<p>  。推荐系统（Recommender systems）</p>
</li>
</ul>
<h3 id="有监督学习介绍"><a href="#有监督学习介绍" class="headerlink" title="有监督学习介绍"></a>有监督学习介绍</h3><h5 id="问题分类："><a href="#问题分类：" class="headerlink" title="问题分类："></a>问题分类：</h5><ul>
<li>预测问题<br>课程内拿房价预测作为示例：具体可以看课程内容<br>预测问题也叫回归问题，具有以下特征：</li>
</ul>
<ol>
<li>预测连续的输出</li>
<li>没有明显得离散划分</li>
</ol>
<ul>
<li>分类问题<br>课程内以肿瘤划分作为示例</li>
</ul>
<h3 id="无监督学习介绍"><a href="#无监督学习介绍" class="headerlink" title="无监督学习介绍"></a>无监督学习介绍</h3><p>在无监督学习里我们获得的是没有标注的数据，将这些数据划分成不同的数据簇</p>
<h5 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h5><p>具体应用示例：</p>
<ol>
<li>新闻划分</li>
<li>基因组排序</li>
<li>分布式计算机集群划分</li>
<li>社交网络划分</li>
<li>天文数据分析</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Introduction</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据分析-数据可视化</title>
    <url>/2020/01/03/2020-01-03-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<p>本文将介绍使用Matplotlib工具，完成Python数据分析-数据可视化</p>
<span id="more"></span>
<h1 id="一-Matplotlib-基本概念"><a href="#一-Matplotlib-基本概念" class="headerlink" title="一. Matplotlib 基本概念"></a>一. Matplotlib 基本概念</h1><p>Matplotlib是python的一个数据可视化工具库。</p>
<p>特点：专门用于开发2D图表(包括3D图表)， 操作简单。</p>
<p>可视化是在整个数据挖掘的关键辅助工具，可以清晰的理解数据，从而调整我们的分析方法。</p>
<h1 id="二-Matplotlib三层结构"><a href="#二-Matplotlib三层结构" class="headerlink" title="二. Matplotlib三层结构"></a>二. Matplotlib三层结构</h1><p><img src="https://img-blog.csdnimg.cn/20190313235406342.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="三层结构"></p>
<h1 id="三-Matplotlib-基本使用"><a href="#三-Matplotlib-基本使用" class="headerlink" title="三. Matplotlib 基本使用"></a>三. Matplotlib 基本使用</h1><h2 id="1-折线图"><a href="#1-折线图" class="headerlink" title="1. 折线图"></a>1. 折线图</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"># 图形显示设置</span><br><span class="line">%matplotlib inline   </span><br><span class="line"> </span><br><span class="line"># 绘制画布-容器层  figsize: 画布长宽属性   dpi: 图象的清晰度</span><br><span class="line">plt.figure(figsize=(16,8), dpi=60)</span><br><span class="line"> </span><br><span class="line"># 绘制折线图-图象层</span><br><span class="line">plt.plot([1,2,3,4,5,6], [22,19,18,25,27,19])</span><br><span class="line"> </span><br><span class="line"># 显示图象</span><br><span class="line"># plt.show()</span><br><span class="line"> </span><br><span class="line"># 保存图象 -注：plt.show()会释放figure资源，保存图片需要将plt.show()注释掉</span><br><span class="line"># 图片的保存路径 -- </span><br><span class="line">plt.savefig(&quot;plot.png&quot;)</span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdn.net/20180917191105231?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"></p>
<h2 id="2-绘制多条折线图"><a href="#2-绘制多条折线图" class="headerlink" title="2. 绘制多条折线图"></a>2. 绘制多条折线图</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 中文显示问题-- 下载中文字体，安装字体-修改配置文件下面手动修改配置</span><br><span class="line"># from pylab import mpl</span><br><span class="line"># mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line"># mpl.rcParams[&quot;axes.unicode_minus&quot;] = False # 解决保存图像是负号&#x27;-&#x27;显示为方块的问题</span><br><span class="line"> </span><br><span class="line"># 准备数据</span><br><span class="line">x = range(60)</span><br><span class="line">y_sh = [random.uniform(26,31) for i in x]</span><br><span class="line">y_bj = [random.uniform(27, 35) for i in x]</span><br><span class="line"> </span><br><span class="line"># 创建画布</span><br><span class="line">plt.figure(figsize=(16,8), dpi=60)</span><br><span class="line"> </span><br><span class="line"># 同一坐标内--绘制多条折线图  （新增）</span><br><span class="line">plt.plot(x, y_sh, label=&quot;sh&quot;)</span><br><span class="line">plt.plot(x, y_bj, label=&quot;bj&quot;, linestyle=&quot;--&quot;, color=&quot;y&quot;)  # 线条颜色，线条样式设置 见下图</span><br><span class="line"> </span><br><span class="line"># 自定义x, y轴 刻度 &amp; 刻度标签 (新增)</span><br><span class="line">x_ticks = range(0, 60, 5)</span><br><span class="line">y_ticks = range(20, 40, 5)</span><br><span class="line"> </span><br><span class="line">x_ticks_label = [&quot;11点&#123;&#125;分&quot;.format(i) for i in x_ticks]</span><br><span class="line"> </span><br><span class="line">plt.xticks(x_ticks, x_ticks_label)</span><br><span class="line">plt.yticks(y_ticks)</span><br><span class="line"> </span><br><span class="line"># 添加辅助描述信息-- x，y轴标签 &amp; 图形标题</span><br><span class="line">plt.xlabel(&quot;时间&quot;)</span><br><span class="line">plt.ylabel(&quot;温度&quot;)</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;两地同一时间温度变化图&quot;)</span><br><span class="line"> </span><br><span class="line"># 添加网格线 - alpha:透明度   （新增）</span><br><span class="line">plt.grid(True, linestyle=&quot;--&quot;, alpha=0.6)</span><br><span class="line"> </span><br><span class="line"># 显示图例 -- loc：位置设置,详见下图  （新增）</span><br><span class="line">plt.legend(loc=&quot;best&quot;)</span><br><span class="line"> </span><br><span class="line"># 显示图象</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdn.net/20180917235044993?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br><strong>附参数表:</strong><br><img src="https://img-blog.csdn.net/20180917235125313?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190310015842874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="3-绘制多个坐标系-—-plt-subplots"><a href="#3-绘制多个坐标系-—-plt-subplots" class="headerlink" title="3. 绘制多个坐标系 — plt.subplots"></a>3. 绘制多个坐标系 — plt.subplots</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 中文显示问题--下面手动修改配置 </span><br><span class="line">from pylab import mpl</span><br><span class="line">mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line">mpl.rcParams[&quot;axes.unicode_minus&quot;] = False</span><br><span class="line"> </span><br><span class="line"># 准备x,y轴数据</span><br><span class="line">x = range(60)</span><br><span class="line">y_sh = [random.uniform(15, 18) for i in x ]</span><br><span class="line">y_bj = [random.uniform(5, 12) for i in x ]</span><br><span class="line"> </span><br><span class="line"># 创建画布--多个坐标轴, 绘制折线图</span><br><span class="line">fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8), dpi=100)</span><br><span class="line"># Returns:  fig: 图对象    ax: 坐标轴对象列表</span><br><span class="line">axes[0].plot(x, y_sh, label=&#x27;上海&#x27;)</span><br><span class="line">axes[1].plot(x, y_bj, label=&#x27;北京&#x27;, color=&#x27;r&#x27;, linestyle=&#x27;--&#x27;)</span><br><span class="line"> </span><br><span class="line"># 显示图例/坐标轴刻度/网格线</span><br><span class="line">axes[0].legend()</span><br><span class="line">axes[1].legend()</span><br><span class="line"> </span><br><span class="line">x_ticks_label = [&#x27;11点&#123;&#125;分&#x27;.format(i) for i in x]</span><br><span class="line">y_ticks = range(40)</span><br><span class="line">axes[0].set_xticks(x[::5], x_ticks_label[::5])</span><br><span class="line">axes[0].set_yticks(y_ticks[::5])</span><br><span class="line">axes[1].set_xticks(x[::5], x_ticks_label[::5])</span><br><span class="line">axes[1].set_yticks(y_ticks[::5])</span><br><span class="line"> </span><br><span class="line">axes[0].grid(True, linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line">axes[1].grid(True, linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line"> </span><br><span class="line"># 添加 标题/坐标轴描述信息</span><br><span class="line">axes[0].set_title(&#x27;上海11点0分到12点之间的温度变化图&#x27;)</span><br><span class="line">axes[0].set_xlabel(&quot;时间&quot;)</span><br><span class="line">axes[0].set_ylabel(&quot;温度&quot;)</span><br><span class="line"> </span><br><span class="line">axes[1].set_title(&#x27;北京11点0分到12点之间的温度变化图&#x27;)</span><br><span class="line">axes[1].set_xlabel(&#x27;时间&#x27;)</span><br><span class="line">axes[1].set_ylabel(&#x27;温度&#x27;)</span><br><span class="line"> </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="4-绘制sin-函数图像-—-plot"><a href="#4-绘制sin-函数图像-—-plot" class="headerlink" title="4. 绘制sin()函数图像 — plot"></a>4. 绘制sin()函数图像 — plot</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">import numpy as np</span><br><span class="line">x = np.linspace(-10, 10, 1000)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制图像，显示图像</span><br><span class="line">plt.figure(figsize=(10, 1), dpi=100)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.grid(linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/2019031201165850.png" alt="在这里插入图片描述"></p>
<h2 id="5-散点图-—-scatter"><a href="#5-散点图-—-scatter" class="headerlink" title="5. 散点图 — scatter"></a>5. 散点图 — scatter</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># -- 案例： 探究房屋面积和房屋价格的关系</span><br><span class="line">from pylab import mpl          # 中文显示问题--下面手动修改配置 </span><br><span class="line">mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line">mpl.rcParams[&quot;axes.unicode_minus&quot;] = False</span><br><span class="line"># 准备数据</span><br><span class="line">x = [225.98, 247.07, 253.14, 457.85, 241.58, 301.01,  20.67, 288.64,</span><br><span class="line">       163.56, 120.06, 207.83, 342.75, 147.9 ,  53.06, 224.72,  29.51,</span><br><span class="line">        21.61, 483.21, 245.25, 399.25, 343.35]</span><br><span class="line">y = [196.63, 203.88, 210.75, 372.74, 202.41, 247.61,  24.9 , 239.34,</span><br><span class="line">       140.32, 104.15, 176.84, 288.23, 128.79,  49.64, 191.74,  33.1 ,</span><br><span class="line">        30.74, 400.02, 205.35, 330.64, 283.45]</span><br><span class="line"> </span><br><span class="line"># 创建画布  -- 绘制散点图 -- 显示图像</span><br><span class="line">plt.figure(figsize=(20,8), dpi=100)</span><br><span class="line">plt.scatter(x, y)</span><br><span class="line"> </span><br><span class="line">plt.title(&#x27;房屋面积和房屋价格的关系--案例测试&#x27;)</span><br><span class="line">plt.xlabel(&#x27;面积&#x27;)</span><br><span class="line">plt.ylabel(&#x27;价格&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190312012238645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="6-柱状图-—-bar"><a href="#6-柱状图-—-bar" class="headerlink" title="6. 柱状图 — bar"></a>6. 柱状图 — bar</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;东方快车谋杀案&#x27;,&#x27;寻梦环游记&#x27;,&#x27;全球风暴&#x27;,&#x27;降魔传&#x27;,&#x27;追捕&#x27;,&#x27;七十七天&#x27;,&#x27;密战&#x27;,&#x27;狂兽&#x27;,&#x27;其它&#x27;]</span><br><span class="line">y = [73853,57767,22354,15969,14839,8725,8716,8318,7916,6764,52222]</span><br><span class="line">x = range(len(movie_name))</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制柱状图，添加标题和格线，显示图像</span><br><span class="line">plt.figure(figsize=(18, 6), dpi=80)</span><br><span class="line">plt.bar(x, y, width=0.5, color=[&#x27;b&#x27;,&#x27;r&#x27;,&#x27;g&#x27;,&#x27;y&#x27;,&#x27;c&#x27;,&#x27;m&#x27;,&#x27;y&#x27;,&#x27;k&#x27;,&#x27;c&#x27;,&#x27;g&#x27;,&#x27;m&#x27;])</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影票房收入对比&quot;)</span><br><span class="line">plt.xticks(x, movie_name)</span><br><span class="line">plt.grid(linestyle=&#x27;--&#x27;, alpha=0.5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190312014545814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="7-柱状图-—-多个指标对比"><a href="#7-柱状图-—-多个指标对比" class="headerlink" title="7. 柱状图 — 多个指标对比"></a>7. 柱状图 — 多个指标对比</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;寻梦环游记&#x27;]</span><br><span class="line">first_day = [10587.6,10062.5,1275.7]</span><br><span class="line">first_weekend=[36224.9,34479.6,11830]</span><br><span class="line">x = range(len(movie_name))</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制柱状图，添加标题/坐标轴刻度标签/网格线/示例， 显示图像</span><br><span class="line">plt.figure(figsize=(10, 5), dpi=80)</span><br><span class="line">plt.bar(x, first_day, width=0.2, label=&quot;首日票房&quot;)</span><br><span class="line">plt.bar([i+0.2 for i in x], first_weekend, width=0.2, label=&quot;首周票房&quot;)</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影首日和首周的票房对比&quot;)</span><br><span class="line">plt.xticks([i+0.1 for i in x], movie_name)     # 修改x轴刻度显示</span><br><span class="line">plt.grid(linestyle=&quot;--&quot;, alpha=0.5)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190312020011216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="8-直方图-—-hist"><a href="#8-直方图-—-hist" class="headerlink" title="8 直方图 — hist"></a>8 直方图 — hist</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据</span><br><span class="line">time = [131,  98, 125, 131, 124, 139, 131, 117, 128, 108, 135, 138, 131, 102, 107, 114, 119, 128, 121, 142, 127, 130, 124, 101, 110, 116, 117, 110, 128, 128, 115,  99, 136, 126, 134,  95, 138, 117, 111,78, 132, 124, 113, 150, 110, 117,  86,  95, 144, 105, 126, 130,126, 130, 126, 116, 123, 106, 112, 138, 123,  86, 101,  99, 136,123, 117, 119, 105, 137, 123, 128, 125, 104, 109, 134, 125, 127,105, 120, 107, 129, 116, 108, 132, 103, 136, 118, 102, 120, 114,105, 115, 132, 145, 119, 121, 112, 139, 125, 138, 109, 132, 134,156, 106, 117, 127, 144, 139, 139, 119, 140,  83, 110, 102,123,107, 143, 115, 136, 118, 139, 123, 112, 118, 125, 109, 119, 133,112, 114, 122, 109, 106, 123, 116, 131, 127, 115, 118, 112, 135,115, 146, 137, 116, 103, 144,  83, 123, 111, 110, 111, 100, 154,136, 100, 118, 119, 133, 134, 106, 129, 126, 110, 111, 109, 141,120, 117, 106, 149, 122, 122, 110, 118, 127, 121, 114, 125, 126,114, 140, 103, 130, 141, 117, 106, 114, 121, 114, 133, 137,  92,121, 112, 146,  97, 137, 105,  98, 117, 112,  81,  97, 139, 113,134, 106, 144, 110, 137, 137, 111, 104, 117, 100, 111, 101, 110,105, 129, 137, 112, 120, 113, 133, 112,  83,  94, 146, 133, 101,131, 116, 111,  84, 137, 115, 122, 106, 144, 109, 123, 116, 111,111, 133, 150]</span><br><span class="line"> </span><br><span class="line"># 创建画布， 绘制直方图， 添加标题/坐标轴刻度标签/网格线</span><br><span class="line">plt.figure(figsize=(20, 8), dpi=80)</span><br><span class="line">distance = 2</span><br><span class="line">group_num = int((max(time)-min(time)) / distance)</span><br><span class="line">plt.hist(time, bins=group_num)</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影时长分布状况&quot;)</span><br><span class="line">plt.xticks(range(min(time), max(time))[::2])</span><br><span class="line">plt.xlabel(&quot;电影时长&quot;)</span><br><span class="line">plt.ylabel(&quot;数量&quot;)</span><br><span class="line">plt.grid(linestyle=&quot;--&quot;, alpha=0.5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190313233637810.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="9-饼图-—-pie"><a href="#9-饼图-—-pie" class="headerlink" title="9 饼图 — pie"></a>9 饼图 — pie</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 准备数据   -- 案例：电影的排片占比</span><br><span class="line">movie_name = [&#x27;雷神3：诸神黄昏&#x27;,&#x27;正义联盟&#x27;,&#x27;东方快车谋杀案&#x27;,&#x27;寻梦环游记&#x27;,&#x27;全球风暴&#x27;,&#x27;降魔传&#x27;,&#x27;追捕&#x27;,&#x27;七十七天&#x27;,&#x27;密战&#x27;,&#x27;狂兽&#x27;,&#x27;其它&#x27;]</span><br><span class="line">place_count = [60605,54546,45819,28243,13270,9945,7679,6799,6101,4621,20105]</span><br><span class="line"> </span><br><span class="line"># 创建画布，绘制饼图，添加标题/坐标轴刻度标签</span><br><span class="line">plt.figure(figsize=(18, 6), dpi=80)</span><br><span class="line">plt.pie(place_count, labels=movie_name, autopct=&quot;%1.2f%%&quot;)</span><br><span class="line">plt.axis(&quot;equal&quot;)          #  坐标轴长宽相等，保证饼图成圆形</span><br><span class="line"> </span><br><span class="line">plt.title(&quot;电影的排片占比&quot;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://img-blog.csdnimg.cn/20190313235128167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1JlZnJhaW5fX1dH,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="10-VENN-韦恩图"><a href="#10-VENN-韦恩图" class="headerlink" title="10.VENN 韦恩图"></a>10.VENN 韦恩图</h2><p><strong>需要先下载matplotlib_venn</strong></p>
<h3 id="10-1-具有2个分组的基本的维恩图"><a href="#10-1-具有2个分组的基本的维恩图" class="headerlink" title="10.1 具有2个分组的基本的维恩图"></a>10.1 具有2个分组的基本的维恩图</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn2</span><br><span class="line"></span><br><span class="line"># 第一种方法，10，5为两组的大小，2为两组交叉大小;</span><br><span class="line"># set_labels为组名</span><br><span class="line"># venn2(subsets = (10, 5, 2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;))</span><br><span class="line"># 设置两组数据为ABCD和DEF</span><br><span class="line">venn2([set([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;]), set([&#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;])])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzRfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<h3 id="10-2-具有3个组的基本维恩图"><a href="#10-2-具有3个组的基本维恩图" class="headerlink" title="10.2 具有3个组的基本维恩图"></a>10.2 具有3个组的基本维恩图</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line"># Import the library</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn3</span><br><span class="line"> </span><br><span class="line"># Make the diagram</span><br><span class="line">venn3(subsets = (10, 8, 22, 6,9,4,2)) # 通过直接设置各部分数据来配置韦恩图</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzZfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 设置三组ABCD、DEF、ADG</span><br><span class="line">venn3([set([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;]), set([&#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;]), set([&#x27;A&#x27;, &#x27;D&#x27;, &#x27;G&#x27;,&#x27;F&#x27;])]) # 设置数据来配置韦恩图</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzdfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<h3 id="10-3-自定义维恩图"><a href="#10-3-自定义维恩图" class="headerlink" title="10.3 自定义维恩图"></a>10.3 自定义维恩图</h3><ol>
<li>自定义标签</li>
<li>自定义维恩图上圆的线条</li>
<li>自定义维恩图上的圆</li>
</ol>
<p><strong>自定义标签</strong></p>
<ul>
<li>get_label_by_id 可查看其源代码 表示分类里不同部分<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">## Venn上的自定义标签 Custom label on Venn</span><br><span class="line"># Import the library</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn3</span><br><span class="line">from matplotlib_venn import venn3_circles</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Custom text labels: change the label of group A</span><br><span class="line">v=venn3(subsets = (10, 8, 22, 6,9,4,2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;, &#x27;Group C&#x27;))</span><br><span class="line"># 单独改变A的标签</span><br><span class="line">v.get_label_by_id(&#x27;A&#x27;).set_text(&#x27;My Favourite group!&#x27;)</span><br></pre></td></tr></table></figure>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzlfMC5wbmc?x-oss-process=image/format,png" alt="在这里插入图片描述"><br><strong>自定义维恩图上圆的线条</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">## 自定义维恩图上圆的线条 Custom Circles lines on Venn</span><br><span class="line"># Line style: can be &#x27;dashed&#x27; or &#x27;dotted&#x27; for example</span><br><span class="line"># 设置维恩图</span><br><span class="line">v = venn3(subsets = (10, 8, 22, 6,9,4,2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;, &#x27;Group C&#x27;))</span><br><span class="line"># 画圆，linestyle线条类型，linewith线宽，color线条颜色</span><br><span class="line">c = venn3_circles(subsets = (10, 8, 22, 6,9,4,2), linestyle=&#x27;dashed&#x27;, linewidth=1, color=&quot;grey&quot;)</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzEwXzAucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"><br><strong>自定义维恩图上的圆</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">## 自定义维恩图上的圆 Custom a circle on Venn</span><br><span class="line"># Change one group only</span><br><span class="line">v=venn3(subsets = (10, 8, 22, 6,9,4,2), set_labels = (&#x27;Group A&#x27;, &#x27;Group B&#x27;, &#x27;Group C&#x27;))</span><br><span class="line">c=venn3_circles(subsets = (10, 8, 22, 6,9,4,2), linestyle=&#x27;dashed&#x27;, linewidth=1, color=&quot;grey&quot;)</span><br><span class="line"># 设置第一个圆的线宽</span><br><span class="line">c[0].set_lw(8.0)</span><br><span class="line"># 设置第一个圆的线形</span><br><span class="line">c[0].set_ls(&#x27;dotted&#x27;)</span><br><span class="line"># 设置第一个圆的填充颜色</span><br><span class="line">c[0].set_color(&#x27;skyblue&#x27;)</span><br><span class="line"> </span><br><span class="line"># Color</span><br><span class="line"># id号</span><br><span class="line"># 如ABC三个簇，010代表非A和B和非C,100代表A和非B和非C</span><br><span class="line"># 设置透明度</span><br><span class="line">v.get_patch_by_id(&#x27;011&#x27;).set_alpha(1.0)</span><br><span class="line"># 设置颜色</span><br><span class="line">v.get_patch_by_id(&#x27;011&#x27;).set_color(&#x27;red&#x27;)</span><br><span class="line"># 打印id号</span><br><span class="line">#v.id2idx</span><br></pre></td></tr></table></figure>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9naXRlZS5jb20vbHVtaW5pb3VzL2FydGljbGVfcGljdHVyZV93YXJlaG91c2UvcmF3L21hc3Rlci9QeXRob24tU3R1ZHktTm90ZXMvVkVOTiUyMERJQUdSQU0vb3V0cHV0XzExXzAucG5n?x-oss-process=image/format,png" alt="在这里插入图片描述"></p>
<h3 id="10-4-修改韦恩图数值"><a href="#10-4-修改韦恩图数值" class="headerlink" title="10.4 修改韦恩图数值"></a>10.4 修改韦恩图数值</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#_*_coding:utf-8_*_</span><br><span class="line"># author    : jmx</span><br><span class="line"># create    : 19-12-16 上午11:08</span><br><span class="line"># filename  : venn.py</span><br><span class="line"># IDE   : PyCharm</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib_venn import venn3, venn3_circles</span><br><span class="line">import matplotlib.font_manager as fm</span><br><span class="line">from pathlib import Path</span><br><span class="line">def venn(train_filename=&#x27;biaozhuData/generate_data/train/pkuverb.txt&#x27;, val_filename=&#x27;biaozhuData/generate_data_2/valid/pkuverb.txt&#x27;, test_filename=&#x27;biaozhuData/generate_data_2/test/pkuverb.txt&#x27;):</span><br><span class="line">	&#x27;&#x27;&#x27;</span><br><span class="line">	训练集、验证集、测试集 数据分布的韦恩图</span><br><span class="line">	:param train_filename:</span><br><span class="line">	:param val_filename:</span><br><span class="line">	:param test_filename:</span><br><span class="line">	:return:</span><br><span class="line">	&#x27;&#x27;&#x27;</span><br><span class="line">	path = Path(train_filename)</span><br><span class="line">	set1 = set(open(test_filename, encoding=&#x27;utf-8&#x27;).readlines())</span><br><span class="line">	set2 = set(open(val_filename, encoding=&#x27;utf-8&#x27;).readlines())</span><br><span class="line">	set3 = set(open(train_filename, encoding=&#x27;utf-8&#x27;).readlines())</span><br><span class="line"></span><br><span class="line">	set_len1 = len(set1)</span><br><span class="line">	set_len2 = len(set2)</span><br><span class="line">	set_len3 = len(set3)</span><br><span class="line"></span><br><span class="line">	v = venn3([set1, set2, set3], (&#x27;test&#x27;, &#x27;valid&#x27;, &#x27;train&#x27;))</span><br><span class="line">	a = v.get_label_by_id(&#x27;100&#x27;).get_text()</span><br><span class="line">	b = v.get_label_by_id(&#x27;010&#x27;).get_text()</span><br><span class="line">	c = v.get_label_by_id(&#x27;001&#x27;).get_text()</span><br><span class="line"></span><br><span class="line">	a = str(round(int(a) / set_len1, 3)) + &#x27; &#x27; + a</span><br><span class="line">	b = str(round(int(b) / set_len2, 3)) + &#x27; &#x27; + b</span><br><span class="line">	c = str(round(int(c) / set_len3, 3)) + &#x27; &#x27; + c</span><br><span class="line"></span><br><span class="line">	v.get_label_by_id(&#x27;100&#x27;).set_text(a)</span><br><span class="line">	v.get_label_by_id(&#x27;010&#x27;).set_text(b)</span><br><span class="line">	v.get_label_by_id(&#x27;001&#x27;).set_text(c)</span><br><span class="line">	plt.title(name[path.stem]+&#x27;集合关系&#x27;, fontproperties=myfont)</span><br><span class="line">	sfname = path.name.replace(path.suffix, &#x27;.png&#x27;)</span><br><span class="line">	savepath = Path(venn_savedir)/sfname</span><br><span class="line">	#plt.show()</span><br><span class="line">	plt.savefig(savepath)</span><br><span class="line">	plt.close()</span><br></pre></td></tr></table></figure>
<h1 id="四-Matplotlib-中文无法显示问题"><a href="#四-Matplotlib-中文无法显示问题" class="headerlink" title="四. Matplotlib 中文无法显示问题"></a>四. Matplotlib 中文无法显示问题</h1><ol>
<li>windows下：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">plt.rcParams[&#x27;font.sans-serif&#x27;]=[&#x27;SimHei&#x27;] #用来正常显示中文标签</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">%matplotlib inline</span><br><span class="line"># 中文显示问题--下面手动修改配置 </span><br><span class="line">from pylab import mpl</span><br><span class="line">mpl.rcParams[&quot;font.sans-serif&quot;] = [&quot;SimHei&quot;]</span><br><span class="line">mpl.rcParams[&quot;axes.unicode_minus&quot;] = False</span><br></pre></td></tr></table></figure>
<ol>
<li>Ubuntu/LInux下</li>
</ol>
<p><a href="https://www.cnblogs.com/panlq/p/9270826.html">Ubuntu解决matplotlib中文无法显示</a></p>
<p>其实这些乱码问题，只是路径等配置问题，上述只是修改默认的配置，可以通过指定字体路径等来解决。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">myfont = fm.FontProperties(fname=r&#x27;1031Competition/font/simsun.ttc&#x27;)  # 设置字体</span><br><span class="line">……</span><br><span class="line">plt.xlabel(&#x27;短语类型&#x27;, fontproperties=myfont)</span><br><span class="line">plt.ylabel(&#x27;比例&#x27;, fontproperties=myfont)</span><br><span class="line">plt.legend(loc=&#x27;upper right&#x27;)</span><br><span class="line">plt.savefig(&#x27;test.png&#x27;)</span><br></pre></td></tr></table></figure>
<p><strong>参考文章</strong><br><a href="https://blog.csdn.net/refrain__wg/article/details/82747254">Matplotlib 数据可视化-基本使用教程</a><br><a href="https://blog.csdn.net/LuohenYJ/article/details/103091081">python基于matplotlib_venn实现维恩图的绘制</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>数据分析</category>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch 多卡训练</title>
    <url>/2021/09/12/2021-09-12-Pytorch%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>本文将介绍Pytorch 多卡训练的原理及实现。多卡训练流程一般如下：</p>
<ol>
<li>指定主机节点</li>
<li>主机节点划分数据，一个batch数据平均分到每个机器上</li>
<li>模型从主机拷贝到各个机器</li>
<li>每个机器进行前向传播</li>
<li>每个机器计算loss损失</li>
<li>主机收集所有loss结果，进行参数更新</li>
<li>将更新后参数模型拷贝给各个机器</li>
</ol>
<span id="more"></span>
<h1 id="Pytorch-多卡训练"><a href="#Pytorch-多卡训练" class="headerlink" title="Pytorch 多卡训练"></a>Pytorch 多卡训练</h1><h2 id="一、多卡训练原理"><a href="#一、多卡训练原理" class="headerlink" title="一、多卡训练原理"></a>一、多卡训练原理</h2><p>多卡训练流程一般如下：</p>
<ol>
<li>指定主机节点</li>
<li>主机节点划分数据，一个batch数据平均分到每个机器上</li>
<li>模型从主机拷贝到各个机器</li>
<li>每个机器进行前向传播</li>
<li>每个机器计算loss损失</li>
<li>主机收集所有loss结果，进行参数更新</li>
<li>将更新后参数模型拷贝给各个机器</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/img_convert/1665788acfe3757d493b3d82422035c1.png" alt="image"></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/bffacf11040884149347a428d6f63260.png" alt="image"></p>
<h2 id="二、单机多卡训练"><a href="#二、单机多卡训练" class="headerlink" title="二、单机多卡训练"></a>二、单机多卡训练</h2><p>使用<strong>torch.nn.DataParallel</strong>(module, device_ids)模块，module为模型，device_ids为并行的GPU id列表</p>
<p>使用方式：将模型调用该接口执行操作</p>
<p><code>model = torch.nn.DataParallel(model)</code></p>
<p>示例：我们假设模型输入为(32, input_dim)，这里的 32 表示batch_size，模型输出为(32, output_dim)，使用 4 个GPU训练。nn.DataParallel起到的作用是将这 32 个样本拆成 4 份，发送给 4 个GPU 分别做 forward，然后生成 4 个大小为(8, output_dim)的输出，然后再将这 4 个输出都收集到cuda:0上并合并成(32, output_dim)。</p>
<p>可以看出，nn.DataParallel没有改变模型的输入输出，因此其他部分的代码不需要做任何更改，非常方便。但弊端是，后续的loss计算只会在cuda:0上进行，没法并行，因此会导致负载不均衡的问题。</p>
<p>通过在模型内置loss计算可以解决上述负载不均衡的情况，最后所得loss进行取平均。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Net:</span><br><span class="line">    def __init__(self,...):</span><br><span class="line">        # code</span><br><span class="line">    </span><br><span class="line">    def forward(self, inputs, labels=None)</span><br><span class="line">        # outputs = fct(inputs)</span><br><span class="line">        # loss_fct = ...</span><br><span class="line">        if labels is not None:</span><br><span class="line">            loss = loss_fct(outputs, labels)  # 在训练模型时直接将labels传入模型，在forward过程中计算loss</span><br><span class="line">            return loss</span><br><span class="line">        else:</span><br><span class="line">            return outputs</span><br></pre></td></tr></table></figure>
<p>按照我们上面提到的模型并行逻辑，在每个GPU上会计算出一个loss，这些loss会被收集到cuda:0上并合并成长度为 4 的张量。这个时候在做backward的之前，必须对将这个loss张量合并成一个标量，一般直接取mean就可以。这在Pytorch官方文档nn.DataParallel函数中有提到：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.</span><br></pre></td></tr></table></figure>
<h2 id="三、多机多卡训练"><a href="#三、多机多卡训练" class="headerlink" title="三、多机多卡训练"></a>三、多机多卡训练</h2><p><strong>该方式也可以实现单机多卡</strong></p>
<p>使用<strong>torch.nn.parallel.DistributedDataParallel</strong>和<strong>torch.utils.data.distributed.DistributedSampler</strong>结合多进程实现。</p>
<ol>
<li><p>从一开始就会启动多个进程(进程数小于等于GPU数)，每个进程独享一个GPU，每个进程都会独立地执行代码。这意味着每个进程都独立地初始化模型、训练，当然，在每次迭代过程中会通过进程间通信共享梯度，整合梯度，然后独立地更新参数。</p>
</li>
<li><p>每个进程都会初始化一份训练数据集，当然它们会使用数据集中的不同记录做训练，这相当于同样的模型喂进去不同的数据做训练，也就是所谓的数据并行。这是通过<strong>torch.utils.data.distributed.DistributedSampler</strong>函数实现的，不过逻辑上也不难想到，只要做一下数据partition，不同进程拿到不同的parition就可以了，官方有一个简单的demo，感兴趣的可以看一下代码实现：Distributed Training</p>
</li>
<li><p>进程通过local_rank变量来标识自己，local_rank为0的为master，其他是slave。这个变量是torch.distributed包帮我们创建的，使用方法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import argparse  # 必须引入 argparse 包</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&quot;--local_rank&quot;, type=int, default=-1)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure>
<p>必须以如下方式运行代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 train.py</span><br></pre></td></tr></table></figure>
<p>这样的话，torch.distributed.launch就以命令行参数的方式将args.local_rank变量注入到每个进程中，每个进程得到的变量值都不相同。比如使用 4 个GPU的话，则 4 个进程获得的args.local_rank值分别为0、1、2、3。</p>
</li>
</ol>
<p>上述命令行参数nproc_per_node表示每个节点需要创建多少个进程(使用几个GPU就创建几个)；nnodes表示使用几个节点，做单机多核训练设为1。</p>
<ol>
<li>因为每个进程都会初始化一份模型，为保证模型初始化过程中生成的随机权重相同，需要设置随机种子。方法如下：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def set_seed(seed):</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>使用方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from torch.utils.data.distributed import DistributedSampler  # 负责分布式dataloader创建，也就是实现上面提到的partition。</span><br><span class="line"></span><br><span class="line"># 负责创建 args.local_rank 变量，并接受 torch.distributed.launch 注入的值</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&quot;--local_rank&quot;, type=int, default=-1)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"># 每个进程根据自己的local_rank设置应该使用的GPU</span><br><span class="line">torch.cuda.set_device(args.local_rank)</span><br><span class="line">device = torch.device(&#x27;cuda&#x27;, args.local_rank)</span><br><span class="line"></span><br><span class="line"># 初始化分布式环境，主要用来帮助进程间通信</span><br><span class="line">torch.distributed.init_process_group(backend=&#x27;nccl&#x27;)</span><br><span class="line"></span><br><span class="line"># 固定随机种子</span><br><span class="line">seed = 42</span><br><span class="line">random.seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line">torch.cuda.manual_seed_all(seed)</span><br><span class="line"></span><br><span class="line"># 初始化模型</span><br><span class="line">model = Net()</span><br><span class="line">model.to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.1)</span><br><span class="line"></span><br><span class="line"># 只 master 进程做 logging，否则输出会很乱</span><br><span class="line">if args.local_rank == 0:</span><br><span class="line">    tb_writer = SummaryWriter(comment=&#x27;ddp-training&#x27;)</span><br><span class="line"></span><br><span class="line"># 分布式数据集</span><br><span class="line">train_sampler = DistributedSampler(train_dataset)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)  # 注意这里的batch_size是每个GPU上的batch_size</span><br><span class="line"></span><br><span class="line"># 分布式模型</span><br><span class="line">model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)</span><br></pre></td></tr></table></figure>
<p><strong>torch.distributed.init_process_group</strong>()包含四个常用的参数：</p>
<ul>
<li>backend: 后端, 实际上是多个机器之间交换数据的协议</li>
<li>init_method: 机器之间交换数据, 需要指定一个主节点, 而这个参数就是指定主节点的</li>
<li>world_size: 介绍都是说是进程, 实际就是机器的个数, 例如两台机器一起训练的话, world_size就设置为2</li>
<li>rank: 区分主节点和从节点的, 主节点为0, 剩余的为了1-(N-1), N为要使用的机器的数量, 也就是world_size</li>
</ul>
<h3 id="后端初始化"><a href="#后端初始化" class="headerlink" title="后端初始化"></a>后端初始化</h3><p>pytorch提供下列常用后端：</p>
<p><img src="https://pic2.zhimg.com/80/v2-b4d4a27387dde5cbd043d883948def09_720w.jpg" alt="image"></p>
<h3 id="初始化init-method"><a href="#初始化init-method" class="headerlink" title="初始化init_method"></a>初始化init_method</h3><ol>
<li>TCP初始化</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line"></span><br><span class="line">dist.init_process_group(backend, init_method=&#x27;tcp://10.1.1.20:23456&#x27;,</span><br><span class="line">                        rank=rank, world_size=world_size)</span><br></pre></td></tr></table></figure>
<p>注意这里使用格式为tcp://ip:端口号, 首先ip地址是你的主节点的ip地址, 也就是rank参数为0的那个主机的ip地址, 然后再选择一个空闲的端口号, 这样就可以初始化init_method了.</p>
<ol>
<li>共享文件系统初始化</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line"></span><br><span class="line">dist.init_process_group(backend, init_method=&#x27;file:///mnt/nfs/sharedfile&#x27;,</span><br><span class="line">                        rank=rank, world_size=world_size)</span><br></pre></td></tr></table></figure>
<h3 id="初始化rank和world-size"><a href="#初始化rank和world-size" class="headerlink" title="初始化rank和world_size"></a>初始化rank和world_size</h3><p>这里其实没有多难, 你需要确保, 不同机器的rank值不同, 但是主机的rank必须为0, 而且使用init_method的ip一定是rank为0的主机, 其次world_size是你的主机数量, 你不能随便设置这个数值, 你的参与训练的主机数量达不到world_size的设置值时, 代码是不会执行的.</p>
<h2 id="四、模型保存"><a href="#四、模型保存" class="headerlink" title="四、模型保存"></a>四、模型保存</h2><p>模型的保存与加载，与单GPU的方式有所不同。这里通通将参数以cpu的方式save进存储, 因为如果是保存的GPU上参数，pth文件中会记录参数属于的GPU号，则加载时会加载到相应的GPU上，这样就会导致如果你GPU数目不够时会在加载模型时报错</p>
<p>或者模型保存时控制进程，只在主进程进行保存。<br>模型保存都是一致的，不过分布式运行有多个进程在同时跑，所以会保存多个模型到存储上，如果使用共享存储就要注意文件名的问题，当然一般只在rank0进程上保存参数即可，因为所有进程的模型参数是同步的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.save(model.module.cpu().state_dict(), &quot;model.pth&quot;)</span><br></pre></td></tr></table></figure>
<p>参数加载：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">param=torch.load(&quot;model.pth&quot;)</span><br></pre></td></tr></table></figure>
<p>以下是huggingface/transformers代码中用到的模型保存代码<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if torch.distributed.get_rank() == 0:</span><br><span class="line">    model_to_save = model.module if hasattr(model, &quot;module&quot;) else model  # Take care of distributed/parallel training</span><br><span class="line">    model_to_save.save_pretrained(args.output_dir)</span><br><span class="line">    tokenizer.save_pretrained(args.output_dir)</span><br></pre></td></tr></table></figure></p>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://zhuanlan.zhihu.com/p/86441879">pytorch多gpu并行训练</a></p>
<p><a href="https://github.com/jia-zhuang/pytorch-multi-gpu-training">PyTorch 单机多GPU 训练方法与原理整理</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention可视化</title>
    <url>/2021/12/24/2021-12-24-Attention%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<p>本文介绍了Attention可视化的方式，包含两种热力图可视化、文本可视化。</p>
<ol>
<li>使用matplotlib与seaborn完成attention矩阵的热力图绘制</li>
<li>根据注意力矩阵，得到html颜色深浅代表相关性强弱</li>
</ol>
<span id="more"></span>
<h1 id="热力图可视化"><a href="#热力图可视化" class="headerlink" title="热力图可视化"></a>热力图可视化</h1><blockquote>
<p>使用matplotlib与seaborn完成attention矩阵的热力图绘制</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"># author:   LZ</span><br><span class="line"># create time:  2021/11/15 14:54</span><br><span class="line"># file: genpic.py</span><br><span class="line"># IDE:  PyCharm</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import pandas as pd</span><br><span class="line">import matplotlib.ticker as ticker</span><br><span class="line">import seaborn as sns</span><br><span class="line">import matplotlib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gperes = &#123;</span><br><span class="line">     &#x27;武&#x27;: &#123;&#x27;武&#x27;: 0.1, &#x27;汉&#x27;: 0.9, &#x27;市&#x27;: 1, &#x27;长&#x27;: 0.05, &#x27;江&#x27;: 0.09, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.2&#125;,</span><br><span class="line">     &#x27;汉&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0.2, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.03, &#x27;江&#x27;: 0.04, &#x27;大&#x27;: 0.02, &#x27;桥&#x27;: 0.07&#125;,</span><br><span class="line">     &#x27;市&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.01, &#x27;江&#x27;: 0.02, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.05&#125;,</span><br><span class="line">     &#x27;长&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0.25, &#x27;江&#x27;: 0.1, &#x27;大&#x27;: 0.08, &#x27;桥&#x27;: 0.09&#125;,</span><br><span class="line">     &#x27;江&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0.15, &#x27;大&#x27;: 0.04, &#x27;桥&#x27;: 0.015&#125;,</span><br><span class="line">     &#x27;大&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0.14, &#x27;桥&#x27;: 0.15&#125;,</span><br><span class="line">     &#x27;桥&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0, &#x27;桥&#x27;: 0.12&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">newgperes = &#123;&#125;</span><br><span class="line">for k, v in gperes.items():</span><br><span class="line">     for w, a in v.items():</span><br><span class="line">          newgperes.setdefault(w, &#123;&#125;)</span><br><span class="line">          newgperes[w][k] = a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.rcParams[&#x27;font.sans-serif&#x27;]=[&#x27;Arial Unicode MS&#x27;]</span><br><span class="line">plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False</span><br><span class="line">gpedata = pd.DataFrame(newgperes)</span><br><span class="line">ax = sns.heatmap(gpedata, cmap=&quot;YlOrRd&quot;)</span><br><span class="line">ax.xaxis.tick_top()</span><br><span class="line">ax.set_yticklabels(ax.get_yticklabels(), rotation=0)</span><br><span class="line">plt.savefig(&#x27;loctest.png&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://s3.bmp.ovh/imgs/2021/12/4100fe169b8b9921.png" alt="gpe.png"></p>
<h1 id="NLP文本注意力可视化"><a href="#NLP文本注意力可视化" class="headerlink" title="NLP文本注意力可视化"></a>NLP文本注意力可视化</h1><blockquote>
<p>根据注意力矩阵，得到html颜色深浅代表相关性强弱</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Credits to Lin Zhouhan(@hantek) for the complete visualization code</span><br><span class="line">import random, os, numpy, scipy</span><br><span class="line">from codecs import open</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def createHTML(texts, weights, savepath):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Creates a html file with text heat.</span><br><span class="line">	weights: attention weights for visualizing</span><br><span class="line">	texts: text on which attention weights are to be visualized</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    fOut = open(savepath, &quot;w&quot;, encoding=&quot;utf-8&quot;)</span><br><span class="line">    part1 = &quot;&quot;&quot;</span><br><span class="line">    &lt;html lang=&quot;en&quot;&gt;</span><br><span class="line">    &lt;head&gt;</span><br><span class="line">    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=utf-8&quot;&gt;</span><br><span class="line">    &lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">    font-family: Sans-Serif;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;/style&gt;</span><br><span class="line">    &lt;/head&gt;</span><br><span class="line">    &lt;body&gt;</span><br><span class="line">    &lt;h3&gt;</span><br><span class="line">    Heatmaps</span><br><span class="line">    &lt;/h3&gt;</span><br><span class="line">    &lt;/body&gt;</span><br><span class="line">    &lt;script&gt;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    part2 = &quot;&quot;&quot;</span><br><span class="line">    var color = &quot;255,0,0&quot;;</span><br><span class="line">    var ngram_length = 3;</span><br><span class="line">    var half_ngram = 1;</span><br><span class="line">    for (var k=0; k &lt; any_text.length; k++) &#123;</span><br><span class="line">    var tokens = any_text[k].split(&quot; &quot;);</span><br><span class="line">    var intensity = new Array(tokens.length);</span><br><span class="line">    var max_intensity = Number.MIN_SAFE_INTEGER;</span><br><span class="line">    var min_intensity = Number.MAX_SAFE_INTEGER;</span><br><span class="line">    for (var i = 0; i &lt; intensity.length; i++) &#123;</span><br><span class="line">    intensity[i] = 0.0;</span><br><span class="line">    for (var j = -half_ngram; j &lt; ngram_length-half_ngram; j++) &#123;</span><br><span class="line">    if (i+j &lt; intensity.length &amp;&amp; i+j &gt; -1) &#123;</span><br><span class="line">    intensity[i] += trigram_weights[k][i + j];</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if (i == 0 || i == intensity.length-1) &#123;</span><br><span class="line">    intensity[i] /= 2.0;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">    intensity[i] /= 3.0;</span><br><span class="line">    &#125;</span><br><span class="line">    if (intensity[i] &gt; max_intensity) &#123;</span><br><span class="line">    max_intensity = intensity[i];</span><br><span class="line">    &#125;</span><br><span class="line">    if (intensity[i] &lt; min_intensity) &#123;</span><br><span class="line">    min_intensity = intensity[i];</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    var denominator = max_intensity - min_intensity;</span><br><span class="line">    for (var i = 0; i &lt; intensity.length; i++) &#123;</span><br><span class="line">    intensity[i] = (intensity[i] - min_intensity) / denominator;</span><br><span class="line">    &#125;</span><br><span class="line">    if (k%2 == 0) &#123;</span><br><span class="line">    var heat_text = &quot;&lt;p&gt;&lt;br&gt;&lt;b&gt;Example:&lt;/b&gt;&lt;br&gt;&quot;;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">    var heat_text = &quot;&lt;b&gt;Example:&lt;/b&gt;&lt;br&gt;&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    var space = &quot;&quot;;</span><br><span class="line">    for (var i = 0; i &lt; tokens.length; i++) &#123;</span><br><span class="line">    heat_text += &quot;&lt;span style=&#x27;background-color:rgba(&quot; + color + &quot;,&quot; + intensity[i] + &quot;)&#x27;&gt;&quot; + space + tokens[i] + &quot;&lt;/span&gt;&quot;;</span><br><span class="line">    if (space == &quot;&quot;) &#123;</span><br><span class="line">    space = &quot; &quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //heat_text += &quot;&lt;p&gt;&quot;;</span><br><span class="line">    document.body.innerHTML += heat_text;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;/script&gt;</span><br><span class="line">    &lt;/html&gt;&quot;&quot;&quot;</span><br><span class="line">    putQuote = lambda x: &quot;\&quot;%s\&quot;&quot; % x</span><br><span class="line">    textsString = &quot;var any_text = [%s];\n&quot; % (&quot;,&quot;.join(map(putQuote, texts)))</span><br><span class="line">    weightsString = &quot;var trigram_weights = [%s];\n&quot; % (&quot;,&quot;.join(map(str, weights)))</span><br><span class="line">    fOut.write(part1)</span><br><span class="line">    fOut.write(textsString)</span><br><span class="line">    fOut.write(weightsString)</span><br><span class="line">    fOut.write(part2)</span><br><span class="line">    fOut.close()</span><br><span class="line"></span><br><span class="line">    return</span><br><span class="line"></span><br><span class="line">gperes = &#123;</span><br><span class="line">     &#x27;武&#x27;: &#123;&#x27;武&#x27;: 0.1, &#x27;汉&#x27;: 0.9, &#x27;市&#x27;: 1, &#x27;长&#x27;: 0.05, &#x27;江&#x27;: 0.09, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.2&#125;,</span><br><span class="line">     &#x27;汉&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0.2, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.03, &#x27;江&#x27;: 0.04, &#x27;大&#x27;: 0.02, &#x27;桥&#x27;: 0.07&#125;,</span><br><span class="line">     &#x27;市&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0.1, &#x27;长&#x27;: 0.01, &#x27;江&#x27;: 0.02, &#x27;大&#x27;: 0.03, &#x27;桥&#x27;: 0.05&#125;,</span><br><span class="line">     &#x27;长&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0.25, &#x27;江&#x27;: 0.1, &#x27;大&#x27;: 0.08, &#x27;桥&#x27;: 0.09&#125;,</span><br><span class="line">     &#x27;江&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0.15, &#x27;大&#x27;: 0.04, &#x27;桥&#x27;: 0.015&#125;,</span><br><span class="line">     &#x27;大&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0.14, &#x27;桥&#x27;: 0.15&#125;,</span><br><span class="line">     &#x27;桥&#x27;: &#123;&#x27;武&#x27;: 0, &#x27;汉&#x27;: 0, &#x27;市&#x27;: 0, &#x27;长&#x27;: 0, &#x27;江&#x27;: 0, &#x27;大&#x27;: 0, &#x27;桥&#x27;: 0.12&#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">matrix = []</span><br><span class="line"># 获取矩阵表示</span><br><span class="line">for k, v in gperes.items():</span><br><span class="line">    matrix.append(list(v.values()))</span><br><span class="line">str_ = &#x27;武 汉 市 长 江 大 桥&#x27;</span><br><span class="line">createHTML([str_] * 7,</span><br><span class="line">           matrix,</span><br><span class="line">           &#x27;./visualization/test.html&#x27;)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.bmp.ovh/imgs/2021/12/04573a3a1f1fb2b6.png" alt="wx20211211-193035@2x.png"></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://blog.csdn.net/qq_38607066/article/details/101345282#t17">https://blog.csdn.net/qq_38607066/article/details/101345282#t17</a></p>
<p><a href="https://blog.csdn.net/u010490755/article/details/89574847">https://blog.csdn.net/u010490755/article/details/89574847</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Nvidia-Docker配置python3与pytorch环境</title>
    <url>/2021/09/28/2021-09-28-Nvidia-Docker%E9%85%8D%E7%BD%AEpython3%E4%B8%8Epytorch%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<p>本文将介绍Nvidia-Docker配置python3与pytorch环境，完成docker容器内安装GPU深度学习环境。<br>TIPS：为了避免下载源过慢，建议添加中科大源/清华源<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">2. sudo sed -i &#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27; /etc/apt/sources.list</span><br><span class="line">3. sudo apt update</span><br></pre></td></tr></table></figure></p>
<span id="more"></span>
<h1 id="一、Docker与Nvidia-docker安装"><a href="#一、Docker与Nvidia-docker安装" class="headerlink" title="一、Docker与Nvidia-docker安装"></a>一、Docker与Nvidia-docker安装</h1><h2 id="TIPS：为了避免下载源过慢，建议添加中科大源-清华源"><a href="#TIPS：为了避免下载源过慢，建议添加中科大源-清华源" class="headerlink" title="TIPS：为了避免下载源过慢，建议添加中科大源/清华源"></a>TIPS：为了避免下载源过慢，建议添加中科大源/清华源</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">2. sudo sed -i &#x27;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#x27; /etc/apt/sources.list</span><br><span class="line">3. sudo apt update</span><br></pre></td></tr></table></figure>
<h2 id="1-Docker安装"><a href="#1-Docker安装" class="headerlink" title="1. Docker安装"></a>1. Docker安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. Update the apt package index and install packages to allow apt to use a repository over HTTPS:</span><br><span class="line"></span><br><span class="line"> sudo apt-get update</span><br><span class="line"> sudo apt-get install \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    gnupg \</span><br><span class="line">    lsb-release</span><br><span class="line">    </span><br><span class="line">2. Add Docker’s official GPG key:</span><br><span class="line"></span><br><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg</span><br><span class="line"></span><br><span class="line">3. Use the following command to set up the stable repository. To add the nightly or test repository, add the word nightly or test (or both) after the word stable in the commands below. Learn about nightly and test channels.</span><br><span class="line"></span><br><span class="line"> echo \</span><br><span class="line">  &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \</span><br><span class="line">  $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br><span class="line">  </span><br><span class="line">4. Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version:</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install docker-ce docker-ce-cli containerd.io</span><br><span class="line"></span><br><span class="line">5. To install a specific version of Docker Engine, list the available versions in the repo, then select and install:</span><br><span class="line"></span><br><span class="line">a. List the versions available in your repo:</span><br><span class="line"></span><br><span class="line">apt-cache madison docker-ce</span><br><span class="line"></span><br><span class="line">b. Install a specific version using the version string from the second column, for example, 5:18.09.1~3-0~ubuntu-xenial.</span><br><span class="line"></span><br><span class="line">sudo apt-get install docker-ce=&lt;VERSION_STRING&gt; docker-ce-cli=&lt;VERSION_STRING&gt; containerd.io</span><br><span class="line"></span><br><span class="line">6. Verify that Docker Engine is installed correctly by running the hello-world image.</span><br><span class="line"></span><br><span class="line">sudo docker run hello-world</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-Nvidia-Docker-安装"><a href="#2-Nvidia-Docker-安装" class="headerlink" title="2. Nvidia-Docker 安装"></a>2. Nvidia-Docker 安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. Setup the stable repository and the GPG key:</span><br><span class="line"></span><br><span class="line">distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line"></span><br><span class="line">2. To get access to experimental features such as CUDA on WSL or the new MIG capability on A100, you may want to add the experimental branch to the repository listing:</span><br><span class="line"></span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-container-runtime/experimental/$distribution/nvidia-container-runtime.list | sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list</span><br><span class="line"></span><br><span class="line">3. Install the nvidia-docker2 package (and dependencies) after updating the package listing:</span><br><span class="line"></span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y nvidia-docker2</span><br><span class="line"></span><br><span class="line">4. Restart the Docker daemon to complete the installation after setting the default runtime:</span><br><span class="line"></span><br><span class="line">sudo systemctl restart docker</span><br><span class="line"></span><br><span class="line">5. At this point, a working setup can be tested by running a base CUDA container:</span><br><span class="line">sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi</span><br></pre></td></tr></table></figure>
<h1 id="二、docker内安装python与pytorch环境"><a href="#二、docker内安装python与pytorch环境" class="headerlink" title="二、docker内安装python与pytorch环境"></a>二、docker内安装python与pytorch环境</h1><blockquote>
<p>拉取nvidia包含cuda的基础镜像。拟安装环境：python3.7, pytorch1.6</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 宿主机：提前在宿主机上下载好安装pip3.7要用到的包</span><br><span class="line">curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py</span><br><span class="line"></span><br><span class="line"># 宿主机与容器传输文件</span><br><span class="line">docker cp a.txt containerid:/path</span><br><span class="line"></span><br><span class="line"># 宿主机：运行ubuntu:18.04容器</span><br><span class="line">docker run -it -d --name=lz-ubuntu -v /root/get-pip.py:/root/get-pip.py ubuntu:18.04</span><br><span class="line"></span><br><span class="line"># 宿主机：进入到容器</span><br><span class="line">docker exec -it lz-ubuntu bash</span><br><span class="line"></span><br><span class="line"># 容器内：可选-安装vim</span><br><span class="line">apt-get update</span><br><span class="line">apt-get install vim -y</span><br><span class="line"></span><br><span class="line"># 容器内：配置pip源，用以加速安装</span><br><span class="line">sudo mkdir ~/.pip</span><br><span class="line">sudo vim ~/.pip/pip.conf</span><br><span class="line"></span><br><span class="line">添加以下内容：</span><br><span class="line">[global]</span><br><span class="line">index-url=https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">[install]</span><br><span class="line">trusted-host=mirrors.aliyun.com</span><br><span class="line"></span><br><span class="line">国内源：</span><br><span class="line">清华：</span><br><span class="line">https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">阿里云：</span><br><span class="line">http://mirrors.aliyun.com/pypi/simple/</span><br><span class="line">中国科技大学 </span><br><span class="line">https://pypi.mirrors.ustc.edu.cn/simple/</span><br><span class="line">华中理工大学：</span><br><span class="line">http://pypi.hustunique.com/</span><br><span class="line">山东理工大学：</span><br><span class="line">http://pypi.sdutlinux.org/</span><br><span class="line">豆瓣：</span><br><span class="line">http://pypi.douban.com/simple/</span><br><span class="line"></span><br><span class="line"># 容器内：可选-配置apt源</span><br><span class="line">mv /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line">vim /etc/apt/sources.list</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line"></span><br><span class="line"># 容器内：更新软件包列表</span><br><span class="line">apt-get update</span><br><span class="line"></span><br><span class="line"># 容器内：可选-安装调试工具</span><br><span class="line">apt-get install iputils-ping net-tools curl</span><br><span class="line"></span><br><span class="line"># 容器内：安装最主要的python包</span><br><span class="line">apt-get install python3.7 python3.7-dev</span><br><span class="line"></span><br><span class="line"># 容器内：安装pip3.7</span><br><span class="line">apt install python3-distutils</span><br><span class="line">python3.7 get-pip.py</span><br><span class="line"></span><br><span class="line"># 容器内：安装pytorch</span><br><span class="line"># CUDA 10.1</span><br><span class="line">pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html</span><br><span class="line"># 安装其他python包</span><br><span class="line">pip install transformers==2.10.0</span><br><span class="line">pip install pytorch-crf==0.7.2</span><br><span class="line">pip install sklearn</span><br><span class="line">pip install seqeval==1.2.2</span><br><span class="line">pip install pandas</span><br><span class="line"></span><br><span class="line"># 时区设置</span><br><span class="line"># 宿主机：从宿主机中拷贝时区文件到容器内，/usr/share/zoneinfo/UCT这个文件是通过软链追溯到的，时区是亚洲/上海</span><br><span class="line">docker cp /usr/share/zoneinfo/UCT  lyz-ubuntu:/etc/</span><br><span class="line"># 容器内：然后在容器内将其改名为/etc/localtime</span><br><span class="line">mv /etc/UCT /etc/localtime</span><br><span class="line"></span><br><span class="line"># 容器内：清理无用的包</span><br><span class="line">apt-get clean</span><br><span class="line">apt-get autoclean</span><br><span class="line">du -sh /var/cache/apt/</span><br><span class="line">rm -rf /var/cache/apt/archives</span><br><span class="line"></span><br><span class="line"># 容器内：清理pip缓存</span><br><span class="line">rm -rf ~/.cache/pip</span><br><span class="line"></span><br><span class="line"># 容器内：清理命令日志</span><br><span class="line">history -c</span><br><span class="line"></span><br><span class="line"># 宿主机：打包镜像</span><br><span class="line">docker commit -a &#x27;提交人&#x27; -m &#x27;描述&#x27; &lt;容器名/ID&gt; &lt;镜像名称&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="三、nvidia-docker-运行"><a href="#三、nvidia-docker-运行" class="headerlink" title="三、nvidia-docker 运行"></a>三、nvidia-docker 运行</h1><blockquote>
<p>nvidia-docker2版本下nvidia-docker run与docker run —runtime=nvidia效果无太大差异</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># pytorch gpu 可运行</span><br><span class="line">docker run -itd --gpus all --name 容器名 -eNVIDIA_DRIVER_CAPABILITIES=compute,utility -e NVIDIA_VISIBLE_DEVICES=all 镜像名</span><br><span class="line"></span><br><span class="line">多出来的东西其实就是这个家伙：NVIDIA_DRIVER_CAPABILITIES=compute,utility</span><br><span class="line">　　</span><br><span class="line">也就是说，如果你不改这个环境变量，宿主机的nvidia driver在容器内是仅作为utility存在的，如果加上compute，宿主机的英伟达driver将对容器提供计算支持（所谓的计算支持也就是cuda支持）。</span><br><span class="line"></span><br><span class="line"># nvidia-docker2验证</span><br><span class="line">nvidia-docker --version</span><br><span class="line"></span><br><span class="line"># nvidia-docker2 测试</span><br><span class="line">docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi</span><br><span class="line"></span><br><span class="line"># nvidia-docker2启动</span><br><span class="line">启动nvidia-docker：</span><br><span class="line">      $: systemctl start nvidia-docker</span><br><span class="line"> 查看docker服务是否启动：</span><br><span class="line">      $: systemctl status nvidia-docker</span><br></pre></td></tr></table></figure>
<h2 id="1-docker-run-参数介绍"><a href="#1-docker-run-参数介绍" class="headerlink" title="1. docker run 参数介绍"></a>1. docker run 参数介绍</h2><p><strong>docker run 常用参数介绍：</strong></p>
<ul>
<li><p>—rm选项，这样在容器退出时就能够自动清理容器内部的文件系统。</p>
</li>
<li><p>—i选项，打开STDIN，用于控制台交互</p>
</li>
<li><p>—t选项，分配tty设备，该可以支持终端登录，默认为false</p>
</li>
<li><p>—d选项，指定容器运行于前台还是后台，默认为false</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">其他参数介绍：</span><br><span class="line">  -u, --user=&quot;&quot;              指定容器的用户  </span><br><span class="line">  -a, --attach=[]            登录容器（必须是以docker run -d启动的容器）</span><br><span class="line">  -w, --workdir=&quot;&quot;           指定容器的工作目录 </span><br><span class="line">  -c, --cpu-shares=0        设置容器CPU权重，在CPU共享场景使用  </span><br><span class="line">  -e, --env=[]               指定环境变量，容器中可以使用该环境变量  </span><br><span class="line">  -m, --memory=&quot;&quot;            指定容器的内存上限  </span><br><span class="line">  -P, --publish-all=false    指定容器暴露的端口  </span><br><span class="line">  -p, --publish=[]           指定容器暴露的端口 </span><br><span class="line">  -h, --hostname=&quot;&quot;          指定容器的主机名  </span><br><span class="line">  -v, --volume=[]            给容器挂载存储卷，挂载到容器的某个目录  </span><br><span class="line">  --volumes-from=[]          给容器挂载其他容器上的卷，挂载到容器的某个目录</span><br><span class="line">  --cap-add=[]               添加权限，权限清单详见：http://linux.die.net/man/7/capabilities  </span><br><span class="line">  --cap-drop=[]              删除权限，权限清单详见：http://linux.die.net/man/7/capabilities  </span><br><span class="line">  --cidfile=&quot;&quot;               运行容器后，在指定文件中写入容器PID值，一种典型的监控系统用法  </span><br><span class="line">  --cpuset=&quot;&quot;                设置容器可以使用哪些CPU，此参数可以用来容器独占CPU  </span><br><span class="line">  --device=[]                添加主机设备给容器，相当于设备直通  </span><br><span class="line">  --dns=[]                   指定容器的dns服务器  </span><br><span class="line">  --dns-search=[]            指定容器的dns搜索域名，写入到容器的/etc/resolv.conf文件  </span><br><span class="line">  --entrypoint=&quot;&quot;            覆盖image的入口点  </span><br><span class="line">  --env-file=[]              指定环境变量文件，文件格式为每行一个环境变量  </span><br><span class="line">  --expose=[]                指定容器暴露的端口，即修改镜像的暴露端口  </span><br><span class="line">  --link=[]                  指定容器间的关联，使用其他容器的IP、env等信息  </span><br><span class="line">  --lxc-conf=[]              指定容器的配置文件，只有在指定--exec-driver=lxc时使用  </span><br><span class="line">  --name=&quot;&quot;                  指定容器名字，后续可以通过名字进行容器管理，links特性需要使用名字  </span><br><span class="line">  --net=&quot;bridge&quot;             容器网络设置:</span><br><span class="line">				                bridge 使用docker daemon指定的网桥     </span><br><span class="line">				                host 	//容器使用主机的网络  </span><br><span class="line">				                container:NAME_or_ID  &gt;//使用其他容器的网路，共享IP和PORT等网络资源  </span><br><span class="line">				                none 容器使用自己的网络（类似--net=bridge），但是不进行配置 </span><br><span class="line">  --privileged=false         指定容器是否为特权容器，特权容器拥有所有的capabilities  </span><br><span class="line">  --restart=&quot;no&quot;             指定容器停止后的重启策略:</span><br><span class="line">				                no：容器退出时不重启  </span><br><span class="line">				                on-failure：容器故障退出（返回值非零）时重启 </span><br><span class="line">				                always：容器退出时总是重启  </span><br><span class="line">  --rm=false                 指定容器停止后自动删除容器(不支持以docker run -d启动的容器)  </span><br><span class="line">  --sig-proxy=true           设置由代理接受并处理信号，但是SIGCHLD、SIGSTOP和SIGKILL不能被代理  </span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<h2 id="2-docker-常用命令"><a href="#2-docker-常用命令" class="headerlink" title="2. docker 常用命令"></a>2. docker 常用命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> ### 显示版本信息 (与python, nvcc相比少了两个‘--’）</span><br><span class="line">$ docker version</span><br><span class="line"></span><br><span class="line">### 了解当前Docker的使用状态（当前容器，镜像数目信息，存储空间占用信息，</span><br><span class="line"># OS内核版本， 发行版本， 硬件资源等）</span><br><span class="line">$ docker info</span><br><span class="line"></span><br><span class="line">### 拉去一个镜像 ( xxxx 表示某个镜像名字，）</span><br><span class="line">$ docker pull xxxx</span><br><span class="line"># e.g.</span><br><span class="line"># docker pull ubuntu</span><br><span class="line"></span><br><span class="line">### 查看系统中已有的镜像(images要带‘s&#x27;)</span><br><span class="line">$ docker images</span><br><span class="line"># e.g.:</span><br><span class="line"># REPOSITORY  TAG    IMAGES ID   CREATED VIRTUAL SIZE</span><br><span class="line"># ubuntu      latest 4ef6axxxxx   5 day ago  84.0M</span><br><span class="line"></span><br><span class="line">### 从镜像创建docker容器</span><br><span class="line">$ docker run -i -t ubuntu /bin/bash </span><br><span class="line"># or</span><br><span class="line">$ docker run -it 4ef /bin/bash</span><br><span class="line"># 其中 -i, 交互模式，让输入输出都在标准控制台进行；-d，则进入后台</span><br><span class="line"># -t, 为新创建的容器分配一个伪终端</span><br><span class="line"># ubuntu, 用于创建容器的镜像名，可用ID来代替（前3位足够）</span><br><span class="line"># /bin/bash， 在新建容器中运行的命令，可以为任意Linux命令</span><br><span class="line"></span><br><span class="line">### 离开当前容器,返回宿主机终端，使用组合键 &quot;Ctrl+P&quot; 和 &quot;Ctrl+Q&quot;</span><br><span class="line"></span><br><span class="line">### 查看当前活动的容器</span><br><span class="line">$ docker ps</span><br><span class="line"># CONTAINER ID  IMAGE  COMMAND  CREATED   STATUS   PORTS NAME</span><br><span class="line"># 610xxxx  ubuntu:latest  &quot;/bin/bash&quot; 1 minute ago Up 1 minute ago prickly_wilson</span><br><span class="line"></span><br><span class="line">### 宿主机终端与某个容器建立连接</span><br><span class="line">$ docker attach 610</span><br><span class="line"></span><br><span class="line">### 从容器创建Docker镜像</span><br><span class="line">$ docker commit -m &quot;hhahaha&quot; 610 ubuntu:hhh</span><br><span class="line"># -m, 新镜像说明</span><br><span class="line"># 610， 某个容器的ID</span><br><span class="line"># ubuntu:hhh， 命名最好不要这么随意</span><br><span class="line"># 那么接下来可以查看新生成的镜像，命令 docker images</span><br><span class="line"></span><br><span class="line">### 基于新的镜像创建一个新的容器(一样的)</span><br><span class="line">$ docker run -it ubuntu:hhh /bin/bash</span><br><span class="line"></span><br><span class="line">### 给镜像重命名(方便记忆)</span><br><span class="line">$ docker tag IMAGEID(image id) REPOSITORY:TAG</span><br><span class="line"></span><br><span class="line">### 给容器重命名</span><br><span class="line">$ docker rename old-container-name new-container-name</span><br></pre></td></tr></table></figure>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://blog.csdn.net/qq_39698985/article/details/109524762">https://blog.csdn.net/qq_39698985/article/details/109524762</a></p>
<p><a href="https://blog.csdn.net/mumoDM/article/details/82503022">https://blog.csdn.net/mumoDM/article/details/82503022</a></p>
<p><a href="https://blog.csdn.net/qq_29518275/article/details/107486028">https://blog.csdn.net/qq_29518275/article/details/107486028</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>Nvidia-Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习基础-神经网络权重初始化</title>
    <url>/2021/09/29/2021-09-29-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
    <content><![CDATA[<p>本文将介绍神经网络权重初始化的原理与实现。首先解决了两个问题：1. 全零初始化是否可以、2. 参数全部相同初始化是否可以</p>
<p>然后介绍了初始化的方式：</p>
<ol>
<li>预训练初始化</li>
<li>随机初始化</li>
<li>固定初始化</li>
</ol>
<span id="more"></span>
<h1 id="一、两个问题"><a href="#一、两个问题" class="headerlink" title="一、两个问题"></a>一、两个问题</h1><blockquote>
<p>假设3层神经网络,输入节点v0，第一层节点v1,v2,v3 第二层节点v4,v5 第三层节点v6。其中vi=f(ai),i=4,5,6 f为激活函数。</p>
</blockquote>
<p><strong>前向传播：</strong></p>
<p><img src="https://img-blog.csdnimg.cn/83cc1293388e439ab8a1d33e6c67396f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBASk1YR09ETFo=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>
<h2 id="1-全零初始化是否可以"><a href="#1-全零初始化是否可以" class="headerlink" title="1. 全零初始化是否可以"></a>1. 全零初始化是否可以</h2><p>一般情况不可以。当全零参数初始化时，除输入节点所有节点值均为0,根据上式除第一层梯度与输入值有关其余均为0.</p>
<p>LR等一层网络可以全零初始化， 网络梯度与输入值有关。仅全零初始化一层也不影响训练，但涉及两层及以上，从涉及层到输入层的梯度都为0,参数无法更新。</p>
<h2 id="2-参数全部相同初始化是否可以"><a href="#2-参数全部相同初始化是否可以" class="headerlink" title="2. 参数全部相同初始化是否可以"></a>2. 参数全部相同初始化是否可以</h2><p>不可以。若初始化为相同的参数，隐藏层所有节点输出相同，梯度也是相同的。相当于输入经过一个节点。</p>
<h1 id="二、参数初始化方式"><a href="#二、参数初始化方式" class="headerlink" title="二、参数初始化方式"></a>二、参数初始化方式</h1><h2 id="1-预训练初始化"><a href="#1-预训练初始化" class="headerlink" title="1. 预训练初始化"></a>1. 预训练初始化</h2><p><strong>pretraining + finetuning</strong>加载已训练好的模型参数，进行下游任务的模型训练。</p>
<h2 id="2-随机初始化"><a href="#2-随机初始化" class="headerlink" title="2. 随机初始化"></a>2. 随机初始化</h2><h3 id="2-1-random-initialization"><a href="#2-1-random-initialization" class="headerlink" title="2.1 random initialization"></a>2.1 random initialization</h3><p><code>random initialization: np.random.randn(m,n)</code></p>
<p>随机产生符合正态分布的m×n维向量</p>
<p>弊端：随机会产生梯度消失，随着网络层次加深，由于链式求导法则，输出越来越接近0</p>
<h3 id="2-2-Xavier-initialization"><a href="#2-2-Xavier-initialization" class="headerlink" title="2.2 Xavier initialization"></a>2.2 Xavier initialization</h3><p><code>tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in)</code></p>
<p><img src="https://pic3.zhimg.com/80/v2-6302a7093b93e1376e54e95033c58086_720w.jpg" alt="image"></p>
<p>M 代表着输入输出维度即node_in,node_out</p>
<p>保证输入输出方差一致</p>
<h3 id="2-3-He-initialization"><a href="#2-3-He-initialization" class="headerlink" title="2.3 He initialization"></a>2.3 He initialization</h3><p><code>tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in/2)</code></p>
<p>适用于RELU激活函数，只有半区有效</p>
<h2 id="3-固定初始化"><a href="#3-固定初始化" class="headerlink" title="3. 固定初始化"></a>3. 固定初始化</h2><p>比如对于偏置（bias）通常用0初始化，LSTM遗忘门偏置通常为1或2，使时序上的梯度变大，对于ReLU神经元，偏置设为0.01，使得训练初期更容易激活。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://note.youdao.com/">https://www.leiphone.com/category/ai/3qMp45aQtbxTdzmK.html</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>权重初始化</tag>
      </tags>
  </entry>
  <entry>
    <title>2022预训练的下一步是什么</title>
    <url>/2021/12/31/2021-12-31-2022%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E4%B8%8B%E4%B8%80%E6%AD%A5%E6%98%AF%E4%BB%80%E4%B9%88/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文内容为自己对2021年自身算法经历的回顾，同时展望了未来研究的方向。如有理解不对的地方，欢迎指正批评。</p>
<p>2021年研究热点</p>
<ul>
<li>大规模预训练</li>
<li>对比学习</li>
<li>prompt</li>
</ul>
<p>展望未来</p>
<p>回顾自身算法经历</p>
<ol>
<li>需求分析</li>
<li>模型选型及设计</li>
<li>数据分析</li>
<li>模型训练及优化</li>
<li>分析负例<ol>
<li>检查数据质量是否过差</li>
<li>根据指标进行分析<ul>
<li>recall低</li>
<li>precision低</li>
</ul>
</li>
</ol>
</li>
</ol>
<span id="more"></span>
<blockquote>
<p>该内容为自己对2021年自身算法经历的回顾，同时展望了未来研究的方向。如有理解不对的地方，欢迎指针批评。</p>
</blockquote>
<h1 id="2021年研究热点"><a href="#2021年研究热点" class="headerlink" title="2021年研究热点"></a>2021年研究热点</h1><h2 id="大规模预训练"><a href="#大规模预训练" class="headerlink" title="大规模预训练"></a>大规模预训练</h2><p>预训练+微调的做法，在多个下游领域取得优异的表现。而在过去的一年里，预训练模型更是在往<strong>大而深</strong>的方向发展。</p>
<p>目前，国内已有智源研究院、鹏城实验室、中科院自动化所、阿里、百度、华为、浪潮等科研院所和企业研相继发出“悟道”、“盘古”、“紫东 · 太初”、M6、PLUG、ERNIE 3.0 等大模型。</p>
<p>但是模型在往大而深方向发展的同时，也存在如下亟待解决的问题：</p>
<ul>
<li>如何解释预训练模型的理论基础（如大模型智能的参数规模极限存在吗）</li>
<li>如何将大模型高效、低成本的应用于实际系统</li>
<li>如何克服构建大模型的数据质量、训练效率、算力消耗、模型交付等诸多障碍</li>
<li>如何解决目前大部分大模型普遍缺乏认知能力的问题</li>
</ul>
<h2 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h2><p><strong>对比学习的出发点在于避免模型坍塌，理想的模型应该符合alignment和uniformity，即语义相近的句子彼此聚集，语义无关的句子均匀分布。</strong></p>
<p>如果仅仅通过数据增强构建正例，随机句子作为负例，并为其打上0，1标签，存在以下问题：</p>
<ul>
<li>数据增强生成正例的变化有限</li>
<li>随机搭配成负例，含有除正例组合外其他组合全部为0的诱导</li>
<li>0，1标签的赋予太过绝对，对相似性表述不够准确</li>
</ul>
<p>因此对比学习的核心思想转变为：</p>
<script type="math/tex; mode=display">score(X,X^{'}) >> score(X,Y)</script><p>其中，X代表原样本，$X^{‘c}$代表数据增强的正样本，Y代表随机选择的负样本。</p>
<p>根据该思想，对比学习采用InfoNCE损失函数：</p>
<script type="math/tex; mode=display">loss = -log \frac{exp(score(X,X^{'}))}{score(X,X^{'})+\sum_{i=1}^{N}score(X,Y_i)}</script><p>通过该损失函数实现正例拉近，负例推远的效果。</p>
<h2 id="prompt"><a href="#prompt" class="headerlink" title="prompt"></a>prompt</h2><p>prompt被誉为NLP领域的新范式，与预训练+微调的范式相比，其过程分为：”pre-train, prompt, and predict”。</p>
<p><strong>prompt的出发点在于以更轻量化的方式利用预训练模型，避免微调与预训练之间存在的差异。</strong></p>
<p>prompt通过构建模版的方式，将下游任务转为与预训练相似的MLM任务，以该方式充分发挥预训练模型的性能。</p>
<p>以文本情感分类任务中，”I love this movie.”句子为例，prompt按照以下方式进行处理：</p>
<ol>
<li>生成prompt句子</li>
</ol>
<p>该步骤完成输入句子到模型输入的映射：</p>
<script type="math/tex; mode=display">x^{'}=f_{prompt}(x)</script><p>其中，$x^{‘}$为生成的prompt句子，x为输入句子，$f_{prompt}$为prompt函数。</p>
<p>在本例中，使用的模版为： “ [X] Overall, it was a [Z] movie.”</p>
<p>因此，得到的，$x^{‘}$为”I love this movie. Overall it was a [Z] movie.”</p>
<ol>
<li>模型预测</li>
</ol>
<p>该步骤将$x^{‘}$输入模型，模型完成模版空白位置的词语预测。</p>
<p>在本例中，模型可能预测为：”excellent”, “great”, “wonderful” 等词语</p>
<ol>
<li>结果映射</li>
</ol>
<p>通常模型预测的词语与任务输出存在一定差距，因此我们需要完成词语到输出结果的映射。</p>
<script type="math/tex; mode=display">y = f(x^{'})</script><p>在本例中，”excellent”, “great”, “wonderful” 等词语映射为标签 “++”</p>
<h1 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a>展望未来</h1><p>首先我认为当前基于数据驱动方法存在如下的问题：</p>
<ol>
<li>长尾效应：自然界中的数据分布就是长尾的，在学习的过程中，模型容易发生过拟合，泛化性较差。</li>
<li>数据噪声：有标签的数据，在标注过程中就不可避免的存在噪声。尤其是多位人员一起标注时，不同标注人员根据自身的理解完成数据的标注，但不同的人自身理解存在偏差，因此标注结果极易存在误差。归根到底：标注的规范难以确定，无法统一大家的知识库。</li>
</ol>
<p>当前我遇到的一些问题分享：模型仍无法很好地处理下述问题：</p>
<blockquote>
<p>太阳有几只眼睛？</p>
<p>姚明与奥尼尔身高谁比较高？</p>
<p>猫咪可以吃生蛋黄吗？猫咪是可以吃蛋黄的。这里特定煮熟的白水蛋，猫咪不能吃生鸡蛋，因为生鸡蛋中有细菌。</p>
<p>物质都是由分子构成的吗？物质都是由分子构成的，分子又由原子构成-错的！因为有些物质是不含分子的。</p>
</blockquote>
<p>这些问题，我总结为两方面的困难：</p>
<ol>
<li>缺乏知识，由于预训练与微调领域存在偏差，模型在下游任务中缺乏特定知识，同时模型在一些常识问题上表现较差。</li>
<li>缺乏深度语义的理解，模型表现的更像通过字面匹配完成任务，推理的成分更弱。</li>
</ol>
<p>当前研究热点仍然在于挖掘预训练模型的能力，但在基于常识性知识与逻辑推理的问题上，这种基于数据驱动的方式从底层就存在问题。引用一下大咖们对2022年的展望。</p>
<blockquote>
<p>大模型一方面在不少问题上取得了以往难以预期的成功，另一方面其巨大的训练能耗和碳排放是不能忽视的问题。个人以为，大模型未来会在一些事关国计民生的重大任务上发挥作用，而在其他一些场景下或许会通过类似集成学习的手段来利用小模型，尤其是通过很少量训练来 “复用” 和集成已有的小模型来达到不错的性能。</p>
<p>我们提出了一个叫做 “学件” 的思路，目前在做一些这方面的探索。大致思想是，假设很多人已经做了模型并且乐意放到某个市场去共享，市场通过建立规约来组织和管理学件，以后的人再做新应用时，就可以不用从头收集数据训练模型，可以先利用规约去市场里找找看是否有比较接近需求的模型，然后拿回家用自己的数据稍微打磨就能用。这其中还有一些技术挑战需要解决，我们正在研究这个方向。</p>
<p>另一方面，有可能通过利用人类的常识和专业领域知识，使模型得以精简，这就要结合逻辑推理和机器学习。逻辑推理比较善于利用人类知识，机器学习比较善于利用数据事实，如何对两者进行有机结合一直是人工智能中的重大挑战问题。麻烦的是逻辑推理是严密的基于数理逻辑的 “从一般到特殊”的演绎过程，机器学习是不那么严密的概率近似正确的 “从特殊到一般”的归纳过程，在方法论上就非常不一样。已经有的探索大体上是以其中某一方为倚重，引入另一方的某些成分，我们最近在探索双方相对均衡互促利用的方式。</p>
</blockquote>
<p>谈谈自己的理解，<strong>预训练模型的方式归根到底仍然属于数据驱动的任务，其通过在大规模数据上学习，推断未知数据的概率。如果说数据中存在表述不准确、表述有歧义或者词汇本身就有多个含义的话，以概率的方式难以解决这些问题。</strong> 而人脑在未知问题上，推理成分居多，以一词多义为例，人类会考虑该词汇有几种用法，考虑在这种上下文语境下使用哪一种用法，所以是否可以建立一套类似于标准公理的语言规范，以该规范为基础，对未知句子进行拆解推理，理解句子的完整含义。通过了解模型的推理过程，模型的可解释性增强。当预测错误时，我们可以进行溯源分析，对模型依赖的知识进行调整，或者让模型学习的更充分。</p>
<p>接下来对自己2022年的期望：</p>
<ol>
<li>自身学习更多模型结构变化的同时，更多地理解业务的架构，明白模型在业务中起的作用。</li>
<li>在算法研究上能够研究的更加深入，希望能够找到解决上述困难的方法。</li>
</ol>
<h1 id="回顾自身算法经历"><a href="#回顾自身算法经历" class="headerlink" title="回顾自身算法经历"></a>回顾自身算法经历</h1><p>2021年自身的算法经历主要分为：实习、算法比赛、项目、论文四部分。在这些经历里面主要接触分类、阅读理解、信息抽取三种任务，评估方式均采用精确率、召回率及F1值。下面将以这些经历为基础，介绍我处理这些任务的方式。</p>
<h2 id="1-需求分析"><a href="#1-需求分析" class="headerlink" title="1. 需求分析"></a>1. 需求分析</h2><p>开展算法工作之前，首先要搞清楚算法需要满足什么样的需求。包括：</p>
<ul>
<li>业务属于什么样的任务</li>
<li>算法需要侧重的方向</li>
<li>训练数据及线上数据的情况</li>
<li>线上的指标</li>
<li>线下的评估方式</li>
<li>……</li>
</ul>
<p><strong>需求分析的目的在于了解业务的需求与算法在业务中起到的作用。</strong></p>
<h2 id="2-模型选型及设计"><a href="#2-模型选型及设计" class="headerlink" title="2. 模型选型及设计"></a>2. 模型选型及设计</h2><p>在明白需求之后，需要根据任务类型选择模型，并根据需求的不同，对模型结构进行调整。如阅读理解任务下：针对多答案、无答案的情况，我们需要调整模型的结构。</p>
<p><strong>模型选型及设计的目的在于选择或设计能够很好地满足业务需求的模型。</strong></p>
<h2 id="3-数据分析"><a href="#3-数据分析" class="headerlink" title="3. 数据分析"></a>3. 数据分析</h2><p><strong>数据分析这一步是最重要的一步，当前模型主要还是以数据驱动，数据对模型的影响很大。</strong> </p>
<p>我主要从以下角度进行分析：</p>
<ul>
<li>数据是否存在噪声：标点、大小写、特殊符号等</li>
<li>训练集测试集分布是否存在差异，测试集能否反映模型在具体业务下的表现</li>
<li>数据存在哪些特征，通过引入额外的特征，模型可以表现地更好</li>
<li>训练集分布：标签分布、长度分布等，是否会给模型带来类别不均衡、长文本等问题</li>
<li>数据量大小，数据量足够时可以继续预训练</li>
</ul>
<p><strong>数据分析的目的在于数据能否充分发挥模型性能，能否得到符合业务需求的模型</strong></p>
<h2 id="4-模型训练及优化"><a href="#4-模型训练及优化" class="headerlink" title="4. 模型训练及优化"></a>4. 模型训练及优化</h2><p><strong>模型进行训练，开始炼丹【调参】。</strong></p>
<ul>
<li>设置合适的超参数【可以通过一些超参数搜索算法】</li>
<li>选择合适的优化器【adam/adamw/sgd】</li>
<li>学习率调整的策略</li>
</ul>
<p><strong>进阶版：</strong></p>
<ul>
<li>对抗训练</li>
<li>对比学习</li>
<li>UDA等数据增强方式</li>
<li>继续预训练</li>
<li>多任务学习</li>
<li>伪标签</li>
<li>SWA</li>
<li>……</li>
</ul>
<h2 id="5-分析负例"><a href="#5-分析负例" class="headerlink" title="5. 分析负例"></a>5. 分析负例</h2><p>该过程同样重要，我们需要了解模型在测试数据上的表现情况，在什么数据表现较差，如何优化这些负例。</p>
<p><strong>在优化过程中，建议记录每一次优化信息，分析模型的提升/降低是否符合自己预期，充分利用每一次实验</strong></p>
<p>下面总结了我在优化过程常用的分析方式：</p>
<h3 id="1-检查数据质量是否过差"><a href="#1-检查数据质量是否过差" class="headerlink" title="1. 检查数据质量是否过差"></a>1. 检查数据质量是否过差</h3><p>这种情况通常表现为数据质量较差，模型在原始数据上表现不佳，精确率与召回率都很低。针对这种情况，需要对数据做必要的预处理，让模型能够更好地学习。</p>
<h3 id="2-根据指标进行分析"><a href="#2-根据指标进行分析" class="headerlink" title="2. 根据指标进行分析"></a>2. 根据指标进行分析</h3><h4 id="recall低"><a href="#recall低" class="headerlink" title="recall低"></a>recall低</h4><p>召回率表示召回的数量，测试集数据未召回较多，则从下列角度检查数据：</p>
<ol>
<li>训练集测试集数据差异是否较大，即训练集中是否存在类似数据，若不存在则引入更多数据或者对该数据进行数据增强。<strong>这种情况，常见原因为数据分布不均衡-少数数据训练不充分；训练集、测试集分布差异较大导致</strong></li>
<li>训练集中存在类似数据，检查训练集中该种情况有无标注错误：漏标、错标。</li>
</ol>
<h4 id="precision低"><a href="#precision低" class="headerlink" title="precision低"></a>precision低</h4><p>精确率表示预测出的准确率，测试集数据分错的较多：</p>
<ol>
<li>检查数据分布，是否数据分布不均衡。<strong>数据不均衡导致模型倾向于预测数量较多的数据，精确率下降</strong></li>
<li>标签定义是否准确，是否存在两类标签混淆的情况。<strong>这种情况，需要考虑对标签进行融合</strong></li>
</ol>
<p>类别不均衡常用解决方式：</p>
<ul>
<li>数据增强</li>
<li>resample</li>
<li>reweight</li>
<li>集成学习</li>
</ul>
<p>数据错误常用解决方式：</p>
<ul>
<li>交叉验证</li>
<li>置信学习</li>
<li>聚类分析</li>
</ul>
<p>接下来的过程则是迭代分析，直到模型性能符合业务需求。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://mp.weixin.qq.com/s/RqkQzeR5BOVpU7tj_zUgqQ">https://mp.weixin.qq.com/s/RqkQzeR5BOVpU7tj_zUgqQ</a></p>
<p><a href="https://www.zhihu.com/question/480187938/answer/2103245373">https://www.zhihu.com/question/480187938/answer/2103245373</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/399295895">https://zhuanlan.zhihu.com/p/399295895</a></p>
]]></content>
      <categories>
        <category>年度总结</category>
        <category>2021</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>年度总结</tag>
        <tag>NLP</tag>
        <tag>预训练</tag>
        <tag>对比学习</tag>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>Python数据分析-数据可视化(二)</title>
    <url>/2022/01/09/2022-01-09-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96(%E4%BA%8C)/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>看到有些论文插图十分简洁美观，于是便摸索一下如何美化一下折线图绘图。本文将在前文<a href="https://jmxgodlz.xyz/2020/01/03/2020-01-03-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/#more">Python数据分析-数据可视化</a>的基础上，介绍折线图格式的调整。</p>
<p>本文使用的画图工具为matplotlib，相关API可访问<a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html">python matplotlib文档</a>。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gy7pemcxsnj30m80ecdh1.jpg" alt=""></p>
<span id="more"></span>
<h1 id="Matplotlib-折线图格式调整"><a href="#Matplotlib-折线图格式调整" class="headerlink" title="Matplotlib 折线图格式调整"></a>Matplotlib 折线图格式调整</h1><p>首先，贴一下文档中折线图绘制的附加参数表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>agg_filter</td>
<td>a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array</td>
</tr>
<tr>
<td>alpha</td>
<td>scalar or None</td>
</tr>
<tr>
<td>animated</td>
<td>bool</td>
</tr>
<tr>
<td>antialiased or aa</td>
<td>bool</td>
</tr>
<tr>
<td>clip_box</td>
<td>Bbox</td>
</tr>
<tr>
<td>clip_on</td>
<td>bool</td>
</tr>
<tr>
<td>clip_path</td>
<td>Patch or (Path, Transform) or None</td>
</tr>
<tr>
<td>color or c</td>
<td>color</td>
</tr>
<tr>
<td>dash_capstyle</td>
<td>CapStyle or {‘butt’, ‘projecting’, ‘round’}</td>
</tr>
<tr>
<td>dash_joinstyle</td>
<td>JoinStyle or {‘miter’, ‘round’, ‘bevel’}</td>
</tr>
<tr>
<td>dashes</td>
<td>sequence of floats (on/off ink in points) or (None, None)</td>
</tr>
<tr>
<td>data</td>
<td>(2, N) array or two 1D arrays</td>
</tr>
<tr>
<td>drawstyle or ds</td>
<td>{‘default’, ‘steps’, ‘steps-pre’, ‘steps-mid’, ‘steps-post’}, default: ‘default’</td>
</tr>
<tr>
<td>figure</td>
<td>Figure</td>
</tr>
<tr>
<td>fillstyle</td>
<td>{‘full’, ‘left’, ‘right’, ‘bottom’, ‘top’, ‘none’}</td>
</tr>
<tr>
<td>gid</td>
<td>str</td>
</tr>
<tr>
<td>in_layout</td>
<td>bool</td>
</tr>
<tr>
<td>label</td>
<td>object</td>
</tr>
<tr>
<td>linestyle or ls</td>
<td>{‘-‘, ‘—‘, ‘-.’, ‘:’, ‘’, (offset, on-off-seq), …}</td>
</tr>
<tr>
<td>linewidth or lw</td>
<td>float</td>
</tr>
<tr>
<td>marker</td>
<td>marker style string, Path or MarkerStyle</td>
</tr>
<tr>
<td>markeredgecolor or mec</td>
<td>color</td>
</tr>
<tr>
<td>markeredgewidth or mew</td>
<td>float</td>
</tr>
<tr>
<td>markerfacecolor or mfc</td>
<td>color</td>
</tr>
<tr>
<td>markerfacecoloralt or mfcalt</td>
<td>color</td>
</tr>
<tr>
<td>markersize or ms</td>
<td>float</td>
</tr>
<tr>
<td>markevery</td>
<td>None or int or (int, int) or slice or list[int] or float or (float, float) or list[bool]</td>
</tr>
<tr>
<td>path_effects</td>
<td>AbstractPathEffect</td>
</tr>
<tr>
<td>picker</td>
<td>float or callable[[Artist, Event], tuple[bool, dict]]</td>
</tr>
<tr>
<td>pickradius</td>
<td>float</td>
</tr>
<tr>
<td>rasterized</td>
<td>bool</td>
</tr>
<tr>
<td>sketch_params</td>
<td>(scale: float, length: float, randomness: float)</td>
</tr>
<tr>
<td>snap</td>
<td>bool or None</td>
</tr>
<tr>
<td>solid_capstyle</td>
<td>CapStyle or {‘butt’, ‘projecting’, ‘round’}</td>
</tr>
<tr>
<td>solid_joinstyle</td>
<td>JoinStyle or {‘miter’, ‘round’, ‘bevel’}</td>
</tr>
<tr>
<td>transform</td>
<td>unknown</td>
</tr>
<tr>
<td>url</td>
<td>str</td>
</tr>
<tr>
<td>visible</td>
<td>bool</td>
</tr>
<tr>
<td>xdata</td>
<td>1D array</td>
</tr>
<tr>
<td>ydata</td>
<td>1D array</td>
</tr>
<tr>
<td>zorder</td>
<td>float</td>
</tr>
</tbody>
</table>
</div>
<p><strong>接下来，我将挑选几个常用的附加参数介绍使用方式与效果。</strong></p>
<h2 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h2><ol>
<li>附加参数名：label</li>
<li>功能：为绘制曲线命名，该名称会在图例显示</li>
<li>使用方式：plt.plot(x,y,label=’example’)</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gy79jv3dhtj30hs0dc3yn.jpg" alt=""></p>
<h2 id="线条颜色"><a href="#线条颜色" class="headerlink" title="线条颜色"></a>线条颜色</h2><ol>
<li>附加参数名：color</li>
<li>功能：选择绘制线条的颜色</li>
<li>使用方式：plt.plot(x,y,color=’r’)</li>
<li>颜色选取方式分为三种：</li>
</ol>
<ul>
<li>用全名或简称 ，如blue或b</li>
<li>16进制 ，如FF00FF</li>
<li>(r, g, b) 或 (r, g, b, a)，如（1,0,1,1） ，其中 r g b a 取均为[0, 1]之间，[0, 1]之间的浮点数的字符串形式，表示灰度值。0表示黑色，1表示白色</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gy7m0t136fj30hs0dc74x.jpg" alt=""></p>
<h2 id="线条形状"><a href="#线条形状" class="headerlink" title="线条形状"></a>线条形状</h2><ol>
<li>附加参数名：linestyle(或ls)</li>
<li>功能：选择绘制线条的形状</li>
<li>使用方式：plt.plot(x,y,linestyle=’:’)或者plt.plot(x,y,ls=’:’)</li>
<li>常用形状：</li>
</ol>
<ul>
<li>-      实线(solid)</li>
<li>—     短线(dashed)</li>
<li>-.     短点相间线(dashdot)</li>
<li>：    虚点线(dotted)</li>
<li>‘’, ‘ ‘, None</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;, linestyle=&#x27;-&#x27;)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;, ls=&#x27;--&#x27;)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;, ls=&#x27;:&#x27;)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6), ls=&#x27;&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gy7m774jovj30hs0dct9g.jpg" alt=""></p>
<h2 id="折点样式"><a href="#折点样式" class="headerlink" title="折点样式"></a>折点样式</h2><ol>
<li>附加参数名：<br>(1)marker — 折点形状</li>
</ol>
<p>(2)markeredgecolor 或 mec — 折点外边颜色</p>
<p>(3)markeredgewidth 或 mew — 折点线宽</p>
<p>(4)markerfacecolor 或 mfc —折点实心颜色</p>
<p>(5)markerfacecoloralt 或 mfcalt</p>
<p>(6)markersize 或 ms —折点大小</p>
<p>折点形状选择如下表:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>character</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>&#39;-&#39;</code></td>
<td>solid line style</td>
</tr>
<tr>
<td><code>&#39;--&#39;</code></td>
<td>dashed line style</td>
</tr>
<tr>
<td><code>&#39;-.&#39;</code></td>
<td>dash-dot line style</td>
</tr>
<tr>
<td><code>&#39;:&#39;</code></td>
<td>dotted line style</td>
</tr>
<tr>
<td><code>&#39;.&#39;</code></td>
<td>point marker</td>
</tr>
<tr>
<td><code>&#39;,&#39;</code></td>
<td>pixel marker</td>
</tr>
<tr>
<td><code>&#39;o&#39;</code></td>
<td>circle marker</td>
</tr>
<tr>
<td><code>&#39;v&#39;</code></td>
<td>triangle_down marker</td>
</tr>
<tr>
<td><code>&#39;^&#39;</code></td>
<td>triangle_up marker</td>
</tr>
<tr>
<td><code>&#39;&lt;&#39;</code></td>
<td>triangle_left marker</td>
</tr>
<tr>
<td><code>&#39;&gt;&#39;</code></td>
<td>triangle_right marker</td>
</tr>
<tr>
<td><code>&#39;1&#39;</code></td>
<td>tri_down marker</td>
</tr>
<tr>
<td><code>&#39;2&#39;</code></td>
<td>tri_up marker</td>
</tr>
<tr>
<td><code>&#39;3&#39;</code></td>
<td>tri_left marker</td>
</tr>
<tr>
<td><code>&#39;4&#39;</code></td>
<td>tri_right marker</td>
</tr>
<tr>
<td><code>&#39;s&#39;</code></td>
<td>square marker</td>
</tr>
<tr>
<td><code>&#39;p&#39;</code></td>
<td>pentagon marker</td>
</tr>
<tr>
<td><code>&#39;*&#39;</code></td>
<td>star marker</td>
</tr>
<tr>
<td><code>&#39;h&#39;</code></td>
<td>hexagon1 marker</td>
</tr>
<tr>
<td><code>&#39;H&#39;</code></td>
<td>hexagon2 marker</td>
</tr>
<tr>
<td><code>&#39;+&#39;</code></td>
<td>plus marker</td>
</tr>
<tr>
<td><code>&#39;x&#39;</code></td>
<td>x marker</td>
</tr>
<tr>
<td><code>&#39;D&#39;</code></td>
<td>diamond marker</td>
</tr>
<tr>
<td><code>&#39;d&#39;</code></td>
<td>thin_diamond marker</td>
</tr>
<tr>
<td>``’</td>
<td>‘``</td>
<td>vline marker</td>
</tr>
<tr>
<td><code>&#39;_&#39;</code></td>
<td>hline marker</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;, linestyle=&#x27;-&#x27;, marker=&#x27;o&#x27;)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;, ls=&#x27;--&#x27;, marker=&#x27;1&#x27;)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;, ls=&#x27;:&#x27;, marker=&#x27;2&#x27;)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6), marker=&#x27;3&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gy7mdyz2wcj30hs0dc3z9.jpg" alt=""></p>
<h2 id="线条透明度"><a href="#线条透明度" class="headerlink" title="线条透明度"></a>线条透明度</h2><ol>
<li>附加参数名：alpha,值在[0,1]之间</li>
<li>功能：选择绘制线条的透明度</li>
<li>使用方式：plt.plot(x,y,alpha=’0.9’)</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">x = range(10)</span><br><span class="line">y = [random.random() for _ in range(10)]</span><br><span class="line">y2 = [random.random() for _ in range(10)]</span><br><span class="line">y3 = [random.random() for _ in range(10)]</span><br><span class="line">y4 = [random.random() for _ in range(10)]</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, y, label=&#x27;example1&#x27;, color=&#x27;blue&#x27;, linestyle=&#x27;-&#x27;, alpha=0.3)</span><br><span class="line">plt.plot(x, y2, label=&#x27;example2&#x27;, color=&#x27;r&#x27;, ls=&#x27;--&#x27;, alpha=0.1)</span><br><span class="line">plt.plot(x, y3, label=&#x27;example3&#x27;, color=&#x27;#00FFFF&#x27;, ls=&#x27;:&#x27;, alpha=0.5)</span><br><span class="line">plt.plot(x, y4, label=&#x27;example4&#x27;, color=(0.4, 0.5, 0.6), ls=&#x27;&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gy7m9a8dwij30hs0dcq38.jpg" alt=""></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>数据分析</category>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>解析NLP竞赛中的提分点-对抗训练</title>
    <url>/2022/01/20/2022-01-20-%E8%A7%A3%E6%9E%90NLP%E4%B8%AD%E7%9A%84%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在NLP比赛中，对抗训练是常见的提分手段。本文将详细介绍对抗训练的场景、作用、类型、具体实现以及未来的展望。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gyk1712imxj30gh06pabe.jpg" alt=""></p>
<span id="more"></span>
<h1 id="对抗训练应用场景"><a href="#对抗训练应用场景" class="headerlink" title="对抗训练应用场景"></a>对抗训练应用场景</h1><p>Szegedy在14年的ICLR中提出了对抗样本的概念。对抗样本可以用来攻击和防御，而对抗训练其实是“对抗”家族中防御的一种方式，其基本原理为：通过添加扰动构建对抗样本，喂入模型一同训练，提高模型遇到对抗样本时的鲁棒性，同时一定程度也能提高模型的表现和泛化能力。</p>
<p>对抗样本一般需要具有两个特点：</p>
<ol>
<li>相对于原始输入，所添加的扰动是微小的；</li>
<li>能使模型犯错。</li>
</ol>
<p>对抗训练的公式如下：</p>
<script type="math/tex; mode=display">
\min _{\theta} \mathbb{E}_{(x, y) \sim \mathcal{D}}\left[\max _{r_{a d v} \in \mathcal{S}} L\left(\theta, x+r_{a d v}, y\right)\right]</script><p>该过程可以分为两步：</p>
<ol>
<li>内部的max过程：寻找让模型犯错最大的扰动</li>
<li>外部的min过程：寻找整体损失最小的参数</li>
</ol>
<p>在图像领域，扰动可以为图像上的噪点，但是在NLP中，如果直接在词编码上加上扰动，输入会偏离原先的语义。由于向量空间中语义相近的词语相互接近，在向量空间中添加微小的扰动的方式并不会对语义带来较大的破坏，因此当前NLP中的对抗训练均针对embedding做扰动。</p>
<h1 id="对抗训练的作用"><a href="#对抗训练的作用" class="headerlink" title="对抗训练的作用"></a>对抗训练的作用</h1><ol>
<li>提高模型应对恶意对抗样本时的鲁棒性。</li>
<li>作为一种正则化方式(regularization)，减少过拟合(overfitting)，提高泛化能力。</li>
</ol>
<p>在NLP任务中，对抗训练的角色不再是为了防御基于梯度的恶意攻击，更多的是作为一种正则化方式(regularization)，提高模型的泛化能力。</p>
<h1 id="对抗训练具体方式FGM-PGD-FreeLB"><a href="#对抗训练具体方式FGM-PGD-FreeLB" class="headerlink" title="对抗训练具体方式FGM/PGD/FreeLB"></a>对抗训练具体方式FGM/PGD/FreeLB</h1><h2 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h2><p>在介绍对抗训练的具体实现之前，本文先介绍下面Pytorch代码中常见的函数：</p>
<p><strong>一般优化流程：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># zero the parameter gradients</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"># forward + backward + optimize</span><br><span class="line">outputs = net(inputs)</span><br><span class="line">loss = criterion(outputs, labels)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p><strong>具体展开流程：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># gradient descent</span><br><span class="line">weights = [0] * n</span><br><span class="line">alpha = 0.0001</span><br><span class="line">max_Iter = 50000</span><br><span class="line">for i in range(max_Iter):</span><br><span class="line">    loss = 0</span><br><span class="line">    d_weights = [0] * n</span><br><span class="line">    for k in range(m):</span><br><span class="line">        h = dot(input[k], weights)</span><br><span class="line">        d_weights = [d_weights[j] + (label[k] - h) * input[k][j] for j in range(n)] # 梯度下降优化</span><br><span class="line">        loss += (label[k] - h) * (label[k] - h) / 2 # 梯度下降优化</span><br><span class="line">    d_weights = [d_weights[k]/m for k in range(n)]</span><br><span class="line">    weights = [weights[k] + alpha * d_weights[k] for k in range(n)]</span><br><span class="line">    if i%10000 == 0:</span><br><span class="line">        print &quot;Iteration %d loss: %f&quot;%(i, loss/m)</span><br><span class="line">        print weights</span><br></pre></td></tr></table></figure>
<p>可以发现它们实际上是一一对应的：</p>
<ul>
<li><strong>optimizer.zero_grad()对应d_weights = [0] * n</strong></li>
</ul>
<p>该步骤将梯度初始化为零（因为一个batch的loss关于weight的导数是所有sample的loss关于weight的导数的累加和）</p>
<ul>
<li><strong>outputs = net(inputs)对应h = dot(input[k], weights)</strong></li>
</ul>
<p>该步骤即前向传播求出预测的值</p>
<ul>
<li><strong>loss = criterion(outputs, labels)对应loss += (label[k] - h) * (label[k] - h) / 2</strong></li>
</ul>
<p>该步骤为求当前具体loss值</p>
<ul>
<li><strong>loss.backward()对应d_weights = [d_weights[j] + (label[k] - h) * input[k][j] for j in range(n)]</strong></li>
</ul>
<p>该步骤即反向传播求梯度</p>
<ul>
<li><strong>optimizer.step()对应weights = [weights[k] + alpha * d_weights[k] for k in range(n)]</strong></li>
</ul>
<p>该步骤即更新所有参数</p>
<h2 id="FGSM-FGM"><a href="#FGSM-FGM" class="headerlink" title="FGSM/FGM"></a>FGSM/FGM</h2><p>该方式的思想为沿着梯度上升方向对扰动可以对模型带来最大的破坏。</p>
<p>FGSM：采用Sign函数对梯度采取max归一化，max归一化是是说如果梯度某个维度上的值为正，则设为1；如果为负，则设为-1；如果为0，则设为0</p>
<p>FGM：采用L2归一化，L2归一化则将梯度的每个维度的值除以梯度的L2范数。 理论上L2归一化更严格的保留了梯度的方向，但是max归一化则不一定和原始梯度的方向相同。</p>
<script type="math/tex; mode=display">
FGSM：\delta=\epsilon Sign(g)</script><script type="math/tex; mode=display">
FGM: \delta = \epsilon (g/||g_2||)</script><script type="math/tex; mode=display">
其中g为梯度g=\nabla x(L(f_{\theta}(X), y))</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FGM</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attack</span>(<span class="params">self, epsilon=<span class="number">1.</span>, emb_name=<span class="string">&#x27;emb.&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name:</span><br><span class="line">                self.backup[name] = param.data.clone()</span><br><span class="line">                norm = torch.norm(param.grad)</span><br><span class="line">                <span class="keyword">if</span> norm != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> torch.isnan(norm):</span><br><span class="line">                    r_at = epsilon * param.grad / norm</span><br><span class="line">                    param.data.add_(r_at)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore</span>(<span class="params">self, emb_name=<span class="string">&#x27;emb.&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name: </span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.backup</span><br><span class="line">                param.data = self.backup[name]</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">fgm = FGM(model)</span><br><span class="line"><span class="keyword">for</span> batch_input, batch_label <span class="keyword">in</span> data:</span><br><span class="line">    <span class="comment"># 正常训练</span></span><br><span class="line">    loss = model(batch_input, batch_label)</span><br><span class="line">    loss.backward() <span class="comment"># 反向传播，得到正常的grad</span></span><br><span class="line">    <span class="comment"># 对抗训练</span></span><br><span class="line">    fgm.attack() <span class="comment"># 在embedding上添加对抗扰动</span></span><br><span class="line">    loss_adv = model(batch_input, batch_label)</span><br><span class="line">    loss_adv.backward() <span class="comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span></span><br><span class="line">    fgm.restore() <span class="comment"># 恢复embedding参数</span></span><br><span class="line">    <span class="comment"># 梯度下降，更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<h3 id="FGM-FGSM-流程总结"><a href="#FGM-FGSM-流程总结" class="headerlink" title="FGM/FGSM 流程总结"></a>FGM/FGSM 流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>执行对抗训练-根据当前梯度对参数值添加扰动；前向传播得到损失与最终梯度</li>
<li>恢复embedding参数</li>
<li>更新本次迭代的参数</li>
</ol>
<p>根据min-max公式可以看出，对抗训练主要完成内部max的过程。FGM/FGSM思想就是沿着梯度上升的方向，找寻最优解。但是<strong>FGM/FGSM 有假设：损失函数是线性或者局部线性。如果不是线性，那梯度提升方向不一定是最优方向。</strong></p>
<h2 id="PGD"><a href="#PGD" class="headerlink" title="PGD"></a>PGD</h2><p>为了解决FGM中线性假设问题，PGD分多次迭代，若扰动超出范围将扰动映射到规定范围内。</p>
<script type="math/tex; mode=display">
X_{t + 1}=\prod _{X+S}(X_t + \epsilon(g_t/||g_t||))</script><script type="math/tex; mode=display">
其中g为梯度g_t=\nabla x_t(L(f_{\theta}(X_t), y))</script><p>虽然PGD很有效，但效率并不高，若经过m次迭代，PGD需要迭代m*(K + 1)次。其代码展示如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PGD</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model</span>):</span></span><br><span class="line">        self.model = model</span><br><span class="line">        self.emb_backup = &#123;&#125;</span><br><span class="line">        self.grad_backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">attack</span>(<span class="params">self, epsilon=<span class="number">1.</span>, alpha=<span class="number">0.3</span>, emb_name=<span class="string">&#x27;emb.&#x27;</span>, is_first_attack=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">if</span> is_first_attack:</span><br><span class="line">                    self.emb_backup[name] = param.data.clone()</span><br><span class="line">                norm = torch.norm(param.grad)</span><br><span class="line">                <span class="keyword">if</span> norm != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> torch.isnan(norm):</span><br><span class="line">                    r_at = alpha * param.grad / norm</span><br><span class="line">                    param.data.add_(r_at)</span><br><span class="line">                    param.data = self.project(name, param.data, epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore</span>(<span class="params">self, emb_name=<span class="string">&#x27;emb.&#x27;</span></span>):</span></span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad <span class="keyword">and</span> emb_name <span class="keyword">in</span> name: </span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.emb_backup</span><br><span class="line">                param.data = self.emb_backup[name]</span><br><span class="line">        self.emb_backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">project</span>(<span class="params">self, param_name, param_data, epsilon</span>):</span></span><br><span class="line">        r = param_data - self.emb_backup[param_name]</span><br><span class="line">        <span class="keyword">if</span> torch.norm(r) &gt; epsilon:</span><br><span class="line">            r = epsilon * r / torch.norm(r)</span><br><span class="line">        <span class="keyword">return</span> self.emb_backup[param_name] + r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backup_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                self.grad_backup[name] = param.grad.clone()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">restore_grad</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> param.requires_grad:</span><br><span class="line">                param.grad = self.grad_backup[name]</span><br><span class="line">                </span><br><span class="line">pgd = PGD(model)</span><br><span class="line">K = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> batch_input, batch_label <span class="keyword">in</span> data:</span><br><span class="line">    <span class="comment"># 正常训练</span></span><br><span class="line">    loss = model(batch_input, batch_label)</span><br><span class="line">    loss.backward() <span class="comment"># 反向传播，得到正常的grad</span></span><br><span class="line">    pgd.backup_grad()</span><br><span class="line">    <span class="comment"># 对抗训练</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        pgd.attack(is_first_attack=(t==<span class="number">0</span>)) <span class="comment"># 在embedding上添加对抗扰动, first attack时备份param.data</span></span><br><span class="line">        <span class="keyword">if</span> t != K-<span class="number">1</span>:</span><br><span class="line">            model.zero_grad()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pgd.restore_grad()</span><br><span class="line">        loss_adv = model(batch_input, batch_label)</span><br><span class="line">        loss_adv.backward() <span class="comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span></span><br><span class="line">    pgd.restore() <span class="comment"># 恢复embedding参数</span></span><br><span class="line">    <span class="comment"># 梯度下降，更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<h3 id="PGD流程总结"><a href="#PGD流程总结" class="headerlink" title="PGD流程总结"></a>PGD流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>备份正常的梯度</li>
<li>执行K次对抗训练<ol>
<li>若t=0，备份参数；梯度清零；前向传播，计算梯度与loss值</li>
<li>若t=K-1；恢复第1步的梯度；前向传播，计算梯度与loss值</li>
</ol>
</li>
<li>恢复3.1中的embedding参数</li>
<li>更新本次迭代的参数</li>
</ol>
<p>PGD执行K次目的为分多步获取<strong>内部max的扰动-扰动表现在参数上</strong>，每一步梯度归零，但是参数值得到了累加:$x^{‘}=x+\sum_{t=0}^{K} r_t$,最后根据参数$x^{‘}$以及初始梯度前向传播计算loss和最终梯度，最后，恢复初始参数，根据最终梯度完成参数更新。</p>
<h2 id="FreeAT"><a href="#FreeAT" class="headerlink" title="FreeAT"></a>FreeAT</h2><p>PGD中进行m次反向传播，m *（K + 1） 次前向传播效率不高</p>
<p>FreeAT把前向传播计算出的梯度也进行回传</p>
<p>对比图为：</p>
<p><img src="https://pic4.zhimg.com/80/v2-b54c33d95d0e8123091297d3c4b89a2f_720w.jpg" alt="image"></p>
<p>进行（m/k）*k=m 次反向传播，（m/k）* k = m次前向传播</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">初始化r=0</span><br><span class="line">对于epoch=1...N/m:</span><br><span class="line">  对于每个x:</span><br><span class="line">    对于每步m:</span><br><span class="line">      1.利用上一步的r，计算x+r的前后向，得到梯度</span><br><span class="line">      2.根据梯度更新参数</span><br><span class="line">      3.根据梯度更新r</span><br></pre></td></tr></table></figure>
<h3 id="FreeAT流程总结"><a href="#FreeAT流程总结" class="headerlink" title="FreeAT流程总结"></a>FreeAT流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>备份正常的梯度</li>
<li>执行K次对抗训练<ol>
<li>前向传播得到梯度与loss值</li>
<li>根据梯度更新参数</li>
<li>根据梯度更新扰动</li>
</ol>
</li>
</ol>
<p><strong>缺点：FreeLB指出，FreeAT的问题在于每次的r对于当前的参数都是次优的（无法最大化loss），因为当前r是由$ r_{t-1} $和$\theta_{t-1}$计算出来的，是对于$\theta_{ t-1 }$的最优。</strong></p>
<h2 id="FreeLB"><a href="#FreeLB" class="headerlink" title="FreeLB"></a>FreeLB</h2><p>FreeLB认为，FreeAT和YOPO对于获得最优r (inner max)的计算都存在问题，因此提出了一种类似PGD的方法。只不过PGD只使用了最后一步x+r输出的梯度，而FreeLB取了每次迭代r输出梯度的平均值，相当于把输入看作一个K倍大的虚拟batch，由[X+r1, X+r2, …, X+rk]拼接而成。具体的公式为：</p>
<script type="math/tex; mode=display">
min_{\theta} E(Z,y) - D(\frac{1}{K} \sum_{t=0}^{K-1}max_{r_t \in L_t} L(f_{\theta}(X+r_t),y))</script><p>PGD公式为:</p>
<script type="math/tex; mode=display">
min_{\theta} E(Z,y) - D(max_{||r|| \le\epsilon} L(f_{\theta}(X+r_t),y))</script><p><strong>FreeLB与PGD区别如下：</strong></p>
<ol>
<li>PGD是迭代K次r后取最后一次扰动的梯度更新参数，FreeLB是取K次迭代中的平均梯度</li>
<li>PGD的扰动范围都在epsilon内，因为流程第3步将梯度归0了，每次投影都会回到以第1步x为圆心，半径是epsilon的圆内，而FreeLB每次的x都会迭代，所以r的范围更加灵活，更可能接近局部最优</li>
</ol>
<p>伪代码为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于每个x:</span><br><span class="line">  1.通过均匀分布初始化r，梯度g为0</span><br><span class="line">  对于每步t=1...K:</span><br><span class="line">    2.根据x+r计算前后向，累计梯度g</span><br><span class="line">    3.更新r</span><br><span class="line">  4.根据g/K更新梯度</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class FreeLB(object):</span><br><span class="line">    def __init__(self, adv_K, adv_lr, adv_init_mag, adv_max_norm=0., adv_norm_type=&#x27;l2&#x27;, base_model=&#x27;bert&#x27;):</span><br><span class="line">        self.adv_K = adv_K</span><br><span class="line">        self.adv_lr = adv_lr</span><br><span class="line">        self.adv_max_norm = adv_max_norm</span><br><span class="line">        self.adv_init_mag = adv_init_mag    # adv-training initialize with what magnitude, 即我们用多大的数值初始化delta</span><br><span class="line">        self.adv_norm_type = adv_norm_type</span><br><span class="line">        self.base_model = base_model</span><br><span class="line">    def attack(self, model, inputs, gradient_accumulation_steps=1):</span><br><span class="line">        input_ids = inputs[&#x27;input_ids&#x27;]</span><br><span class="line">        if isinstance(model, torch.nn.DataParallel):</span><br><span class="line">            embeds_init = getattr(model.module, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">        else:</span><br><span class="line">            embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">        if self.adv_init_mag &gt; 0:   # 影响attack首步是基于原始梯度(delta=0)，还是对抗梯度(delta!=0)</span><br><span class="line">            input_mask = inputs[&#x27;attention_mask&#x27;].to(embeds_init)</span><br><span class="line">            input_lengths = torch.sum(input_mask, 1)</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                delta = torch.zeros_like(embeds_init).uniform_(-1, 1) * input_mask.unsqueeze(2)</span><br><span class="line">                dims = input_lengths * embeds_init.size(-1)</span><br><span class="line">                mag = self.adv_init_mag / torch.sqrt(dims)</span><br><span class="line">                delta = (delta * mag.view(-1, 1, 1)).detach()</span><br><span class="line">            elif self.adv_norm_type == &quot;linf&quot;:</span><br><span class="line">                delta = torch.zeros_like(embeds_init).uniform_(-self.adv_init_mag, self.adv_init_mag)</span><br><span class="line">                delta = delta * input_mask.unsqueeze(2)</span><br><span class="line">        else:</span><br><span class="line">            delta = torch.zeros_like(embeds_init)  # 扰动初始化</span><br><span class="line">        loss, logits = None, None</span><br><span class="line">        for astep in range(self.adv_K):</span><br><span class="line">            delta.requires_grad_()</span><br><span class="line">            inputs[&#x27;inputs_embeds&#x27;] = delta + embeds_init  # 累积一次扰动delta</span><br><span class="line">            inputs[&#x27;input_ids&#x27;] = None</span><br><span class="line">            outputs = model(**inputs)</span><br><span class="line">            loss, logits = outputs[:2]  # model outputs are always tuple in transformers (see doc)</span><br><span class="line">            loss = loss.mean()  # mean() to average on multi-gpu parallel training</span><br><span class="line">            loss = loss / gradient_accumulation_steps</span><br><span class="line">            loss.backward()</span><br><span class="line">            delta_grad = delta.grad.clone().detach()  # 备份扰动的grad</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1).view(-1, 1, 1)</span><br><span class="line">                denorm = torch.clamp(denorm, min=1e-8)</span><br><span class="line">                delta = (delta + self.adv_lr * delta_grad / denorm).detach()</span><br><span class="line">                if self.adv_max_norm &gt; 0:</span><br><span class="line">                    delta_norm = torch.norm(delta.view(delta.size(0), -1).float(), p=2, dim=1).detach()</span><br><span class="line">                    exceed_mask = (delta_norm &gt; self.adv_max_norm).to(embeds_init)</span><br><span class="line">                    reweights = (self.adv_max_norm / delta_norm * exceed_mask + (1 - exceed_mask)).view(-1, 1, 1)</span><br><span class="line">                    delta = (delta * reweights).detach()</span><br><span class="line">            elif self.adv_norm_type == &quot;linf&quot;:</span><br><span class="line">                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1, p=float(&quot;inf&quot;)).view(-1, 1, 1)  # p=&#x27;inf&#x27;,无穷范数，获取绝对值最大者</span><br><span class="line">                denorm = torch.clamp(denorm, min=1e-8)  # 类似np.clip，将数值夹逼到(min, max)之间</span><br><span class="line">                delta = (delta + self.adv_lr * delta_grad / denorm).detach()  # 计算该步的delta，然后累加到原delta值上(梯度上升)</span><br><span class="line">                if self.adv_max_norm &gt; 0:</span><br><span class="line">                    delta = torch.clamp(delta, -self.adv_max_norm, self.adv_max_norm).detach()</span><br><span class="line">            else:</span><br><span class="line">                raise ValueError(&quot;Norm type &#123;&#125; not specified.&quot;.format(self.adv_norm_type))</span><br><span class="line">            if isinstance(model, torch.nn.DataParallel):  </span><br><span class="line">                embeds_init = getattr(model.module, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">            else:</span><br><span class="line">                embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids)</span><br><span class="line">        return loss, logits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if args.do_adv:</span><br><span class="line">    inputs = &#123;</span><br><span class="line">        &quot;input_ids&quot;: input_ids,</span><br><span class="line">        &quot;bbox&quot;: layout,</span><br><span class="line">        &quot;token_type_ids&quot;: segment_ids,</span><br><span class="line">        &quot;attention_mask&quot;: input_mask,</span><br><span class="line">        &quot;masked_lm_labels&quot;: lm_label_ids</span><br><span class="line">    &#125;</span><br><span class="line">    loss, prediction_scores = freelb.attack(model, inputs)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">scheduler.step()</span><br><span class="line">model.zero_grad()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class FreeLB():</span><br><span class="line">    def __init__(self, model, args, optimizer, base_model=&#x27;xlm-roberta&#x27;):</span><br><span class="line">        self.args = args</span><br><span class="line">        self.model = model</span><br><span class="line">        self.adv_K = self.args.adv_K</span><br><span class="line">        self.adv_lr = self.args.adv_lr</span><br><span class="line">        self.adv_max_norm = self.args.adv_max_norm</span><br><span class="line">        self.adv_init_mag = self.args.adv_init_mag  # adv-training initialize with what magnitude, 即我们用多大的数值初始化delta</span><br><span class="line">        self.adv_norm_type = self.args.adv_norm_type</span><br><span class="line">        self.base_model = base_model</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line"></span><br><span class="line">    def attack(self, model, inputs):</span><br><span class="line">        args = self.args</span><br><span class="line">        input_ids = inputs[&#x27;input_ids&#x27;]</span><br><span class="line">        #获取初始化时的embedding</span><br><span class="line">        embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids.to(args.device))</span><br><span class="line"></span><br><span class="line">        if self.adv_init_mag &gt; 0:   # 影响attack首步是基于原始梯度(delta=0)，还是对抗梯度(delta!=0)</span><br><span class="line">            input_mask = inputs[&#x27;attention_mask&#x27;].to(embeds_init)</span><br><span class="line">            input_lengths = torch.sum(input_mask, 1)</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                delta = torch.zeros_like(embeds_init).uniform_(-1, 1) * input_mask.unsqueeze(2)</span><br><span class="line">                dims = input_lengths * embeds_init.size(-1)</span><br><span class="line">                mag = self.adv_init_mag / torch.sqrt(dims)</span><br><span class="line">                delta = (delta * mag.view(-1, 1, 1)).detach()</span><br><span class="line">        else:</span><br><span class="line">            delta = torch.zeros_like(embeds_init)  # 扰动初始化</span><br><span class="line">        # loss, logits = None, None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        for astep in range(self.adv_K):</span><br><span class="line">            delta.requires_grad_()</span><br><span class="line">            inputs[&#x27;inputs_embeds&#x27;] = delta + embeds_init  # 累积一次扰动delta</span><br><span class="line">            # inputs[&#x27;input_ids&#x27;] = None</span><br><span class="line">            loss, _ = model(input_ids=None,</span><br><span class="line">                            attention_mask=inputs[&quot;attention_mask&quot;].to(args.device),</span><br><span class="line">                             token_type_ids=inputs[&quot;token_type_ids&quot;].to(args.device),</span><br><span class="line">                             labels=inputs[&quot;sl_labels&quot;].to(args.device),</span><br><span class="line">                            inputs_embeds=inputs[&quot;inputs_embeds&quot;].to(args.device))</span><br><span class="line"></span><br><span class="line">            loss = loss / self.adv_K # 求平均的梯度</span><br><span class="line"></span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            if astep == self.adv_K - 1:</span><br><span class="line">                # further updates on delta</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">            delta_grad = delta.grad.clone().detach()  # 备份扰动的grad</span><br><span class="line">            if self.adv_norm_type == &quot;l2&quot;:</span><br><span class="line">                denorm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1).view(-1, 1, 1)</span><br><span class="line">                denorm = torch.clamp(denorm, min=1e-8)</span><br><span class="line">                delta = (delta + self.adv_lr * delta_grad / denorm).detach()</span><br><span class="line">                if self.adv_max_norm &gt; 0:</span><br><span class="line">                    delta_norm = torch.norm(delta.view(delta.size(0), -1).float(), p=2, dim=1).detach()</span><br><span class="line">                    exceed_mask = (delta_norm &gt; self.adv_max_norm).to(embeds_init)</span><br><span class="line">                    reweights = (self.adv_max_norm / delta_norm * exceed_mask + (1 - exceed_mask)).view(-1, 1, 1)</span><br><span class="line">                    delta = (delta * reweights).detach()</span><br><span class="line">            else:</span><br><span class="line">                raise ValueError(&quot;Norm type &#123;&#125; not specified.&quot;.format(self.adv_norm_type))</span><br><span class="line"></span><br><span class="line">            embeds_init = getattr(model, self.base_model).embeddings.word_embeddings(input_ids.to(args.device))</span><br><span class="line">        return loss</span><br><span class="line"></span><br><span class="line">for batch_input, batch_label in data:</span><br><span class="line">    # 正常训练</span><br><span class="line">    loss = model(batch_input, batch_label)</span><br><span class="line">    loss.backward() # 反向传播，得到正常的grad</span><br><span class="line">    # 对抗训练</span><br><span class="line">    freelb = FreeLB( model, args, optimizer, base_model)</span><br><span class="line">    loss_adv = freelb.attack(model, batch_input)</span><br><span class="line">    loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span><br><span class="line">    # 梯度下降，更新参数</span><br><span class="line">    optimizer.step()</span><br><span class="line">    model.zero_grad()</span><br></pre></td></tr></table></figure>
<h3 id="FreeLB流程总结"><a href="#FreeLB流程总结" class="headerlink" title="FreeLB流程总结"></a>FreeLB流程总结</h3><ol>
<li>正常的前向传播-得到梯度与loss值</li>
<li>备份正常的梯度</li>
<li>执行K次对抗训练<ol>
<li>前向传播，计算梯度与loss值</li>
<li>梯度累加</li>
<li>根据梯度计算扰动</li>
</ol>
</li>
<li>恢复初始embedding参数</li>
<li>更新本次迭代的参数</li>
</ol>
<p>该方法与FreeAT一样都想高效的利用两种梯度。<strong>不同的是，该方法并不是每次都进行更新，而是将参数梯度累积起来，用累积的梯度对参数更新</strong>。</p>
<h2 id="通用范式"><a href="#通用范式" class="headerlink" title="通用范式"></a>通用范式</h2><p>通过对上述几种对抗训练方式的学习，不难看出对抗训练的目的为完成<strong>内部max的任务，找出最大扰动的最优解</strong>。具体表现为：求解最大扰动更新参数；根据参数进行前向传播得到loss与最终梯度；恢复最初的参数值；利用最终的梯度对最初的参数值进行更新。所以通用流程表示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 正常的前向传播-得到梯度与loss值</span><br><span class="line">2. 备份正常的参数</span><br><span class="line">3. 求解扰动最优值，更新参数</span><br><span class="line">4. 根据更新后参数以及最初梯度，前向传播得到最终梯度</span><br><span class="line">5. 恢复最初的参数</span><br><span class="line">6. 根据最初的参数与最终梯度，完成参数的更新</span><br></pre></td></tr></table></figure>
<p><strong>不同对抗训练方式体现为求解扰动最优值的方式不同：</strong></p>
<ul>
<li>FGM/FGSM最优值求解方式为：一步到位，根据最初的梯度与参数值，得到扰动值</li>
<li>PGD最优值求解方式为：多步走，每一步根据上一步的参数获取扰动并更新参数，最终得到多步累加的扰动值</li>
<li>FreeAT最优值求解方式为：与PGD一样分多步，但是该方法相当于：每一步根据上一步参数和梯度获取的扰动值就是最终扰动值</li>
<li>FreeLB最优值求解方式为：与PGD一样分多步，但是在最后进行梯度更新的时候，最终梯度为初始梯度加上每一步梯度的平均值</li>
</ul>
<h1 id="对抗训练展望"><a href="#对抗训练展望" class="headerlink" title="对抗训练展望"></a>对抗训练展望</h1><h2 id="虚拟对抗训练"><a href="#虚拟对抗训练" class="headerlink" title="虚拟对抗训练"></a>虚拟对抗训练</h2><p><strong>那什么是虚拟对抗训练(VAT)呢</strong>？</p>
<blockquote>
<p>VAT不需要标签信息，可应用于无监督学习，其梯度上升的方向是能使预测的输出分布偏离现状的方向，而传统对抗训练课找的是使模型预测最大地偏离label的方向。因此，VAT不使用真实label，而是“虚拟”label——当前模型的预测结果。</p>
</blockquote>
<p>该部分可以查看JayJay的博客：<a href="https://zhuanlan.zhihu.com/p/345264876">虚拟对抗训练：让预训练模型再次强大！</a></p>
<h1 id="延伸思考"><a href="#延伸思考" class="headerlink" title="延伸思考"></a>延伸思考</h1><h2 id="对抗训练与梯度惩罚"><a href="#对抗训练与梯度惩罚" class="headerlink" title="对抗训练与梯度惩罚"></a>对抗训练与梯度惩罚</h2><p>该内容为苏神在博客<a href="https://kexue.fm/archives/7234">对抗训练浅谈：意义、方法和思考（附Keras实现）</a>中所提及：</p>
<p>假设已经得到对抗扰动Δx，那么我们在更新θ时，考虑对$L(x+Δx,y;θ)$的展开：</p>
<script type="math/tex; mode=display">
\min_{\theta}\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[L(x+\Delta x, y;\theta)\right]\\ 
\approx\, \min_{\theta}\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[L(x, y;\theta)+\langle\nabla_x L(x, y;\theta), \Delta x\rangle\right]</script><p>对应的θ的梯度为：</p>
<script type="math/tex; mode=display">
\nabla_{\theta}L(x, y;\theta)+\langle\nabla_{\theta}\nabla_x L(x, y;\theta), \Delta x\rangle</script><p>代入$\Delta x=\epsilon \nabla_x L(x, y;\theta)$，得到</p>
<script type="math/tex; mode=display">
\nabla_{\theta}L(x, y;\theta)+\epsilon\langle\nabla_{\theta}\nabla_x L(x, y;\theta), \nabla_x L(x, y;\theta)\rangle\\ 
=\,\nabla_{\theta}\left(L(x, y;\theta)+\frac{1}{2}\epsilon\left\Vert\nabla_x L(x, y;\theta)\right\Vert^2\right)</script><p>这个结果表示，对输入样本施加$\epsilon \nabla_x L(x, y;\theta)$的对抗扰动，一定程度上等价于往loss里边加入“梯度惩罚”</p>
<script type="math/tex; mode=display">
\frac{1}{2}\epsilon\left\Vert\nabla_x L(x, y;\theta)\right\Vert^2</script><p>如果对抗扰动是$\nabla_x L(x, y;\theta)/\Vert \nabla_x L(x, y;\theta)\Vert$，那么对应的梯度惩罚项则是$\epsilon\left\Vert\nabla_x L(x, y;\theta)\right\Vert$（少了个1/2，也少了个2次方）。</p>
<p>事实上，这个结果不是新的，它首先出现论文<a href="https://arxiv.org/abs/1711.09404">《Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients》</a>里。只不过这篇文章不容易搜到，因为你一旦搜索“adversarial training gradient penalty”等关键词，出来的结果几乎都是WGAN-GP相关的东西。</p>
<h2 id="词向量空间"><a href="#词向量空间" class="headerlink" title="词向量空间"></a>词向量空间</h2><p>NLP中对抗训练目前的方式均是对embedding向量空间添加扰动，那么向量空间究竟什么样呢？在对比学习的研究中<Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere>，同样提出一个好的对比学习系统应该具体两个特点：</p>
<ul>
<li><strong>Alignment：</strong>指的是相似的例子，也就是正例，映射到单位超球面后，应该有接近的特征，也即是说，在超球面上距离比较近</li>
<li><strong>Uniformity：</strong>指的是系统应该倾向在特征里保留尽可能多的信息，这等价于使得映射到单位超球面的特征，尽可能均匀地分布在球面上，分布得越均匀，意味着保留的信息越充分。分布均匀意味着两两有差异，也意味着各自保有独有信息，这代表信息保留充分。</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gykdip46wnj30nm0w4wgj.jpg" alt=""></p>
<p>极端情况下会出现模型塌缩的情况，即所有特征映射到同一点：</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gykdod8h59j31180u0taa.jpg" alt=""></p>
<p>笔者认为，对抗训练在词向量层添加扰动，与对比学习类似，实现相似的例子在向量空间中相接近的目的，完成输入发生微小改变，输出改变幅度也不大的任务。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>对抗训练</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>对抗训练</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络调参-warmup and decay</title>
    <url>/2022/01/25/2022-01-25-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%B0%83%E5%8F%82-warmup%20and%20decay/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文将介绍神经网络调参技巧：warmup，decay。反向传播主要完成参数更新：$\theta_t=\theta_{t-1}-\alpha * g_t$，其中$\alpha$为学习率，$g_t$为梯度更新量，而warmup、decay就是调整$\alpha$的方式，优化器决定梯度更新的方式即$g_t$的计算方式。衰减方式如下图所示：</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gyppski86oj30hs0dcab6.jpg" alt=""><br><span id="more"></span></p>
<h1 id="warmup-and-decay"><a href="#warmup-and-decay" class="headerlink" title="warmup and decay"></a>warmup and decay</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Warmup and Decay是模型训练过程中，一种学习率（learning rate）的调整策略。</p>
<p>Warmup是在ResNet论文中提到的一种学习率预热的方法，它在训练开始的时候先选择使用一个较小的学习率，训练了一些epoches或者steps(比如4个epoches,10000steps),再修改为预先设置的学习来进行训练。</p>
<p>同理，Decay是学习率衰减方法，它指定在训练到一定epoches或者steps后，按照线性或者余弦函数等方式，将学习率降低至指定值。一般，使用Warmup and Decay，学习率会遵循从小到大，再减小的规律。</p>
<h2 id="为什么要warmup"><a href="#为什么要warmup" class="headerlink" title="为什么要warmup"></a>为什么要warmup</h2><p>这里引用知乎：<a href="https://www.zhihu.com/question/338066667/answer/771252708的讨论：">https://www.zhihu.com/question/338066667/answer/771252708的讨论：</a><br><strong>SGD训练中常见的方式是初始较大的学习率，然后衰减为小的学习率，而warmup是先以较小的学习率上升到初始学习率，然后再衰减到小的学习率上，那么为什么warmup有效。</strong></p>
<h3 id="直观上解释"><a href="#直观上解释" class="headerlink" title="直观上解释"></a>直观上解释</h3><p>深层网络随机初始化差异较大，如果一开始以较大的学习率，初始学习带来的偏差在后续学习过程中难以纠正。</p>
<p>训练刚开始时梯度更新较大，若学习率设置较大则更新的幅度较大，该类型与传统学习率先大后小方式不同的原因在于起初浅层网络幅度大的更新并不会导致方向错误。</p>
<h3 id="理论上解释"><a href="#理论上解释" class="headerlink" title="理论上解释"></a>理论上解释</h3><p><strong>warmup带来的优点包含：</strong></p>
<ul>
<li>缓解模型在初期对mini-batch过拟合的现象</li>
<li>保持模型深层的稳定性</li>
</ul>
<p><strong>给出三个论文中的结论：</strong></p>
<ol>
<li>当batch大小增加时，学习率也可以成倍增加</li>
<li>限制大batch训练的是高学习率带来的训练不稳定性</li>
<li>warmup主要限制深层的权重变化，并且冻结深层权重的变化可以取得相似的效果</li>
</ol>
<h4 id="batch与学习率大小的关系"><a href="#batch与学习率大小的关系" class="headerlink" title="batch与学习率大小的关系"></a>batch与学习率大小的关系</h4><p>假设现在模型已经train到第t步，权重为$w_t$，我们有k个mini-batch，每个mini-batch大小为n，记为$\mathcal{B}_{1:k}$ 。下面我们来看，以学习率 $\eta$训k次 $\mathcal{B}_{1:k}$ 和以学习率 $\hat{\eta}$ 一次训练$\mathcal{B}$时学习率的关系。</p>
<p>假设我们用的是SGD，那么训k次后我们可以得到：</p>
<script type="math/tex; mode=display">
w_{t+k}=w_{t}-\eta \frac{1}{n} \sum_{j<k} \sum_{x \in \mathcal{B}_{j}} \nabla l\left(x, w_{t+j}\right)</script><p>如果我们一次训练就可以得到：</p>
<script type="math/tex; mode=display">
\hat{w}_{t+1}=w_{t}-\hat{\eta} \frac{1}{k n} \sum_{j<k} \sum_{x \in \mathcal{B}_{j}} \nabla l\left(x, w_{t}\right)</script><p>其中$w_{t+k}$与$\hat{w}_{t+1}$代表按上述方式训练k次与1次，完成参数更新后的参数。显然，这两个是不一样的。但如果我们假设$\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)$，那么令$\hat{\eta}=k\eta $就可以保证 <script type="math/tex">\hat{w}_{t+1} \approx w_{t+k}</script> 。那么，在什么时候 $\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)$ 可能不成立呢？[1]告诉我们有两种情况：</p>
<ul>
<li>在训练的开始阶段，模型权重迅速改变</li>
<li>Mini-batch 大小较小，样本方差较大</li>
</ul>
<p>第一种情况，模型初始参数分布取决于初始化方式，初始数据对于模型都是初次修正，所以梯度更新较大，若一开始以较大的学习率学习，易对数据造成过拟合，需要经过之后更多轮的训练进行修正。</p>
<p>第二种情况，在训练的过程中，如果有mini-batch内的数据分布方差特别大，这会导致模型学习剧烈波动，使其学得的权重很不稳定，这在训练初期最为明显，最后期较为缓解。</p>
<p>针对上述两种情况，并不能简单的成倍增长学习率$\hat{\eta}=k\eta$,因为此时不符合$\nabla l\left(x, w_{t}\right) \approx \nabla l\left(x, w_{t+j}\right)$假设。此时要么更改学习率增长方式[warmup]，要么解决这两种情况[数据预处理以减小样本方差]。</p>
<h4 id="warmup与模型学习的稳定性"><a href="#warmup与模型学习的稳定性" class="headerlink" title="warmup与模型学习的稳定性"></a>warmup与模型学习的稳定性</h4><p>该部分通过一些论文实验结果，推断有了warmup之后模型能够学习的更稳定。</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gyq8344bmwj311n0d90vs.jpg" alt=""></p>
<p><strong>上图表示有了warmup之后，模型能够学习的更加稳定。</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gyq84sqtxlj310y09q762.jpg" alt=""></p>
<p><strong>上图b，c表示有了warmup之后，模型最后几层的相似性增加，避免模型不稳定的改变。</strong></p>
<h1 id="学习率衰减策略"><a href="#学习率衰减策略" class="headerlink" title="学习率衰减策略"></a>学习率衰减策略</h1><h2 id="可视化代码"><a href="#可视化代码" class="headerlink" title="可视化代码"></a>可视化代码</h2><p><strong>下列各种学习率衰减策略均采用warmup，为了图片反应的更加直观：起始学习率设置为1，warmup 步数为20，总步数为100。通常warmup步数可以设置为总步数的10%，参照BERT的经验策略。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># author： JMXGODLZZ</span><br><span class="line"># datetime： 2022/1/23 下午7:10 </span><br><span class="line"># ide： PyCharm</span><br><span class="line">import keras</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">from learningrateSchedules import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_cosine_with_hard_restarts_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_polynomial_decay_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_step_schedule_with_warmup</span><br><span class="line">from learningrateSchedules import get_exp_schedule_with_warmup</span><br><span class="line">init_lr = 1</span><br><span class="line">warmupsteps = 20</span><br><span class="line">totalsteps = 100</span><br><span class="line"></span><br><span class="line">lrs = get_linear_schedule_with_warmup(1, warmupsteps, totalsteps)</span><br><span class="line">cos_warm_lrs = get_cosine_schedule_with_warmup(1, warmupsteps, totalsteps)</span><br><span class="line">cos_hard_warm_lrs = get_cosine_with_hard_restarts_schedule_with_warmup(1, warmupsteps, totalsteps, 2)</span><br><span class="line">poly_warm_lrs = get_polynomial_decay_schedule_with_warmup(1, warmupsteps, totalsteps, 0, 5)</span><br><span class="line">step_warm_lrs = get_step_schedule_with_warmup(1, warmupsteps, totalsteps)</span><br><span class="line">exp_warm_lrs = get_exp_schedule_with_warmup(1, warmupsteps, totalsteps, 0.9)</span><br><span class="line">x = list(range(totalsteps))</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x, lrs, label=&#x27;linear_warmup&#x27;, color=&#x27;k&#x27;)</span><br><span class="line">plt.plot(x, cos_warm_lrs, label=&#x27;cosine_warmup&#x27;, color=&#x27;b&#x27;)</span><br><span class="line">plt.plot(x, cos_hard_warm_lrs, label=&#x27;cosine_cy2_warmup&#x27;, color=&#x27;g&#x27;)</span><br><span class="line">plt.plot(x, poly_warm_lrs, label=&#x27;polynomial_warmup_pw5&#x27;, color=&#x27;r&#x27;)</span><br><span class="line">plt.plot(x, step_warm_lrs, label=&#x27;step_warmup&#x27;, color=&#x27;purple&#x27;)</span><br><span class="line">plt.plot(x, exp_warm_lrs, label=&#x27;exp_warmup&#x27;, color=&#x27;orange&#x27;)</span><br><span class="line">plt.xlabel(&#x27;steps&#x27;)</span><br><span class="line">plt.ylabel(&#x27;learning rate&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNly1gyppski86oj30hs0dcab6.jpg" alt=""></p>
<h2 id="指数衰减学习率"><a href="#指数衰减学习率" class="headerlink" title="指数衰减学习率"></a>指数衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_exp_schedule_with_warmup(learning_rate, num_warmup_steps, num_training_steps, gamma, last_epoch=-1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span><br><span class="line">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        stepmi = (current_step - num_warmup_steps)</span><br><span class="line">        return pow(gamma, stepmi)</span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="余弦衰减学习率"><a href="#余弦衰减学习率" class="headerlink" title="余弦衰减学习率"></a>余弦衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_cosine_schedule_with_warmup(</span><br><span class="line">    learning_rate, num_warmup_steps: int, num_training_steps: int, num_cycles: float = 0.5, last_epoch: int = -1</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases following the values of the cosine function between the</span><br><span class="line">    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the</span><br><span class="line">    initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        num_cycles (:obj:`float`, `optional`, defaults to 0.5):</span><br><span class="line">            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0</span><br><span class="line">            following a half-cosine).</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))</span><br><span class="line">        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))</span><br><span class="line"></span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="线性衰减学习率"><a href="#线性衰减学习率" class="headerlink" title="线性衰减学习率"></a>线性衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_linear_schedule_with_warmup(learning_rate, num_warmup_steps, num_training_steps, last_epoch=-1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span><br><span class="line">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        return max(</span><br><span class="line">            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))</span><br><span class="line">        )</span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="阶梯衰减学习率"><a href="#阶梯衰减学习率" class="headerlink" title="阶梯衰减学习率"></a>阶梯衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_step_schedule_with_warmup(learning_rate, num_warmup_steps, num_training_steps, last_epoch=-1):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after</span><br><span class="line">    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        stepmi = (current_step - num_warmup_steps) // 20 + 1</span><br><span class="line">        return pow(0.5, stepmi)</span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="多项式衰减学习率"><a href="#多项式衰减学习率" class="headerlink" title="多项式衰减学习率"></a>多项式衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_polynomial_decay_schedule_with_warmup(</span><br><span class="line">    learning_rate, num_warmup_steps, num_training_steps, lr_end=1e-7, power=1.0, last_epoch=-1</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the</span><br><span class="line">    optimizer to end lr defined by `lr_end`, after a warmup period during which it increases linearly from 0 to the</span><br><span class="line">    initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        lr_end (:obj:`float`, `optional`, defaults to 1e-7):</span><br><span class="line">            The end LR.</span><br><span class="line">        power (:obj:`float`, `optional`, defaults to 1.0):</span><br><span class="line">            Power factor.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Note: `power` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT</span><br><span class="line">    implementation at</span><br><span class="line">    https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    lr_init = learning_rate</span><br><span class="line">    if not (lr_init &gt; lr_end):</span><br><span class="line">        raise ValueError(f&quot;lr_end (&#123;lr_end&#125;) must be be smaller than initial lr (&#123;lr_init&#125;)&quot;)</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step: int):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        elif current_step &gt; num_training_steps:</span><br><span class="line">            return lr_end / lr_init  # as LambdaLR multiplies by lr_init</span><br><span class="line">        else:</span><br><span class="line">            lr_range = lr_init - lr_end</span><br><span class="line">            decay_steps = num_training_steps - num_warmup_steps</span><br><span class="line">            pct_remaining = 1 - (current_step - num_warmup_steps) / decay_steps</span><br><span class="line">            decay = lr_range * pct_remaining ** power + lr_end</span><br><span class="line">            return decay / lr_init  # as LambdaLR multiplies by lr_init</span><br><span class="line"></span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h2 id="余弦循环衰减学习率"><a href="#余弦循环衰减学习率" class="headerlink" title="余弦循环衰减学习率"></a>余弦循环衰减学习率</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_cosine_with_hard_restarts_schedule_with_warmup(</span><br><span class="line">    learning_rate, num_warmup_steps: int, num_training_steps: int, num_cycles: int = 1, last_epoch: int = -1</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Create a schedule with a learning rate that decreases following the values of the cosine function between the</span><br><span class="line">    initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases</span><br><span class="line">    linearly between 0 and the initial lr set in the optimizer.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        optimizer (:class:`~torch.optim.Optimizer`):</span><br><span class="line">            The optimizer for which to schedule the learning rate.</span><br><span class="line">        num_warmup_steps (:obj:`int`):</span><br><span class="line">            The number of steps for the warmup phase.</span><br><span class="line">        num_training_steps (:obj:`int`):</span><br><span class="line">            The total number of training steps.</span><br><span class="line">        num_cycles (:obj:`int`, `optional`, defaults to 1):</span><br><span class="line">            The number of hard restarts to use.</span><br><span class="line">        last_epoch (:obj:`int`, `optional`, defaults to -1):</span><br><span class="line">            The index of the last epoch when resuming training.</span><br><span class="line"></span><br><span class="line">    Return:</span><br><span class="line">        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def lr_lambda(current_step):</span><br><span class="line">        if current_step &lt; num_warmup_steps:</span><br><span class="line">            return float(current_step) / float(max(1, num_warmup_steps))</span><br><span class="line">        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))</span><br><span class="line">        if progress &gt;= 1.0:</span><br><span class="line">            return 0.0</span><br><span class="line">        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(num_cycles) * progress) % 1.0))))</span><br><span class="line"></span><br><span class="line">    lrs = []</span><br><span class="line">    for current_step in range(num_training_steps):</span><br><span class="line">        cur_lr = lr_lambda(current_step) * learning_rate</span><br><span class="line">        lrs.append(cur_lr)</span><br><span class="line">    return lrs</span><br></pre></td></tr></table></figure>
<h1 id="学习率衰减实现"><a href="#学习率衰减实现" class="headerlink" title="学习率衰减实现"></a>学习率衰减实现</h1><h2 id="Pytorch学习率策略"><a href="#Pytorch学习率策略" class="headerlink" title="Pytorch学习率策略"></a>Pytorch学习率策略</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if args.scheduler == &quot;constant_schedule&quot;:</span><br><span class="line">    scheduler = get_constant_schedule(optimizer)</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;constant_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_constant_schedule_with_warmup(</span><br><span class="line">        optimizer, num_warmup_steps=args.warmup_steps</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;linear_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;cosine_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_cosine_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">        num_cycles=args.cosine_schedule_num_cycles,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;cosine_with_hard_restarts_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">        num_cycles=args.cosine_schedule_num_cycles,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">elif args.scheduler == &quot;polynomial_decay_schedule_with_warmup&quot;:</span><br><span class="line">    scheduler = get_polynomial_decay_schedule_with_warmup(</span><br><span class="line">        optimizer,</span><br><span class="line">        num_warmup_steps=args.warmup_steps,</span><br><span class="line">        num_training_steps=t_total,</span><br><span class="line">        lr_end=args.polynomial_decay_schedule_lr_end,</span><br><span class="line">        power=args.polynomial_decay_schedule_power,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">else:</span><br><span class="line">    raise ValueError(&quot;&#123;&#125; is not a valid scheduler.&quot;.format(args.scheduler))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="keras学习率策略"><a href="#keras学习率策略" class="headerlink" title="keras学习率策略"></a>keras学习率策略</h2><ul>
<li>Keras提供了四种衰减策略分别是ExponentialDecay(指数衰减)、 PiecewiseConstantDecay(分段常数衰减) 、 PolynomialDecay(多项式衰减)和InverseTimeDecay(逆时间衰减)。只要在Optimizer中指定衰减策略，一行代码就能实现，在以下方法一中详细介绍。</li>
<li>如果想要自定义学习率的衰减，有第二种方法，更加灵活，需要使用callbacks来实现动态、自定义学习率衰减策略，方法二中将详细介绍。</li>
<li>如果两种方法同时使用，默认优先使用第二种，第一种方法将被忽略。</li>
</ul>
<h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><h4 id="指数衰减"><a href="#指数衰减" class="headerlink" title="指数衰减"></a>指数衰减</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(</span><br><span class="line">    initial_learning_rate=1e-2,</span><br><span class="line">    decay_steps=10000,</span><br><span class="line">    decay_rate=0.96)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h4 id="分段衰减"><a href="#分段衰减" class="headerlink" title="分段衰减"></a>分段衰减</h4><p>[0~1000]的steps，学习率为1.0,[10001～9000]的steps，学习率为0.5，其他steps，学习率为0.1</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">step = tf.Variable(0, trainable=False)</span><br><span class="line">boundaries = [1000, 10000]</span><br><span class="line">values = [1.0, 0.5, 0.1]</span><br><span class="line">learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)</span><br><span class="line">lr_scheduler = learning_rate_fn(step)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h4 id="多项式衰减"><a href="#多项式衰减" class="headerlink" title="多项式衰减"></a>多项式衰减</h4><p>在10000步中从0.1衰减到0.001，使用开根式( power=0.5)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">start_lr = 0.1</span><br><span class="line">end_lr = 0.001</span><br><span class="line">decay_steps = 10000</span><br><span class="line">lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(</span><br><span class="line">    start_lr,</span><br><span class="line">    decay_steps,</span><br><span class="line">    end_lr,</span><br><span class="line">    power=0.5)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h4 id="逆时间衰减"><a href="#逆时间衰减" class="headerlink" title="逆时间衰减"></a>逆时间衰减</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">initial_lr = 0.1</span><br><span class="line">decay_steps = 1.0</span><br><span class="line">decay_rate = 0.5</span><br><span class="line">lr_scheduler = keras.optimizers.schedules.InverseTimeDecay(</span><br><span class="line">  initial_lr, decay_steps, decay_rate)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=lr_scheduler)</span><br></pre></td></tr></table></figure>
<h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><h4 id="自定义指数衰减"><a href="#自定义指数衰减" class="headerlink" title="自定义指数衰减"></a>自定义指数衰减</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 第一步：自定义指数衰减策略</span><br><span class="line">def step_decay(epoch):</span><br><span class="line">    init_lr = 0.1</span><br><span class="line">    drop=0.5</span><br><span class="line">    epochs_drop=10</span><br><span class="line">    if epoch&lt;100:</span><br><span class="line">        return init_lr</span><br><span class="line">    else:</span><br><span class="line">        return init_lr*pow(drop,floor(1+epoch)/epochs_drop)</span><br><span class="line">        </span><br><span class="line"># ……</span><br><span class="line"># 第二步：用LearningRateScheduler封装学习率衰减策略</span><br><span class="line">lr_callback = LearningRateScheduler(step_decay)</span><br><span class="line"># 第三步：加入callbacks</span><br><span class="line">model = KerasClassifier(build_fn = create_model,epochs=200,batch_size=5,verbose=1,callbacks=[checkpoint,lr_callback])</span><br><span class="line">model.fit(X,Y)</span><br></pre></td></tr></table></figure>
<h4 id="动态修改学习率"><a href="#动态修改学习率" class="headerlink" title="动态修改学习率"></a>动态修改学习率</h4><p>ReduceLROnPlateau(monitor=’val_acc’, mode=’max’,min_delta=0.1,factor=0.2,patience=5, min_lr=0.001)</p>
<p>训练集连续patience个epochs的val_acc小于min_delta时，学习率将会乘以factor。mode可以选择max或者min，根据monitor的选择而灵活设定。min_lr是学习率的最低值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 第一步：ReduceLROnPlateau定义学习动态变化策略</span><br><span class="line">reduce_lr_callback = ReduceLROnPlateau(monitor=&#x27;val_acc&#x27;, factor=0.2,patience=5, min_lr=0.001)</span><br><span class="line"># 第二步：加入callbacks</span><br><span class="line">model = KerasClassifier(build_fn = create_model,epochs=200,batch_size=5,verbose=1,callbacks=[checkpoint,reduce_lr_callback])</span><br><span class="line">model.fit(X,Y)</span><br></pre></td></tr></table></figure>
<h3 id="Keras学习率回显代码"><a href="#Keras学习率回显代码" class="headerlink" title="Keras学习率回显代码"></a>Keras学习率回显代码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_lr_metric(optimizer):</span><br><span class="line">    def lr(y_true, y_pred):</span><br><span class="line">        return optimizer.lr</span><br><span class="line">    return lr</span><br><span class="line"> </span><br><span class="line">x = Input((50,))</span><br><span class="line">out = Dense(1, activation=&#x27;sigmoid&#x27;)(x)</span><br><span class="line">model = Model(x, out)</span><br><span class="line"> </span><br><span class="line">optimizer = Adam(lr=0.001)</span><br><span class="line">lr_metric = get_lr_metric(optimizer)</span><br><span class="line">model.compile(loss=&#x27;binary_crossentropy&#x27;, optimizer=optimizer, metrics=[&#x27;acc&#x27;, lr_metric])</span><br><span class="line"> </span><br><span class="line"># reducing the learning rate by half every 2 epochs</span><br><span class="line">cbks = [LerningRateScheduler(lambda epoch: 0.001 * 0.5 ** (epoch // 2)),</span><br><span class="line">        TensorBoard(write_graph=False)]</span><br><span class="line">X = np.random.rand(1000, 50)</span><br><span class="line">Y = np.random.randint(2, size=1000)</span><br><span class="line">model.fit(X, Y, epochs=10, callbacks=cbks)</span><br></pre></td></tr></table></figure>
<h1 id="分层学习率设置"><a href="#分层学习率设置" class="headerlink" title="分层学习率设置"></a>分层学习率设置</h1><p>有时候我们需要为模型中不同层设置不同学习率大小，比如微调预训练模型时，预训练层数设置较小的学习率进行学习，而其他层以正常大小进行学习。这里给出苏神给出的keras实现，其通过参数变换实现调整学习率的目的：</p>
<p>梯度下降公式如下：</p>
<script type="math/tex; mode=display">
\boldsymbol{\theta}_{n+1}=\boldsymbol{\theta}_{n}-\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\label{eq:sgd-1}</script><p>考虑变换$\boldsymbol{\theta}=\lambda \boldsymbol{\phi}$,其中λ是一个固定的标量，<strong>ϕ</strong>也是参数。现在来优化<strong>ϕ</strong>，相应的更新公式为：</p>
<script type="math/tex; mode=display">
\begin{aligned}\boldsymbol{\phi}_{n+1}=&\boldsymbol{\phi}_{n}-\alpha \frac{\partial L(\lambda\boldsymbol{\phi}_{n})}{\partial \boldsymbol{\phi}_n}\\ 
=&\boldsymbol{\phi}_{n}-\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\frac{\partial \boldsymbol{\theta}_{n}}{\partial \boldsymbol{\phi}_n}\\ 
=&\boldsymbol{\phi}_{n}-\lambda\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\end{aligned}</script><p>然后通过链式求导法则，再上述等式两边同时乘以λ：</p>
<script type="math/tex; mode=display">
\lambda\boldsymbol{\phi}_{n+1}=\lambda\boldsymbol{\phi}_{n}-\lambda^2\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\quad\Rightarrow\quad\boldsymbol{\theta}_{n+1}=\boldsymbol{\theta}_{n}-\lambda^2\alpha \frac{\partial L(\boldsymbol{\theta}_{n})}{\partial \boldsymbol{\theta}_n}\label{eq:sgd-2}</script><blockquote>
<p>在SGD优化器中，如果做参数变换<strong>θ</strong>=λ<strong>ϕ</strong>，那么等价的结果是学习率从α变成了$\lambda^2\alpha$。</p>
<p>不过，在自适应学习率优化器（比如RMSprop、Adam等），情况有点不一样，因为自适应学习率使用梯度（作为分母）来调整了学习率，抵消了一个λ</p>
<p>在RMSprop、Adam等自适应学习率优化器中，如果做参数变换<strong>θ</strong>=λ<strong>ϕ</strong>，那么等价的结果是学习率从α变成了λα。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import keras.backend as K</span><br><span class="line"></span><br><span class="line">class SetLearningRate:</span><br><span class="line">    &quot;&quot;&quot;层的一个包装，用来设置当前层的学习率</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, layer, lamb, is_ada=False):</span><br><span class="line">        self.layer = layer</span><br><span class="line">        self.lamb = lamb # 学习率比例</span><br><span class="line">        self.is_ada = is_ada # 是否自适应学习率优化器</span><br><span class="line"></span><br><span class="line">    def __call__(self, inputs):</span><br><span class="line">        with K.name_scope(self.layer.name):</span><br><span class="line">            if not self.layer.built:</span><br><span class="line">                input_shape = K.int_shape(inputs)</span><br><span class="line">                self.layer.build(input_shape)</span><br><span class="line">                self.layer.built = True</span><br><span class="line">                if self.layer._initial_weights is not None:</span><br><span class="line">                    self.layer.set_weights(self.layer._initial_weights)</span><br><span class="line">        for key in [&#x27;kernel&#x27;, &#x27;bias&#x27;, &#x27;embeddings&#x27;, &#x27;depthwise_kernel&#x27;, &#x27;pointwise_kernel&#x27;, &#x27;recurrent_kernel&#x27;, &#x27;gamma&#x27;, &#x27;beta&#x27;]:</span><br><span class="line">            if hasattr(self.layer, key):</span><br><span class="line">                weight = getattr(self.layer, key)</span><br><span class="line">                if self.is_ada:</span><br><span class="line">                    lamb = self.lamb # 自适应学习率优化器直接保持lamb比例</span><br><span class="line">                else:</span><br><span class="line">                    lamb = self.lamb**0.5 # SGD（包括动量加速），lamb要开平方</span><br><span class="line">                K.set_value(weight, K.eval(weight) / lamb) # 更改初始化</span><br><span class="line">                setattr(self.layer, key, weight * lamb) # 按比例替换</span><br><span class="line">        return self.layer(inputs)</span><br><span class="line"> </span><br><span class="line">x_in = Input(shape=(None,))</span><br><span class="line">x = x_in</span><br><span class="line"></span><br><span class="line"># 默认情况下是x = Embedding(100, 1000, weights=[word_vecs])(x)</span><br><span class="line"># 下面这一句表示：后面将会用自适应学习率优化器，并且Embedding层以总体的十分之一的学习率更新。</span><br><span class="line"># word_vecs是预训练好的词向量</span><br><span class="line">x = SetLearningRate(Embedding(100, 1000, weights=[word_vecs]), 0.1, True)(x)</span><br><span class="line"></span><br><span class="line"># 后面部分自己想象了～</span><br><span class="line">x = LSTM(100)(x)</span><br><span class="line"></span><br><span class="line">model = Model(x_in, x)</span><br><span class="line">model.compile(loss=&#x27;mse&#x27;, optimizer=&#x27;adam&#x27;) # 用自适应学习率优化器优化</span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://jishuin.proginn.com/p/763bfbd51f6b">https://jishuin.proginn.com/p/763bfbd51f6b</a></p>
<p><a href="https://www.zhihu.com/question/338066667/answer/771252708">https://www.zhihu.com/question/338066667/answer/771252708</a></p>
<p><a href="https://kexue.fm/archives/6418">https://kexue.fm/archives/6418</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>warmup</tag>
        <tag>decay</tag>
        <tag>学习率</tag>
      </tags>
  </entry>
  <entry>
    <title>不要停止预训练实战-Roberta与Albert</title>
    <url>/2022/03/20/2022-03-20-%E9%A2%84%E8%AE%AD%E7%BB%83%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文在LCQMC数据集上，再次对roberta、albert模型进行预训练，详细介绍了预训练的过程并对比了预训练前后的结果。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">roberta</td>
<td style="text-align:center">0.88503</td>
<td style="text-align:center">0.86344</td>
</tr>
<tr>
<td style="text-align:center">albert</td>
<td style="text-align:center">0.85662</td>
<td style="text-align:center">0.84960</td>
</tr>
<tr>
<td style="text-align:center">预训练后roberta</td>
<td style="text-align:center"><strong>0.89343</strong></td>
<td style="text-align:center">0.85328</td>
</tr>
<tr>
<td style="text-align:center">预训练后albert</td>
<td style="text-align:center">0.84958</td>
<td style="text-align:center"><strong>0.85224</strong></td>
</tr>
</tbody>
</table>
</div>
<span id="more"></span>
<h1 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h1><p>根据选取数据集，转为预训练格式数据，完成roberta、albert的预训练，并对比在该数据集上，预训练前后的具体任务指标。</p>
<h1 id="任务数据集"><a href="#任务数据集" class="headerlink" title="任务数据集"></a>任务数据集</h1><p>LCQMC数据集</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">训练集</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">238766</td>
<td style="text-align:center">8802</td>
<td style="text-align:center">12500</td>
</tr>
</tbody>
</table>
</div>
<p>LCQMC数据集的长度分布如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h0gke2qbgrj20hs0dcglw.jpg" alt=""></p>
<h1 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h1><p>代码链接：<a href="https://github.com/447428054/Pretrain/tree/master/LcqmcExample">https://github.com/447428054/Pretrain/tree/master/LcqmcExample</a></p>
<p>预训练环境：tensorflow1.14</p>
<p>预训练代码执行顺序：</p>
<ol>
<li>bash create_pretrain_data_lz.sh</li>
<li>bash pretrain_lz.sh</li>
</ol>
<p>LCQMC微调代码:</p>
<ol>
<li>python task_sentence_similarity_lcqmc_roberta.py</li>
<li>python task_sentence_similarity_lcqmc_albert.py</li>
</ol>
<p>TIPS:</p>
<p>记得修改文件路径</p>
<h1 id="预训练数据生成"><a href="#预训练数据生成" class="headerlink" title="预训练数据生成"></a>预训练数据生成</h1><p>预训练代码读取生成的record，数据处理代码首先读取不同文件，每个文件格式为：<strong>每一行存放一个句子，不同文档之间以空行分割</strong></p>
<p>我们将LCQMC中相似的句子作为一个文档，不相似的分开</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">谁有狂三这张高清的</span><br><span class="line"></span><br><span class="line">这张高清图，谁有</span><br><span class="line"></span><br><span class="line">英雄联盟什么英雄最好</span><br><span class="line">英雄联盟最好英雄是什么</span><br><span class="line"></span><br><span class="line">这是什么意思，被蹭网吗</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="roberta的预训练数据处理"><a href="#roberta的预训练数据处理" class="headerlink" title="roberta的预训练数据处理"></a>roberta的预训练数据处理</h2><ol>
<li><p>每个文件中，一个sentence占一行，不同document之间加一个空行分割<br>[[‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘？’], [‘有’, ‘人’, ‘知’, ‘道’, ‘名’, ‘字’, ‘吗’]]</p>
</li>
<li><p>从一个文档中连续的获得文本，直到达到最大长度。如果是从下一个文档中获得，那么加上一个分隔符.将长度限制修改了，因为lcqmc句子都偏短<br>[‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘？’, ‘有’, ‘人’, ‘知’, ‘道’, ‘名’, ‘字’, ‘吗’]</p>
</li>
<li><p>对于获取之后的文本，进行全词分词： 判断每个字符起始长度3以内的，是否在分词里面，在的话添加##标记<br>[‘有’, ‘##人’, ‘知’, ‘##道’, ‘叫’, ‘什’, ‘##么’, ‘名’, ‘##字’, ‘吗’, ‘[UNK]’, ‘？’, ‘有’, ‘##人’, ‘知’, ‘##道’, ‘名’, ‘##字’, ‘吗’]</p>
</li>
<li><p>对获得的token序列，进行掩码:返回 掩码结果，掩码的位置，掩码的标签<br>[‘[CLS]’, ‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘[MASK]’, ‘有’, ‘人’, ‘[MASK]’, ‘[MASK]’, ‘名’, ‘字’, ‘吗’, ‘[SEP]’]<br>[12, 15, 16]<br>[‘？’, ‘知’, ‘##道’]</p>
</li>
</ol>
<p>run_pretraining.py需要注释TPU的引用<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># *tpu_cluster_resolver* = tf.contrib.cluster_resolver.TPUClusterResolver( # TODO</span><br><span class="line">#       tpu=FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)</span><br></pre></td></tr></table></figure></p>
<h2 id="albert的预训练数据处理"><a href="#albert的预训练数据处理" class="headerlink" title="albert的预训练数据处理"></a>albert的预训练数据处理</h2><ol>
<li><p>每个文件中，一个sentence占一行，不同document之间加一个空行分割<br>[[‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘字’, ‘吗’, ‘[UNK]’, ‘？’], [‘有’, ‘人’, ‘知’, ‘道’, ‘名’, ‘字’, ‘吗’]]</p>
</li>
<li><p>从一个文档中获取sentence,sentece进行全词分词，当长度达到最大长度或者遍历完整个文档了，A[SEP]B 随机分割句子，50%概率交换顺序，得到SOP标签<br>tokenA:[‘有’, ‘##人’, ‘知’, ‘##道’, ‘叫’, ‘什’, ‘##么’, ‘名’, ‘##字’, ‘吗’, ‘[UNK]’, ‘？’]<br>tokenB:[‘有’, ‘##人’, ‘知’, ‘##道’, ‘名’, ‘##字’, ‘吗’]</p>
</li>
</ol>
<p>只有一句话,构不成SOP任务的就continue</p>
<ol>
<li>对获得的token序列，进行掩码:返回 掩码结果，掩码的位置，掩码的标签<br>tokens:[‘[CLS]’, ‘有’, ‘人’, ‘知’, ‘道’, ‘叫’, ‘什’, ‘么’, ‘名’, ‘[MASK]’, ‘吗’, ‘[UNK]’, ‘？’, ‘[SEP]’, ‘[MASK]’, ‘人’, ‘知’, ‘[MASK]’, ‘名’, ‘字’, ‘吗’, ‘[SEP]’]<br>masked_lm_positions:[9, 14, 17]<br>masked_lm_labels:[‘##字’, ‘有’, ‘##道’]<br>is_random_next:False</li>
</ol>
<h1 id="预训练代码"><a href="#预训练代码" class="headerlink" title="预训练代码"></a>预训练代码</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="Roberta"><a href="#Roberta" class="headerlink" title="Roberta"></a>Roberta</h3><p><strong>整个模型结构与BERT相同，整体流程如下：</strong></p>
<ol>
<li><strong>输入的token 经过embedding</strong></li>
<li><strong>再加上token type id 与position embedding</strong></li>
<li><strong>进入transfomer层，每一个transformer又由多头attention、层归一化、残差结构、前馈神经网络构成</strong></li>
<li><strong>获取CLS输出与整个句子的输出</strong></li>
</ol>
<p><strong>整个代码结构跟流程相同：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">with tf.variable_scope(scope, default_name=&quot;bert&quot;):</span><br><span class="line">  with tf.variable_scope(&quot;embeddings&quot;):</span><br><span class="line">    # Perform embedding lookup on the word ids.</span><br><span class="line">    (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        vocab_size=config.vocab_size,</span><br><span class="line">        embedding_size=config.hidden_size,</span><br><span class="line">        initializer_range=config.initializer_range,</span><br><span class="line">        word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    # Add positional embeddings and token type embeddings, then layer</span><br><span class="line">    # normalize and perform dropout.</span><br><span class="line">    self.embedding_output = embedding_postprocessor(</span><br><span class="line">        input_tensor=self.embedding_output,</span><br><span class="line">        use_token_type=True,</span><br><span class="line">        token_type_ids=token_type_ids,</span><br><span class="line">        token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">        token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">        use_position_embeddings=True,</span><br><span class="line">        position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">        initializer_range=config.initializer_range,</span><br><span class="line">        max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">        dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">  with tf.variable_scope(&quot;encoder&quot;):</span><br><span class="line">    # This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span><br><span class="line">    # mask of shape [batch_size, seq_length, seq_length] which is used</span><br><span class="line">    # for the attention scores.</span><br><span class="line">    attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">        input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">    # Run the stacked transformer.</span><br><span class="line">    # `sequence_output` shape = [batch_size, seq_length, hidden_size].</span><br><span class="line">    self.all_encoder_layers = transformer_model(</span><br><span class="line">        input_tensor=self.embedding_output,</span><br><span class="line">        attention_mask=attention_mask,</span><br><span class="line">        hidden_size=config.hidden_size,</span><br><span class="line">        num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">        num_attention_heads=config.num_attention_heads,</span><br><span class="line">        intermediate_size=config.intermediate_size,</span><br><span class="line">        intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">        hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">        attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">        initializer_range=config.initializer_range,</span><br><span class="line">        do_return_all_layers=True)</span><br><span class="line"></span><br><span class="line">  self.sequence_output = self.all_encoder_layers[-1] # [batch_size, seq_length, hidden_size]</span><br><span class="line">  # The &quot;pooler&quot; converts the encoded sequence tensor of shape</span><br><span class="line">  # [batch_size, seq_length, hidden_size] to a tensor of shape</span><br><span class="line">  # [batch_size, hidden_size]. This is necessary for segment-level</span><br><span class="line">  # (or segment-pair-level) classification tasks where we need a fixed</span><br><span class="line">  # dimensional representation of the segment.</span><br><span class="line">  with tf.variable_scope(&quot;pooler&quot;):</span><br><span class="line">    # We &quot;pool&quot; the model by simply taking the hidden state corresponding</span><br><span class="line">    # to the first token. We assume that this has been pre-trained</span><br><span class="line">    first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)</span><br><span class="line">    self.pooled_output = tf.layers.dense(</span><br><span class="line">        first_token_tensor,</span><br><span class="line">        config.hidden_size,</span><br><span class="line">        activation=tf.tanh,</span><br><span class="line">        kernel_initializer=create_initializer(config.initializer_range))</span><br></pre></td></tr></table></figure>
<h4 id="embedding-lookup"><a href="#embedding-lookup" class="headerlink" title="embedding_lookup"></a>embedding_lookup</h4><p><strong>与常规神经网络中词嵌入类似</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def embedding_lookup(input_ids,</span><br><span class="line">                     vocab_size,</span><br><span class="line">                     embedding_size=128,</span><br><span class="line">                     initializer_range=0.02,</span><br><span class="line">                     word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">                     use_one_hot_embeddings=False):</span><br><span class="line">  &quot;&quot;&quot;Looks up words embeddings for id tensor.</span><br><span class="line">  Args:</span><br><span class="line">    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span><br><span class="line">      ids.</span><br><span class="line">    vocab_size: int. Size of the embedding vocabulary.</span><br><span class="line">    embedding_size: int. Width of the word embeddings.</span><br><span class="line">    initializer_range: float. Embedding initialization range.</span><br><span class="line">    word_embedding_name: string. Name of the embedding table.</span><br><span class="line">    use_one_hot_embeddings: bool. If True, use one-hot method for word</span><br><span class="line">      embeddings. If False, use `tf.gather()`.</span><br><span class="line">  Returns:</span><br><span class="line">    float Tensor of shape [batch_size, seq_length, embedding_size].</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  # This function assumes that the input is of shape [batch_size, seq_length,</span><br><span class="line">  # num_inputs].</span><br><span class="line">  #</span><br><span class="line">  # If the input is a 2D tensor of shape [batch_size, seq_length], we</span><br><span class="line">  # reshape to [batch_size, seq_length, 1].</span><br><span class="line">  if input_ids.shape.ndims == 2:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[-1])</span><br><span class="line"></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [-1])</span><br><span class="line">  if use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  else:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[0:-1] + [input_shape[-1] * embedding_size])</span><br><span class="line">  return (output, embedding_table)</span><br></pre></td></tr></table></figure>
<h4 id="embedding-postprocessor"><a href="#embedding-postprocessor" class="headerlink" title="embedding_postprocessor"></a>embedding_postprocessor</h4><p><strong>加上token type id与可学习的position id 词嵌入，postprocessor指在embedding之后进行层归一化与dropout</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def embedding_postprocessor(input_tensor,</span><br><span class="line">                            use_token_type=False,</span><br><span class="line">                            token_type_ids=None,</span><br><span class="line">                            token_type_vocab_size=16,</span><br><span class="line">                            token_type_embedding_name=&quot;token_type_embeddings&quot;,</span><br><span class="line">                            use_position_embeddings=True,</span><br><span class="line">                            position_embedding_name=&quot;position_embeddings&quot;,</span><br><span class="line">                            initializer_range=0.02,</span><br><span class="line">                            max_position_embeddings=512,</span><br><span class="line">                            dropout_prob=0.1):</span><br><span class="line">  &quot;&quot;&quot;Performs various post-processing on a word embedding tensor.</span><br><span class="line">  Args:</span><br><span class="line">    input_tensor: float Tensor of shape [batch_size, seq_length,</span><br><span class="line">      embedding_size].</span><br><span class="line">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span><br><span class="line">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span><br><span class="line">      Must be specified if `use_token_type` is True.</span><br><span class="line">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span><br><span class="line">    token_type_embedding_name: string. The name of the embedding table variable</span><br><span class="line">      for token type ids.</span><br><span class="line">    use_position_embeddings: bool. Whether to add position embeddings for the</span><br><span class="line">      position of each token in the sequence.</span><br><span class="line">    position_embedding_name: string. The name of the embedding table variable</span><br><span class="line">      for positional embeddings.</span><br><span class="line">    initializer_range: float. Range of the weight initialization.</span><br><span class="line">    max_position_embeddings: int. Maximum sequence length that might ever be</span><br><span class="line">      used with this model. This can be longer than the sequence length of</span><br><span class="line">      input_tensor, but cannot be shorter.</span><br><span class="line">    dropout_prob: float. Dropout probability applied to the final output tensor.</span><br><span class="line">  Returns:</span><br><span class="line">    float tensor with same shape as `input_tensor`.</span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: One of the tensor shapes or input values is invalid.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">  batch_size = input_shape[0]</span><br><span class="line">  seq_length = input_shape[1]</span><br><span class="line">  width = input_shape[2]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  if use_token_type:</span><br><span class="line">    if token_type_ids is None:</span><br><span class="line">      raise ValueError(&quot;`token_type_ids` must be specified if&quot;</span><br><span class="line">                       &quot;`use_token_type` is True.&quot;)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    # This vocab will be small so we always do one-hot here, since it is always</span><br><span class="line">    # faster for a small vocabulary.</span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [-1])</span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  if use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    with tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      # Since the position embedding table is a learned variable, we create it</span><br><span class="line">      # using a (long) sequence length `max_position_embeddings`. The actual</span><br><span class="line">      # sequence length might be shorter than this, for faster training of</span><br><span class="line">      # tasks that do not have long sequences.</span><br><span class="line">      #</span><br><span class="line">      # So `full_position_embeddings` is effectively an embedding table</span><br><span class="line">      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span><br><span class="line">      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span><br><span class="line">      # perform a slice.</span><br><span class="line">      position_embeddings = tf.slice(full_position_embeddings, [0, 0],</span><br><span class="line">                                     [seq_length, -1])</span><br><span class="line">      num_dims = len(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      # Only the last two dimensions are relevant (`seq_length` and `width`), so</span><br><span class="line">      # we broadcast among the first dimensions, which is typically just</span><br><span class="line">      # the batch size.</span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      for _ in range(num_dims - 2):</span><br><span class="line">        position_broadcast_shape.append(1)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  return </span><br></pre></td></tr></table></figure>
<h4 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h4><p><strong>对于每一个Transformer结构如下：</strong></p>
<ol>
<li><strong>多头attention</strong></li>
<li><strong>拼接多头输出，经过隐藏层映射</strong></li>
<li><strong>经过dropout+残差+层归一化</strong></li>
<li><strong>前馈神经网络</strong></li>
<li><strong>经过dropout+残差+层归一化</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def transformer_model(input_tensor,</span><br><span class="line">                      attention_mask=None,</span><br><span class="line">                      hidden_size=768,</span><br><span class="line">                      num_hidden_layers=12,</span><br><span class="line">                      num_attention_heads=12,</span><br><span class="line">                      intermediate_size=3072,</span><br><span class="line">                      intermediate_act_fn=gelu,</span><br><span class="line">                      hidden_dropout_prob=0.1,</span><br><span class="line">                      attention_probs_dropout_prob=0.1,</span><br><span class="line">                      initializer_range=0.02,</span><br><span class="line">                      do_return_all_layers=False):</span><br><span class="line">  &quot;&quot;&quot;Multi-headed, multi-layer Transformer from &quot;Attention is All You Need&quot;.</span><br><span class="line">  This is almost an exact implementation of the original Transformer encoder.</span><br><span class="line">  See the original paper:</span><br><span class="line">  https://arxiv.org/abs/1706.03762</span><br><span class="line">  Also see:</span><br><span class="line">  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span><br><span class="line">  Args:</span><br><span class="line">    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span><br><span class="line">    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span><br><span class="line">      seq_length], with 1 for positions that can be attended to and 0 in</span><br><span class="line">      positions that should not be.</span><br><span class="line">    hidden_size: int. Hidden size of the Transformer.</span><br><span class="line">    num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span><br><span class="line">    num_attention_heads: int. Number of attention heads in the Transformer.</span><br><span class="line">    intermediate_size: int. The size of the &quot;intermediate&quot; (a.k.a., feed</span><br><span class="line">      forward) layer.</span><br><span class="line">    intermediate_act_fn: function. The non-linear activation function to apply</span><br><span class="line">      to the output of the intermediate/feed-forward layer.</span><br><span class="line">    hidden_dropout_prob: float. Dropout probability for the hidden layers.</span><br><span class="line">    attention_probs_dropout_prob: float. Dropout probability of the attention</span><br><span class="line">      probabilities.</span><br><span class="line">    initializer_range: float. Range of the initializer (stddev of truncated</span><br><span class="line">      normal).</span><br><span class="line">    do_return_all_layers: Whether to also return all layers or just the final</span><br><span class="line">      layer.</span><br><span class="line">  Returns:</span><br><span class="line">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span><br><span class="line">    hidden layer of the Transformer.</span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: A Tensor shape or parameter is invalid.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  if hidden_size % num_attention_heads != 0:</span><br><span class="line">    raise ValueError(</span><br><span class="line">        &quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">        &quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">  batch_size = input_shape[0]</span><br><span class="line">  seq_length = input_shape[1]</span><br><span class="line">  input_width = input_shape[2]</span><br><span class="line"></span><br><span class="line">  # The Transformer performs sum residuals on all layers so the input needs</span><br><span class="line">  # to be the same as the hidden size.</span><br><span class="line">  if input_width != hidden_size:</span><br><span class="line">    raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  # We keep the representation as a 2D tensor to avoid re-shaping it back and</span><br><span class="line">  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span><br><span class="line">  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span><br><span class="line">  # help the optimizer.</span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  for layer_idx in range(num_hidden_layers):</span><br><span class="line">    with tf.variable_scope(&quot;layer_%d&quot; % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      with tf.variable_scope(&quot;attention&quot;):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=True,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = None</span><br><span class="line">        if len(attention_heads) == 1:</span><br><span class="line">          attention_output = attention_heads[0]</span><br><span class="line">        else:</span><br><span class="line">          # In the case where we have other sequences, we just concatenate</span><br><span class="line">          # them to the self-attention head before the projection.</span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=-1)</span><br><span class="line"></span><br><span class="line">        # Run a linear projection of `hidden_size` then add a residual</span><br><span class="line">        # with `layer_input`.</span><br><span class="line">        with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      # The activation is only applied to the &quot;intermediate&quot; hidden layer.</span><br><span class="line">      with tf.variable_scope(&quot;intermediate&quot;):</span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      # Down-project back to `hidden_size` then add the residual.</span><br><span class="line">      with tf.variable_scope(&quot;output&quot;):</span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  if do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    for layer_output in all_layer_outputs:</span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    return final_outputs</span><br><span class="line">  else:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    return final_output</span><br></pre></td></tr></table></figure>
<p><strong>其中，多头attention结构如下：</strong></p>
<ol>
<li><p><strong>针对输入向量与输出向量，生成Q、K、V向量，隐藏层维度为num heads * head size</strong></p>
<p><strong>self-attention中 输入输出来源相同。Q来源于输入向量，V来源于输出向量。“目的在于计算输入向量 对于 不同输出的 权重</strong>”</p>
<p><strong>若需要对注意力进行掩码，对得分减去很大的值，最终softmax之后得到的影响就非常小</strong></p>
</li>
<li><p><strong>针对Q，K 进行缩放点积计算，再经过softmax</strong></p>
</li>
<li><p><strong>将第二步结果与V相乘得到上下文向量</strong></p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def attention_layer(from_tensor,</span><br><span class="line">                    to_tensor,</span><br><span class="line">                    attention_mask=None,</span><br><span class="line">                    num_attention_heads=1,</span><br><span class="line">                    size_per_head=512,</span><br><span class="line">                    query_act=None,</span><br><span class="line">                    key_act=None,</span><br><span class="line">                    value_act=None,</span><br><span class="line">                    attention_probs_dropout_prob=0.0,</span><br><span class="line">                    initializer_range=0.02,</span><br><span class="line">                    do_return_2d_tensor=False,</span><br><span class="line">                    batch_size=None,</span><br><span class="line">                    from_seq_length=None,</span><br><span class="line">                    to_seq_length=None):</span><br><span class="line">  &quot;&quot;&quot;Performs multi-headed attention from `from_tensor` to `to_tensor`.</span><br><span class="line">  This is an implementation of multi-headed attention based on &quot;Attention</span><br><span class="line">  is all you Need&quot;. If `from_tensor` and `to_tensor` are the same, then</span><br><span class="line">  this is self-attention. Each timestep in `from_tensor` attends to the</span><br><span class="line">  corresponding sequence in `to_tensor`, and returns a fixed-with vector.</span><br><span class="line">  This function first projects `from_tensor` into a &quot;query&quot; tensor and</span><br><span class="line">  `to_tensor` into &quot;key&quot; and &quot;value&quot; tensors. These are (effectively) a list</span><br><span class="line">  of tensors of length `num_attention_heads`, where each tensor is of shape</span><br><span class="line">  [batch_size, seq_length, size_per_head].</span><br><span class="line">  Then, the query and key tensors are dot-producted and scaled. These are</span><br><span class="line">  softmaxed to obtain attention probabilities. The value tensors are then</span><br><span class="line">  interpolated by these probabilities, then concatenated back to a single</span><br><span class="line">  tensor and returned.</span><br><span class="line">  In practice, the multi-headed attention are done with transposes and</span><br><span class="line">  reshapes rather than actual separate tensors.</span><br><span class="line">  Args:</span><br><span class="line">    from_tensor: float Tensor of shape [batch_size, from_seq_length,</span><br><span class="line">      from_width].</span><br><span class="line">    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].</span><br><span class="line">    attention_mask: (optional) int32 Tensor of shape [batch_size,</span><br><span class="line">      from_seq_length, to_seq_length]. The values should be 1 or 0. The</span><br><span class="line">      attention scores will effectively be set to -infinity for any positions in</span><br><span class="line">      the mask that are 0, and will be unchanged for positions that are 1.</span><br><span class="line">    num_attention_heads: int. Number of attention heads.</span><br><span class="line">    size_per_head: int. Size of each attention head.</span><br><span class="line">    query_act: (optional) Activation function for the query transform.</span><br><span class="line">    key_act: (optional) Activation function for the key transform.</span><br><span class="line">    value_act: (optional) Activation function for the value transform.</span><br><span class="line">    attention_probs_dropout_prob: (optional) float. Dropout probability of the</span><br><span class="line">      attention probabilities.</span><br><span class="line">    initializer_range: float. Range of the weight initializer.</span><br><span class="line">    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size</span><br><span class="line">      * from_seq_length, num_attention_heads * size_per_head]. If False, the</span><br><span class="line">      output will be of shape [batch_size, from_seq_length, num_attention_heads</span><br><span class="line">      * size_per_head].</span><br><span class="line">    batch_size: (Optional) int. If the input is 2D, this might be the batch size</span><br><span class="line">      of the 3D version of the `from_tensor` and `to_tensor`.</span><br><span class="line">    from_seq_length: (Optional) If the input is 2D, this might be the seq length</span><br><span class="line">      of the 3D version of the `from_tensor`.</span><br><span class="line">    to_seq_length: (Optional) If the input is 2D, this might be the seq length</span><br><span class="line">      of the 3D version of the `to_tensor`.</span><br><span class="line">  Returns:</span><br><span class="line">    float Tensor of shape [batch_size, from_seq_length,</span><br><span class="line">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span><br><span class="line">      true, this will be of shape [batch_size * from_seq_length,</span><br><span class="line">      num_attention_heads * size_per_head]).</span><br><span class="line">  Raises:</span><br><span class="line">    ValueError: Any of the arguments or tensor shapes are invalid.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,</span><br><span class="line">                           seq_length, width):</span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])</span><br><span class="line">    return output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])</span><br><span class="line"></span><br><span class="line">  if len(from_shape) != len(to_shape):</span><br><span class="line">    raise ValueError(</span><br><span class="line">        &quot;The rank of `from_tensor` must match the rank of `to_tensor`.&quot;)</span><br><span class="line"></span><br><span class="line">  if len(from_shape) == 3:</span><br><span class="line">    batch_size = from_shape[0]</span><br><span class="line">    from_seq_length = from_shape[1]</span><br><span class="line">    to_seq_length = to_shape[1]</span><br><span class="line">  elif len(from_shape) == 2:</span><br><span class="line">    if (batch_size is None or from_seq_length is None or to_seq_length is None):</span><br><span class="line">      raise ValueError(</span><br><span class="line">          &quot;When passing in rank 2 tensors to attention_layer, the values &quot;</span><br><span class="line">          &quot;for `batch_size`, `from_seq_length`, and `to_seq_length` &quot;</span><br><span class="line">          &quot;must all be specified.&quot;)</span><br><span class="line"></span><br><span class="line">  # Scalar dimensions referenced here:</span><br><span class="line">  #   B = batch size (number of sequences)</span><br><span class="line">  #   F = `from_tensor` sequence length</span><br><span class="line">  #   T = `to_tensor` sequence length</span><br><span class="line">  #   N = `num_attention_heads`</span><br><span class="line">  #   H = `size_per_head`</span><br><span class="line"></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">  # `query_layer` = [B*F, N*H]</span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=&quot;query&quot;,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # `key_layer` = [B*T, N*H]</span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=&quot;key&quot;,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # `value_layer` = [B*T, N*H]</span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=&quot;value&quot;,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  # `query_layer` = [B, N, F, H]</span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  # `key_layer` = [B, N, T, H]</span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  # Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw</span><br><span class="line">  # attention scores.</span><br><span class="line">  # `attention_scores` = [B, N, F, T]</span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)</span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 1.0 / math.sqrt(float(size_per_head)))</span><br><span class="line"></span><br><span class="line">  if attention_mask is not None:</span><br><span class="line">    # `attention_mask` = [B, 1, F, T]</span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[1])</span><br><span class="line"></span><br><span class="line">    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span><br><span class="line">    # masked positions, this operation will create a tensor which is 0.0 for</span><br><span class="line">    # positions we want to attend and -10000.0 for masked positions.</span><br><span class="line">    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0</span><br><span class="line"></span><br><span class="line">    # Since we are adding it to the raw scores before the softmax, this is</span><br><span class="line">    # effectively the same as removing these entirely.</span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  # Normalize the attention scores to probabilities.</span><br><span class="line">  # `attention_probs` = [B, N, F, T]</span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  # This is actually dropping out entire tokens to attend to, which might</span><br><span class="line">  # seem a bit unusual, but is taken from the original Transformer paper.</span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  # `value_layer` = [B, T, N, H]</span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  # `value_layer` = [B, N, T, H]</span><br><span class="line">  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])</span><br><span class="line"></span><br><span class="line">  # `context_layer` = [B, N, F, H]</span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  # `context_layer` = [B, F, N, H]</span><br><span class="line">  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])</span><br><span class="line"></span><br><span class="line">  if do_return_2d_tensor:</span><br><span class="line">    # `context_layer` = [B*F, N*H]</span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  else:</span><br><span class="line">    # `context_layer` = [B, F, N*H]</span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  return context_layer</span><br></pre></td></tr></table></figure>
<h3 id="Albert"><a href="#Albert" class="headerlink" title="Albert"></a>Albert</h3><p><strong>Albert整体结构与BERT相似，改动有三点：</strong></p>
<ol>
<li><p><strong>词嵌入层由Vocab <em> Hidden 分解为 Vocab </em> Embedding + Embedding * Hidden</strong></p>
</li>
<li><p><strong>跨层参数共享，主要是全连接层与注意力层的共享</strong></p>
<p><strong>Tensorflow 中 通过get variable 与 变量域Variable Scope完成参数共享</strong></p>
</li>
<li><p><strong>段落连续的SOP任务替换原先NSP任务，SOP任务中文档连续语句为正例，调换顺序后为负例</strong></p>
</li>
</ol>
<p><strong>代码中同时更新了层归一化的顺序：pre-Layer Normalization can converge fast and better. check paper: ON LAYER NORMALIZATION IN THE TRANSFORMER ARCHITECTURE</strong></p>
<p><strong>模型结构改动主要涉及前两点，接下来我们从代码层面来看这些改动：</strong></p>
<h4 id="embedding-lookup-factorized"><a href="#embedding-lookup-factorized" class="headerlink" title="embedding_lookup_factorized"></a>embedding_lookup_factorized</h4><p><strong>主要拆分为两次矩阵运算，embedding size在中间过渡</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def embedding_lookup_factorized(input_ids, # Factorized embedding parameterization provide by albert</span><br><span class="line">                     vocab_size,</span><br><span class="line">                     hidden_size,</span><br><span class="line">                     embedding_size=128,</span><br><span class="line">                     initializer_range=0.02,</span><br><span class="line">                     word_embedding_name=&quot;word_embeddings&quot;,</span><br><span class="line">                     use_one_hot_embeddings=False):</span><br><span class="line">    &quot;&quot;&quot;Looks up words embeddings for id tensor, but in a factorized style followed by albert. it is used to reduce much percentage of parameters previous exists.</span><br><span class="line">       Check &quot;Factorized embedding parameterization&quot; session in the paper.</span><br><span class="line">     Args:</span><br><span class="line">       input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span><br><span class="line">         ids.</span><br><span class="line">       vocab_size: int. Size of the embedding vocabulary.</span><br><span class="line">       embedding_size: int. Width of the word embeddings.</span><br><span class="line">       initializer_range: float. Embedding initialization range.</span><br><span class="line">       word_embedding_name: string. Name of the embedding table.</span><br><span class="line">       use_one_hot_embeddings: bool. If True, use one-hot method for word</span><br><span class="line">         embeddings. If False, use `tf.gather()`.</span><br><span class="line">     Returns:</span><br><span class="line">       float Tensor of shape [batch_size, seq_length, embedding_size].</span><br><span class="line">     &quot;&quot;&quot;</span><br><span class="line">    # This function assumes that the input is of shape [batch_size, seq_length,</span><br><span class="line">    # num_inputs].</span><br><span class="line">    #</span><br><span class="line">    # If the input is a 2D tensor of shape [batch_size, seq_length], we</span><br><span class="line">    # reshape to [batch_size, seq_length, 1].</span><br><span class="line"></span><br><span class="line">    # 1.first project one-hot vectors into a lower dimensional embedding space of size E</span><br><span class="line">    print(&quot;embedding_lookup_factorized. factorized embedding parameterization is used.&quot;)</span><br><span class="line">    if input_ids.shape.ndims == 2:</span><br><span class="line">        input_ids = tf.expand_dims(input_ids, axis=[-1])  # shape of input_ids is:[ batch_size, seq_length, 1]</span><br><span class="line"></span><br><span class="line">    embedding_table = tf.get_variable(  # [vocab_size, embedding_size]</span><br><span class="line">        name=word_embedding_name,</span><br><span class="line">        shape=[vocab_size, embedding_size],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">    flat_input_ids = tf.reshape(input_ids, [-1])  # one rank. shape as (batch_size * sequence_length,)</span><br><span class="line">    if use_one_hot_embeddings:</span><br><span class="line">        one_hot_input_ids = tf.one_hot(flat_input_ids,depth=vocab_size)  # one_hot_input_ids=[batch_size * sequence_length,vocab_size]</span><br><span class="line">        output_middle = tf.matmul(one_hot_input_ids, embedding_table)  # output=[batch_size * sequence_length,embedding_size]</span><br><span class="line">    else:</span><br><span class="line">        output_middle = tf.gather(embedding_table,flat_input_ids)  # [vocab_size, embedding_size]*[batch_size * sequence_length,]---&gt;[batch_size * sequence_length,embedding_size]</span><br><span class="line"></span><br><span class="line">    # 2. project vector(output_middle) to the hidden space</span><br><span class="line">    project_variable = tf.get_variable(  # [embedding_size, hidden_size]</span><br><span class="line">        name=word_embedding_name+&quot;_2&quot;,</span><br><span class="line">        shape=[embedding_size, hidden_size],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    output = tf.matmul(output_middle, project_variable) # ([batch_size * sequence_length, embedding_size] * [embedding_size, hidden_size])---&gt;[batch_size * sequence_length, hidden_size]</span><br><span class="line">    # reshape back to 3 rank</span><br><span class="line">    input_shape = get_shape_list(input_ids)  # input_shape=[ batch_size, seq_length, 1]</span><br><span class="line">    batch_size, sequene_length, _=input_shape</span><br><span class="line">    output = tf.reshape(output, (batch_size,sequene_length,hidden_size))  # output=[batch_size, sequence_length, hidden_size]</span><br><span class="line">    return (output, embedding_table, project_variable)</span><br></pre></td></tr></table></figure>
<h4 id="prelln-transformer-model"><a href="#prelln-transformer-model" class="headerlink" title="prelln_transformer_model"></a>prelln_transformer_model</h4><p>将Layer Norm放在Attention前面，使训练过程收敛的更快更好。使用Tensorflow的变量域，完成参数共享。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def prelln_transformer_model(input_tensor,</span><br><span class="line">						attention_mask=None,</span><br><span class="line">						hidden_size=768,</span><br><span class="line">						num_hidden_layers=12,</span><br><span class="line">						num_attention_heads=12,</span><br><span class="line">						intermediate_size=3072,</span><br><span class="line">						intermediate_act_fn=gelu,</span><br><span class="line">						hidden_dropout_prob=0.1,</span><br><span class="line">						attention_probs_dropout_prob=0.1,</span><br><span class="line">						initializer_range=0.02,</span><br><span class="line">						do_return_all_layers=False,</span><br><span class="line">						shared_type=&#x27;all&#x27;, # None,</span><br><span class="line">						adapter_fn=None):</span><br><span class="line">	&quot;&quot;&quot;Multi-headed, multi-layer Transformer from &quot;Attention is All You Need&quot;.</span><br><span class="line">	This is almost an exact implementation of the original Transformer encoder.</span><br><span class="line">	See the original paper:</span><br><span class="line">	https://arxiv.org/abs/1706.03762</span><br><span class="line">	Also see:</span><br><span class="line">	https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span><br><span class="line">	Args:</span><br><span class="line">		input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span><br><span class="line">		attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span><br><span class="line">			seq_length], with 1 for positions that can be attended to and 0 in</span><br><span class="line">			positions that should not be.</span><br><span class="line">		hidden_size: int. Hidden size of the Transformer.</span><br><span class="line">		num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span><br><span class="line">		num_attention_heads: int. Number of attention heads in the Transformer.</span><br><span class="line">		intermediate_size: int. The size of the &quot;intermediate&quot; (a.k.a., feed</span><br><span class="line">			forward) layer.</span><br><span class="line">		intermediate_act_fn: function. The non-linear activation function to apply</span><br><span class="line">			to the output of the intermediate/feed-forward layer.</span><br><span class="line">		hidden_dropout_prob: float. Dropout probability for the hidden layers.</span><br><span class="line">		attention_probs_dropout_prob: float. Dropout probability of the attention</span><br><span class="line">			probabilities.</span><br><span class="line">		initializer_range: float. Range of the initializer (stddev of truncated</span><br><span class="line">			normal).</span><br><span class="line">		do_return_all_layers: Whether to also return all layers or just the final</span><br><span class="line">			layer.</span><br><span class="line">	Returns:</span><br><span class="line">		float Tensor of shape [batch_size, seq_length, hidden_size], the final</span><br><span class="line">		hidden layer of the Transformer.</span><br><span class="line">	Raises:</span><br><span class="line">		ValueError: A Tensor shape or parameter is invalid.</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	if hidden_size % num_attention_heads != 0:</span><br><span class="line">		raise ValueError(</span><br><span class="line">				&quot;The hidden size (%d) is not a multiple of the number of attention &quot;</span><br><span class="line">				&quot;heads (%d)&quot; % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">	attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line"></span><br><span class="line">	input_shape = bert_utils.get_shape_list(input_tensor, expected_rank=3)</span><br><span class="line">	batch_size = input_shape[0]</span><br><span class="line">	seq_length = input_shape[1]</span><br><span class="line">	input_width = input_shape[2]</span><br><span class="line"></span><br><span class="line">	# The Transformer performs sum residuals on all layers so the input needs</span><br><span class="line">	# to be the same as the hidden size.</span><br><span class="line">	if input_width != hidden_size:</span><br><span class="line">		raise ValueError(&quot;The width of the input tensor (%d) != hidden size (%d)&quot; %</span><br><span class="line">										 (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">	# We keep the representation as a 2D tensor to avoid re-shaping it back and</span><br><span class="line">	# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span><br><span class="line">	# the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span><br><span class="line">	# help the optimizer.</span><br><span class="line">	prev_output = bert_utils.reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">	all_layer_outputs = []</span><br><span class="line"></span><br><span class="line">	def layer_scope(idx, shared_type):</span><br><span class="line">		if shared_type == &#x27;all&#x27;:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_shared&quot;,</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention&#x27;,</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate&#x27;,</span><br><span class="line">				&#x27;output&#x27;:&#x27;output&#x27;</span><br><span class="line">			&#125;</span><br><span class="line">		elif shared_type == &#x27;attention&#x27;:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_shared&quot;,</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention&#x27;,</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate_&#123;&#125;&#x27;.format(idx),</span><br><span class="line">				&#x27;output&#x27;:&#x27;output_&#123;&#125;&#x27;.format(idx)</span><br><span class="line">			&#125;</span><br><span class="line">		elif shared_type == &#x27;ffn&#x27;:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_shared&quot;,</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention_&#123;&#125;&#x27;.format(idx),</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate&#x27;,</span><br><span class="line">				&#x27;output&#x27;:&#x27;output&#x27;</span><br><span class="line">			&#125;</span><br><span class="line">		else:</span><br><span class="line">			tmp = &#123;</span><br><span class="line">				&quot;layer&quot;:&quot;layer_&#123;&#125;&quot;.format(idx),</span><br><span class="line">				&#x27;attention&#x27;:&#x27;attention&#x27;,</span><br><span class="line">				&#x27;intermediate&#x27;:&#x27;intermediate&#x27;,</span><br><span class="line">				&#x27;output&#x27;:&#x27;output&#x27;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">		return tmp</span><br><span class="line"></span><br><span class="line">	all_layer_outputs = []</span><br><span class="line"></span><br><span class="line">	for layer_idx in range(num_hidden_layers):</span><br><span class="line"></span><br><span class="line">		idx_scope = layer_scope(layer_idx, shared_type)</span><br><span class="line"></span><br><span class="line">		with tf.variable_scope(idx_scope[&#x27;layer&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">			layer_input = prev_output</span><br><span class="line"></span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;attention&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				attention_heads = []</span><br><span class="line"></span><br><span class="line">				with tf.variable_scope(&quot;output&quot;, reuse=tf.AUTO_REUSE):</span><br><span class="line">					layer_input_pre = layer_norm(layer_input)</span><br><span class="line"></span><br><span class="line">				with tf.variable_scope(&quot;self&quot;):</span><br><span class="line">					attention_head = attention_layer(</span><br><span class="line">							from_tensor=layer_input_pre,</span><br><span class="line">							to_tensor=layer_input_pre,</span><br><span class="line">							attention_mask=attention_mask,</span><br><span class="line">							num_attention_heads=num_attention_heads,</span><br><span class="line">							size_per_head=attention_head_size,</span><br><span class="line">							attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">							initializer_range=initializer_range,</span><br><span class="line">							do_return_2d_tensor=True,</span><br><span class="line">							batch_size=batch_size,</span><br><span class="line">							from_seq_length=seq_length,</span><br><span class="line">							to_seq_length=seq_length)</span><br><span class="line">					attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">				attention_output = None</span><br><span class="line">				if len(attention_heads) == 1:</span><br><span class="line">					attention_output = attention_heads[0]</span><br><span class="line">				else:</span><br><span class="line">					# In the case where we have other sequences, we just concatenate</span><br><span class="line">					# them to the self-attention head before the projection.</span><br><span class="line">					attention_output = tf.concat(attention_heads, axis=-1)</span><br><span class="line"></span><br><span class="line">				# Run a linear projection of `hidden_size` then add a residual</span><br><span class="line">				# with `layer_input`.</span><br><span class="line">				with tf.variable_scope(&quot;output&quot;, reuse=tf.AUTO_REUSE):</span><br><span class="line">					attention_output = tf.layers.dense(</span><br><span class="line">							attention_output,</span><br><span class="line">							hidden_size,</span><br><span class="line">							kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">					attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">					# attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line">					attention_output = attention_output + layer_input</span><br><span class="line"></span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;output&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				attention_output_pre = layer_norm(attention_output)</span><br><span class="line"></span><br><span class="line">			# The activation is only applied to the &quot;intermediate&quot; hidden layer.</span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;intermediate&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				intermediate_output = tf.layers.dense(</span><br><span class="line">						attention_output_pre,</span><br><span class="line">						intermediate_size,</span><br><span class="line">						activation=intermediate_act_fn,</span><br><span class="line">						kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">			# Down-project back to `hidden_size` then add the residual.</span><br><span class="line">			with tf.variable_scope(idx_scope[&#x27;output&#x27;], reuse=tf.AUTO_REUSE):</span><br><span class="line">				layer_output = tf.layers.dense(</span><br><span class="line">						intermediate_output,</span><br><span class="line">						hidden_size,</span><br><span class="line">						kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">				layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">				# layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">				layer_output = layer_output + attention_output</span><br><span class="line">				prev_output = layer_output</span><br><span class="line">				all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">	if do_return_all_layers:</span><br><span class="line">		final_outputs = []</span><br><span class="line">		for layer_output in all_layer_outputs:</span><br><span class="line">			final_output = bert_utils.reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">			final_outputs.append(final_output)</span><br><span class="line">		return final_outputs</span><br><span class="line">	else:</span><br><span class="line">		final_output = bert_utils.reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">		return final_output</span><br></pre></td></tr></table></figure>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="MLM"><a href="#MLM" class="headerlink" title="MLM"></a>MLM</h3><p><strong>主要流程为：</strong></p>
<ol>
<li><strong>提取模型输出中mask position位置的向量</strong></li>
<li><strong>经过变换 每一个输出vocab size大小</strong></li>
<li><strong>与标签计算交叉熵损失</strong></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_masked_lm_output(albert_config, input_tensor, output_weights, positions,</span><br><span class="line">                         label_ids, label_weights):</span><br><span class="line">  &quot;&quot;&quot;Get loss and log probs for the masked LM.&quot;&quot;&quot;</span><br><span class="line">  input_tensor = gather_indexes(input_tensor, positions)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  with tf.variable_scope(&quot;cls/predictions&quot;):</span><br><span class="line">    # We apply one more non-linear transformation before the output layer.</span><br><span class="line">    # This matrix is not used after pre-training.</span><br><span class="line">    with tf.variable_scope(&quot;transform&quot;):</span><br><span class="line">      input_tensor = tf.layers.dense(</span><br><span class="line">          input_tensor,</span><br><span class="line">          units=albert_config.embedding_size,</span><br><span class="line">          activation=modeling.get_activation(albert_config.hidden_act),</span><br><span class="line">          kernel_initializer=modeling.create_initializer(</span><br><span class="line">              albert_config.initializer_range))</span><br><span class="line">      input_tensor = modeling.layer_norm(input_tensor)</span><br><span class="line"></span><br><span class="line">    # The output weights are the same as the input embeddings, but there is</span><br><span class="line">    # an output-only bias for each token.</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        &quot;output_bias&quot;,</span><br><span class="line">        shape=[albert_config.vocab_size],</span><br><span class="line">        initializer=tf.zeros_initializer())</span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=-1)</span><br><span class="line"></span><br><span class="line">    label_ids = tf.reshape(label_ids, [-1])</span><br><span class="line">    label_weights = tf.reshape(label_weights, [-1])</span><br><span class="line"></span><br><span class="line">    one_hot_labels = tf.one_hot(</span><br><span class="line">        label_ids, depth=albert_config.vocab_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    # The `positions` tensor might be zero-padded (if the sequence is too</span><br><span class="line">    # short to have the maximum number of predictions). The `label_weights`</span><br><span class="line">    # tensor has a value of 1.0 for every real prediction and 0.0 for the</span><br><span class="line">    # padding predictions.</span><br><span class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])</span><br><span class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</span><br><span class="line">    denominator = tf.reduce_sum(label_weights) + 1e-5</span><br><span class="line">    loss = numerator / denominator</span><br><span class="line"></span><br><span class="line">  return (loss, per_example_loss, log_probs)</span><br></pre></td></tr></table></figure>
<h3 id="SOP"><a href="#SOP" class="headerlink" title="SOP"></a>SOP</h3><p>主要流程为：</p>
<ol>
<li>模型输出向量 转换为 输出为2维的向量</li>
<li>与标签计算交叉熵损失</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_sentence_order_output(albert_config, input_tensor, labels):</span><br><span class="line">  &quot;&quot;&quot;Get loss and log probs for the next sentence prediction.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  # Simple binary classification. Note that 0 is &quot;next sentence&quot; and 1 is</span><br><span class="line">  # &quot;random sentence&quot;. This weight matrix is not used after pre-training.</span><br><span class="line">  with tf.variable_scope(&quot;cls/seq_relationship&quot;):</span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        &quot;output_weights&quot;,</span><br><span class="line">        shape=[2, albert_config.hidden_size],</span><br><span class="line">        initializer=modeling.create_initializer(</span><br><span class="line">            albert_config.initializer_range))</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        &quot;output_bias&quot;, shape=[2], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=True)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=-1)</span><br><span class="line">    labels = tf.reshape(labels, [-1])</span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)</span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)</span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line">    return (loss, per_example_loss, log_probs)</span><br></pre></td></tr></table></figure>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">模型</th>
<th style="text-align:center">验证集</th>
<th style="text-align:center">测试集</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">roberta</td>
<td style="text-align:center">0.88503</td>
<td style="text-align:center">0.86344</td>
</tr>
<tr>
<td style="text-align:center">albert</td>
<td style="text-align:center">0.85662</td>
<td style="text-align:center">0.84960</td>
</tr>
<tr>
<td style="text-align:center">预训练后roberta</td>
<td style="text-align:center"><strong>0.89343</strong></td>
<td style="text-align:center">0.85328</td>
</tr>
<tr>
<td style="text-align:center">预训练后albert</td>
<td style="text-align:center">0.84958</td>
<td style="text-align:center"><strong>0.85224</strong></td>
</tr>
</tbody>
</table>
</div>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>模型根据验证集结果保存最优模型，因此测试集上表现不一定是最优的，我们主要看在验证集上的表现。以上模型在相同参数下只跑了一次，因此结果会略有浮动。</p>
<ol>
<li>roberta在预训练后效果取得提升，经过再次预训练，模型领域与微调领域更加接近，效果更好</li>
<li>Albert预训练后效果下降，可能与我们构建数据的方式有关，构建的数据与SOP任务并不符合，可以尝试更符合要求的数据进行测试。</li>
</ol>
<h1 id="TO-DO"><a href="#TO-DO" class="headerlink" title="TO DO"></a>TO DO</h1><ul>
<li>MACBert-20220319暂未开源预训练代码</li>
<li>根据SpanBert改为n-gram 掩码与SBO任务</li>
<li>Pytorch与keras的预训练</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>预训练</category>
      </categories>
      <tags>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>不要停止预训练实战(二)-一日看尽MLM</title>
    <url>/2022/05/30/2022-05-30-%E4%B8%8D%E8%A6%81%E5%81%9C%E6%AD%A2%E9%A2%84%E8%AE%AD%E7%BB%83%E5%AE%9E%E6%88%98(%E4%BA%8C)-%E4%B8%80%E6%97%A5%E7%9C%8B%E5%B0%BDMLM/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h2qp6pp8d8j21lk0sedik.jpg" alt=""></p>
<p>本文在上文<a href="https://jmxgodlz.xyz/2022/03/20/2022-03-20-预训练实战/#more">不要停止预训练实战-Roberta与Albert</a>的基础上，进一步完成以下内容：</p>
<ul>
<li>keras预训练</li>
<li>N-gram掩码任务 </li>
<li>Span掩码任务 </li>
</ul>
<span id="more"></span>
<h1 id="掩码任务"><a href="#掩码任务" class="headerlink" title="掩码任务"></a>掩码任务</h1><p>BERT等预训练模型中掩码任务主要涉及下列要素：</p>
<ul>
<li>掩码比例</li>
<li>替换策略</li>
<li>掩码方式</li>
</ul>
<h2 id="掩码比例"><a href="#掩码比例" class="headerlink" title="掩码比例"></a>掩码比例</h2><p>常用掩码比例设置为15%，该比例经过许多研究，已证明该比例能够取得很好的效果。</p>
<p>从理论上来说，笔者从网上找到的说法为：“当取15%时，恰好大概7个词mask一个，正好就是CBOW中，长度为7的滑动窗口的中心词，因此会有比较好的效果”</p>
<p>而近日丹琦大佬等人的论文<a href="https://arxiv.org/abs/2202.08005">Should You Mask 15% in Masked Language Modeling?</a>表明掩码40%能够取得与15%差不多的效果。</p>
<p>该论文表明<strong>“所谓的optimal masking rate并不是一个一成不变的神奇数字，而是一个随着模型大小、mask策略、训练recipe、下游任务变化的函数。”</strong></p>
<h2 id="替换策略"><a href="#替换策略" class="headerlink" title="替换策略"></a>替换策略</h2><p>常用的替换策略如下：</p>
<ul>
<li>80%词语替换为[MASK]</li>
<li>10%词语保持不变</li>
<li>10%词语随机替换为其他词语</li>
</ul>
<p>这样做的目的在于强迫模型学习词语上下文的语义信息。任何一个词语都有可能被替换，不仅靠当前词语，还需要利用上下文的信息预测当前词语。</p>
<p>但是[MASK]标签并未出现在下游任务中，因此<strong>存在预训练与微调的不一致问题。</strong></p>
<p>MacBERT提出<strong>MLM as correction</strong>的方法，替换策略如下：</p>
<ul>
<li>80%词语替换为同义词</li>
<li>10%词语保持不变</li>
<li>10%词语随机替换为其他词语</li>
</ul>
<p>MacBERT论文中与下列替换策略进行对比，对比结果如图所示：</p>
<ul>
<li>MacBERT：80%替换为同义词，10%替换为随机词语，10%保持不变；</li>
<li>Random Replace：90%替换为随机词语，10%保持不变；</li>
<li>Partial Mask：同原生的BERT一样，80%替换为[MASK]，10%替换为随机词语，10%保持不变；</li>
<li>ALL Mask：90%替换为[MASK]，10%保持不变。</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h2qhfc7hgpj20sg12uwib.jpg" alt="ss"></p>
<p>图中横坐标代表训练步数，纵坐标代表EM值。第一幅图是CMRC数据集结果，第二幅图是DRCD数据集结果。</p>
<h2 id="掩码方式"><a href="#掩码方式" class="headerlink" title="掩码方式"></a>掩码方式</h2><p>目前的掩码方式主要分为以下几种：</p>
<ul>
<li>单词掩码</li>
<li>全词掩码</li>
<li>实体掩码</li>
<li>N-gram掩码</li>
<li>Span掩码</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">中文</th>
<th style="text-align:center">英文</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">原句</td>
<td style="text-align:center">使用语言模型来预测下一个词的概率。</td>
<td style="text-align:center">we use a language model to predict the probability of the next word.</td>
</tr>
<tr>
<td style="text-align:center">分词</td>
<td style="text-align:center">使用 语言 模型 来 预测 下 一个 词 的 概率 。</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center">BERT Tokenizer</td>
<td style="text-align:center">使 用 语 言 模 型 来 预 测 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a language <strong>model</strong> to <strong>pre ##di ##ct</strong> the <strong>pro ##ba ##bility</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">单词掩码</td>
<td style="text-align:center">使 用 语 言 <strong>[M]</strong> 型 来 <strong>[M]</strong> 测 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a language <strong>[M]</strong> to <strong>[M] ##di ##ct</strong> the <strong>pro [M] ##bility</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">全词掩码</td>
<td style="text-align:center">使 用 语 言 <strong>[M] [M]</strong> 来 <strong>[M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a language <strong>[M]</strong> to <strong>[M] [M] [M]</strong> the <strong>[M] [M] [M]</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">实体掩码</td>
<td style="text-align:center">使 用 <strong>[M] [M] [M] [M]</strong> 来 <strong>[M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a <strong>[M] [M]</strong> to <strong>[M] [M] [M]</strong> the <strong>[M] [M] [M]</strong> of the next word.</td>
</tr>
<tr>
<td style="text-align:center">N-gram掩码</td>
<td style="text-align:center">使 用 <strong>[M] [M] [M] [M]</strong> 来 <strong>[M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a <strong>[M] [M]</strong> to <strong>[M] [M] [M]</strong> the <strong>[M] [M] [M] [M] [M]</strong> next word.</td>
</tr>
<tr>
<td style="text-align:center">Span掩码</td>
<td style="text-align:center">使 用 <strong>[M] [M] [M] [M] [M] [M] [M]</strong> 下 一 个 词 的 概 率 。</td>
<td style="text-align:center">we use a <strong>[M] [M] [M] [M] [M] [M]</strong> the <strong>[M] [M] [M] [M] [M]</strong> next word.</td>
</tr>
<tr>
<td style="text-align:center">MAC掩码</td>
<td style="text-align:center">使 用 语 法 建 模 来 预 见 下 一 个 词 的 几 率 。</td>
<td style="text-align:center">we use a <strong>text system</strong> to <strong>ca ##lc ##ulate</strong> the <strong>po ##si ##bility</strong> of the next word.</td>
</tr>
</tbody>
</table>
</div>
<h3 id="全词掩码"><a href="#全词掩码" class="headerlink" title="全词掩码"></a>全词掩码</h3><p>以分词结果为最小粒度，完成掩码任务。</p>
<h3 id="N-gram掩码"><a href="#N-gram掩码" class="headerlink" title="N-gram掩码"></a>N-gram掩码</h3><p>同样以分词结果为最小粒度，以n-gram取词语进行掩码。</p>
<p>例如MacBERT采用基于分词的n-gram masking，1-gram~4gram Masking的概率分别是40%、30%、20%、10%。</p>
<h3 id="实体掩码"><a href="#实体掩码" class="headerlink" title="实体掩码"></a>实体掩码</h3><p>代表模型为：<strong>ERNIE</strong></p>
<p>引入命名实体信息，将实体作为最小粒度，进行掩码。</p>
<h3 id="Span掩码"><a href="#Span掩码" class="headerlink" title="Span掩码"></a>Span掩码</h3><p>代表模型为：<strong>SpanBERT</strong></p>
<p>以上做法让人认为，或许必须得引入类似词边界信息才能帮助训练。但前不久的 MASS 模型，却表明可能并不需要，随机遮盖可能效果也很好，于是就有SpanBERT的 idea：</p>
<p>根据<strong>几何分布</strong>，先随机选择一段（span）的<strong>长度</strong>，之后再根据均匀分布随机选择这一段的<strong>起始位置</strong>，最后按照长度遮盖。文中使用几何分布取 <em>p=0.2</em>，最大长度只能是 10，利用此方案获得平均采样长度分布。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>相关代码实现可见：</p>
<p><a href="https://github.com/447428054/Pretrain/tree/master/KerasExample/pretraining">https://github.com/447428054/Pretrain/tree/master/KerasExample/pretraining</a></p>
<p>Span掩码核心代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def __init__(</span><br><span class="line">    self, tokenizer, word_segment, lower=1, upper=10, p=0.3, mask_rate=0.15, sequence_length=512</span><br><span class="line">):</span><br><span class="line">    &quot;&quot;&quot;参数说明：</span><br><span class="line">        tokenizer必须是bert4keras自带的tokenizer类；</span><br><span class="line">        word_segment是任意分词函数。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    super(TrainingDatasetRoBERTa, self).__init__(tokenizer, sequence_length)</span><br><span class="line">    self.word_segment = word_segment</span><br><span class="line">    self.mask_rate = mask_rate</span><br><span class="line"></span><br><span class="line">    self.lower = lower</span><br><span class="line">    self.upper = upper</span><br><span class="line">    self.p = p</span><br><span class="line"></span><br><span class="line">    self.lens = list(range(self.lower, self.upper + 1))</span><br><span class="line">    self.len_distrib = [self.p * (1-self.p)**(i - self.lower) for i in range(self.lower, self.upper + 1)] if self.p &gt;= 0 else None</span><br><span class="line">    self.len_distrib = [x / (sum(self.len_distrib)) for x in self.len_distrib]</span><br><span class="line">    print(self.len_distrib, self.lens)</span><br><span class="line"></span><br><span class="line">def sentence_process(self, text):</span><br><span class="line">    &quot;&quot;&quot;单个文本的处理函数</span><br><span class="line">    流程：分词，然后转id，按照mask_rate构建全词mask的序列</span><br><span class="line">          来指定哪些token是否要被mask</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    word_tokens = self.tokenizer.tokenize(text=text)[1:-1]</span><br><span class="line">    word_token_ids = self.tokenizer.tokens_to_ids(word_tokens)</span><br><span class="line"></span><br><span class="line">    sent_length = len(word_tokens)</span><br><span class="line">    mask_num = math.ceil(sent_length * self.mask_rate)</span><br><span class="line">    mask = set()</span><br><span class="line">    spans = []</span><br><span class="line"></span><br><span class="line">    while len(mask) &lt; mask_num:</span><br><span class="line">        span_len = np.random.choice(self.lens, p=self.len_distrib) # 随机选择span长度</span><br><span class="line"></span><br><span class="line">        anchor = np.random.choice(sent_length)</span><br><span class="line">        if anchor in mask: # 随机生成起点</span><br><span class="line">            continue</span><br><span class="line">        left1 = anchor</span><br><span class="line">        spans.append([left1, left1])</span><br><span class="line">        right1 = min(anchor + span_len, sent_length)</span><br><span class="line">        for i in range(left1, right1):</span><br><span class="line">            if len(mask) &gt;= mask_num:</span><br><span class="line">                break</span><br><span class="line">            mask.add(i)</span><br><span class="line">            spans[-1][-1] = i</span><br><span class="line"></span><br><span class="line">    spans = merge_intervals(spans)</span><br><span class="line">    word_mask_ids = [0] * len(word_tokens)</span><br><span class="line">    for (st, ed) in spans:</span><br><span class="line">        for idx in range(st, ed + 1):</span><br><span class="line">            wid = word_token_ids[idx]</span><br><span class="line">            word_mask_ids[idx] = self.token_process(wid) + 1</span><br><span class="line"></span><br><span class="line">    return [word_token_ids, word_mask_ids]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>预训练</category>
      </categories>
      <tags>
        <tag>预训练</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP技能树学习路线-（一）路线总览.md</title>
    <url>/2022/06/25/2022-06-25-NLP%E6%8A%80%E8%83%BD%E6%A0%91%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF-%EF%BC%88%E4%B8%80%EF%BC%89%E8%B7%AF%E7%BA%BF%E6%80%BB%E8%A7%88/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>偶然发现一位韩国小哥整理了NLP的学习路线：<a href="https://github.com/graykode/nlp-roadmap">nlp-roadmap</a>，其中知识点覆盖很全面。由于里面内容都是英文的，笔者重新用xmind绘制学习路线图，并结合自己的理解，将一些名词翻译为中文名词。</p>
<p>笔者打算根据该学习路线，对自己的知识体系进行查漏补缺，并在该系列文章记录自己学习过程。本部分内容旨在贴出各部分学习路线图，总览待学习内容。</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h3kgqimr09j21dc0u0788.jpg" alt=""></p>
<span id="more"></span>
<h1 id="概率统计"><a href="#概率统计" class="headerlink" title="概率统计"></a>概率统计</h1><p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h3kgl2gpjuj213z0u0tc3.jpg" alt=""></p>
<p>概率统计是人工智能算法的基础，图中分为贝叶斯、信息理论、模型、采样、基础五部分。笔者知识体系在“基础、采样、贝叶斯部分”存在缺漏，将在后面对该部分内容展开学习。</p>
<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h1><p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h3kgp8m5exj21370u0gou.jpg" alt=""></p>
<p>机器学习算法也是NLP的基石，含有许多基础概念，图中分为训练、降维、聚类、非概率、线性回归、逻辑回归、正则化七部分。笔者知识体系在“降维、聚类”存在缺漏，将在后面对该部分内容展开学习。</p>
<h1 id="NLP下的数据挖掘"><a href="#NLP下的数据挖掘" class="headerlink" title="NLP下的数据挖掘"></a>NLP下的数据挖掘</h1><p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h3kgpw1yppj20xv0u0acz.jpg" alt=""></p>
<p>NLP下的数据挖掘包含常见NLP数据处理操作，图中分为基础流程、序列标注、词嵌入、NLP基本假设、图、文档六部分。笔者知识体系在“主题模型、NLP基本假设、图”存在缺漏，将在后面对该部分内容展开学习。</p>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h3kgqimr09j21dc0u0788.jpg" alt=""></p>
<p>该部分正式介绍NLP的学习路线，分为基础学习、分布式特征、具体任务、语言模型四部分。笔者知识体系在一些具体模型存在缺漏，将在后面对该部分内容展开学习。</p>
<p>若大家需要思维导图源文件可以私我，有兴趣一起学习也可以私我加好友～</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
        <category>学习路线</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>真香～BERT在MAC Pytorch的使用.md</title>
    <url>/2022/07/16/2022-07-16-%E7%9C%9F%E9%A6%99%EF%BD%9EBERT%E5%9C%A8MAC%20Pytorch%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>终于，Pytorch也支持MAC的硬件加速，两个字评价一下感受：真香～</p>
<p>周末笔者在自己机器上完成环境安装，笔者机器环境如下：</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h495zuj103j20vy0bkq3r.jpg" alt=""></p>
<p>接着，笔者在该文用卷积、BERT模型对比了有无MAC硬件加速的模型运行时间</p>
<span id="more"></span>
<h1 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h1><p>按照官网给出的命令，即可完成安装MAC硬件加速版pytorch。</p>
<p><a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a></p>
<blockquote>
<p>conda install pytorch torchvision torchaudio -c pytorch</p>
</blockquote>
<h1 id="简单测试"><a href="#简单测试" class="headerlink" title="简单测试"></a>简单测试</h1><p>利用卷积操作，测试有无硬件加速的效果。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev = &#x27;mps:0&#x27;</span><br><span class="line"></span><br><span class="line">conv = torch.nn.Conv2d(10, 10, 3).to(dev)</span><br><span class="line"></span><br><span class="line">img = torch.randn(64, 10, 64, 64).to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line"></span><br><span class="line">for i in range(1000):</span><br><span class="line"></span><br><span class="line">    conv(img)</span><br><span class="line"></span><br><span class="line">t1 = time.time()</span><br><span class="line"></span><br><span class="line">print(&#x27;Use mps, time:&#123;&#125;&#x27;.format(t1-t0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev = &#x27;cpu&#x27;</span><br><span class="line"></span><br><span class="line">conv = torch.nn.Conv2d(10, 10, 3).to(dev)</span><br><span class="line"></span><br><span class="line">img = torch.randn(64, 10, 64, 64).to(dev)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t0 = time.time()</span><br><span class="line"></span><br><span class="line">for i in range(1000):</span><br><span class="line"></span><br><span class="line">    conv(img)</span><br><span class="line"></span><br><span class="line">t1 = time.time()</span><br><span class="line"></span><br><span class="line">print(&#x27;Use cpu, time:&#123;&#125;&#x27;.format(t1-t0))</span><br></pre></td></tr></table></figure>
<h2 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h2><p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h48wmn5k4vj20fo03674a.jpg" alt=""></p>
<h1 id="BERT测试"><a href="#BERT测试" class="headerlink" title="BERT测试"></a>BERT测试</h1><p>使用huggingface的glue代码作示例。</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>运行下述代码完成数据下载工作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;&#x27;&#x27; Script for downloading all GLUE data.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Note: for legal reasons, we are unable to host MRPC.</span><br><span class="line"></span><br><span class="line">You can either use the version hosted by the SentEval team, which is already tokenized,</span><br><span class="line"></span><br><span class="line">or you can download the original data from (https://download.microsoft.com/download/D/4/6/D46FF87A-F6B9-4252-AA8B-3604ED519838/MSRParaphraseCorpus.msi) and extract the data from it manually.</span><br><span class="line"></span><br><span class="line">For Windows users, you can run the .msi file. For Mac and Linux users, consider an external library such as &#x27;cabextract&#x27; (see below for an example).</span><br><span class="line"></span><br><span class="line">You should then rename and place specific files in a folder (see below for an example).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mkdir MRPC</span><br><span class="line"></span><br><span class="line">cabextract MSRParaphraseCorpus.msi -d MRPC</span><br><span class="line"></span><br><span class="line">cat MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D | tr -d $&#x27;\r&#x27; &gt; MRPC/msr_paraphrase_train.txt</span><br><span class="line"></span><br><span class="line">cat MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61 | tr -d $&#x27;\r&#x27; &gt; MRPC/msr_paraphrase_test.txt</span><br><span class="line"></span><br><span class="line">rm MRPC/_*</span><br><span class="line"></span><br><span class="line">rm MSRParaphraseCorpus.msi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1/30/19: It looks like SentEval is no longer hosting their extracted and tokenized MRPC data, so you&#x27;ll need to download the data from the original source for now.</span><br><span class="line"></span><br><span class="line">2/11/19: It looks like SentEval actually *is* hosting the extracted data. Hooray!</span><br><span class="line"></span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">import shutil</span><br><span class="line"></span><br><span class="line">import argparse</span><br><span class="line"></span><br><span class="line">import tempfile</span><br><span class="line"></span><br><span class="line">import urllib.request</span><br><span class="line"></span><br><span class="line">import zipfile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASKS = [&quot;CoLA&quot;, &quot;SST&quot;, &quot;MRPC&quot;, &quot;QQP&quot;, &quot;STS&quot;, &quot;MNLI&quot;, &quot;QNLI&quot;, &quot;RTE&quot;, &quot;WNLI&quot;, &quot;diagnostic&quot;]</span><br><span class="line"></span><br><span class="line">TASK2PATH = &#123;&quot;CoLA&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/CoLA.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;SST&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/SST-2.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;QQP&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;STS&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/STS-B.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;MNLI&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/MNLI.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;QNLI&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/QNLIv2.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;RTE&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/RTE.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;WNLI&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/WNLI.zip&#x27;,</span><br><span class="line"></span><br><span class="line">             &quot;diagnostic&quot;: &#x27;https://dl.fbaipublicfiles.com/glue/data/AX.tsv&#x27;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MRPC_TRAIN = &#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt&#x27;</span><br><span class="line"></span><br><span class="line">MRPC_TEST = &#x27;https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def download_and_extract(task, data_dir):</span><br><span class="line"></span><br><span class="line">    print(&quot;Downloading and extracting %s...&quot; % task)</span><br><span class="line"></span><br><span class="line">    if task == &quot;MNLI&quot;:</span><br><span class="line"></span><br><span class="line">        print(</span><br><span class="line"></span><br><span class="line">            &quot;\tNote (12/10/20): This script no longer downloads SNLI. You will need to manually download and format the data to use SNLI.&quot;)</span><br><span class="line"></span><br><span class="line">    data_file = &quot;%s.zip&quot; % task</span><br><span class="line"></span><br><span class="line">    urllib.request.urlretrieve(TASK2PATH[task], data_file)</span><br><span class="line"></span><br><span class="line">    with zipfile.ZipFile(data_file) as zip_ref:</span><br><span class="line"></span><br><span class="line">        zip_ref.extractall(data_dir)</span><br><span class="line"></span><br><span class="line">    os.remove(data_file)</span><br><span class="line"></span><br><span class="line">    print(&quot;\tCompleted!&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def format_mrpc(data_dir, path_to_data):</span><br><span class="line"></span><br><span class="line">    print(&quot;Processing MRPC...&quot;)</span><br><span class="line"></span><br><span class="line">    mrpc_dir = os.path.join(data_dir, &quot;MRPC&quot;)</span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(mrpc_dir):</span><br><span class="line"></span><br><span class="line">        os.mkdir(mrpc_dir)</span><br><span class="line"></span><br><span class="line">    if path_to_data:</span><br><span class="line"></span><br><span class="line">        mrpc_train_file = os.path.join(path_to_data, &quot;msr_paraphrase_train.txt&quot;)</span><br><span class="line"></span><br><span class="line">        mrpc_test_file = os.path.join(path_to_data, &quot;msr_paraphrase_test.txt&quot;)</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line"></span><br><span class="line">            mrpc_train_file = os.path.join(mrpc_dir, &quot;msr_paraphrase_train.txt&quot;)</span><br><span class="line"></span><br><span class="line">            mrpc_test_file = os.path.join(mrpc_dir, &quot;msr_paraphrase_test.txt&quot;)</span><br><span class="line"></span><br><span class="line">            URLLIB.urlretrieve(MRPC_TRAIN, mrpc_train_file)</span><br><span class="line"></span><br><span class="line">            URLLIB.urlretrieve(MRPC_TEST, mrpc_test_file)</span><br><span class="line"></span><br><span class="line">        except urllib.error.HTTPError:</span><br><span class="line"></span><br><span class="line">            print(&quot;Error downloading MRPC&quot;)</span><br><span class="line"></span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">    assert os.path.isfile(mrpc_train_file), &quot;Train data not found at %s&quot; % mrpc_train_file</span><br><span class="line"></span><br><span class="line">    assert os.path.isfile(mrpc_test_file), &quot;Test data not found at %s&quot; % mrpc_test_file</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    with io.open(mrpc_test_file, encoding=&#x27;utf-8&#x27;) as data_fh, \</span><br><span class="line"></span><br><span class="line">            io.open(os.path.join(mrpc_dir, &quot;test.tsv&quot;), &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as test_fh:</span><br><span class="line"></span><br><span class="line">        header = data_fh.readline()</span><br><span class="line"></span><br><span class="line">        test_fh.write(&quot;index\t#1 ID\t#2 ID\t#1 String\t#2 String\n&quot;)</span><br><span class="line"></span><br><span class="line">        for idx, row in enumerate(data_fh):</span><br><span class="line"></span><br><span class="line">            label, id1, id2, s1, s2 = row.strip().split(&#x27;\t&#x27;)</span><br><span class="line"></span><br><span class="line">            test_fh.write(&quot;%d\t%s\t%s\t%s\t%s\n&quot; % (idx, id1, id2, s1, s2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line"></span><br><span class="line">        URLLIB.urlretrieve(TASK2PATH[&quot;MRPC&quot;], os.path.join(mrpc_dir, &quot;dev_ids.tsv&quot;))</span><br><span class="line"></span><br><span class="line">    except KeyError or urllib.error.HTTPError:</span><br><span class="line"></span><br><span class="line">        print(&quot;\tError downloading standard development IDs for MRPC. You will need to manually split your data.&quot;)</span><br><span class="line"></span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    dev_ids = []</span><br><span class="line"></span><br><span class="line">    with io.open(os.path.join(mrpc_dir, &quot;dev_ids.tsv&quot;), encoding=&#x27;utf-8&#x27;) as ids_fh:</span><br><span class="line"></span><br><span class="line">        for row in ids_fh:</span><br><span class="line"></span><br><span class="line">            dev_ids.append(row.strip().split(&#x27;\t&#x27;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    with io.open(mrpc_train_file, encoding=&#x27;utf-8&#x27;) as data_fh, \</span><br><span class="line"></span><br><span class="line">            io.open(os.path.join(mrpc_dir, &quot;train.tsv&quot;), &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as train_fh, \</span><br><span class="line"></span><br><span class="line">            io.open(os.path.join(mrpc_dir, &quot;dev.tsv&quot;), &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as dev_fh:</span><br><span class="line"></span><br><span class="line">        header = data_fh.readline()</span><br><span class="line"></span><br><span class="line">        train_fh.write(header)</span><br><span class="line"></span><br><span class="line">        dev_fh.write(header)</span><br><span class="line"></span><br><span class="line">        for row in data_fh:</span><br><span class="line"></span><br><span class="line">            label, id1, id2, s1, s2 = row.strip().split(&#x27;\t&#x27;)</span><br><span class="line"></span><br><span class="line">            if [id1, id2] in dev_ids:</span><br><span class="line"></span><br><span class="line">                dev_fh.write(&quot;%s\t%s\t%s\t%s\t%s\n&quot; % (label, id1, id2, s1, s2))</span><br><span class="line"></span><br><span class="line">            else:</span><br><span class="line"></span><br><span class="line">                train_fh.write(&quot;%s\t%s\t%s\t%s\t%s\n&quot; % (label, id1, id2, s1, s2))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    print(&quot;\tCompleted!&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def download_diagnostic(data_dir):</span><br><span class="line"></span><br><span class="line">    print(&quot;Downloading and extracting diagnostic...&quot;)</span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(os.path.join(data_dir, &quot;diagnostic&quot;)):</span><br><span class="line"></span><br><span class="line">        os.mkdir(os.path.join(data_dir, &quot;diagnostic&quot;))</span><br><span class="line"></span><br><span class="line">    data_file = os.path.join(data_dir, &quot;diagnostic&quot;, &quot;diagnostic.tsv&quot;)</span><br><span class="line"></span><br><span class="line">    urllib.request.urlretrieve(TASK2PATH[&quot;diagnostic&quot;], data_file)</span><br><span class="line"></span><br><span class="line">    print(&quot;\tCompleted!&quot;)</span><br><span class="line"></span><br><span class="line">    return</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tasks(task_names):</span><br><span class="line"></span><br><span class="line">    task_names = task_names.split(&#x27;,&#x27;)</span><br><span class="line"></span><br><span class="line">    if &quot;all&quot; in task_names:</span><br><span class="line"></span><br><span class="line">        tasks = TASKS</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line"></span><br><span class="line">        tasks = []</span><br><span class="line"></span><br><span class="line">        for task_name in task_names:</span><br><span class="line"></span><br><span class="line">            assert task_name in TASKS, &quot;Task %s not found!&quot; % task_name</span><br><span class="line"></span><br><span class="line">            tasks.append(task_name)</span><br><span class="line"></span><br><span class="line">    return tasks</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(arguments):</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">    parser.add_argument(&#x27;--data_dir&#x27;, help=&#x27;directory to save data to&#x27;, type=str, default=&#x27;glue_data&#x27;)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(&#x27;--tasks&#x27;, help=&#x27;tasks to download data for as a comma separated string&#x27;,</span><br><span class="line"></span><br><span class="line">                        type=str, default=&#x27;all&#x27;)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(&#x27;--path_to_mrpc&#x27;,</span><br><span class="line"></span><br><span class="line">                        help=&#x27;path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt&#x27;,</span><br><span class="line"></span><br><span class="line">                        type=str, default=&#x27;&#x27;)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args(arguments)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(args.data_dir):</span><br><span class="line"></span><br><span class="line">        os.mkdir(args.data_dir)</span><br><span class="line"></span><br><span class="line">    tasks = get_tasks(args.tasks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    for task in tasks:</span><br><span class="line"></span><br><span class="line">        if task == &#x27;MRPC&#x27;:</span><br><span class="line"></span><br><span class="line">            format_mrpc(args.data_dir, args.path_to_mrpc)</span><br><span class="line"></span><br><span class="line">        elif task == &#x27;diagnostic&#x27;:</span><br><span class="line"></span><br><span class="line">            download_diagnostic(args.data_dir)</span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line"></span><br><span class="line">            download_and_extract(task, args.data_dir)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">    sys.exit(main(sys.argv[1:]))</span><br></pre></td></tr></table></figure>
<h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>requirements内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">accelerate</span><br><span class="line"></span><br><span class="line">datasets &gt;= 1.8.0</span><br><span class="line"></span><br><span class="line">sentencepiece != 0.1.92</span><br><span class="line"></span><br><span class="line">scipy</span><br><span class="line"></span><br><span class="line">scikit-learn</span><br><span class="line"></span><br><span class="line">protobuf</span><br><span class="line"></span><br><span class="line">numpy==1.17.3</span><br><span class="line"></span><br><span class="line">#torch &gt;= 1.3</span><br></pre></td></tr></table></figure>
<h2 id="代码准备"><a href="#代码准备" class="headerlink" title="代码准备"></a>代码准备</h2><p>利用huggingface的<strong>run_glue_no_trainer.py</strong>。</p>
<p>运行脚本如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export TASK_NAME=mrpc</span><br><span class="line"></span><br><span class="line">python run_glue_no_trainer.py \</span><br><span class="line">  --model_name_or_path Pretrained_LMs/bert-base-cased \</span><br><span class="line">  --task_name $TASK_NAME \</span><br><span class="line">  --max_length 128 \</span><br><span class="line">  --per_device_train_batch_size 32 \</span><br><span class="line">  --learning_rate 2e-5 \</span><br><span class="line">  --num_train_epochs 3 \</span><br><span class="line">  --output_dir ./output/$TASK_NAME/</span><br></pre></td></tr></table></figure>
<p>在代码中修改运行设备方式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">    accelerator.state.device = &#x27;mps&#x27;</span><br><span class="line"></span><br><span class="line">    print(&#x27;-&#x27; * 100)</span><br><span class="line"></span><br><span class="line">    print(accelerator.state.device)</span><br><span class="line"></span><br><span class="line">    print(&#x27;-&#x27; * 100)</span><br></pre></td></tr></table></figure>
<h2 id="运行结果-1"><a href="#运行结果-1" class="headerlink" title="运行结果"></a>运行结果</h2><p>CPU下运行时间约1h：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Num processes: 1</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: cpu</span><br><span class="line">...</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ - ***** Running training *****</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Num examples = 3668</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Num Epochs = 3</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Instantaneous batch size per device = 32</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Total train batch size (w. parallel, distributed &amp; accumulation) = 32</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Gradient Accumulation steps = 1</span><br><span class="line">07/16/2022 17:13:00 - INFO - __main__ -   Total optimization steps = 345</span><br><span class="line">  2%|███▌                                                                                                                                                                                                       | 6/345 [01:06&lt;1:03:49, 11.30s/it]</span><br></pre></td></tr></table></figure>
<p>硬件加速下运行时间约20min：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Num processes: 1</span><br><span class="line">Process index: 0</span><br><span class="line">Local process index: 0</span><br><span class="line">Device: mps</span><br><span class="line">...</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ - ***** Running training *****</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Num examples = 3668</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Num Epochs = 3</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Instantaneous batch size per device = 32</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Total train batch size (w. parallel, distributed &amp; accumulation) = 32</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Gradient Accumulation steps = 1</span><br><span class="line">07/16/2022 17:14:29 - INFO - __main__ -   Total optimization steps = 345</span><br><span class="line">  5%|██████████▋                                                                                                                                                                                                 | 18/345 [01:03&lt;20:14,  3.71s/it]</span><br></pre></td></tr></table></figure>
<p>观察MAC活动监视器，可以看到程序确实有用到GPU硬件加速。</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h496jqdjjbj21fu05m754.jpg" alt=""></p>
<h2 id="bug-fix"><a href="#bug-fix" class="headerlink" title="bug fix"></a>bug fix</h2><p>在运行过程中出现如下错误：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OMP: Error #15: Initializing libomp.dylib, but found libiomp5.dylib already initialize异常</span><br></pre></td></tr></table></figure>
<p>参照该<a href="http://t.zoukankan.com/yxym2016-p-13900887.html">链接</a>解决了问题，<strong>如果Python是基本于Conda安装的，则Conda上的numpy包中的mkl很容易与系统内库发生冲突，可选择update numpy package in Conda或者设置为系统库。</strong></p>
<p><strong>解决方案</strong>:降低numpy的版本，此处笔者将版本降低到1.17.3</p>
<blockquote>
<p>pip install numpy==1.17.3</p>
</blockquote>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>一文梳理NLP主要模型发展脉络.md</title>
    <url>/2022/07/31/2022-07-31-%E4%B8%80%E6%96%87%E6%A2%B3%E7%90%86NLP%E4%B8%BB%E8%A6%81%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95%E8%84%89%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文根据笔者所学知识，对NLP主要模型的发展脉络作梳理，目的在于了解主流技术的前世今生，如有理解错误的地方，麻烦指正～</p>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24egy1h335vr18jdj22fa0u043j.jpg" alt=""></p>
<p>下面将依次介绍RNN、LSTM、GRU、Encoder-Deocder、Transformer、BERT设计的出发点，模型结构不作详细介绍。</p>
<span id="more"></span>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>自然语言处理的数据类型多为文本类型，文本数据的上下文关系具有较强的序列特征。同时，RNN模型具有“上一时刻输出作为下一时刻的输入”的特征，该特征能够很好的处理序列数据。因此，RNN模型相较于其他模型，更适合处理自然语言处理任务。</p>
<p>当待处理序列长度较长时，RNN模型在反向传播的过程中，受链式求导法则的影响，当求导过程导数过小或过大时，会导致梯度消失或梯度爆炸。</p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>RNN模型的权重矩阵在时间维度上是共享的。LSTM相较于RNN模型，通过引入门控机制，缓解梯度消失，那么<strong>LSTM如何避免梯度消失？</strong></p>
<p>这里给出几个关键结论，详细分析后续开一篇介绍。</p>
<ul>
<li><p>RNN模型在时间维度共享参数矩阵，因此RNN模型总的梯度等于各时间的梯度之和，$g=\sum{g_t}$。</p>
</li>
<li><p>RNN中总的梯度不会消失，只是远距离梯度消失，梯度被近距离梯度主导，无法捕获远距离特征。</p>
</li>
<li><p>梯度消失的本质：由于RNN模型在时间维度共享参数矩阵，导致针对隐藏状态h求导时，循环计算矩阵乘法，最终梯度上出现了参数矩阵的累乘。</p>
</li>
<li><p>LSTM缓解梯度消失的本质：引入门控机制，将矩阵乘法转为逐元素相乘的哈达马积:$c_{t}=f_{t} \odot c_{t-1}+i_{t} \odot \tanh \left(W_{c}\left[h_{t-1}, x_{t}\right]+b_{c}\right)$</p>
</li>
</ul>
<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>GRU与LSTM模型相同，引入门控机制，避免梯度消失。区别在于，GRU只用了重置门与更新门两个门结构，参数量较LSTM少，训练速度更快。</p>
<h1 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h1><p>RNN模型“上一时刻输出作为下一时刻的输入”的特征，也存在模型输入输出一直是等长的问题。Encoder-Decoder模型通过编码器与解码器两个部分，解决了输入输出定长的问题。其中Encoder端负责文本序列的特征表示获取，Decoder端根据特征向量解码输出序列。</p>
<p>但Encoder-Decoder模型仍然存在以下问题：</p>
<ul>
<li><p>文本序列的特征表示向量选取</p>
</li>
<li><p>特征表示向量包含特征的有限性</p>
</li>
<li><p>OOV问题</p>
</li>
</ul>
<p>第一个与第二个问题通过<strong>注意力机制</strong>解决，利用注意力机制有选择的关注文本序列重要的特征。</p>
<p>第三个问题则通过<strong>拷贝机制</strong>以及<strong>Subword编码</strong>解决。</p>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer模型主要包含多头自注意力模块、前馈神经网络、残差结构与Dropout，其中核心模块为<strong>多头自注意力模块</strong>，各组件的功能如下：</p>
<ul>
<li><p><strong>自注意力机制</strong>在编码器端有选择的关注文本序列重要的特征，解决文本序列的特征表示向量选取及该向量包含特征的有限性问题。</p>
</li>
<li><p><strong>多头机制</strong>中每一头映射到不同空间，得到不同侧重点的特征表示，使得特征表示的更充分。</p>
</li>
<li><p><strong>残差结构</strong>有效避免梯度消失。</p>
</li>
<li><p><strong>Dropout</strong>有效避免过拟合。</p>
</li>
<li><p><strong>前馈神经网络</strong>完成隐含层到输出空间的映射</p>
</li>
</ul>
<p>接下来将重点介绍<strong>Transformer</strong>模型的<strong>优点</strong>。</p>
<h2 id="1-Transformer能够实现长距离依赖"><a href="#1-Transformer能够实现长距离依赖" class="headerlink" title="1. Transformer能够实现长距离依赖"></a>1. Transformer能够实现长距离依赖</h2><p>在自注意力机制中，每个字符能够与其他所有字符计算注意力得分。这种计算方式未考虑时序特征，能够捕获长距离依赖。</p>
<p>但该方式缺点在于，注意力得分计算的时间复杂度为$O(n^2)$，当输入序列较长时，时间复杂度过高，因此不适合处理过长数据。</p>
<h2 id="2-Transformer能够实现并行化"><a href="#2-Transformer能够实现并行化" class="headerlink" title="2. Transformer能够实现并行化"></a>2. Transformer能够实现并行化</h2><p>假设输入序列为（a,b,c,d）</p>
<p>传统RNN需要计算a的embedding向量得到$e_a$,再经过特征抽取得到$h_a$,然后以相同方式计算b,c,d。</p>
<p>Transformer通过self-attention机制，每个词都可以与全部序列交互，应此模型可以同时处理整个序列，得到$e_a,e_b,e_c,e_d$，然后再一起计算$h_a,h_b,h_c,h_d$。</p>
<h2 id="3-Transformer适合预训练"><a href="#3-Transformer适合预训练" class="headerlink" title="3. Transformer适合预训练"></a>3. Transformer适合预训练</h2><ul>
<li><p>RNN模型作出输入数据具有时序性的假设，将前一时刻的输出作为下一时刻的输入。</p>
</li>
<li><p>CNN模型基于输入数据为图像的假设，在结构中加入一些特质【如卷积生成特征】，使前向传播更加高效，降低网络的参数量。</p>
</li>
</ul>
<p>与CNN、RNN模型不同， Transformer 模型是一种灵活的架构，对输入数据的结构无限制，因此适合在大规模数据上进行预训练，但该点也带来了Transformer模型在小规模数据集上泛化性差的问题。改进方法包括引入结构偏差或正则化，对大规模未标记数据进行预训练等。</p>
<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>首先，BERT模型参照GPT模型，采样预训练-微调两阶段的训练方式。但是，与GPT使用Transformer解码器部分不同，BERT为了充分利用上下文信息，使用Transformer编码器部分作为模型结构。</p>
<h3 id="训练任务"><a href="#训练任务" class="headerlink" title="训练任务"></a>训练任务</h3><p>如果BERT与GPT同样使用语言模型作为学习任务，则模型存在标签泄露的问题【一个词的上下文包含了另一个词的预测目标】。因此为了利用上下文信息，BERT提出MLM掩码语言模型任务，通过上下文预测遮盖词，MLM有关介绍可见-<a href="https://jmxgodlz.xyz/2022/05/30/2022-05-30-不要停止预训练实战(二">不要停止预训练实战(二)-一日看尽MLM</a>-一日看尽MLM/#more)。</p>
<h3 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h3><p>针对Transformer结构及预训练方式，BERT模型仍存在以下改进点：</p>
<ul>
<li><p>训练方式：针对掩码方式与多任务训练方式进行改进，调整NSP训练任务与掩码的方式</p>
</li>
<li><p>模型结构调整：针对Transformer $O(n^2)$的时间复杂度以及输入结构无假设的两点，调整模型结构</p>
</li>
<li><p>架构调整：轻量化结构、加强跨块连接、自适应计算时间、分治策略的Transformer</p>
</li>
<li><p>预训练：使用完整Encoder模型，如T5，BART模型</p>
</li>
<li><p>多模态等下游任务应用</p>
</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h4ludsfcu4j20sq12jwm7.jpg" alt=""></p>
<p>BERT模型未来发展方向主要包含：更大更深的模型、多模态、跨语言、小样本、模型蒸馏，有关讨论可见-<a href="https://jmxgodlz.xyz/2021/12/31/2021-12-31-2022预训练的下一步是什么/">2022预训练的下一步是什么</a></p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://arxiv.org/pdf/2106.04554.pdf">https://arxiv.org/pdf/2106.04554.pdf</a></p>
<p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>预训练</tag>
      </tags>
  </entry>
</search>
